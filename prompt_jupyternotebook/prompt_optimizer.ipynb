{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cdf8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from prompters import padding\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from data import FewShotLearningDatasetParallel\n",
    "from experiment_builder import ExperimentBuilder\n",
    "import prompters\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c5c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DATASET_DIR'] = os.path.join(os.getcwd(), \"datasets\")\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"alfa+maml\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "    \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"attenuate\": False,\n",
    "  \"alfa\": True,\n",
    "  \"random_init\": False,\n",
    "  \"backbone\": \"4-CONV\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137b9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f197d3c34aab4095bc23b2d0f9aa6b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\anaconda3\\envs\\metal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(84),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR100(\"./data\", transform=preprocess,\n",
    "                          download=True, train=True)\n",
    "\n",
    "val_dataset = CIFAR100(\"./data\", transform=preprocess,\n",
    "                        download=True, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=25, pin_memory=True,\n",
    "                          num_workers=16, shuffle=True)\n",
    "\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e9eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "targets = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4])\n",
    "targets = torch.Tensor(targets)\n",
    "targets = targets.type(torch.LongTensor)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a044d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 3, 84, 84])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
      "        4], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f485233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "class MAMLFewShotClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, im_shape, device, args):\n",
    "        \n",
    "        super(MAMLFewShotClassifier, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.batch_size = args.batch_size\n",
    "        self.use_cuda = args.use_cuda\n",
    "        self.im_shape = im_shape\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.classifier = VGGReLUNormNetwork(im_shape=self.im_shape, num_output_classes=self.args.\n",
    "                                                 num_classes_per_set,\n",
    "                                                 args=args, device=device, meta_classifier=True).to(device=self.device)\n",
    "        \n",
    "        \n",
    "        self.prompter = prompters.padding(args=args, prompt_size=10, image_size=self.im_shape)\n",
    "        \n",
    "        \n",
    "        self.optimizer_all = optim.Adam(self.trainable_parameters(), lr=args.meta_learning_rate, amsgrad=False)\n",
    "        self.scheduler_all = optim.lr_scheduler.CosineAnnealingLR(optimizer=self.optimizer_all, T_max=self.args.total_epochs,\n",
    "                                                              eta_min=self.args.min_learning_rate)\n",
    "        \n",
    "    def trainable_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                yield param\n",
    "                \n",
    "                \n",
    "    def get_inner_loop_parameter_dict(self, params):\n",
    "        \n",
    "        param_dict = dict()\n",
    "        for name, param in params:\n",
    "            if param.requires_grad:\n",
    "                param_dict[name] = param.to(device=device)\n",
    "\n",
    "        return param_dict\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        loss, prompt_preds = self.net_forward(x,y)\n",
    "        observer = self.meta_update(loss)\n",
    "        return observer\n",
    "    \n",
    "    \n",
    "    def forward_grad(self, x, y):\n",
    "        \n",
    "        loss, prompt_preds = self.net_forward(x,y)\n",
    "        \n",
    "        prompt_grads = self.meta_update_grad(loss)\n",
    "        \n",
    "        return prompt_grads\n",
    "        \n",
    "    def forward_prompt_backward(self, x, y):\n",
    "        \n",
    "        loss, prompt_preds = self.net_forward(x,y)\n",
    "        \n",
    "        prompt_grads = self.meta_prompt_update(loss)\n",
    "        \n",
    "        return prompt_grads\n",
    "    \n",
    "    def forward_classifier_backward(self, x, y):\n",
    "        \n",
    "        loss, prompt_preds = self.net_forward(x,y)\n",
    "        \n",
    "        prompt_grads = self.meta_classifier_update(loss)\n",
    "        \n",
    "        return prompt_grads\n",
    "    \n",
    "        \n",
    "    def net_forward(self, x, y):\n",
    "        \n",
    "        names_prompt_weights_copy = self.get_inner_loop_parameter_dict(self.prompter.named_parameters())\n",
    "        names_weights_copy = self.get_inner_loop_parameter_dict(self.classifier.named_parameters())\n",
    "\n",
    "        names_prompt_weights_copy = {\n",
    "                        name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                            [1] + [1 for i in range(len(value.shape))]) for\n",
    "                        name, value in names_prompt_weights_copy.items()}\n",
    "\n",
    "        names_weights_copy = {\n",
    "                        name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                            [1] + [1 for i in range(len(value.shape))]) for\n",
    "                        name, value in names_weights_copy.items()}\n",
    "        \n",
    "        \n",
    "        for param in self.prompter.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        prompted_images = self.prompter.forward(x=images, params=names_prompt_weights_copy)\n",
    "        prompt_preds = self.classifier.forward(prompted_images, params=names_weights_copy,num_step=4)\n",
    "        \n",
    "        loss = F.cross_entropy(input=prompt_preds, target=y)\n",
    "        \n",
    "        return loss, prompt_preds\n",
    "        \n",
    "    \n",
    "    def meta_update_grad(self, loss):\n",
    "        \n",
    "        for param in self.classifier.parameters():\n",
    "            print(param.requires_grad)\n",
    "        \n",
    "        prompt_grads = torch.autograd.grad(loss, self.classifier.parameters(), allow_unused=True)\n",
    "        \n",
    "        return prompt_grads\n",
    "        \n",
    "        \n",
    "    def meta_prompt_update(self, loss):\n",
    "        \n",
    "        print(\"meta_prompt_update\")\n",
    "        \n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for param in self.prompter.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.optimizer_all.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        for name, param in self.prompter.named_parameters():\n",
    "            print(param.grad)\n",
    "        \n",
    "        self.optimizer_all.step()\n",
    "        \n",
    "        \n",
    "    def meta_classifier_update(self, loss):\n",
    "        \n",
    "        print(\"meta_classifier_update\")\n",
    "        \n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for param in self.prompter.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.optimizer_all.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        for name, param in self.classifier.named_parameters():\n",
    "            print(param.grad)\n",
    "        \n",
    "        self.optimizer_all.step()\n",
    "    \n",
    "    \n",
    "    def unify_test(self, images, targets):\n",
    "        prompt_grads1 = model.forward_grad(images, targets)\n",
    "        \n",
    "        \n",
    "        ## loss.bac\n",
    "        model.forward_prompt_backward(images, targets)\n",
    "        model.forward_classifier_backward(images, targets)\n",
    "        \n",
    "        return  prompt_grads1\n",
    "    \n",
    "    \n",
    "    def meta_update(self, loss):\n",
    "        \n",
    "        print(\"loss == \", loss)\n",
    "        \n",
    "        #print(self.classifier.state_dict())\n",
    "        print(self.prompter.state_dict())\n",
    "        print(\"==============================\")\n",
    "        \n",
    "        #### Prompter Update ####\n",
    "        \n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.prompter.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.optimizer_all.zero_grad()\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer_all.step()\n",
    "\n",
    "        print(self.prompter.state_dict())\n",
    "        print(\"==============================\")\n",
    "        \n",
    "        \n",
    "        #### Classifier Update ####        \n",
    "                \n",
    "        self.classifier.zero_grad()\n",
    "        self.prompter.zero_grad()\n",
    "        self.optimizer_all.zero_grad()\n",
    "        \n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in self.prompter.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        loss.backward()\n",
    "        self.optimizer_all.step()\n",
    "\n",
    "\n",
    "        print(self.prompter.state_dict())\n",
    "        print(\"==============================\")\n",
    "        \n",
    "        \n",
    "#########\n",
    "model = MAMLFewShotClassifier(args=args, device=device, im_shape=(2, 3, args.image_height, args.image_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9101d7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16416\\679165686.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16416\\3668596552.py\u001b[0m in \u001b[0;36mforward_grad\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprompt_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mprompt_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprompt_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16416\\3668596552.py\u001b[0m in \u001b[0;36mmeta_update_grad\u001b[1;34m(self, loss)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mprompt_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_unused\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprompt_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    226\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[0;32m    227\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "grad = model.forward_grad(images, targets)\n",
    "\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966ada2",
   "metadata": {},
   "source": [
    "### 1) retain_graph= True 사용법 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60e4089",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ==  tensor(2.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "OrderedDict([('pad_dict.pad_up', tensor([[[-0.4146,  0.1824,  1.5668,  ...,  0.1607,  2.1045,  1.0947],\n",
      "         [-0.5065,  0.7279, -0.3725,  ...,  1.4798,  0.7338,  0.6663],\n",
      "         [-1.2741,  0.3628,  0.7863,  ..., -0.0027, -1.0407,  0.1453],\n",
      "         ...,\n",
      "         [ 0.2212, -0.4895, -0.3429,  ...,  0.0668, -0.1131, -1.5486],\n",
      "         [ 0.7786,  0.7920, -1.4037,  ...,  0.0371, -0.6172,  2.3837],\n",
      "         [ 0.7835, -0.7619,  0.7989,  ...,  1.6326,  0.7327,  0.7296]],\n",
      "\n",
      "        [[-0.7939, -0.3529,  0.5046,  ...,  0.1915,  0.1670, -0.1800],\n",
      "         [-0.3602,  0.2038, -1.6841,  ...,  0.9995,  1.6749, -1.1009],\n",
      "         [ 0.7550, -0.4750,  1.3468,  ...,  1.2652,  1.3309,  0.3080],\n",
      "         ...,\n",
      "         [ 0.6359, -0.0728,  0.0797,  ...,  0.3535,  0.6812,  0.1956],\n",
      "         [-0.1643, -0.1860, -0.4835,  ...,  1.2388, -2.0533,  1.8488],\n",
      "         [-0.7935,  0.2394, -0.0627,  ..., -0.7649, -0.4766, -0.7666]],\n",
      "\n",
      "        [[-1.7111, -1.3933,  0.4745,  ..., -0.2788,  0.5213,  0.8936],\n",
      "         [-1.3884,  0.8352, -0.2777,  ..., -1.9325, -0.6108,  0.5314],\n",
      "         [ 0.5707, -0.5858,  1.1467,  ...,  2.3346, -1.6414,  0.4267],\n",
      "         ...,\n",
      "         [-0.3738,  0.5415, -1.3405,  ...,  0.9669, -0.2362,  0.7365],\n",
      "         [-0.5219,  0.5006, -1.5435,  ..., -0.0616,  0.8177, -2.2697],\n",
      "         [-1.0231, -0.5833,  0.8018,  ...,  1.5748,  0.8328,  0.5056]]])), ('pad_dict.pad_down', tensor([[[-8.1703e-02,  2.2916e+00, -6.1332e-01,  ..., -8.1384e-01,\n",
      "           9.7043e-01,  2.6132e-01],\n",
      "         [ 1.3835e-01, -2.2624e-01, -1.5803e+00,  ..., -8.1355e-01,\n",
      "          -2.9778e-01,  1.5555e-01],\n",
      "         [-2.1831e-01, -1.1998e-01,  1.0761e+00,  ...,  2.4380e-01,\n",
      "          -2.5510e-01, -1.0426e+00],\n",
      "         ...,\n",
      "         [-1.0378e+00, -1.5219e+00, -6.4705e-01,  ..., -4.4522e-02,\n",
      "           4.6554e-01, -3.7556e-01],\n",
      "         [-1.1685e+00,  5.1325e-01, -8.1190e-02,  ...,  1.9611e-02,\n",
      "          -4.3971e-01, -2.1646e-03],\n",
      "         [ 1.1985e+00, -1.4467e-01, -6.6738e-01,  ..., -7.9180e-02,\n",
      "           3.3487e-01, -4.5103e-01]],\n",
      "\n",
      "        [[ 2.0585e+00,  5.3043e-01, -9.2709e-01,  ...,  4.0468e-01,\n",
      "          -1.3157e+00, -7.3585e-01],\n",
      "         [ 4.5015e-01, -1.1464e+00,  9.3523e-01,  ...,  5.8760e-01,\n",
      "           1.6358e+00,  1.4020e-01],\n",
      "         [ 1.1843e+00,  2.3404e-01,  8.0572e-01,  ...,  9.1341e-01,\n",
      "          -7.2279e-01,  2.6756e-02],\n",
      "         ...,\n",
      "         [ 4.5380e-01, -1.1979e+00,  1.6398e-02,  ..., -4.5099e-01,\n",
      "          -3.4685e-01, -1.0131e+00],\n",
      "         [-4.3208e-01, -1.4492e+00,  4.0964e-01,  ...,  1.1071e-01,\n",
      "           2.1424e+00,  1.0988e+00],\n",
      "         [ 2.4960e-01,  9.2314e-01,  1.0807e+00,  ...,  1.1188e+00,\n",
      "          -9.1339e-01,  2.1272e+00]],\n",
      "\n",
      "        [[ 3.7408e-01, -1.6177e+00, -1.4742e-01,  ...,  1.3844e+00,\n",
      "          -4.1888e-01, -4.8448e-01],\n",
      "         [ 8.3224e-01, -1.8966e-01,  1.2365e+00,  ..., -6.9109e-02,\n",
      "          -1.4996e+00, -1.5250e+00],\n",
      "         [ 1.2617e+00,  9.9440e-01,  7.3575e-01,  ...,  2.1162e+00,\n",
      "          -8.9877e-01, -7.5857e-01],\n",
      "         ...,\n",
      "         [-1.2162e+00,  3.7648e-01, -1.7277e-01,  ...,  1.6797e-01,\n",
      "           6.0119e-01, -1.2600e+00],\n",
      "         [-1.2046e+00, -1.4846e+00, -1.1103e+00,  ...,  8.6326e-01,\n",
      "          -9.0107e-02, -1.0916e+00],\n",
      "         [-3.6331e-01,  7.7547e-01, -1.0943e+00,  ..., -1.1483e+00,\n",
      "           2.1842e-01, -8.4640e-01]]])), ('pad_dict.pad_left', tensor([[[ 0.7243,  0.6484, -0.6521,  ...,  0.0765,  0.9157,  2.4355],\n",
      "         [-0.7616,  0.9010,  0.3883,  ...,  0.9050, -1.6289,  0.1383],\n",
      "         [ 1.1405,  0.6562,  0.2126,  ...,  0.0701, -0.8255, -1.4401],\n",
      "         ...,\n",
      "         [ 1.1946,  2.3328,  0.5878,  ...,  0.4945,  1.6036, -1.0677],\n",
      "         [-1.1601, -0.5951,  0.0938,  ...,  0.6100,  1.6859, -0.0796],\n",
      "         [ 0.0270, -0.6414, -1.7248,  ..., -0.8020,  0.3394,  0.7573]],\n",
      "\n",
      "        [[ 1.2639, -0.3635, -0.4210,  ...,  0.8204,  1.3218, -0.6058],\n",
      "         [ 0.7892,  1.0210,  0.0594,  ..., -0.9163, -0.6725,  0.1150],\n",
      "         [-0.5013,  0.3831, -0.3519,  ...,  0.5203, -0.4022,  0.4268],\n",
      "         ...,\n",
      "         [ 0.4979, -1.3923, -0.5601,  ...,  1.4953, -1.3575, -0.2928],\n",
      "         [ 0.5975, -0.4692,  0.2975,  ...,  0.0748,  0.7706, -1.1438],\n",
      "         [-0.5697, -0.3587, -1.6538,  ..., -0.4472, -0.2709, -0.8696]],\n",
      "\n",
      "        [[-0.4818,  0.7072,  0.3276,  ..., -0.5718,  0.3538,  0.4825],\n",
      "         [ 1.0532, -0.0377, -1.4780,  ..., -0.7843, -3.2990,  0.1751],\n",
      "         [-1.4767,  1.2823,  0.0349,  ...,  0.3182, -1.0416,  0.0870],\n",
      "         ...,\n",
      "         [ 0.3445,  0.6090, -0.1063,  ...,  0.6283,  0.3449,  0.3097],\n",
      "         [-0.9427,  1.5671, -1.9178,  ...,  0.1839,  0.3783, -1.1539],\n",
      "         [-0.1335, -0.4503,  0.2189,  ..., -0.8191, -1.3945, -1.4043]]])), ('pad_dict.pad_right', tensor([[[-1.4904, -0.3915, -0.1280,  ..., -0.8355, -0.4231,  0.0706],\n",
      "         [-1.1861,  0.8003, -0.4810,  ...,  0.8722,  0.1767,  0.5872],\n",
      "         [ 0.6127, -1.8293, -1.5412,  ..., -0.2255, -0.2346, -0.8133],\n",
      "         ...,\n",
      "         [ 0.5332, -0.4565, -0.3029,  ..., -0.4215, -0.1378,  0.8761],\n",
      "         [-0.4000, -0.3743,  0.5285,  ..., -0.9932, -1.0560, -0.5289],\n",
      "         [-1.0852,  0.9917, -0.3390,  ...,  1.1403, -0.3372, -1.0287]],\n",
      "\n",
      "        [[ 1.3508,  0.5049,  1.0408,  ...,  1.6541,  0.2943,  0.0811],\n",
      "         [ 0.0534, -1.7410,  1.8998,  ..., -0.3751, -1.4061,  0.3124],\n",
      "         [-0.3752,  0.2623,  0.2444,  ..., -1.0547,  1.9007, -1.6026],\n",
      "         ...,\n",
      "         [-0.5576, -0.6948, -0.9127,  ..., -0.6735, -0.0322, -0.3401],\n",
      "         [ 1.2601,  0.7556, -0.2203,  ...,  0.2148,  0.0173, -1.2489],\n",
      "         [-0.1089, -0.1516, -0.8241,  ..., -0.2861,  2.2312,  0.8231]],\n",
      "\n",
      "        [[-0.6088, -2.0127, -2.0678,  ...,  1.1086,  0.1221,  1.5632],\n",
      "         [ 1.4923, -1.1345, -1.6645,  ...,  2.5211, -0.6708,  0.0689],\n",
      "         [-0.0680, -1.0545,  1.3854,  ..., -0.7426,  2.5344, -0.1288],\n",
      "         ...,\n",
      "         [-0.3342, -1.4591,  0.2485,  ...,  0.1895,  1.8896,  1.0880],\n",
      "         [-0.8566, -0.0588, -0.3063,  ...,  0.8177,  0.7949,  1.1032],\n",
      "         [ 0.5265,  0.3984,  0.5228,  ..., -0.9762, -0.2190,  0.8811]]]))])\n",
      "==============================\n",
      "OrderedDict([('pad_dict.pad_up', tensor([[[-4.1555e-01,  1.8139e-01,  1.5658e+00,  ...,  1.5975e-01,\n",
      "           2.1035e+00,  1.0957e+00],\n",
      "         [-5.0548e-01,  7.2693e-01, -3.7154e-01,  ...,  1.4788e+00,\n",
      "           7.3281e-01,  6.6727e-01],\n",
      "         [-1.2731e+00,  3.6184e-01,  7.8533e-01,  ..., -1.7361e-03,\n",
      "          -1.0397e+00,  1.4433e-01],\n",
      "         ...,\n",
      "         [ 2.2022e-01, -4.8847e-01, -3.4390e-01,  ...,  6.5805e-02,\n",
      "          -1.1406e-01, -1.5496e+00],\n",
      "         [ 7.7962e-01,  7.9300e-01, -1.4047e+00,  ...,  3.8109e-02,\n",
      "          -6.1619e-01,  2.3827e+00],\n",
      "         [ 7.8245e-01, -7.6293e-01,  7.9789e-01,  ...,  1.6336e+00,\n",
      "           7.3374e-01,  7.2863e-01]],\n",
      "\n",
      "        [[-7.9485e-01, -3.5192e-01,  5.0359e-01,  ...,  1.9248e-01,\n",
      "           1.6796e-01, -1.8099e-01],\n",
      "         [-3.6124e-01,  2.0484e-01, -1.6831e+00,  ...,  1.0005e+00,\n",
      "           1.6739e+00, -1.1019e+00],\n",
      "         [ 7.5599e-01, -4.7604e-01,  1.3478e+00,  ...,  1.2642e+00,\n",
      "           1.3319e+00,  3.0897e-01],\n",
      "         ...,\n",
      "         [ 6.3689e-01, -7.3798e-02,  7.8741e-02,  ...,  3.5452e-01,\n",
      "           6.8019e-01,  1.9661e-01],\n",
      "         [-1.6534e-01, -1.8504e-01, -4.8451e-01,  ...,  1.2378e+00,\n",
      "          -2.0543e+00,  1.8478e+00],\n",
      "         [-7.9454e-01,  2.4037e-01, -6.1685e-02,  ..., -7.6387e-01,\n",
      "          -4.7558e-01, -7.6763e-01]],\n",
      "\n",
      "        [[-1.7121e+00, -1.3943e+00,  4.7352e-01,  ..., -2.7777e-01,\n",
      "           5.2028e-01,  8.9460e-01],\n",
      "         [-1.3894e+00,  8.3617e-01, -2.7865e-01,  ..., -1.9315e+00,\n",
      "          -6.0978e-01,  5.3239e-01],\n",
      "         [ 5.6973e-01, -5.8481e-01,  1.1477e+00,  ...,  2.3356e+00,\n",
      "          -1.6424e+00,  4.2571e-01],\n",
      "         ...,\n",
      "         [-3.7482e-01,  5.4046e-01, -1.3415e+00,  ...,  9.6791e-01,\n",
      "          -2.3720e-01,  7.3754e-01],\n",
      "         [-5.2087e-01,  4.9961e-01, -1.5445e+00,  ..., -6.0624e-02,\n",
      "           8.1669e-01, -2.2687e+00],\n",
      "         [-1.0241e+00, -5.8427e-01,  8.0277e-01,  ...,  1.5738e+00,\n",
      "           8.3379e-01,  5.0457e-01]]])), ('pad_dict.pad_down', tensor([[[-0.0827,  2.2906, -0.6123,  ..., -0.8128,  0.9714,  0.2623],\n",
      "         [ 0.1394, -0.2252, -1.5813,  ..., -0.8146, -0.2988,  0.1565],\n",
      "         [-0.2193, -0.1210,  1.0751,  ...,  0.2428, -0.2541, -1.0416],\n",
      "         ...,\n",
      "         [-1.0368, -1.5229, -0.6460,  ..., -0.0435,  0.4665, -0.3746],\n",
      "         [-1.1695,  0.5122, -0.0802,  ...,  0.0206, -0.4387, -0.0032],\n",
      "         [ 1.1975, -0.1457, -0.6664,  ..., -0.0782,  0.3339, -0.4500]],\n",
      "\n",
      "        [[ 2.0595,  0.5314, -0.9281,  ...,  0.4057, -1.3147, -0.7369],\n",
      "         [ 0.4511, -1.1454,  0.9342,  ...,  0.5886,  1.6368,  0.1412],\n",
      "         [ 1.1833,  0.2350,  0.8067,  ...,  0.9124, -0.7238,  0.0278],\n",
      "         ...,\n",
      "         [ 0.4548, -1.1969,  0.0154,  ..., -0.4520, -0.3479, -1.0121],\n",
      "         [-0.4331, -1.4482,  0.4106,  ...,  0.1097,  2.1434,  1.0978],\n",
      "         [ 0.2506,  0.9241,  1.0817,  ...,  1.1178, -0.9144,  2.1282]],\n",
      "\n",
      "        [[ 0.3751, -1.6167, -0.1464,  ...,  1.3854, -0.4199, -0.4855],\n",
      "         [ 0.8312, -0.1907,  1.2375,  ..., -0.0701, -1.5006, -1.5260],\n",
      "         [ 1.2607,  0.9934,  0.7368,  ...,  2.1152, -0.8978, -0.7596],\n",
      "         ...,\n",
      "         [-1.2152,  0.3775, -0.1738,  ...,  0.1670,  0.6022, -1.2590],\n",
      "         [-1.2056, -1.4836, -1.1093,  ...,  0.8643, -0.0911, -1.0906],\n",
      "         [-0.3623,  0.7745, -1.0933,  ..., -1.1473,  0.2174, -0.8474]]])), ('pad_dict.pad_left', tensor([[[ 0.7253,  0.6494, -0.6511,  ...,  0.0755,  0.9147,  2.4345],\n",
      "         [-0.7626,  0.9000,  0.3873,  ...,  0.9060, -1.6279,  0.1393],\n",
      "         [ 1.1395,  0.6572,  0.2116,  ...,  0.0711, -0.8245, -1.4411],\n",
      "         ...,\n",
      "         [ 1.1956,  2.3318,  0.5868,  ...,  0.4935,  1.6046, -1.0667],\n",
      "         [-1.1591, -0.5961,  0.0928,  ...,  0.6110,  1.6849, -0.0786],\n",
      "         [ 0.0280, -0.6404, -1.7238,  ..., -0.8010,  0.3384,  0.7563]],\n",
      "\n",
      "        [[ 1.2649, -0.3645, -0.4200,  ...,  0.8194,  1.3208, -0.6068],\n",
      "         [ 0.7882,  1.0220,  0.0604,  ..., -0.9153, -0.6735,  0.1140],\n",
      "         [-0.5003,  0.3821, -0.3509,  ...,  0.5193, -0.4032,  0.4278],\n",
      "         ...,\n",
      "         [ 0.4989, -1.3913, -0.5591,  ...,  1.4943, -1.3565, -0.2938],\n",
      "         [ 0.5965, -0.4702,  0.2965,  ...,  0.0758,  0.7716, -1.1448],\n",
      "         [-0.5687, -0.3597, -1.6548,  ..., -0.4482, -0.2699, -0.8686]],\n",
      "\n",
      "        [[-0.4828,  0.7082,  0.3286,  ..., -0.5708,  0.3528,  0.4815],\n",
      "         [ 1.0522, -0.0367, -1.4770,  ..., -0.7833, -3.2980,  0.1761],\n",
      "         [-1.4777,  1.2833,  0.0359,  ...,  0.3172, -1.0426,  0.0860],\n",
      "         ...,\n",
      "         [ 0.3435,  0.6100, -0.1073,  ...,  0.6273,  0.3439,  0.3107],\n",
      "         [-0.9437,  1.5681, -1.9188,  ...,  0.1829,  0.3773, -1.1529],\n",
      "         [-0.1325, -0.4513,  0.2179,  ..., -0.8201, -1.3935, -1.4053]]])), ('pad_dict.pad_right', tensor([[[-1.4914, -0.3905, -0.1270,  ..., -0.8365, -0.4241,  0.0696],\n",
      "         [-1.1871,  0.7993, -0.4820,  ...,  0.8732,  0.1777,  0.5862],\n",
      "         [ 0.6137, -1.8303, -1.5402,  ..., -0.2245, -0.2336, -0.8143],\n",
      "         ...,\n",
      "         [ 0.5342, -0.4555, -0.3039,  ..., -0.4225, -0.1368,  0.8751],\n",
      "         [-0.4010, -0.3733,  0.5275,  ..., -0.9942, -1.0570, -0.5279],\n",
      "         [-1.0842,  0.9907, -0.3400,  ...,  1.1393, -0.3362, -1.0297]],\n",
      "\n",
      "        [[ 1.3518,  0.5039,  1.0418,  ...,  1.6551,  0.2933,  0.0821],\n",
      "         [ 0.0544, -1.7400,  1.8988,  ..., -0.3741, -1.4071,  0.3114],\n",
      "         [-0.3762,  0.2613,  0.2434,  ..., -1.0557,  1.8997, -1.6016],\n",
      "         ...,\n",
      "         [-0.5566, -0.6958, -0.9117,  ..., -0.6745, -0.0332, -0.3391],\n",
      "         [ 1.2591,  0.7566, -0.2213,  ...,  0.2158,  0.0183, -1.2479],\n",
      "         [-0.1079, -0.1506, -0.8231,  ..., -0.2851,  2.2322,  0.8221]],\n",
      "\n",
      "        [[-0.6078, -2.0117, -2.0668,  ...,  1.1096,  0.1231,  1.5642],\n",
      "         [ 1.4913, -1.1355, -1.6655,  ...,  2.5201, -0.6698,  0.0699],\n",
      "         [-0.0690, -1.0555,  1.3844,  ..., -0.7416,  2.5354, -0.1278],\n",
      "         ...,\n",
      "         [-0.3332, -1.4581,  0.2495,  ...,  0.1905,  1.8906,  1.0890],\n",
      "         [-0.8576, -0.0578, -0.3053,  ...,  0.8167,  0.7939,  1.1022],\n",
      "         [ 0.5255,  0.3974,  0.5238,  ..., -0.9772, -0.2200,  0.8801]]]))])\n",
      "==============================\n",
      "OrderedDict([('pad_dict.pad_up', tensor([[[-4.1622e-01,  1.8072e-01,  1.5651e+00,  ...,  1.5908e-01,\n",
      "           2.1029e+00,  1.0964e+00],\n",
      "         [-5.0481e-01,  7.2626e-01, -3.7087e-01,  ...,  1.4781e+00,\n",
      "           7.3214e-01,  6.6794e-01],\n",
      "         [-1.2724e+00,  3.6117e-01,  7.8466e-01,  ..., -1.0661e-03,\n",
      "          -1.0390e+00,  1.4366e-01],\n",
      "         ...,\n",
      "         [ 2.1955e-01, -4.8780e-01, -3.4457e-01,  ...,  6.5134e-02,\n",
      "          -1.1473e-01, -1.5503e+00],\n",
      "         [ 7.8029e-01,  7.9367e-01, -1.4053e+00,  ...,  3.8779e-02,\n",
      "          -6.1552e-01,  2.3821e+00],\n",
      "         [ 7.8178e-01, -7.6360e-01,  7.9722e-01,  ...,  1.6343e+00,\n",
      "           7.3441e-01,  7.2796e-01]],\n",
      "\n",
      "        [[-7.9552e-01, -3.5125e-01,  5.0292e-01,  ...,  1.9315e-01,\n",
      "           1.6863e-01, -1.8166e-01],\n",
      "         [-3.6191e-01,  2.0551e-01, -1.6825e+00,  ...,  1.0012e+00,\n",
      "           1.6732e+00, -1.1026e+00],\n",
      "         [ 7.5666e-01, -4.7671e-01,  1.3484e+00,  ...,  1.2636e+00,\n",
      "           1.3326e+00,  3.0964e-01],\n",
      "         ...,\n",
      "         [ 6.3756e-01, -7.4468e-02,  7.8070e-02,  ...,  3.5520e-01,\n",
      "           6.7952e-01,  1.9728e-01],\n",
      "         [-1.6601e-01, -1.8437e-01, -4.8518e-01,  ...,  1.2372e+00,\n",
      "          -2.0550e+00,  1.8471e+00],\n",
      "         [-7.9521e-01,  2.4104e-01, -6.1015e-02,  ..., -7.6320e-01,\n",
      "          -4.7491e-01, -7.6830e-01]],\n",
      "\n",
      "        [[-1.7128e+00, -1.3949e+00,  4.7285e-01,  ..., -2.7710e-01,\n",
      "           5.1961e-01,  8.9527e-01],\n",
      "         [-1.3901e+00,  8.3684e-01, -2.7932e-01,  ..., -1.9309e+00,\n",
      "          -6.0911e-01,  5.3306e-01],\n",
      "         [ 5.6906e-01, -5.8414e-01,  1.1484e+00,  ...,  2.3363e+00,\n",
      "          -1.6430e+00,  4.2504e-01],\n",
      "         ...,\n",
      "         [-3.7549e-01,  5.3979e-01, -1.3422e+00,  ...,  9.6858e-01,\n",
      "          -2.3787e-01,  7.3821e-01],\n",
      "         [-5.2020e-01,  4.9894e-01, -1.5452e+00,  ..., -5.9954e-02,\n",
      "           8.1602e-01, -2.2680e+00],\n",
      "         [-1.0247e+00, -5.8494e-01,  8.0344e-01,  ...,  1.5731e+00,\n",
      "           8.3446e-01,  5.0390e-01]]])), ('pad_dict.pad_down', tensor([[[-0.0834,  2.2900, -0.6117,  ..., -0.8122,  0.9721,  0.2630],\n",
      "         [ 0.1400, -0.2246, -1.5820,  ..., -0.8152, -0.2995,  0.1572],\n",
      "         [-0.2200, -0.1216,  1.0744,  ...,  0.2421, -0.2534, -1.0410],\n",
      "         ...,\n",
      "         [-1.0361, -1.5236, -0.6454,  ..., -0.0429,  0.4672, -0.3739],\n",
      "         [-1.1702,  0.5116, -0.0795,  ...,  0.0213, -0.4380, -0.0038],\n",
      "         [ 1.1968, -0.1463, -0.6657,  ..., -0.0775,  0.3332, -0.4494]],\n",
      "\n",
      "        [[ 2.0602,  0.5321, -0.9288,  ...,  0.4063, -1.3141, -0.7375],\n",
      "         [ 0.4518, -1.1447,  0.9336,  ...,  0.5893,  1.6374,  0.1419],\n",
      "         [ 1.1826,  0.2357,  0.8074,  ...,  0.9117, -0.7245,  0.0284],\n",
      "         ...,\n",
      "         [ 0.4555, -1.1962,  0.0147,  ..., -0.4527, -0.3485, -1.0114],\n",
      "         [-0.4337, -1.4475,  0.4113,  ...,  0.1090,  2.1441,  1.0971],\n",
      "         [ 0.2513,  0.9248,  1.0824,  ...,  1.1171, -0.9151,  2.1289]],\n",
      "\n",
      "        [[ 0.3757, -1.6160, -0.1457,  ...,  1.3861, -0.4205, -0.4862],\n",
      "         [ 0.8306, -0.1913,  1.2382,  ..., -0.0708, -1.5013, -1.5267],\n",
      "         [ 1.2600,  0.9927,  0.7374,  ...,  2.1145, -0.8971, -0.7602],\n",
      "         ...,\n",
      "         [-1.2145,  0.3781, -0.1744,  ...,  0.1663,  0.6029, -1.2583],\n",
      "         [-1.2063, -1.4829, -1.1087,  ...,  0.8649, -0.0918, -1.0899],\n",
      "         [-0.3616,  0.7738, -1.0926,  ..., -1.1467,  0.2168, -0.8481]]])), ('pad_dict.pad_left', tensor([[[ 0.7260,  0.6501, -0.6504,  ...,  0.0748,  0.9141,  2.4338],\n",
      "         [-0.7633,  0.8994,  0.3867,  ...,  0.9066, -1.6272,  0.1399],\n",
      "         [ 1.1389,  0.6579,  0.2110,  ...,  0.0718, -0.8238, -1.4418],\n",
      "         ...,\n",
      "         [ 1.1962,  2.3311,  0.5861,  ...,  0.4928,  1.6053, -1.0661],\n",
      "         [-1.1584, -0.5967,  0.0921,  ...,  0.6116,  1.6842, -0.0779],\n",
      "         [ 0.0287, -0.6398, -1.7231,  ..., -0.8003,  0.3377,  0.7557]],\n",
      "\n",
      "        [[ 1.2656, -0.3651, -0.4193,  ...,  0.8187,  1.3201, -0.6075],\n",
      "         [ 0.7875,  1.0227,  0.0610,  ..., -0.9146, -0.6742,  0.1134],\n",
      "         [-0.4996,  0.3814, -0.3503,  ...,  0.5186, -0.4038,  0.4285],\n",
      "         ...,\n",
      "         [ 0.4996, -1.3906, -0.5585,  ...,  1.4936, -1.3559, -0.2945],\n",
      "         [ 0.5958, -0.4709,  0.2958,  ...,  0.0765,  0.7722, -1.1455],\n",
      "         [-0.5680, -0.3603, -1.6555,  ..., -0.4488, -0.2692, -0.8680]],\n",
      "\n",
      "        [[-0.4835,  0.7089,  0.3292,  ..., -0.5702,  0.3522,  0.4809],\n",
      "         [ 1.0515, -0.0360, -1.4763,  ..., -0.7827, -3.2974,  0.1768],\n",
      "         [-1.4784,  1.2840,  0.0365,  ...,  0.3165, -1.0433,  0.0854],\n",
      "         ...,\n",
      "         [ 0.3428,  0.6107, -0.1079,  ...,  0.6266,  0.3432,  0.3114],\n",
      "         [-0.9444,  1.5687, -1.9195,  ...,  0.1822,  0.3766, -1.1522],\n",
      "         [-0.1319, -0.4519,  0.2172,  ..., -0.8207, -1.3929, -1.4059]]])), ('pad_dict.pad_right', tensor([[[-1.4921, -0.3898, -0.1263,  ..., -0.8372, -0.4247,  0.0689],\n",
      "         [-1.1877,  0.7986, -0.4827,  ...,  0.8739,  0.1783,  0.5856],\n",
      "         [ 0.6144, -1.8309, -1.5395,  ..., -0.2238, -0.2329, -0.8150],\n",
      "         ...,\n",
      "         [ 0.5349, -0.4548, -0.3046,  ..., -0.4232, -0.1362,  0.8744],\n",
      "         [-0.4016, -0.3726,  0.5268,  ..., -0.9949, -1.0577, -0.5272],\n",
      "         [-1.0835,  0.9900, -0.3407,  ...,  1.1386, -0.3355, -1.0304]],\n",
      "\n",
      "        [[ 1.3525,  0.5033,  1.0425,  ...,  1.6557,  0.2926,  0.0827],\n",
      "         [ 0.0551, -1.7393,  1.8981,  ..., -0.3734, -1.4078,  0.3107],\n",
      "         [-0.3769,  0.2606,  0.2428,  ..., -1.0564,  1.8990, -1.6009],\n",
      "         ...,\n",
      "         [-0.5559, -0.6965, -0.9110,  ..., -0.6752, -0.0339, -0.3384],\n",
      "         [ 1.2584,  0.7573, -0.2219,  ...,  0.2165,  0.0190, -1.2472],\n",
      "         [-0.1072, -0.1499, -0.8225,  ..., -0.2844,  2.2329,  0.8214]],\n",
      "\n",
      "        [[-0.6072, -2.0110, -2.0661,  ...,  1.1103,  0.1238,  1.5649],\n",
      "         [ 1.4906, -1.1362, -1.6662,  ...,  2.5194, -0.6692,  0.0706],\n",
      "         [-0.0697, -1.0561,  1.3838,  ..., -0.7410,  2.5361, -0.1271],\n",
      "         ...,\n",
      "         [-0.3325, -1.4574,  0.2502,  ...,  0.1911,  1.8912,  1.0897],\n",
      "         [-0.8583, -0.0571, -0.3047,  ...,  0.8160,  0.7932,  1.1015],\n",
      "         [ 0.5249,  0.3968,  0.5244,  ..., -0.9779, -0.2207,  0.8794]]]))])\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "observer = model.forward(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd90c6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16416\\1950648944.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbefore_update_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobserver\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'before_update_classifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbefore_update_prompter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobserver\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'before_update_prompter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mone_update_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobserver\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'one_update_classifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mone_update_prompter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobserver\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'one_update_prompter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "before_update_classifier = observer['before_update_classifier']\n",
    "before_update_prompter = observer['before_update_prompter']\n",
    "\n",
    "one_update_classifier = observer['one_update_classifier']\n",
    "one_update_prompter = observer['one_update_prompter']\n",
    "\n",
    "two_update_classifier = observer['two_update_classifier']\n",
    "two_update_prompter = observer['two_update_prompter']\n",
    "\n",
    "\n",
    "for param in before_update_classifier.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ae6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in one_update_classifier.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in two_update_classifier.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d12ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in before_update_prompter.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c32cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in one_update_prompter.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1a042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in two_update_prompter.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba217e21",
   "metadata": {},
   "source": [
    "### 2) torch.autograd.grad와 loss.backward를 사용해서 gradient를 구한 것이 같은지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624678f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_grads1 = model.unify_test(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115105d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
