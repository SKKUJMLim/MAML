{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb3f6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c51c0520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eafe3d5",
   "metadata": {},
   "source": [
    "# 0. 사용할 Dataset 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b159aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import basic_utils\n",
    "\n",
    "csv_file = \"sample_loss_log_MAML.csv\"\n",
    "\n",
    "datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "# datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6799c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_5way_5shot_6647\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 101,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "428adcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_5way_5shot_6647\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70f45f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6359555553396543,\n",
       " 'best_val_iter': 49000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 98,\n",
       " 'train_loss_mean': 0.6403910377025605,\n",
       " 'train_loss_std': 0.1255260544055785,\n",
       " 'train_accuracy_mean': 0.7639866684675216,\n",
       " 'train_accuracy_std': 0.06118401968549816,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.9505268172423045,\n",
       " 'val_loss_std': 0.15325273037358733,\n",
       " 'val_accuracy_mean': 0.6287999994556109,\n",
       " 'val_accuracy_std': 0.06361374163962585,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-0.0255, -0.0844,  0.0421],\n",
       "                         [-0.0809, -0.0591, -0.0046],\n",
       "                         [-0.0325,  0.1015, -0.0134]],\n",
       "               \n",
       "                        [[ 0.0662, -0.0385,  0.0898],\n",
       "                         [-0.0224,  0.0329,  0.0889],\n",
       "                         [-0.0375,  0.0514, -0.0026]],\n",
       "               \n",
       "                        [[ 0.0667, -0.0065, -0.0467],\n",
       "                         [ 0.0322,  0.0595, -0.0776],\n",
       "                         [-0.0642,  0.0074, -0.0751]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0714,  0.1215,  0.0690],\n",
       "                         [ 0.0657, -0.0497,  0.0189],\n",
       "                         [-0.1058, -0.0985, -0.0763]],\n",
       "               \n",
       "                        [[-0.0225,  0.0162, -0.0007],\n",
       "                         [ 0.0545, -0.0631, -0.0476],\n",
       "                         [ 0.0542, -0.0559,  0.0761]],\n",
       "               \n",
       "                        [[-0.0650,  0.0606, -0.0772],\n",
       "                         [-0.0224,  0.0663,  0.0857],\n",
       "                         [-0.0305, -0.0117, -0.0181]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0324, -0.0386,  0.0566],\n",
       "                         [-0.0143, -0.0344, -0.0204],\n",
       "                         [ 0.0755,  0.0579, -0.0644]],\n",
       "               \n",
       "                        [[-0.0797, -0.0395,  0.0388],\n",
       "                         [ 0.0813, -0.0030,  0.0204],\n",
       "                         [-0.0698, -0.0439,  0.0303]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0722,  0.0254],\n",
       "                         [ 0.0777,  0.0115, -0.0638],\n",
       "                         [-0.0668,  0.0245,  0.0016]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0359, -0.0860, -0.0662],\n",
       "                         [-0.0050, -0.0928,  0.0520],\n",
       "                         [-0.0283,  0.0588, -0.0609]],\n",
       "               \n",
       "                        [[ 0.0648,  0.0180,  0.0776],\n",
       "                         [ 0.0499, -0.0222, -0.0206],\n",
       "                         [ 0.0198,  0.0787,  0.0061]],\n",
       "               \n",
       "                        [[ 0.0096, -0.0855, -0.0410],\n",
       "                         [-0.0569,  0.0049,  0.0344],\n",
       "                         [-0.0491,  0.0319,  0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0077,  0.0151, -0.0113],\n",
       "                         [ 0.0198,  0.0805, -0.0301],\n",
       "                         [ 0.0457,  0.0801, -0.0407]],\n",
       "               \n",
       "                        [[ 0.0811,  0.0580,  0.1031],\n",
       "                         [ 0.0596,  0.0067,  0.0002],\n",
       "                         [-0.0017, -0.0517,  0.0510]],\n",
       "               \n",
       "                        [[-0.0385, -0.0292, -0.0301],\n",
       "                         [-0.0170, -0.0443,  0.0586],\n",
       "                         [-0.0917, -0.0518,  0.0024]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0323,  0.0842, -0.0393],\n",
       "                         [-0.0391,  0.0747,  0.0626],\n",
       "                         [ 0.0786,  0.0831,  0.0922]],\n",
       "               \n",
       "                        [[-0.0269, -0.0427,  0.0134],\n",
       "                         [ 0.0379, -0.0726, -0.0621],\n",
       "                         [-0.0407, -0.0233, -0.0411]],\n",
       "               \n",
       "                        [[ 0.0461, -0.0238,  0.0385],\n",
       "                         [-0.0098, -0.0332, -0.0305],\n",
       "                         [-0.0480, -0.0654, -0.0453]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.0175,  0.0289,  0.0017, -0.0053,  0.0039,  0.0073,  0.0086,  0.0064,\n",
       "                        0.0009, -0.0196,  0.0216,  0.0025, -0.0158,  0.0111,  0.0017,  0.0114,\n",
       "                        0.0064,  0.0143,  0.0169, -0.0116, -0.0180, -0.0081, -0.0033, -0.0074,\n",
       "                       -0.0037,  0.0029,  0.0011,  0.0125,  0.0148,  0.0091,  0.0051, -0.0029,\n",
       "                       -0.0089,  0.0250,  0.0072, -0.0091,  0.0011,  0.0039,  0.0159,  0.0169,\n",
       "                       -0.0124, -0.0267,  0.0020, -0.0068, -0.0071,  0.0006, -0.0248, -0.0077,\n",
       "                        0.0044,  0.0075, -0.0075,  0.0021,  0.0150, -0.0009, -0.0197, -0.0072,\n",
       "                        0.0030,  0.0047, -0.0043, -0.0002, -0.0109, -0.0002,  0.0212,  0.0022,\n",
       "                       -0.0252, -0.0014,  0.0105, -0.0034, -0.0002, -0.0239, -0.0024, -0.0128,\n",
       "                        0.0037,  0.0041,  0.0119, -0.0083,  0.0055, -0.0148,  0.0025, -0.0067,\n",
       "                        0.0161,  0.0038, -0.0007,  0.0080,  0.0148, -0.0060, -0.0045,  0.0282,\n",
       "                       -0.0009, -0.0187, -0.0101, -0.0353,  0.0054,  0.0013, -0.0063, -0.0174,\n",
       "                       -0.0096, -0.0242,  0.0046, -0.0104, -0.0169,  0.0135,  0.0113,  0.0116,\n",
       "                        0.0058, -0.0031, -0.0009, -0.0004,  0.0068, -0.0032, -0.0076,  0.0107,\n",
       "                        0.0072, -0.0025, -0.0163,  0.0089,  0.0125, -0.0108,  0.0024,  0.0226,\n",
       "                       -0.0144,  0.0032, -0.0118, -0.0031,  0.0021, -0.0323,  0.0027, -0.0002],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1631,  0.1944, -0.1138,  0.1166,  0.1172, -0.0240, -0.2111, -0.1388,\n",
       "                       -0.1821, -0.0969,  0.2105, -0.1737,  0.3352, -0.0586,  0.3851,  0.2701,\n",
       "                        0.0005,  0.0484,  0.2549, -0.0579,  0.0446, -0.1371, -0.0903,  0.0135,\n",
       "                        0.0157, -0.0954, -0.0991,  0.3129,  0.2739,  0.1327,  0.0106, -0.0495,\n",
       "                        0.1543, -0.1553, -0.0760, -0.0628,  0.0274,  0.1642, -0.1468, -0.0709,\n",
       "                        0.0016,  0.3324, -0.1130,  0.1172,  0.2649, -0.1730, -0.1429, -0.1181,\n",
       "                       -0.0757,  0.0019,  0.2784, -0.0801,  0.3400,  0.2097, -0.0662, -0.0745,\n",
       "                       -0.0332,  0.2216,  0.0059,  0.3655, -0.1121,  0.2333,  0.0753, -0.1723,\n",
       "                        0.1420, -0.0555,  0.2159, -0.0682,  0.3034, -0.0017,  0.1455,  0.3801,\n",
       "                       -0.0174, -0.1359,  0.2083, -0.1117,  0.0331,  0.3195,  0.1055, -0.1900,\n",
       "                        0.4710, -0.0528, -0.1267, -0.0556,  0.3039,  0.3539,  0.3447,  0.2085,\n",
       "                       -0.1442,  0.1741, -0.1334,  0.0495,  0.0896,  0.1874, -0.0507,  0.2346,\n",
       "                        0.0595,  0.0425, -0.0443, -0.1186,  0.0008,  0.0851, -0.1202, -0.0469,\n",
       "                       -0.0340,  0.0512,  0.1297, -0.1034,  0.0106,  0.2907,  0.2411,  0.4341,\n",
       "                       -0.0890,  0.0216, -0.0916,  0.1807,  0.0130,  0.4608, -0.0359,  0.4015,\n",
       "                       -0.0879, -0.0734, -0.1045, -0.1029,  0.3831, -0.0157, -0.0910, -0.0575],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.9696, 1.1085, 0.8447, 1.1021, 0.9693, 0.9709, 0.9787, 0.9343, 0.8471,\n",
       "                       0.9381, 1.1029, 0.9399, 1.1080, 0.9657, 1.1290, 1.1905, 0.8940, 1.0025,\n",
       "                       0.9887, 0.9973, 0.9491, 0.9175, 0.9180, 0.9997, 0.9615, 0.9512, 0.9187,\n",
       "                       0.9743, 1.0115, 1.0331, 1.0684, 0.9661, 1.1221, 0.9391, 0.9968, 0.9734,\n",
       "                       0.9412, 1.0812, 0.9791, 1.0023, 1.0361, 1.1265, 0.9472, 1.1115, 0.9799,\n",
       "                       0.9660, 0.9552, 0.9922, 0.9178, 1.0445, 0.9584, 0.9479, 1.0931, 0.9649,\n",
       "                       0.9693, 0.9627, 0.9913, 1.0533, 1.0207, 1.0808, 1.0113, 1.0104, 0.9997,\n",
       "                       0.9408, 1.0169, 0.9237, 0.9685, 1.0111, 1.1905, 0.9716, 1.0283, 1.1558,\n",
       "                       0.9849, 0.9728, 0.9640, 0.9509, 1.0301, 1.1699, 1.1360, 0.8957, 1.0791,\n",
       "                       1.0465, 0.9238, 1.0184, 1.0511, 1.1042, 0.9738, 1.0620, 0.8964, 0.9912,\n",
       "                       0.8152, 0.9297, 0.9573, 1.0192, 0.9643, 0.9865, 0.9691, 0.9859, 0.9162,\n",
       "                       1.0091, 0.9118, 1.0460, 0.9582, 1.0327, 0.9797, 1.0288, 1.0011, 0.9599,\n",
       "                       1.0142, 1.0149, 0.9774, 1.1019, 1.0000, 0.9757, 0.9746, 1.1659, 0.9524,\n",
       "                       1.0279, 1.0224, 1.1533, 0.9746, 0.9960, 0.9669, 0.9357, 1.0148, 0.9732,\n",
       "                       0.9627, 0.9500], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 0.0073,  0.0615,  0.0120],\n",
       "                         [-0.0109,  0.0211, -0.0202],\n",
       "                         [ 0.0240,  0.0670, -0.0142]],\n",
       "               \n",
       "                        [[ 0.0408,  0.0463, -0.0002],\n",
       "                         [-0.0267, -0.0258, -0.0401],\n",
       "                         [-0.0322,  0.0412, -0.0157]],\n",
       "               \n",
       "                        [[ 0.0051, -0.0471, -0.0257],\n",
       "                         [-0.0522, -0.0002,  0.0558],\n",
       "                         [ 0.0197,  0.0114, -0.0256]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0189, -0.0542, -0.0090],\n",
       "                         [-0.0584, -0.0753, -0.0285],\n",
       "                         [-0.0501, -0.0588, -0.0454]],\n",
       "               \n",
       "                        [[ 0.0147,  0.0626, -0.0057],\n",
       "                         [-0.0027,  0.0439, -0.0190],\n",
       "                         [ 0.0332, -0.0397,  0.0007]],\n",
       "               \n",
       "                        [[-0.0371,  0.0505, -0.0220],\n",
       "                         [-0.0072,  0.0231,  0.0021],\n",
       "                         [-0.0321, -0.0171,  0.0332]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0149, -0.0397,  0.0341],\n",
       "                         [ 0.0416,  0.0181, -0.0336],\n",
       "                         [ 0.0185,  0.0093,  0.0077]],\n",
       "               \n",
       "                        [[ 0.0276,  0.0237, -0.0054],\n",
       "                         [-0.0764,  0.0186,  0.0145],\n",
       "                         [-0.0378, -0.0316,  0.0317]],\n",
       "               \n",
       "                        [[ 0.0133, -0.0043, -0.0575],\n",
       "                         [-0.0120, -0.0424, -0.0046],\n",
       "                         [-0.0632, -0.0027, -0.0540]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0045, -0.0387, -0.0022],\n",
       "                         [-0.0536, -0.0996, -0.0563],\n",
       "                         [-0.0248,  0.0001, -0.0399]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0114, -0.0219],\n",
       "                         [ 0.0516,  0.0429,  0.0258],\n",
       "                         [ 0.0443, -0.0104,  0.0394]],\n",
       "               \n",
       "                        [[-0.0065, -0.0368,  0.0156],\n",
       "                         [ 0.0126, -0.0725, -0.0404],\n",
       "                         [-0.0613, -0.0766, -0.0209]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0573, -0.0550, -0.0113],\n",
       "                         [-0.0121,  0.0007, -0.0628],\n",
       "                         [ 0.0059,  0.0141, -0.0043]],\n",
       "               \n",
       "                        [[ 0.0305, -0.0125, -0.0485],\n",
       "                         [ 0.0321,  0.0384, -0.0741],\n",
       "                         [ 0.0067,  0.0431, -0.0392]],\n",
       "               \n",
       "                        [[-0.0431,  0.0168, -0.0467],\n",
       "                         [-0.0372,  0.0102,  0.0053],\n",
       "                         [ 0.0024,  0.0143,  0.0398]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0455,  0.0046, -0.0200],\n",
       "                         [-0.0405, -0.0600, -0.0352],\n",
       "                         [-0.0408, -0.0624, -0.0764]],\n",
       "               \n",
       "                        [[ 0.0437,  0.0360,  0.0398],\n",
       "                         [-0.0529, -0.0126, -0.0394],\n",
       "                         [-0.0420,  0.0150, -0.0273]],\n",
       "               \n",
       "                        [[-0.0355, -0.0435,  0.0117],\n",
       "                         [-0.0331,  0.0213,  0.0256],\n",
       "                         [ 0.0059, -0.0061, -0.0219]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0504,  0.0111,  0.0091],\n",
       "                         [ 0.0700,  0.0102, -0.0127],\n",
       "                         [ 0.0472, -0.0750, -0.0749]],\n",
       "               \n",
       "                        [[ 0.0126,  0.0457,  0.0538],\n",
       "                         [-0.0469, -0.0615, -0.0298],\n",
       "                         [-0.0257,  0.0307,  0.0064]],\n",
       "               \n",
       "                        [[ 0.0381,  0.0234,  0.0319],\n",
       "                         [ 0.0427, -0.0436, -0.0046],\n",
       "                         [-0.0133, -0.0470, -0.0157]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0282, -0.0291, -0.0278],\n",
       "                         [ 0.0064, -0.0879, -0.0409],\n",
       "                         [-0.0045, -0.0201, -0.0508]],\n",
       "               \n",
       "                        [[ 0.0012,  0.0317,  0.0562],\n",
       "                         [ 0.0444,  0.0208,  0.0051],\n",
       "                         [-0.0373,  0.0474,  0.0533]],\n",
       "               \n",
       "                        [[-0.0329, -0.0453, -0.0214],\n",
       "                         [-0.0277,  0.0045,  0.0224],\n",
       "                         [-0.0115,  0.0191, -0.0240]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0053, -0.0619,  0.0081],\n",
       "                         [-0.0290, -0.0561,  0.0356],\n",
       "                         [-0.0297,  0.0281,  0.0076]],\n",
       "               \n",
       "                        [[ 0.0019,  0.0412, -0.0531],\n",
       "                         [-0.0054,  0.0489, -0.0376],\n",
       "                         [-0.0378, -0.0377,  0.0578]],\n",
       "               \n",
       "                        [[-0.0227,  0.0314,  0.0516],\n",
       "                         [ 0.0181, -0.0008, -0.0313],\n",
       "                         [-0.0155,  0.0141,  0.0279]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0321, -0.0038,  0.0526],\n",
       "                         [-0.0037,  0.0310, -0.0090],\n",
       "                         [-0.0042, -0.0025, -0.0319]],\n",
       "               \n",
       "                        [[ 0.0066,  0.0332,  0.0456],\n",
       "                         [-0.0186, -0.0539, -0.0037],\n",
       "                         [-0.0198, -0.0442,  0.0125]],\n",
       "               \n",
       "                        [[-0.0379, -0.0151,  0.0192],\n",
       "                         [-0.0437,  0.0384,  0.0344],\n",
       "                         [ 0.0192,  0.0085,  0.0302]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0173, -0.0070, -0.0065],\n",
       "                         [ 0.0314,  0.0438, -0.0579],\n",
       "                         [-0.0199,  0.0290, -0.0254]],\n",
       "               \n",
       "                        [[-0.0348,  0.0351,  0.0423],\n",
       "                         [ 0.0056, -0.0599, -0.0338],\n",
       "                         [-0.0596, -0.0250,  0.0199]],\n",
       "               \n",
       "                        [[-0.0150, -0.0501,  0.0067],\n",
       "                         [-0.0007,  0.0117,  0.0053],\n",
       "                         [ 0.0055, -0.0287,  0.0358]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0159, -0.0148, -0.0628],\n",
       "                         [ 0.0441,  0.0680, -0.0351],\n",
       "                         [ 0.0223,  0.0213, -0.0230]],\n",
       "               \n",
       "                        [[-0.0644, -0.0820, -0.0552],\n",
       "                         [ 0.0076,  0.0235, -0.0232],\n",
       "                         [-0.0340, -0.0002, -0.0556]],\n",
       "               \n",
       "                        [[-0.0215,  0.0316,  0.0109],\n",
       "                         [-0.0233,  0.0090,  0.0530],\n",
       "                         [-0.0053, -0.0364, -0.0194]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-1.6949e-02, -8.4122e-03, -6.4646e-03, -4.2481e-03, -2.1994e-03,\n",
       "                        3.4437e-03, -1.9195e-03,  2.9394e-03, -8.6836e-04,  2.9146e-03,\n",
       "                       -4.8363e-03, -2.9786e-03,  8.2595e-04,  2.0065e-03,  1.7538e-03,\n",
       "                        5.7412e-03, -3.8066e-05,  3.0353e-03, -2.8385e-03,  2.7388e-03,\n",
       "                       -5.7158e-03,  1.9158e-03, -2.4051e-03, -2.9564e-03,  2.5613e-03,\n",
       "                        2.3439e-04, -2.2504e-03,  7.3651e-03, -1.2441e-03, -8.3805e-05,\n",
       "                        8.6782e-03, -3.5325e-03,  3.8182e-03,  1.5983e-03,  1.2204e-03,\n",
       "                       -6.0784e-03, -4.5049e-04,  4.5410e-03,  3.2182e-04, -2.8716e-03,\n",
       "                       -2.2109e-03,  3.7276e-03,  7.1509e-04, -2.3736e-03,  3.0177e-04,\n",
       "                        3.1425e-03,  4.0651e-03, -3.7909e-03, -1.2114e-03, -1.6332e-03,\n",
       "                       -8.7488e-03, -9.9171e-04,  8.2588e-04, -7.7100e-04, -8.6759e-03,\n",
       "                        3.9878e-03, -2.7320e-03, -8.2349e-03,  2.0059e-03,  6.6739e-04,\n",
       "                        2.1313e-03, -2.2211e-03,  1.5898e-03,  6.0315e-04, -7.0056e-03,\n",
       "                        3.8265e-03,  4.4242e-03, -4.5509e-03,  2.2188e-04, -2.7838e-03,\n",
       "                        1.4363e-03,  4.0279e-04, -1.7379e-03, -4.3296e-03,  2.3067e-03,\n",
       "                       -5.1976e-03, -2.9156e-04, -1.3852e-03,  7.8145e-03,  1.6117e-03,\n",
       "                        6.3610e-03, -2.0517e-03, -5.3275e-03,  4.3522e-03, -2.8026e-03,\n",
       "                        3.9732e-03,  5.5070e-03, -7.5503e-05,  1.0788e-03,  2.9781e-03,\n",
       "                       -9.4866e-03,  5.1489e-04, -3.9218e-04, -5.8387e-05, -1.9815e-03,\n",
       "                        6.0945e-04,  1.0402e-03,  9.8662e-04, -5.3993e-04, -2.6299e-04,\n",
       "                        1.4962e-03, -5.3848e-04,  1.7470e-03, -3.8175e-03, -1.7810e-03,\n",
       "                       -1.8436e-03, -2.5974e-03,  2.9317e-03,  2.3702e-03,  2.2829e-04,\n",
       "                        2.0404e-03,  5.7490e-03,  1.5104e-03,  7.5078e-04, -1.6547e-03,\n",
       "                        4.6222e-03,  1.2253e-04,  3.9792e-03,  4.9803e-03,  4.8457e-04,\n",
       "                       -3.8067e-03, -2.3458e-03,  1.2213e-03, -6.6374e-03, -5.1143e-03,\n",
       "                       -1.9425e-03, -1.7261e-03,  2.3845e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0239, -0.1029, -0.1356, -0.1313, -0.1094, -0.1200, -0.0928, -0.1694,\n",
       "                       -0.0216, -0.0378, -0.0961, -0.0686, -0.2043, -0.1061, -0.1571, -0.1857,\n",
       "                       -0.0801,  0.1526, -0.0903, -0.1624, -0.2084, -0.0794, -0.1526, -0.0904,\n",
       "                       -0.1907, -0.0218, -0.1831,  0.0054, -0.0388, -0.1296,  0.0191, -0.1421,\n",
       "                       -0.0195, -0.1232, -0.0756, -0.1430, -0.2357, -0.0668, -0.1038, -0.1295,\n",
       "                       -0.1906, -0.0274, -0.1887, -0.1435, -0.1363, -0.1023, -0.1372, -0.0549,\n",
       "                       -0.0128, -0.1253, -0.1443, -0.0114, -0.0838, -0.1771, -0.0118, -0.0546,\n",
       "                       -0.0105, -0.1224, -0.1915, -0.1396, -0.1079, -0.0798, -0.1026, -0.0963,\n",
       "                       -0.0723, -0.0284, -0.0720, -0.0425, -0.1336, -0.0733, -0.1823, -0.0852,\n",
       "                       -0.0766, -0.0089, -0.0376, -0.1518, -0.0774, -0.1717,  0.0542, -0.1189,\n",
       "                       -0.0779, -0.1253, -0.0342, -0.0763, -0.1555, -0.0973, -0.0166, -0.0855,\n",
       "                       -0.0579, -0.1178, -0.1892, -0.0419, -0.0477, -0.0605, -0.0799, -0.0898,\n",
       "                       -0.0811, -0.0729, -0.1795, -0.1760, -0.1528, -0.1409, -0.1406, -0.1007,\n",
       "                       -0.1355, -0.1034, -0.0196, -0.1602, -0.0788, -0.0995, -0.1055, -0.0866,\n",
       "                       -0.0884, -0.0945, -0.1266, -0.0264, -0.0842, -0.1382, -0.0980, -0.1124,\n",
       "                       -0.1105, -0.0172, -0.1221, -0.0662, -0.0260, -0.0930, -0.0811, -0.1032],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9428, 0.9862, 1.0353, 1.0197, 0.9484, 1.0319, 1.0427, 0.9799, 1.0889,\n",
       "                       0.9699, 1.0409, 0.9796, 0.9760, 1.0362, 1.0216, 1.0216, 1.0361, 0.9781,\n",
       "                       1.0763, 1.0203, 1.0362, 1.0268, 1.0446, 0.9615, 1.0413, 1.0226, 1.0333,\n",
       "                       1.0005, 1.0836, 1.0583, 0.9296, 0.9788, 1.0094, 0.9958, 1.0039, 0.9190,\n",
       "                       1.0555, 0.9978, 0.9221, 0.9902, 1.0340, 0.9762, 1.0726, 1.0702, 0.9993,\n",
       "                       0.9641, 1.0156, 1.0302, 1.0744, 0.9556, 0.9469, 0.9848, 0.9398, 0.9957,\n",
       "                       0.9438, 1.0218, 1.0183, 0.9649, 0.9851, 0.9523, 0.9629, 0.9934, 0.9863,\n",
       "                       0.9597, 0.9350, 1.0053, 1.0129, 1.0250, 0.9279, 1.0033, 0.9282, 0.9721,\n",
       "                       1.0010, 0.9276, 1.0228, 0.9233, 1.0320, 1.0337, 0.9309, 0.9750, 1.0079,\n",
       "                       0.9799, 1.0670, 0.9271, 1.0485, 0.9817, 0.9526, 0.9983, 1.1004, 0.9980,\n",
       "                       1.0839, 1.0081, 0.9850, 1.0833, 0.9840, 0.9629, 0.9918, 0.9759, 1.0162,\n",
       "                       1.0476, 0.9885, 1.0907, 0.9968, 1.0194, 0.9749, 0.9604, 0.9823, 0.9796,\n",
       "                       1.0114, 1.0355, 0.9358, 1.0666, 0.9849, 1.0066, 0.9954, 1.0633, 1.0803,\n",
       "                       0.9390, 0.9113, 1.0046, 0.9884, 1.0092, 1.0470, 0.9643, 0.9522, 1.0208,\n",
       "                       1.0418, 0.9650], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-5.6612e-04,  5.5613e-02, -1.4964e-02],\n",
       "                         [-3.1003e-02, -5.2417e-03, -3.5244e-02],\n",
       "                         [ 1.5153e-02, -2.5045e-02,  8.3373e-03]],\n",
       "               \n",
       "                        [[ 2.6745e-02, -1.6569e-02,  9.6491e-03],\n",
       "                         [-2.3582e-02, -1.7072e-02, -3.0692e-02],\n",
       "                         [ 8.8099e-03,  1.9431e-02, -1.0071e-02]],\n",
       "               \n",
       "                        [[ 2.2886e-02,  1.2657e-02,  2.7444e-02],\n",
       "                         [ 6.7986e-02,  2.3340e-02, -6.3387e-02],\n",
       "                         [-8.0550e-03,  4.3950e-03, -2.6644e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.2784e-02, -4.7080e-02, -5.8843e-02],\n",
       "                         [ 5.8877e-02,  3.3930e-02,  5.3777e-02],\n",
       "                         [-3.0208e-06, -3.6364e-02, -3.0298e-02]],\n",
       "               \n",
       "                        [[-1.8547e-02, -3.3489e-02,  1.4912e-02],\n",
       "                         [ 4.4668e-02,  1.5992e-02, -3.8668e-02],\n",
       "                         [ 1.9676e-02,  3.2789e-02,  3.8013e-02]],\n",
       "               \n",
       "                        [[ 9.6036e-03,  1.6508e-02,  6.3899e-02],\n",
       "                         [-2.2815e-02, -2.2302e-02, -3.1161e-02],\n",
       "                         [ 1.2610e-03,  2.5403e-03,  4.1579e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3809e-03,  3.2775e-03,  3.4006e-02],\n",
       "                         [ 1.2260e-02,  2.4693e-03, -3.2722e-02],\n",
       "                         [-2.4378e-02, -6.5934e-02, -2.4807e-02]],\n",
       "               \n",
       "                        [[-4.2487e-02, -2.5946e-02, -2.8232e-02],\n",
       "                         [-1.6780e-02,  6.6708e-03, -4.2812e-03],\n",
       "                         [-1.7184e-02, -2.3446e-03,  1.5289e-02]],\n",
       "               \n",
       "                        [[ 8.0496e-03,  5.3279e-02,  9.2603e-03],\n",
       "                         [ 3.7587e-02,  3.2692e-02,  6.3311e-03],\n",
       "                         [ 4.8233e-02,  1.8544e-02, -1.1102e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.0004e-02,  3.0527e-02, -4.1664e-02],\n",
       "                         [-5.6780e-02, -4.1942e-02, -5.5847e-02],\n",
       "                         [-4.5592e-02, -2.2262e-02, -7.3276e-02]],\n",
       "               \n",
       "                        [[-2.0432e-02, -1.4052e-02,  1.8676e-03],\n",
       "                         [-9.4970e-02, -1.9388e-02, -1.8318e-02],\n",
       "                         [-6.2985e-02, -3.7837e-02, -3.0712e-02]],\n",
       "               \n",
       "                        [[-9.6093e-03, -8.8685e-02, -7.3937e-02],\n",
       "                         [-2.9326e-02,  7.3677e-04,  8.5064e-03],\n",
       "                         [-4.6906e-02, -1.3926e-02, -2.8744e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.4945e-02, -1.6051e-03, -6.4338e-02],\n",
       "                         [-9.1217e-04, -1.0536e-02,  1.0642e-02],\n",
       "                         [ 5.3609e-02,  2.5719e-02,  1.7490e-02]],\n",
       "               \n",
       "                        [[ 3.2974e-02,  7.0537e-02,  6.3091e-02],\n",
       "                         [-2.3824e-02,  3.8106e-02,  3.0877e-03],\n",
       "                         [-8.1763e-02, -1.9468e-02, -8.9834e-02]],\n",
       "               \n",
       "                        [[ 5.2738e-02,  1.4020e-02,  6.0283e-02],\n",
       "                         [-8.0464e-03,  9.1787e-02,  2.4450e-02],\n",
       "                         [-7.7595e-04,  4.2054e-02, -5.3030e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.1894e-02,  7.4259e-03, -4.8836e-02],\n",
       "                         [-3.9580e-03, -3.1565e-02, -1.5890e-02],\n",
       "                         [-2.4345e-02, -1.7730e-02,  5.6163e-02]],\n",
       "               \n",
       "                        [[-1.3640e-02, -2.9441e-02, -1.1606e-02],\n",
       "                         [ 6.9270e-05, -5.4901e-02, -6.5660e-02],\n",
       "                         [-7.3340e-02, -7.6530e-02, -5.1404e-02]],\n",
       "               \n",
       "                        [[-1.6647e-02,  2.0942e-02,  5.6214e-02],\n",
       "                         [ 1.0635e-02,  3.8857e-02,  2.6654e-02],\n",
       "                         [-1.2568e-02, -2.9424e-04,  1.8442e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.2160e-02, -4.8316e-02, -5.4221e-02],\n",
       "                         [ 3.5441e-02, -3.3133e-02, -1.6483e-02],\n",
       "                         [ 1.4458e-02,  2.3074e-02,  1.1153e-02]],\n",
       "               \n",
       "                        [[-4.4017e-02,  5.9920e-03,  1.8519e-02],\n",
       "                         [-1.0042e-02,  1.2894e-02, -3.1734e-02],\n",
       "                         [-4.1772e-03,  5.4501e-02,  8.8462e-03]],\n",
       "               \n",
       "                        [[-2.9016e-02,  7.5986e-03, -2.8667e-02],\n",
       "                         [ 3.5275e-02,  5.9086e-03,  3.4249e-02],\n",
       "                         [ 6.0786e-02,  2.2129e-02,  1.7514e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.5898e-02,  3.3125e-02,  1.3987e-02],\n",
       "                         [ 1.1372e-02,  4.8750e-03, -5.3042e-03],\n",
       "                         [-1.7770e-02, -3.6008e-02, -4.0320e-02]],\n",
       "               \n",
       "                        [[-8.7624e-03, -1.3168e-02,  6.0018e-02],\n",
       "                         [-6.0902e-03, -3.3698e-03, -2.7294e-02],\n",
       "                         [-3.2872e-02,  3.5987e-02, -7.3005e-02]],\n",
       "               \n",
       "                        [[ 1.0989e-02, -4.6545e-02, -6.0721e-02],\n",
       "                         [-7.2877e-02, -8.1973e-02, -7.7765e-02],\n",
       "                         [-3.0052e-02,  2.0828e-02, -2.5616e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.8925e-02,  4.8917e-02,  2.8418e-02],\n",
       "                         [-2.1285e-02, -2.5500e-02, -7.5975e-03],\n",
       "                         [-2.8078e-03,  2.9815e-02, -1.3520e-02]],\n",
       "               \n",
       "                        [[ 2.1450e-04, -2.8705e-02,  2.4119e-02],\n",
       "                         [-3.6257e-03, -3.2389e-02,  4.0854e-02],\n",
       "                         [-3.2382e-02,  3.5661e-03,  7.8317e-03]],\n",
       "               \n",
       "                        [[-3.0559e-02,  2.0490e-02,  5.0351e-03],\n",
       "                         [ 5.0072e-02,  4.6661e-02, -1.5058e-02],\n",
       "                         [ 1.7608e-02, -1.3727e-02,  2.3573e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.6052e-02, -3.8395e-03,  3.6573e-02],\n",
       "                         [-5.7949e-04,  7.1536e-02, -1.0412e-02],\n",
       "                         [-1.5482e-03,  2.2744e-02, -8.5390e-03]],\n",
       "               \n",
       "                        [[-6.3845e-02, -4.6169e-02, -4.0007e-02],\n",
       "                         [-1.1293e-02, -3.5827e-02,  1.3014e-02],\n",
       "                         [ 5.1841e-02, -1.6890e-02, -7.2922e-02]],\n",
       "               \n",
       "                        [[ 4.1673e-02,  1.7039e-02, -8.0290e-02],\n",
       "                         [-4.1658e-04, -2.3544e-02, -1.0676e-01],\n",
       "                         [ 4.5347e-03, -2.7998e-02, -2.3365e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.9852e-02,  5.9324e-03,  9.6843e-04],\n",
       "                         [-1.5981e-02, -3.1834e-02,  5.2805e-02],\n",
       "                         [-3.6868e-02, -1.7340e-02,  3.9554e-02]],\n",
       "               \n",
       "                        [[ 6.7321e-02,  2.9623e-02,  5.4673e-03],\n",
       "                         [ 2.0529e-02,  6.5971e-03, -2.3391e-02],\n",
       "                         [ 1.7442e-02,  3.5414e-02,  3.6108e-02]],\n",
       "               \n",
       "                        [[-3.3463e-02, -2.1611e-02, -1.9678e-02],\n",
       "                         [-4.8424e-02,  3.5010e-02,  3.9424e-02],\n",
       "                         [-5.9420e-03,  4.3907e-02,  3.3360e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9797e-02,  4.3752e-02,  1.3179e-02],\n",
       "                         [ 4.3918e-02,  3.1366e-02, -2.0022e-02],\n",
       "                         [-3.3028e-02, -2.8256e-02, -3.0205e-02]],\n",
       "               \n",
       "                        [[-1.2938e-02,  1.2830e-02,  2.9501e-02],\n",
       "                         [-1.7726e-02, -7.1185e-03, -3.7811e-02],\n",
       "                         [ 2.8611e-02, -2.0118e-02, -6.1311e-02]],\n",
       "               \n",
       "                        [[-1.0113e-02,  5.4758e-02,  2.1487e-02],\n",
       "                         [ 3.2794e-03,  1.9351e-02, -2.8451e-02],\n",
       "                         [ 8.8019e-02, -1.0072e-02, -3.6928e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 4.6061e-04, -2.6240e-03, -4.8668e-03,  1.3499e-03, -6.7688e-04,\n",
       "                       -1.3051e-03, -3.9467e-04,  1.6521e-03, -7.8187e-03,  4.3409e-03,\n",
       "                       -2.4901e-03, -1.1735e-03, -1.9624e-04, -7.4011e-03,  1.8107e-05,\n",
       "                       -4.4562e-03,  2.1683e-03, -2.7531e-03,  9.8409e-05,  1.1074e-03,\n",
       "                       -5.6326e-03,  6.3480e-03,  5.0846e-03,  1.4609e-03, -3.3035e-03,\n",
       "                       -3.3362e-03,  9.9636e-04, -5.7492e-04, -3.1613e-03, -1.0247e-02,\n",
       "                        4.1812e-03, -1.9299e-03,  4.8126e-03,  6.2209e-03,  9.1613e-03,\n",
       "                       -3.4160e-03,  1.0048e-02, -1.8047e-02,  1.5066e-04, -9.0010e-04,\n",
       "                       -3.3024e-03,  1.1539e-02,  1.5924e-03, -7.8076e-04,  2.7959e-03,\n",
       "                       -3.8437e-03,  5.5420e-04,  5.6630e-03,  1.7700e-03, -1.1816e-03,\n",
       "                       -3.2343e-03, -1.1565e-03,  5.6338e-03, -2.6592e-03, -2.2296e-03,\n",
       "                       -7.1357e-04,  1.0998e-03, -8.9221e-04, -3.3544e-03, -5.9220e-03,\n",
       "                        3.0695e-03, -2.3067e-03,  7.3235e-03,  6.6048e-03,  1.7583e-03,\n",
       "                        1.2300e-03, -8.0600e-04, -1.3774e-02,  2.6487e-03, -2.2824e-03,\n",
       "                       -4.4781e-03, -4.0840e-03, -4.4604e-04, -9.0035e-04,  1.4574e-04,\n",
       "                        2.9544e-03,  1.5378e-03,  3.9039e-03, -1.7558e-03,  3.4783e-03,\n",
       "                       -3.1708e-03,  6.7397e-03,  2.6774e-03, -4.4915e-03,  1.7781e-03,\n",
       "                        1.4992e-03,  2.2443e-03, -9.6971e-04, -1.8576e-03,  2.4945e-03,\n",
       "                        9.1154e-03,  4.9740e-03,  3.8111e-03,  4.0308e-04,  3.2366e-03,\n",
       "                       -1.2821e-03, -6.6860e-04, -6.5602e-03,  2.9548e-03,  3.9707e-03,\n",
       "                        3.1116e-03, -3.0379e-03,  1.4848e-03, -2.9219e-03, -2.7217e-03,\n",
       "                       -3.1725e-03, -2.3843e-03, -2.0663e-04, -3.5932e-03, -2.9701e-03,\n",
       "                        2.3363e-03, -4.0607e-03,  1.1661e-03,  3.8860e-03, -7.5729e-03,\n",
       "                       -2.9014e-03,  7.2572e-04, -6.2508e-03,  1.2011e-04,  2.4776e-03,\n",
       "                        1.8391e-03, -9.3481e-04,  2.6883e-03, -1.7414e-03,  3.9286e-03,\n",
       "                        3.7851e-03,  5.6336e-03,  5.9318e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.1391, -0.2048, -0.1393, -0.1732, -0.1353, -0.1210, -0.1603, -0.2054,\n",
       "                       -0.0353, -0.1253, -0.1125, -0.0369, -0.2840, -0.1378, -0.1706, -0.2121,\n",
       "                       -0.0803, -0.1494, -0.1289, -0.1470, -0.0888, -0.0605, -0.1538, -0.0884,\n",
       "                       -0.0962, -0.0889, -0.1651, -0.1394, -0.2496, -0.1341, -0.1365, -0.1336,\n",
       "                       -0.2978, -0.0906, -0.1493, -0.2138, -0.0082, -0.2035, -0.2011, -0.2006,\n",
       "                       -0.1547, -0.2458, -0.1979, -0.0222, -0.1107, -0.1439, -0.0984, -0.0936,\n",
       "                       -0.1489, -0.1794, -0.1324, -0.1430, -0.2192, -0.1099, -0.1431, -0.1055,\n",
       "                       -0.1645, -0.1377, -0.1245, -0.0560, -0.2493, -0.0998, -0.1132, -0.2081,\n",
       "                       -0.0350, -0.1173, -0.0356, -0.0940, -0.1193, -0.1307, -0.0620, -0.2028,\n",
       "                       -0.1571, -0.1631, -0.0327, -0.1306, -0.0988, -0.1368, -0.0456, -0.0868,\n",
       "                       -0.1458, -0.1827, -0.1800, -0.1469, -0.0483, -0.1266, -0.1537, -0.1395,\n",
       "                       -0.1806, -0.0839, -0.2118, -0.0111, -0.1126, -0.1722, -0.1317, -0.1179,\n",
       "                       -0.1163, -0.1933, -0.1038, -0.1531, -0.1672, -0.1171, -0.0923, -0.0193,\n",
       "                       -0.0547, -0.2908, -0.1291, -0.1831, -0.1212, -0.1551, -0.0190, -0.0806,\n",
       "                       -0.2205, -0.0160,  0.0108, -0.1951, -0.1521, -0.3187, -0.1491, -0.1353,\n",
       "                       -0.0793, -0.1182,  0.0170, -0.1368, -0.1985, -0.1302, -0.1994, -0.1754],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([1.0277, 1.0128, 0.9804, 0.9433, 0.9412, 0.9652, 0.9658, 0.9749, 1.0517,\n",
       "                       1.0016, 0.9643, 1.0066, 1.0405, 0.9912, 0.9841, 0.9218, 1.1151, 1.0733,\n",
       "                       0.9509, 0.9652, 1.0227, 0.9606, 1.0235, 0.9114, 1.0268, 1.0234, 1.0819,\n",
       "                       0.9045, 1.0696, 1.0600, 0.9874, 1.0240, 1.0417, 1.0809, 0.9790, 1.0562,\n",
       "                       0.9363, 1.1194, 0.9735, 0.9849, 0.9253, 1.0497, 1.0019, 0.9294, 0.9515,\n",
       "                       1.0383, 1.0002, 0.9651, 0.9030, 0.9744, 1.0132, 0.9474, 1.0187, 0.9983,\n",
       "                       0.9703, 0.9965, 0.9904, 0.9778, 0.9497, 1.0112, 1.0529, 0.9374, 0.9905,\n",
       "                       1.0483, 1.0328, 1.0214, 0.9460, 0.9351, 0.9422, 0.9163, 0.9332, 1.0385,\n",
       "                       0.9540, 0.9346, 0.9807, 0.9988, 1.0332, 0.9307, 0.9741, 0.9708, 1.0372,\n",
       "                       0.9618, 0.9719, 1.0110, 0.9977, 0.9824, 0.9189, 0.9367, 1.1518, 0.9375,\n",
       "                       1.0127, 0.9777, 0.9833, 1.0089, 0.9567, 0.9944, 0.9539, 0.9537, 0.9933,\n",
       "                       1.0257, 0.9628, 0.9432, 0.9521, 0.9561, 1.0270, 1.0515, 1.0031, 0.9811,\n",
       "                       1.0540, 0.9654, 1.0987, 0.9105, 0.8965, 0.9892, 0.9852, 0.9682, 0.8834,\n",
       "                       1.0933, 1.0380, 0.9390, 0.9505, 1.0529, 1.0772, 1.0426, 0.9539, 1.0445,\n",
       "                       0.9834, 1.0106], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-5.4627e-02,  7.3330e-04,  1.0867e-02],\n",
       "                         [-1.2974e-02,  1.8572e-02, -8.2295e-03],\n",
       "                         [-6.2027e-03,  3.7114e-03,  6.4246e-02]],\n",
       "               \n",
       "                        [[-4.5182e-02, -5.4583e-02, -4.1419e-02],\n",
       "                         [-4.6200e-02, -4.0708e-02, -3.4592e-02],\n",
       "                         [-3.8282e-02,  5.8337e-03, -4.8956e-02]],\n",
       "               \n",
       "                        [[ 6.5746e-02,  1.5244e-02,  8.7147e-02],\n",
       "                         [-3.4472e-02, -2.5536e-02,  2.4144e-02],\n",
       "                         [-3.4479e-02,  4.1504e-02,  2.4551e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.6482e-02, -7.8470e-03, -3.9585e-02],\n",
       "                         [ 2.1164e-02, -1.7489e-02,  1.2096e-02],\n",
       "                         [ 2.4554e-02,  6.7232e-02,  4.6818e-02]],\n",
       "               \n",
       "                        [[-1.3402e-02, -3.8527e-03, -2.4207e-02],\n",
       "                         [ 1.4606e-02, -7.9428e-03, -1.4672e-02],\n",
       "                         [ 2.4145e-02, -2.7772e-02, -3.5707e-02]],\n",
       "               \n",
       "                        [[ 5.1828e-02, -9.1739e-03,  8.9307e-03],\n",
       "                         [ 3.5144e-03, -6.4678e-03, -4.3382e-02],\n",
       "                         [-9.9676e-03, -3.8737e-02, -2.5570e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4494e-03, -8.8308e-03, -2.3047e-02],\n",
       "                         [ 5.3864e-03, -3.2619e-02, -1.8617e-02],\n",
       "                         [ 3.7532e-03, -3.0453e-03, -3.3701e-02]],\n",
       "               \n",
       "                        [[-1.7086e-02, -1.4788e-02, -3.8940e-03],\n",
       "                         [-6.3392e-02, -3.3190e-02, -4.4878e-02],\n",
       "                         [-3.9955e-02,  1.3318e-02,  1.3630e-02]],\n",
       "               \n",
       "                        [[-2.2971e-02,  1.3972e-02,  4.7967e-02],\n",
       "                         [ 2.2433e-02,  3.2503e-03, -3.8789e-04],\n",
       "                         [ 1.0756e-02,  1.0665e-02,  4.0786e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.7207e-02, -3.5612e-02,  3.2990e-02],\n",
       "                         [-1.0289e-02, -5.2001e-02,  2.2325e-02],\n",
       "                         [ 4.3186e-02, -5.0887e-03, -3.1521e-02]],\n",
       "               \n",
       "                        [[-7.2463e-03,  2.2586e-02, -4.4914e-02],\n",
       "                         [-2.0083e-02, -1.9633e-04, -3.0193e-02],\n",
       "                         [ 3.4482e-02, -5.3447e-03,  2.6395e-02]],\n",
       "               \n",
       "                        [[ 2.2625e-02, -2.5691e-03, -5.2997e-02],\n",
       "                         [-3.9718e-02, -5.9920e-02, -7.0250e-02],\n",
       "                         [-3.1720e-02, -5.6156e-03, -3.6102e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.1116e-02,  4.1400e-02,  7.7896e-02],\n",
       "                         [-5.2314e-03,  5.7153e-02,  6.5850e-02],\n",
       "                         [ 6.1248e-02,  3.1912e-02,  8.1214e-03]],\n",
       "               \n",
       "                        [[-1.7594e-02, -2.7667e-02,  6.5020e-03],\n",
       "                         [-2.5013e-02,  3.1521e-02,  1.0809e-02],\n",
       "                         [-4.3111e-03, -8.4143e-05, -1.1235e-02]],\n",
       "               \n",
       "                        [[ 2.0829e-02,  8.6329e-03, -4.2440e-02],\n",
       "                         [-2.5366e-02,  2.7901e-02, -9.4518e-03],\n",
       "                         [ 8.4475e-03,  5.2818e-02, -3.1964e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2820e-02, -1.8504e-02, -6.6513e-02],\n",
       "                         [-1.3898e-03, -2.1544e-02, -1.7567e-02],\n",
       "                         [-5.2538e-02,  1.3536e-02,  1.3664e-03]],\n",
       "               \n",
       "                        [[ 5.7266e-02,  3.3359e-02,  2.5032e-03],\n",
       "                         [ 6.3723e-02,  6.3739e-02,  5.4753e-02],\n",
       "                         [-1.3828e-02,  1.9169e-02,  1.1911e-02]],\n",
       "               \n",
       "                        [[-2.6761e-02,  1.5723e-02, -1.2717e-02],\n",
       "                         [-2.4329e-02,  3.9293e-03, -3.9389e-02],\n",
       "                         [ 1.6449e-02,  1.2233e-02, -1.2991e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0369e-02,  2.0749e-02,  7.4456e-02],\n",
       "                         [ 7.0774e-02,  4.0400e-02,  2.0487e-02],\n",
       "                         [ 3.9115e-02,  3.1699e-02,  2.0401e-02]],\n",
       "               \n",
       "                        [[-2.9395e-02,  2.5912e-02, -1.5670e-02],\n",
       "                         [ 3.6106e-02, -5.6428e-03, -1.2668e-02],\n",
       "                         [ 1.1267e-02, -1.9298e-02,  1.0444e-02]],\n",
       "               \n",
       "                        [[ 6.6017e-02,  9.4842e-02,  1.0661e-01],\n",
       "                         [ 5.6030e-02,  4.9520e-02, -1.6742e-02],\n",
       "                         [ 3.1501e-02,  5.3863e-02,  6.5480e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.5279e-03,  2.6637e-02, -1.5511e-02],\n",
       "                         [ 1.0471e-02, -2.2111e-02, -1.6427e-02],\n",
       "                         [ 6.6035e-02,  2.6296e-02,  9.1037e-04]],\n",
       "               \n",
       "                        [[-1.9806e-02, -5.3623e-03, -2.7325e-02],\n",
       "                         [-2.5083e-02, -1.0296e-02, -1.6746e-02],\n",
       "                         [ 2.9134e-02, -5.8477e-03, -2.6059e-03]],\n",
       "               \n",
       "                        [[-3.5956e-02,  9.9449e-03,  7.3160e-03],\n",
       "                         [-3.3580e-02, -1.2026e-02,  1.8119e-02],\n",
       "                         [-5.9664e-02, -2.7382e-02, -9.3631e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0678e-02,  2.4934e-02,  2.0218e-02],\n",
       "                         [ 4.2124e-02,  6.6200e-03,  6.8485e-02],\n",
       "                         [ 4.8555e-02,  3.4361e-03, -4.3516e-03]],\n",
       "               \n",
       "                        [[ 2.7361e-02, -1.3040e-02,  1.2497e-02],\n",
       "                         [ 2.7245e-02,  7.9802e-03,  6.1445e-02],\n",
       "                         [ 2.1408e-02, -2.5271e-02, -1.4236e-02]],\n",
       "               \n",
       "                        [[ 2.6388e-02,  1.0168e-02,  5.2068e-03],\n",
       "                         [-1.7016e-02, -1.5717e-02, -2.3489e-02],\n",
       "                         [ 2.1516e-03,  2.9778e-02, -4.5113e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0460e-02, -1.1264e-02,  1.8085e-02],\n",
       "                         [ 6.7404e-02, -4.3560e-03,  7.5595e-02],\n",
       "                         [ 3.1166e-02,  8.9611e-02,  2.6672e-02]],\n",
       "               \n",
       "                        [[ 3.6279e-03, -4.4067e-02,  2.6130e-02],\n",
       "                         [-2.2956e-02,  3.1149e-02,  1.0416e-02],\n",
       "                         [ 2.2577e-02, -3.9788e-02,  4.6509e-02]],\n",
       "               \n",
       "                        [[ 1.1394e-02,  2.5146e-03, -3.5653e-02],\n",
       "                         [-4.5091e-02,  1.8573e-02, -5.8179e-02],\n",
       "                         [-2.0710e-02, -1.1970e-02, -3.7716e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.0215e-02, -2.5592e-02, -7.2966e-03],\n",
       "                         [-6.4089e-03,  8.9794e-03,  4.4473e-03],\n",
       "                         [-5.4829e-02, -2.1084e-02,  1.4473e-03]],\n",
       "               \n",
       "                        [[ 5.4302e-02,  6.5507e-02,  8.2479e-02],\n",
       "                         [ 5.4617e-02,  6.6363e-02,  1.0264e-01],\n",
       "                         [ 6.9296e-02,  1.1446e-01,  1.0007e-01]],\n",
       "               \n",
       "                        [[-2.8678e-02, -2.0243e-02, -2.5346e-02],\n",
       "                         [-5.8157e-02, -1.5913e-02, -2.3890e-02],\n",
       "                         [ 6.0083e-03, -2.9671e-02, -3.1314e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5507e-02, -1.0065e-02,  3.0881e-02],\n",
       "                         [ 3.1270e-03, -3.6857e-02,  1.6174e-02],\n",
       "                         [-4.4302e-02, -1.9763e-02, -2.2433e-02]],\n",
       "               \n",
       "                        [[ 4.3493e-02,  8.6395e-02,  3.7174e-02],\n",
       "                         [ 2.9114e-02,  2.4833e-02, -1.4103e-02],\n",
       "                         [ 3.0108e-02,  8.4913e-02,  2.5513e-02]],\n",
       "               \n",
       "                        [[ 5.9403e-03,  9.7578e-03, -2.9943e-02],\n",
       "                         [ 1.4497e-03,  2.5015e-02, -1.4867e-02],\n",
       "                         [ 3.0806e-02,  3.3726e-02,  1.3615e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.0357,  0.0166,  0.0038, -0.0068, -0.0011, -0.0061, -0.0103, -0.0034,\n",
       "                       -0.0166,  0.0006,  0.0040, -0.0227, -0.0109, -0.0218, -0.0041,  0.0159,\n",
       "                        0.0249,  0.0085,  0.0003, -0.0096, -0.0051,  0.0175, -0.0161,  0.0572,\n",
       "                       -0.0335,  0.0107,  0.0247,  0.0042,  0.0377, -0.0171, -0.1015, -0.0228,\n",
       "                       -0.0087, -0.0797, -0.0014,  0.0367, -0.0072, -0.0147,  0.0137, -0.0163,\n",
       "                       -0.0089,  0.0034, -0.0174, -0.0188, -0.0105,  0.0368, -0.0123, -0.0034,\n",
       "                        0.0002,  0.0054, -0.0139,  0.0104, -0.0051, -0.0017,  0.0151, -0.0027,\n",
       "                        0.0474,  0.0014,  0.0009,  0.0025, -0.0011, -0.0130,  0.0087, -0.0031,\n",
       "                       -0.0616, -0.0055, -0.0170, -0.0149, -0.0131, -0.0219, -0.0087,  0.0045,\n",
       "                        0.0055,  0.0209, -0.0233, -0.0038, -0.0206, -0.0121,  0.0124, -0.0020,\n",
       "                       -0.0793,  0.0078,  0.0091, -0.0271,  0.0183, -0.0040, -0.0276, -0.0200,\n",
       "                       -0.0018, -0.0120,  0.0062,  0.0035, -0.0193, -0.0043,  0.0060, -0.0033,\n",
       "                        0.0979,  0.0046,  0.0230, -0.0159,  0.0214,  0.0229, -0.0133,  0.0055,\n",
       "                        0.0117, -0.0413,  0.0519, -0.0069, -0.0121,  0.0104,  0.0603,  0.0050,\n",
       "                       -0.0129, -0.0009,  0.0235,  0.0443, -0.0130, -0.0091, -0.0486,  0.0067,\n",
       "                       -0.0302,  0.0049, -0.0119, -0.0003,  0.0028, -0.0006, -0.0140, -0.0030],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([ 0.0080, -0.1051, -0.0583, -0.2114, -0.0156, -0.3025, -0.1431, -0.1690,\n",
       "                       -0.1414, -0.1600, -0.0668, -0.2989, -0.3124, -0.2248, -0.1399, -0.2922,\n",
       "                       -0.1752, -0.2401, -0.0152, -0.2591, -0.2447, -0.1701, -0.1470, -0.2224,\n",
       "                       -0.0362, -0.2455, -0.1750, -0.0396, -0.2098, -0.1359, -0.3048, -0.0343,\n",
       "                        0.0020, -0.1888, -0.2021, -0.0549, -0.1270, -0.1581, -0.1407, -0.0158,\n",
       "                       -0.0772, -0.0932, -0.1724, -0.2669, -0.1154, -0.2852, -0.1887, -0.0445,\n",
       "                       -0.1951, -0.1330, -0.0634, -0.0719, -0.0925, -0.0487, -0.1158, -0.0116,\n",
       "                       -0.3001, -0.2889, -0.1159, -0.0192, -0.1558, -0.1853, -0.1915, -0.1553,\n",
       "                       -0.1455, -0.2951, -0.2013,  0.0113, -0.0357, -0.2242, -0.0636, -0.3184,\n",
       "                       -0.2581, -0.1903, -0.0680, -0.1121, -0.2960, -0.1810, -0.0241, -0.2858,\n",
       "                       -0.0688, -0.1451, -0.0308, -0.1205, -0.3220, -0.3186, -0.0910, -0.1209,\n",
       "                       -0.0116, -0.1675, -0.0689, -0.1094, -0.1885, -0.1492,  0.0643, -0.2327,\n",
       "                       -0.1868, -0.1117, -0.1298, -0.1076, -0.2204, -0.2048, -0.1689, -0.1174,\n",
       "                       -0.1029, -0.1313, -0.2718, -0.1771, -0.0555, -0.1106, -0.2870, -0.0997,\n",
       "                       -0.1625, -0.1487, -0.3023, -0.2512, -0.2352, -0.0299, -0.0183, -0.0160,\n",
       "                       -0.2046, -0.2446, -0.2953, -0.1546, -0.1913, -0.0767, -0.0154, -0.0886],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([1.3516, 1.1140, 1.1335, 1.0717, 1.3636, 0.7596, 1.1063, 1.2343, 1.0519,\n",
       "                       1.5841, 1.0131, 1.0762, 1.1994, 1.4031, 1.1075, 0.9533, 1.0498, 1.0185,\n",
       "                       1.1805, 1.0876, 0.7843, 1.0940, 1.0523, 0.8384, 1.0683, 0.7602, 0.9357,\n",
       "                       1.2673, 0.9396, 1.0553, 0.9735, 1.3504, 1.2847, 1.0209, 1.1235, 1.0936,\n",
       "                       1.1686, 0.9953, 1.0376, 1.3279, 1.3441, 1.0842, 1.2170, 1.0637, 1.1351,\n",
       "                       1.0148, 1.0655, 1.2049, 1.2501, 1.0279, 1.1874, 1.2345, 1.1056, 1.6950,\n",
       "                       1.0704, 1.3799, 0.9915, 1.0065, 1.1655, 1.3675, 1.2508, 0.9559, 0.8257,\n",
       "                       1.2192, 1.0724, 1.2390, 0.9456, 1.2447, 1.2510, 0.8256, 1.2509, 0.9981,\n",
       "                       1.2127, 1.0307, 1.1309, 1.1541, 1.0804, 1.2754, 1.1647, 0.7533, 1.2607,\n",
       "                       1.0583, 1.2901, 0.9747, 0.7526, 1.0185, 1.1555, 1.1468, 1.3335, 1.1562,\n",
       "                       1.3142, 1.2976, 1.2818, 1.0301, 1.4043, 0.9505, 0.9716, 1.0872, 1.1282,\n",
       "                       1.0112, 0.9712, 0.9378, 1.1418, 1.2569, 1.0719, 1.2371, 0.8701, 1.1466,\n",
       "                       1.2526, 1.2469, 0.7931, 1.2396, 1.0986, 1.1581, 0.7296, 0.8709, 1.0038,\n",
       "                       1.3437, 1.2107, 1.2521, 0.9667, 1.2116, 0.9963, 1.1073, 1.0488, 1.2081,\n",
       "                       1.1719, 1.4677], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-4.9095e-03, -1.0411e-02,  5.6891e-03,  ..., -4.5805e-03,\n",
       "                         3.6054e-03,  7.6953e-04],\n",
       "                       [-1.1041e-02, -4.2250e-03,  1.1841e-02,  ..., -7.0961e-03,\n",
       "                         1.0293e-02, -1.3303e-02],\n",
       "                       [ 1.2086e-02,  3.3581e-03,  1.2667e-05,  ..., -2.6210e-03,\n",
       "                         6.9780e-03, -2.0183e-02],\n",
       "                       [-1.2930e-02, -5.7542e-03,  8.0124e-03,  ..., -9.6367e-03,\n",
       "                         2.3009e-04, -4.6410e-03],\n",
       "                       [-7.4690e-03, -9.8778e-03,  1.0700e-02,  ..., -4.4659e-03,\n",
       "                         9.0888e-03, -5.3043e-03]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0318,  0.0158,  0.0506, -0.1405,  0.1046], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.481154550075531,\n",
       "   1.3577819538116456,\n",
       "   1.3073870079517365,\n",
       "   1.2560416444540023,\n",
       "   1.2139344412088393,\n",
       "   1.1738374639749527,\n",
       "   1.1513489400148391,\n",
       "   1.1156441736221314,\n",
       "   1.1000408185720443,\n",
       "   1.1022559398412703,\n",
       "   1.0600120342969894,\n",
       "   1.0610931652784348,\n",
       "   1.0509943833351136,\n",
       "   1.0297512702941896,\n",
       "   1.0259157729148864,\n",
       "   1.0170646781921386,\n",
       "   1.0025434201955796,\n",
       "   0.9979545515775681,\n",
       "   0.9981109420061112,\n",
       "   0.983209489941597,\n",
       "   0.9851244906187058,\n",
       "   0.9585923129320144,\n",
       "   0.9533718949556351,\n",
       "   0.9537534577846527,\n",
       "   0.9565720618963242,\n",
       "   0.9413936160802842,\n",
       "   0.9310720965862275,\n",
       "   0.9261977025270463,\n",
       "   0.927066831946373,\n",
       "   0.9329067448377609,\n",
       "   0.912413406252861,\n",
       "   0.9153839558362961,\n",
       "   0.8964474000930787,\n",
       "   0.8877046661376953,\n",
       "   0.8930694156885147,\n",
       "   0.8728554663658142,\n",
       "   0.8791505703926087,\n",
       "   0.8727824296951294,\n",
       "   0.876187061548233,\n",
       "   0.8546114403605461,\n",
       "   0.8619972977638245,\n",
       "   0.8517441082596778,\n",
       "   0.8415768438577652,\n",
       "   0.8492840473651886,\n",
       "   0.843542504310608,\n",
       "   0.8249881924390793,\n",
       "   0.8268159502148629,\n",
       "   0.8218312242031097,\n",
       "   0.8166390725374222,\n",
       "   0.8181124683618546,\n",
       "   0.8208748137354851,\n",
       "   0.8045872502923012,\n",
       "   0.8046765422224998,\n",
       "   0.7958034241199493,\n",
       "   0.8077864516973495,\n",
       "   0.7975693554878235,\n",
       "   0.790548490345478,\n",
       "   0.7801121383309364,\n",
       "   0.7803864649534226,\n",
       "   0.7814394291639328,\n",
       "   0.7828030265569687,\n",
       "   0.7666941681504249,\n",
       "   0.7606288375258445,\n",
       "   0.7601050273180008,\n",
       "   0.7594932028055191,\n",
       "   0.7595013257265091,\n",
       "   0.7552100366353989,\n",
       "   0.7360448144674301,\n",
       "   0.7414670512080193,\n",
       "   0.7494295625090599,\n",
       "   0.7371460669636727,\n",
       "   0.7371414548754692,\n",
       "   0.7387767916321755,\n",
       "   0.7346621882319451,\n",
       "   0.729343874335289,\n",
       "   0.7154981326460839,\n",
       "   0.7241853189468384,\n",
       "   0.7200521076917649,\n",
       "   0.7168885003328324,\n",
       "   0.7137106335759162,\n",
       "   0.704644744694233,\n",
       "   0.712933917939663,\n",
       "   0.7101412235498429,\n",
       "   0.6976962233185768,\n",
       "   0.6835261573195457,\n",
       "   0.6959152084589004,\n",
       "   0.6946925252676011,\n",
       "   0.6936626036167145,\n",
       "   0.6855437688231468,\n",
       "   0.6757366685271263,\n",
       "   0.6853866739869118,\n",
       "   0.6831811745166778,\n",
       "   0.669134651184082,\n",
       "   0.674112578690052,\n",
       "   0.6681912115812302,\n",
       "   0.6822096589207649,\n",
       "   0.6632646207809448,\n",
       "   0.6614208935499192,\n",
       "   0.6562968887090683],\n",
       "  'train_loss_std': [0.1826323852296664,\n",
       "   0.1304803816692784,\n",
       "   0.13473082044900705,\n",
       "   0.1264148518281296,\n",
       "   0.1309927541645151,\n",
       "   0.13041765483289183,\n",
       "   0.13595038986421248,\n",
       "   0.1282007766301599,\n",
       "   0.125523515884704,\n",
       "   0.12920474467392193,\n",
       "   0.13149506488701052,\n",
       "   0.14458368235504027,\n",
       "   0.13563396875998307,\n",
       "   0.1340685289431707,\n",
       "   0.1269818281368983,\n",
       "   0.13682861508570374,\n",
       "   0.13270618698630288,\n",
       "   0.13596521509053586,\n",
       "   0.13223582941469023,\n",
       "   0.14225810632161273,\n",
       "   0.13308502067756578,\n",
       "   0.12679021754557895,\n",
       "   0.1413390589588845,\n",
       "   0.1359445235222156,\n",
       "   0.13791269515262905,\n",
       "   0.14443757268196517,\n",
       "   0.13392421180560674,\n",
       "   0.13951165945190877,\n",
       "   0.13877931301643015,\n",
       "   0.1358298267473773,\n",
       "   0.13501797264604687,\n",
       "   0.1349164184967577,\n",
       "   0.13695523673551566,\n",
       "   0.1451483073167463,\n",
       "   0.13982310347214305,\n",
       "   0.14314590149939144,\n",
       "   0.1363050076063552,\n",
       "   0.14002422585038526,\n",
       "   0.14759149039856237,\n",
       "   0.14450472539140388,\n",
       "   0.13188158767524813,\n",
       "   0.1350245778423253,\n",
       "   0.12905263484849128,\n",
       "   0.13764389183831624,\n",
       "   0.13881268250807685,\n",
       "   0.1354287122386498,\n",
       "   0.13985902688178561,\n",
       "   0.13178799312143952,\n",
       "   0.1342801378552925,\n",
       "   0.13451919669575796,\n",
       "   0.12829729237562648,\n",
       "   0.14118260486342377,\n",
       "   0.1308724887873948,\n",
       "   0.13919309404204383,\n",
       "   0.14872840812009974,\n",
       "   0.12976774577852346,\n",
       "   0.12731262007723013,\n",
       "   0.13881869922137072,\n",
       "   0.13462943655004367,\n",
       "   0.13770271167993803,\n",
       "   0.13847298443940023,\n",
       "   0.140252588668378,\n",
       "   0.1382651704000421,\n",
       "   0.13250643508855578,\n",
       "   0.13567018807668704,\n",
       "   0.14114834201387394,\n",
       "   0.13345703945506426,\n",
       "   0.13655771799823885,\n",
       "   0.12798300171630722,\n",
       "   0.1373892975461676,\n",
       "   0.12939987675503944,\n",
       "   0.1371566591935981,\n",
       "   0.1380250530490947,\n",
       "   0.13158649704415568,\n",
       "   0.13477428405938588,\n",
       "   0.13420257553948292,\n",
       "   0.1407853177902386,\n",
       "   0.14061222669207435,\n",
       "   0.13677513237132138,\n",
       "   0.13573551411253723,\n",
       "   0.13190207288199612,\n",
       "   0.132529215541455,\n",
       "   0.13839148092956385,\n",
       "   0.130502342266507,\n",
       "   0.1302888656064939,\n",
       "   0.1315539211756657,\n",
       "   0.12965038840868331,\n",
       "   0.13434221155013185,\n",
       "   0.13612430655629618,\n",
       "   0.13094215138925933,\n",
       "   0.13315066921480567,\n",
       "   0.13470950451381783,\n",
       "   0.1279483889866256,\n",
       "   0.13657829005808028,\n",
       "   0.12884267597012028,\n",
       "   0.14187728639938374,\n",
       "   0.12587805437370733,\n",
       "   0.13249774895751423,\n",
       "   0.13068641180660404],\n",
       "  'train_accuracy_mean': [0.4261333337724209,\n",
       "   0.4493066667318344,\n",
       "   0.4682266671061516,\n",
       "   0.4951066664457321,\n",
       "   0.5127066660523415,\n",
       "   0.5336266663074494,\n",
       "   0.5442933322191238,\n",
       "   0.5627199993133545,\n",
       "   0.5695866670012474,\n",
       "   0.565373331964016,\n",
       "   0.588639999628067,\n",
       "   0.5888799980282784,\n",
       "   0.5933466667532921,\n",
       "   0.601453332722187,\n",
       "   0.6016533324122428,\n",
       "   0.603493331849575,\n",
       "   0.6135866670608521,\n",
       "   0.6145866674780845,\n",
       "   0.6122266656756401,\n",
       "   0.6214533323645591,\n",
       "   0.6191599994301796,\n",
       "   0.6309333310723305,\n",
       "   0.6338266662359238,\n",
       "   0.6329333322644234,\n",
       "   0.6336533327102661,\n",
       "   0.6381600015163421,\n",
       "   0.6439333313703537,\n",
       "   0.6469600001573562,\n",
       "   0.6427999994754792,\n",
       "   0.6396266660690307,\n",
       "   0.6505066667199135,\n",
       "   0.6491199991106987,\n",
       "   0.6559199989438057,\n",
       "   0.6606533325314522,\n",
       "   0.6594800000190735,\n",
       "   0.6673466663360595,\n",
       "   0.6655200003981591,\n",
       "   0.6673333329558373,\n",
       "   0.6650399996638298,\n",
       "   0.6748266662359238,\n",
       "   0.6705600000619888,\n",
       "   0.6772666668891907,\n",
       "   0.6799200012087822,\n",
       "   0.6772000007033349,\n",
       "   0.6801466667056084,\n",
       "   0.6869466667175292,\n",
       "   0.686026665687561,\n",
       "   0.6902533336877823,\n",
       "   0.6920133324265481,\n",
       "   0.6886266642808914,\n",
       "   0.68794666659832,\n",
       "   0.6986266648769379,\n",
       "   0.6940266666412354,\n",
       "   0.699613334774971,\n",
       "   0.6942933332920075,\n",
       "   0.7006933341026306,\n",
       "   0.7033600001335144,\n",
       "   0.7074799988865852,\n",
       "   0.7057733334302903,\n",
       "   0.7049866656064987,\n",
       "   0.7036266678571701,\n",
       "   0.7119333344697952,\n",
       "   0.7138933347463607,\n",
       "   0.715853333234787,\n",
       "   0.7141999994516373,\n",
       "   0.7140400002002716,\n",
       "   0.7168533338308334,\n",
       "   0.725626667380333,\n",
       "   0.7227066665887832,\n",
       "   0.7193200001716614,\n",
       "   0.725413333773613,\n",
       "   0.7234800004959107,\n",
       "   0.723253332734108,\n",
       "   0.7253733327388764,\n",
       "   0.7260933326482772,\n",
       "   0.7316666649580001,\n",
       "   0.7288933338522912,\n",
       "   0.7306133338212967,\n",
       "   0.7303333345651627,\n",
       "   0.7314266653060914,\n",
       "   0.735506667137146,\n",
       "   0.7332399996519089,\n",
       "   0.7349199990034103,\n",
       "   0.7404933340549469,\n",
       "   0.7425599994659424,\n",
       "   0.7415066667795182,\n",
       "   0.7392666659355164,\n",
       "   0.7413466668128967,\n",
       "   0.7460399987697601,\n",
       "   0.746693333029747,\n",
       "   0.7457199994325637,\n",
       "   0.7445866672992706,\n",
       "   0.7506400004625321,\n",
       "   0.7486000003814697,\n",
       "   0.7513066667318344,\n",
       "   0.7477466658353805,\n",
       "   0.7541600004434585,\n",
       "   0.7561466666460037,\n",
       "   0.7561733330488205],\n",
       "  'train_accuracy_std': [0.06591361322123487,\n",
       "   0.06227793756495563,\n",
       "   0.06835308367811276,\n",
       "   0.06538203616534438,\n",
       "   0.06831468476400906,\n",
       "   0.06832798748275302,\n",
       "   0.07193369262052633,\n",
       "   0.06729224365959229,\n",
       "   0.06510100054305314,\n",
       "   0.06479930119070763,\n",
       "   0.06795959040083484,\n",
       "   0.07165900098586855,\n",
       "   0.06741513048744034,\n",
       "   0.0680922502859073,\n",
       "   0.06673413008510871,\n",
       "   0.06553351161599054,\n",
       "   0.06747923416971312,\n",
       "   0.06650268395691467,\n",
       "   0.0683710270020473,\n",
       "   0.07152263884476014,\n",
       "   0.06612652072070295,\n",
       "   0.06206032981381363,\n",
       "   0.0703668251657764,\n",
       "   0.06648789418471994,\n",
       "   0.06728701246625754,\n",
       "   0.07155660559517912,\n",
       "   0.066334473167138,\n",
       "   0.06834229755388486,\n",
       "   0.06930627723079129,\n",
       "   0.06537307174004116,\n",
       "   0.06784433944616584,\n",
       "   0.06691306983620103,\n",
       "   0.0668841311562931,\n",
       "   0.069106165892124,\n",
       "   0.06901591305095696,\n",
       "   0.06915219765764169,\n",
       "   0.0647862887261049,\n",
       "   0.06623258767813986,\n",
       "   0.0718419459488728,\n",
       "   0.07169497800973891,\n",
       "   0.06508761424464551,\n",
       "   0.0657706116016513,\n",
       "   0.06296801911217323,\n",
       "   0.06590080222099384,\n",
       "   0.06840842025300041,\n",
       "   0.06627492772510031,\n",
       "   0.06543352288977157,\n",
       "   0.06403907092916886,\n",
       "   0.06840104002154876,\n",
       "   0.06458743119987016,\n",
       "   0.06211642687526532,\n",
       "   0.06738927239924632,\n",
       "   0.06253929844881151,\n",
       "   0.06659951138714797,\n",
       "   0.07149553758333171,\n",
       "   0.06295719321740238,\n",
       "   0.06128475736175209,\n",
       "   0.06637372883572769,\n",
       "   0.06555880956323262,\n",
       "   0.0679099067738081,\n",
       "   0.06434354386270719,\n",
       "   0.06817295001884771,\n",
       "   0.06506917563911233,\n",
       "   0.061723078532587426,\n",
       "   0.0640697890322431,\n",
       "   0.0658114347887569,\n",
       "   0.06567976711477352,\n",
       "   0.06569548135653552,\n",
       "   0.061942325345967345,\n",
       "   0.06522289835833038,\n",
       "   0.06401411539999399,\n",
       "   0.06662449371062536,\n",
       "   0.06412274824441129,\n",
       "   0.06401123695914206,\n",
       "   0.06404671884218813,\n",
       "   0.06506594910096727,\n",
       "   0.06656206531730184,\n",
       "   0.06710573129074256,\n",
       "   0.06630150024425562,\n",
       "   0.06420529746841241,\n",
       "   0.06424250044563738,\n",
       "   0.06427503455762233,\n",
       "   0.06539770887594931,\n",
       "   0.06324433107517077,\n",
       "   0.06368971393939064,\n",
       "   0.06288010494075127,\n",
       "   0.06242716640396464,\n",
       "   0.06472409529633813,\n",
       "   0.06565402924516457,\n",
       "   0.0637313745094133,\n",
       "   0.06218461308944313,\n",
       "   0.06251565245503438,\n",
       "   0.06131912101622321,\n",
       "   0.06498748440065961,\n",
       "   0.062471889729398905,\n",
       "   0.06593726125482527,\n",
       "   0.059265363121513114,\n",
       "   0.06267656471033763,\n",
       "   0.061985131336572095],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.471176369190216,\n",
       "   1.4203386040528616,\n",
       "   1.3806963209311167,\n",
       "   1.3533262141545614,\n",
       "   1.3228069829940796,\n",
       "   1.2923235789934795,\n",
       "   1.2658109680811565,\n",
       "   1.2479879687229791,\n",
       "   1.22375756641229,\n",
       "   1.2312382558981578,\n",
       "   1.2062380532423655,\n",
       "   1.201440934141477,\n",
       "   1.1956076474984487,\n",
       "   1.193373558918635,\n",
       "   1.1765884820620218,\n",
       "   1.17311465660731,\n",
       "   1.187868253191312,\n",
       "   1.1736191300551098,\n",
       "   1.1559246160586676,\n",
       "   1.1491659792264302,\n",
       "   1.1453562692801158,\n",
       "   1.1545843807856242,\n",
       "   1.1417370017369588,\n",
       "   1.1338486299912134,\n",
       "   1.1240334182977676,\n",
       "   1.1281356239318847,\n",
       "   1.1178909635543823,\n",
       "   1.1150680551926295,\n",
       "   1.103643309076627,\n",
       "   1.1039624404907227,\n",
       "   1.0983565264940263,\n",
       "   1.0944552268584569,\n",
       "   1.0878954009215036,\n",
       "   1.0826534553368887,\n",
       "   1.084435091416041,\n",
       "   1.0796767693758011,\n",
       "   1.0691722273826598,\n",
       "   1.075462252298991,\n",
       "   1.0627307752768198,\n",
       "   1.069868769844373,\n",
       "   1.058972169359525,\n",
       "   1.0591596617301304,\n",
       "   1.056293647289276,\n",
       "   1.0516797292232514,\n",
       "   1.0464338860909144,\n",
       "   1.04096601943175,\n",
       "   1.0390473582347235,\n",
       "   1.0437529691060383,\n",
       "   1.0289837962388992,\n",
       "   1.0326292578379312,\n",
       "   1.0349435208241144,\n",
       "   1.03252683142821,\n",
       "   1.0310525498787562,\n",
       "   1.0104590525229773,\n",
       "   1.0139344509442647,\n",
       "   1.0106956521670023,\n",
       "   1.032102631131808,\n",
       "   1.0090903135140736,\n",
       "   1.0105778209368388,\n",
       "   1.0125532406568527,\n",
       "   1.0054045470555624,\n",
       "   0.9964819796880087,\n",
       "   0.9941920403639476,\n",
       "   0.9932256497939428,\n",
       "   0.9877218504746755,\n",
       "   0.9872963384787241,\n",
       "   0.9900266283750534,\n",
       "   0.9877198950449626,\n",
       "   0.9829322874546051,\n",
       "   0.9856233690182368,\n",
       "   0.9900106473763783,\n",
       "   0.9901331992944081,\n",
       "   0.9795691275596619,\n",
       "   0.9728402330478032,\n",
       "   0.9765372564395268,\n",
       "   0.9615845106045405,\n",
       "   0.9775967270135879,\n",
       "   0.9674115592241287,\n",
       "   0.9670706542332967,\n",
       "   0.9565932681163152,\n",
       "   0.9645783323049545,\n",
       "   0.954222569068273,\n",
       "   0.9548276960849762,\n",
       "   0.9615878440936406,\n",
       "   0.9601044146219889,\n",
       "   0.9499993185202281,\n",
       "   0.951361569960912,\n",
       "   0.9399580576022466,\n",
       "   0.9446669640143712,\n",
       "   0.9545165171225866,\n",
       "   0.9450459994872411,\n",
       "   0.9451093810796738,\n",
       "   0.9568006992340088,\n",
       "   0.9583671281735102,\n",
       "   0.9470258406798044,\n",
       "   0.9410540425777435,\n",
       "   0.9342471953233084,\n",
       "   0.9384942634900411,\n",
       "   0.9439093967278799],\n",
       "  'val_loss_std': [0.11203740130864129,\n",
       "   0.10715929112252705,\n",
       "   0.09827341826141245,\n",
       "   0.10125932640351137,\n",
       "   0.10043295801446125,\n",
       "   0.10740659524183253,\n",
       "   0.11049734687425121,\n",
       "   0.11983441107245077,\n",
       "   0.11930227996812569,\n",
       "   0.11529669617151428,\n",
       "   0.11472055706061447,\n",
       "   0.11461747819445803,\n",
       "   0.1188223925285219,\n",
       "   0.12176546787710572,\n",
       "   0.11885147128588246,\n",
       "   0.11581106846402765,\n",
       "   0.11909995483656824,\n",
       "   0.12196800945397059,\n",
       "   0.12398465967775586,\n",
       "   0.12429102030429559,\n",
       "   0.12626138369322082,\n",
       "   0.12399722233293942,\n",
       "   0.12768290815715796,\n",
       "   0.12657912390755227,\n",
       "   0.1270241844349698,\n",
       "   0.12868818947660365,\n",
       "   0.1277192766153749,\n",
       "   0.1314605584614048,\n",
       "   0.13090726697788443,\n",
       "   0.1326811614541339,\n",
       "   0.12793874582377707,\n",
       "   0.1295033536529318,\n",
       "   0.13508594559952292,\n",
       "   0.1256389745632251,\n",
       "   0.1276599462085666,\n",
       "   0.13297767868085042,\n",
       "   0.13163224099730259,\n",
       "   0.1336244294053933,\n",
       "   0.13202423489457363,\n",
       "   0.1308216289214086,\n",
       "   0.1334729536791329,\n",
       "   0.13540187290704866,\n",
       "   0.1331894646012306,\n",
       "   0.13424498999450782,\n",
       "   0.13375660790981328,\n",
       "   0.13291695676870965,\n",
       "   0.13276573331422253,\n",
       "   0.13617518701132503,\n",
       "   0.13315697343223637,\n",
       "   0.131602270458581,\n",
       "   0.13268724730948828,\n",
       "   0.133248200070377,\n",
       "   0.1334498839268254,\n",
       "   0.13341375022411908,\n",
       "   0.1332544375230059,\n",
       "   0.13522877668616448,\n",
       "   0.13646125097249184,\n",
       "   0.13827714189175325,\n",
       "   0.13632873247995536,\n",
       "   0.1397483090422933,\n",
       "   0.14107534466995766,\n",
       "   0.13191791023486107,\n",
       "   0.13609270302166718,\n",
       "   0.1373436265325103,\n",
       "   0.13782754732955282,\n",
       "   0.13881340803437905,\n",
       "   0.13607888968501025,\n",
       "   0.13811387686912519,\n",
       "   0.13513214094033638,\n",
       "   0.13763618903021407,\n",
       "   0.14120659189166673,\n",
       "   0.13886795139057656,\n",
       "   0.13668628195830615,\n",
       "   0.13594511421080996,\n",
       "   0.1339807457042434,\n",
       "   0.13636625945424435,\n",
       "   0.14274674896351155,\n",
       "   0.13894214011758702,\n",
       "   0.13304951145628094,\n",
       "   0.13509201511513172,\n",
       "   0.14027035249475067,\n",
       "   0.13705290740066123,\n",
       "   0.13767863220396992,\n",
       "   0.1387881382663321,\n",
       "   0.13586516452145064,\n",
       "   0.14269873294848856,\n",
       "   0.13649695558837954,\n",
       "   0.13155420116093455,\n",
       "   0.13704813390960915,\n",
       "   0.14400514639874926,\n",
       "   0.14107233424250729,\n",
       "   0.14060849573919304,\n",
       "   0.14711724698209286,\n",
       "   0.1410907367007103,\n",
       "   0.13930975593546616,\n",
       "   0.14175003598064614,\n",
       "   0.14033359981714238,\n",
       "   0.13476484911151734,\n",
       "   0.13750004130782806],\n",
       "  'val_accuracy_mean': [0.4045111114283403,\n",
       "   0.41722222248713176,\n",
       "   0.4324888893961906,\n",
       "   0.4456444451212883,\n",
       "   0.45973333438237507,\n",
       "   0.47355555643637975,\n",
       "   0.485844445625941,\n",
       "   0.49588888804117837,\n",
       "   0.5084666676322619,\n",
       "   0.5044666656851768,\n",
       "   0.5158888885378837,\n",
       "   0.5172444433967273,\n",
       "   0.5201333321134249,\n",
       "   0.5220222216844559,\n",
       "   0.5298222202062607,\n",
       "   0.53033333192269,\n",
       "   0.5256666652361552,\n",
       "   0.5339777773618698,\n",
       "   0.5385111107428868,\n",
       "   0.5424444432059924,\n",
       "   0.543244443833828,\n",
       "   0.5373111102978388,\n",
       "   0.5459999985496203,\n",
       "   0.550755555431048,\n",
       "   0.553444446225961,\n",
       "   0.5526666683952014,\n",
       "   0.5570666654904683,\n",
       "   0.5590888892610868,\n",
       "   0.5632888871431351,\n",
       "   0.563711110452811,\n",
       "   0.5653777765234311,\n",
       "   0.568422221938769,\n",
       "   0.5713999994595845,\n",
       "   0.5731777773300807,\n",
       "   0.5676222208142281,\n",
       "   0.5737999984622002,\n",
       "   0.5795555543899537,\n",
       "   0.5761999988555908,\n",
       "   0.5801555547118187,\n",
       "   0.5775555542111397,\n",
       "   0.584933332502842,\n",
       "   0.584577779173851,\n",
       "   0.5832666645447413,\n",
       "   0.5870222210884094,\n",
       "   0.5891777774691582,\n",
       "   0.5897111116846403,\n",
       "   0.5926888887087504,\n",
       "   0.5913777764638265,\n",
       "   0.5966444445649782,\n",
       "   0.5947777767976125,\n",
       "   0.593177777826786,\n",
       "   0.5961777770519257,\n",
       "   0.5967333329717318,\n",
       "   0.6039111100633939,\n",
       "   0.5999999997019768,\n",
       "   0.6032444436351458,\n",
       "   0.5947777771949768,\n",
       "   0.6045777769883474,\n",
       "   0.6029333333174388,\n",
       "   0.6017555550734202,\n",
       "   0.6071777763962746,\n",
       "   0.6112444439530372,\n",
       "   0.6100222223997116,\n",
       "   0.6110222214460372,\n",
       "   0.6153333310286204,\n",
       "   0.6140222209692001,\n",
       "   0.6119555546840032,\n",
       "   0.6158888883392016,\n",
       "   0.6154888861378034,\n",
       "   0.6144222230712573,\n",
       "   0.612022221883138,\n",
       "   0.6133555539449056,\n",
       "   0.6153111106157303,\n",
       "   0.6194888902703921,\n",
       "   0.616088885863622,\n",
       "   0.6242444427808126,\n",
       "   0.6175777745246888,\n",
       "   0.6214222213625908,\n",
       "   0.6218444431821505,\n",
       "   0.6247555573781332,\n",
       "   0.622711109717687,\n",
       "   0.6277111102143923,\n",
       "   0.6264444435636203,\n",
       "   0.6237333337465922,\n",
       "   0.6238444447517395,\n",
       "   0.6310888887445132,\n",
       "   0.6279999990264574,\n",
       "   0.6320666670799255,\n",
       "   0.6319333316882452,\n",
       "   0.630444445113341,\n",
       "   0.6305555550257365,\n",
       "   0.6290666659673055,\n",
       "   0.6284222219387691,\n",
       "   0.6274222214023272,\n",
       "   0.6303555554151535,\n",
       "   0.6347555540998777,\n",
       "   0.6359333327412605,\n",
       "   0.6359555553396543,\n",
       "   0.6325333312153816],\n",
       "  'val_accuracy_std': [0.05353799162002322,\n",
       "   0.0524720981914053,\n",
       "   0.05174179064930801,\n",
       "   0.0545594439699906,\n",
       "   0.05402814078556591,\n",
       "   0.05838780569726321,\n",
       "   0.058177808417806676,\n",
       "   0.059355695041157855,\n",
       "   0.061534849233953345,\n",
       "   0.05942979616153935,\n",
       "   0.06035072230962436,\n",
       "   0.057592111503469996,\n",
       "   0.05651335922892677,\n",
       "   0.06100806981501532,\n",
       "   0.06108741345146006,\n",
       "   0.05955669406457232,\n",
       "   0.058520333169778685,\n",
       "   0.06090453432575346,\n",
       "   0.06073382762738213,\n",
       "   0.0630527898408561,\n",
       "   0.06264413916069356,\n",
       "   0.0613480611002943,\n",
       "   0.06201911090369558,\n",
       "   0.0629110434808522,\n",
       "   0.062440392068660824,\n",
       "   0.06351319518242336,\n",
       "   0.06256483259522035,\n",
       "   0.06216751088082387,\n",
       "   0.06503216901734982,\n",
       "   0.063530729283033,\n",
       "   0.06337751299008496,\n",
       "   0.06272361094956662,\n",
       "   0.06447941994903779,\n",
       "   0.06387816796777328,\n",
       "   0.06152727250719634,\n",
       "   0.06408735869920205,\n",
       "   0.06256571892982157,\n",
       "   0.06175642809294948,\n",
       "   0.06404262086135958,\n",
       "   0.06290223682003454,\n",
       "   0.0639579955819006,\n",
       "   0.06262474690427144,\n",
       "   0.06504126291026241,\n",
       "   0.06563552119824073,\n",
       "   0.06647131003645682,\n",
       "   0.06358946703232453,\n",
       "   0.06259830473072665,\n",
       "   0.06436952090747526,\n",
       "   0.06365764528204357,\n",
       "   0.06382953621904496,\n",
       "   0.06245690339964687,\n",
       "   0.06149903542250107,\n",
       "   0.06350382894868392,\n",
       "   0.064204242829744,\n",
       "   0.06414104917256028,\n",
       "   0.06333795125922861,\n",
       "   0.06269611338101945,\n",
       "   0.06368271627047645,\n",
       "   0.06318921597259596,\n",
       "   0.0649081370904879,\n",
       "   0.0637927615753104,\n",
       "   0.06103617205479102,\n",
       "   0.06279065355775032,\n",
       "   0.06318924604310566,\n",
       "   0.06395484523229095,\n",
       "   0.06394186781302506,\n",
       "   0.06258764194025813,\n",
       "   0.06308949158158543,\n",
       "   0.06251712203552993,\n",
       "   0.06230509942086057,\n",
       "   0.06541423204172483,\n",
       "   0.061592445558287426,\n",
       "   0.06380363086714479,\n",
       "   0.06302408409497127,\n",
       "   0.06400549391932762,\n",
       "   0.06282414847505599,\n",
       "   0.0629419452437504,\n",
       "   0.06382191338091253,\n",
       "   0.061903194602919105,\n",
       "   0.061412204716973555,\n",
       "   0.06305833634416222,\n",
       "   0.06295163999330895,\n",
       "   0.06317485019100758,\n",
       "   0.06443362498009035,\n",
       "   0.06333829041430022,\n",
       "   0.06522325974022641,\n",
       "   0.06261463607075052,\n",
       "   0.060682747250622435,\n",
       "   0.06417772774643522,\n",
       "   0.06386289312281059,\n",
       "   0.06522998233880463,\n",
       "   0.06278136106601999,\n",
       "   0.06413203697901643,\n",
       "   0.06247979930146581,\n",
       "   0.06308154220281362,\n",
       "   0.06139410959916963,\n",
       "   0.06245279278670863,\n",
       "   0.06198397255791851,\n",
       "   0.06265919627346102],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2ada4",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f54da463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAMLFewShotClassifier(\n",
       "  (classifier): VGGReLUNormNetwork(\n",
       "    (layer_dict): ModuleDict(\n",
       "      (conv0): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (linear): MetaLinearLayer()\n",
       "    )\n",
       "  )\n",
       "  (inner_loop_optimizer): GradientDescentLearningRule()\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "maml_system.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbfddb9",
   "metadata": {},
   "source": [
    "# 2. Samlpe 별 loss 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "70b81648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the loss of 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No engine for filetype: 'csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOptionError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    826\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m                     \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"io.excel.{ext}.writer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\pandas\\_config\\config.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\pandas\\_config\\config.py\u001b[0m in \u001b[0;36m_get_option\u001b[1;34m(pat, silent)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_single_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\pandas\\_config\\config.py\u001b[0m in \u001b[0;36m_get_single_key\u001b[1;34m(pat, silent)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0m_warn_if_deprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mOptionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No such keys(s): {repr(pat)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOptionError\u001b[0m: \"No such keys(s): 'io.excel.csv.writer'\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31680\\32199210.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;31m# 파일이 존재하면 append (mode='a')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;31m#df.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mgenerated_alpha_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001b[0m\n\u001b[0;32m   2289\u001b[0m             \u001b[0mfreeze_panes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2290\u001b[0m             \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2291\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2292\u001b[0m         )\n\u001b[0;32m   2293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[1;31m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             writer = ExcelWriter(  # type: ignore[abstract]\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m             )\n\u001b[0;32m    837\u001b[0m             \u001b[0mneed_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    829\u001b[0m                         \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"writer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No engine for filetype: '{ext}'\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"xlwt\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No engine for filetype: 'csv'"
     ]
    }
   ],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "task_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        task_idx = task_idx + 1\n",
    "        print(f\"Calculating the loss of {task_idx}\")\n",
    "        \n",
    "        num_steps=5\n",
    "        for num_step in range(num_steps):            \n",
    "            support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "                    x=x_support_set_task,\n",
    "                    y=y_support_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    backup_running_statistics=num_step == 0,\n",
    "                    training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase='test',\n",
    "                    epoch=0\n",
    "                )\n",
    "        \n",
    "            # 데이터프레임 생성\n",
    "            df = pd.DataFrame({\n",
    "                'Task_Idx' : [task_idx] * len(support_loss_seperate),\n",
    "                'Step': [num_step] * len(support_loss_seperate),\n",
    "                'Sample_Index': list(range(len(support_loss_seperate))),\n",
    "                'Loss': support_loss_seperate.detach().cpu().numpy()\n",
    "            })\n",
    "\n",
    "            # 파일이 존재하지 않으면 새로운 파일 생성 (mode='w')\n",
    "            if not os.path.exists(csv_file):\n",
    "                df.to_csv(csv_file, mode='w', index=False, encoding='utf-8')\n",
    "            else:\n",
    "                # 파일이 존재하면 append (mode='a')\n",
    "                df.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8')\n",
    "            \n",
    "            generated_alpha_params = {}\n",
    "            \n",
    "            names_weights_copy, names_grads_copy = maml_system.model.apply_inner_loop_update(\n",
    "                loss=support_loss,\n",
    "                support_loss_seperate=support_loss_seperate,\n",
    "                names_weights_copy=names_weights_copy,\n",
    "                alpha=generated_alpha_params,\n",
    "                use_second_order=args.second_order,\n",
    "                current_step_idx=num_step,\n",
    "                current_iter=maml_system.state['current_iter'],\n",
    "                training_phase='test')\n",
    "                      \n",
    "#             if num_step==4:\n",
    "#                 target_loss, target_preds, _, _ = self.net_forward(x=x_target_set_task,\n",
    "#                                                                  y=y_target_set_task, weights=names_weights_copy,\n",
    "#                                                                  backup_running_statistics=False, training=True,\n",
    "#                                                                  num_step=num_step, training_phase=training_phase,\n",
    "#                                                                  epoch=epoch)\n",
    "\n",
    "print(\"Loss_per_sample_MAML End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
