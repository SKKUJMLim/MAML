{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb3f6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c51c0520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec9ad8",
   "metadata": {},
   "source": [
    "# 0. 사용할 Dataset 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b159aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import basic_utils\n",
    "\n",
    "csv_file = \"sample_loss_log_Arbiter.csv\"\n",
    "\n",
    "datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "# datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6799c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 150,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "428adcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 206209, 'train': 448695, 'val': 124261}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70f45f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6804888878266017,\n",
       " 'best_val_iter': 48000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 96,\n",
       " 'train_loss_mean': 0.43696876338124274,\n",
       " 'train_loss_std': 0.11899634825869217,\n",
       " 'train_accuracy_mean': 0.8393600009679795,\n",
       " 'train_accuracy_std': 0.04968647847224927,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.8797975228230158,\n",
       " 'val_loss_std': 0.14933626051120422,\n",
       " 'val_accuracy_mean': 0.6671111124753952,\n",
       " 'val_accuracy_std': 0.06162330829347762,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0119, -0.0783,  0.0471],\n",
       "                         [-0.0337, -0.0394, -0.0029],\n",
       "                         [-0.0114,  0.1011, -0.0270]],\n",
       "               \n",
       "                        [[ 0.0543, -0.0757,  0.0729],\n",
       "                         [-0.0279,  0.0095,  0.0631],\n",
       "                         [-0.0564,  0.0283, -0.0307]],\n",
       "               \n",
       "                        [[ 0.0551, -0.0214, -0.0238],\n",
       "                         [ 0.0318,  0.0585, -0.0529],\n",
       "                         [-0.0555,  0.0224, -0.0380]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0290,  0.0915,  0.0294],\n",
       "                         [ 0.0589, -0.0530,  0.0072],\n",
       "                         [-0.0728, -0.0588, -0.0368]],\n",
       "               \n",
       "                        [[-0.0285,  0.0207, -0.0114],\n",
       "                         [ 0.0535, -0.0732, -0.0583],\n",
       "                         [ 0.0665, -0.0495,  0.0936]],\n",
       "               \n",
       "                        [[-0.0595,  0.0688, -0.0786],\n",
       "                         [-0.0144,  0.0512,  0.0714],\n",
       "                         [-0.0151, -0.0190, -0.0073]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0207, -0.0247,  0.0535],\n",
       "                         [-0.0216, -0.0386, -0.0291],\n",
       "                         [ 0.0783,  0.0574, -0.0583]],\n",
       "               \n",
       "                        [[-0.0595, -0.0170,  0.0433],\n",
       "                         [ 0.0773, -0.0029,  0.0148],\n",
       "                         [-0.0614, -0.0359,  0.0413]],\n",
       "               \n",
       "                        [[ 0.0428, -0.0598,  0.0216],\n",
       "                         [ 0.0625,  0.0139, -0.0654],\n",
       "                         [-0.0731,  0.0277,  0.0206]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0146, -0.0782, -0.0442],\n",
       "                         [ 0.0018, -0.0615,  0.0815],\n",
       "                         [-0.0160,  0.0738, -0.0339]],\n",
       "               \n",
       "                        [[ 0.0401, -0.0032,  0.0578],\n",
       "                         [ 0.0257, -0.0285, -0.0380],\n",
       "                         [-0.0029,  0.0422, -0.0140]],\n",
       "               \n",
       "                        [[ 0.0070, -0.0918, -0.0341],\n",
       "                         [-0.0506,  0.0196,  0.0447],\n",
       "                         [-0.0400,  0.0330,  0.0539]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0250, -0.0173, -0.0438],\n",
       "                         [ 0.0012,  0.0625, -0.0575],\n",
       "                         [ 0.0516,  0.0827, -0.0507]],\n",
       "               \n",
       "                        [[ 0.0414,  0.0121,  0.0630],\n",
       "                         [ 0.0157, -0.0384, -0.0286],\n",
       "                         [-0.0238, -0.0805,  0.0333]],\n",
       "               \n",
       "                        [[-0.0101,  0.0044, -0.0011],\n",
       "                         [-0.0062, -0.0224,  0.0907],\n",
       "                         [-0.0599, -0.0274,  0.0261]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0617,  0.0193, -0.0449],\n",
       "                         [-0.0503, -0.0053,  0.0150],\n",
       "                         [ 0.0657,  0.0504,  0.0714]],\n",
       "               \n",
       "                        [[-0.0406, -0.0672,  0.0387],\n",
       "                         [ 0.0675, -0.0936, -0.0482],\n",
       "                         [ 0.0114,  0.0257,  0.0186]],\n",
       "               \n",
       "                        [[-0.0050, -0.0624,  0.0595],\n",
       "                         [ 0.0094, -0.0603, -0.0113],\n",
       "                         [-0.0017, -0.0097,  0.0217]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([ 3.2064e-03,  2.2750e-03,  2.9653e-03,  6.2620e-04, -1.1196e-03,\n",
       "                        2.4475e-04,  3.0839e-04,  7.5104e-04, -1.3262e-03,  1.9373e-03,\n",
       "                       -8.5747e-05,  6.8855e-05, -8.5974e-04, -2.0086e-04, -1.5836e-03,\n",
       "                        1.2017e-03,  1.8619e-04,  6.8974e-04,  2.4037e-04,  2.5924e-04,\n",
       "                        5.3871e-04,  1.0490e-04, -1.7351e-04,  2.2444e-03, -1.1746e-04,\n",
       "                       -1.6944e-04, -1.1623e-03,  1.1516e-03, -1.0374e-03, -7.8554e-04,\n",
       "                        1.3992e-03, -4.8341e-05, -1.2321e-03,  8.1837e-04, -1.0687e-03,\n",
       "                       -5.5406e-04, -3.5855e-04,  4.0901e-04,  1.6019e-04,  2.0375e-03,\n",
       "                        6.7179e-04, -1.4186e-03, -4.6522e-04, -1.4137e-03, -2.5477e-03,\n",
       "                       -1.4701e-04, -8.7582e-04,  1.6826e-04,  6.0122e-04,  3.0951e-04,\n",
       "                       -3.2774e-04,  3.7013e-04, -2.6205e-04, -3.9112e-04, -7.8049e-04,\n",
       "                        6.4286e-05,  4.6858e-04, -1.2535e-03, -8.0630e-04, -1.3598e-03,\n",
       "                        2.9400e-04, -8.3335e-05, -2.2883e-04, -2.8881e-04,  6.8960e-04,\n",
       "                        1.9144e-04, -1.2655e-03,  5.6683e-04, -1.5471e-03, -2.3163e-03,\n",
       "                       -1.7717e-04,  1.2247e-03,  7.4493e-04, -6.4990e-05,  4.7143e-04,\n",
       "                       -3.1657e-04,  1.5947e-04, -6.9534e-04,  8.2283e-04,  1.0679e-03,\n",
       "                        4.2437e-04,  5.5755e-04,  6.6508e-04, -9.4820e-05,  1.1998e-03,\n",
       "                        1.0556e-03,  6.5581e-04,  2.1397e-04, -2.9487e-04, -9.2134e-04,\n",
       "                       -3.3791e-03,  1.0446e-04, -3.7145e-03,  8.6221e-04,  1.4224e-03,\n",
       "                       -1.5094e-03, -1.3303e-03,  2.7911e-04,  1.2918e-03,  3.4310e-04,\n",
       "                        6.6754e-04, -4.4113e-04,  1.5244e-03,  5.9242e-04,  2.7710e-04,\n",
       "                       -1.3899e-04, -1.6411e-04,  3.4560e-04, -2.4607e-05, -7.4244e-04,\n",
       "                       -1.7956e-03, -7.5043e-04,  3.6309e-05, -2.4353e-03,  7.0268e-04,\n",
       "                       -1.5031e-03, -2.7050e-04,  2.7530e-03,  2.6133e-04,  8.2874e-04,\n",
       "                       -1.0530e-04,  1.2980e-04, -1.7328e-03, -7.8338e-04,  2.6865e-03,\n",
       "                        1.1230e-03, -2.7002e-03,  5.4408e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1378,  0.0538,  0.2400, -0.1525, -0.0334,  0.0384, -0.2466, -0.2329,\n",
       "                       -0.1545, -0.2193, -0.2052, -0.2204,  0.0759, -0.1541,  0.1363, -0.1131,\n",
       "                        0.0513,  0.0346, -0.0120, -0.1044, -0.0991, -0.0715, -0.1772, -0.0068,\n",
       "                       -0.0244, -0.2676, -0.1263, -0.0687, -0.0598,  0.0629, -0.1141, -0.0720,\n",
       "                       -0.0821, -0.2274, -0.1394, -0.1412, -0.1776, -0.0818, -0.1910, -0.1527,\n",
       "                       -0.0911,  0.1181, -0.2539, -0.1044,  0.1065, -0.2242, -0.0869, -0.2152,\n",
       "                       -0.0841, -0.1468,  0.1673, -0.1306,  0.1114, -0.0344,  0.0129, -0.1129,\n",
       "                       -0.0481, -0.0213, -0.1226,  0.1226, -0.2320, -0.1327,  0.0212, -0.2950,\n",
       "                       -0.2464, -0.2773, -0.1163, -0.1985, -0.0769,  0.0053,  0.0150, -0.1322,\n",
       "                       -0.0899, -0.1963,  0.0882, -0.0510, -0.0939, -0.0255, -0.0797, -0.1043,\n",
       "                        0.0091, -0.1259, -0.2229, -0.1629, -0.0331,  0.1197,  0.0862, -0.0512,\n",
       "                       -0.1740, -0.0383,  0.0170, -0.1527,  0.1436, -0.0127, -0.1004,  0.3284,\n",
       "                        0.0619,  0.1024,  0.0198, -0.1475, -0.2012, -0.0284,  0.0037,  0.0313,\n",
       "                       -0.1933, -0.1574, -0.0799, -0.1897,  0.0015,  0.0153,  0.0048, -0.2459,\n",
       "                       -0.0390, -0.0269, -0.1148, -0.0846, -0.1770, -0.0473, -0.1074,  0.1073,\n",
       "                       -0.1443, -0.1456, -0.1895, -0.2159,  0.2116, -0.1382, -0.2688, -0.2406],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.0363, 1.0874, 1.0278, 0.9423, 0.9718, 1.0252, 0.9010, 0.8775, 0.9501,\n",
       "                       0.9340, 0.9001, 0.8838, 1.0811, 0.9591, 1.0570, 1.0455, 1.1228, 0.9803,\n",
       "                       0.9896, 0.9420, 0.9721, 0.8768, 0.8985, 0.9997, 0.9565, 0.8651, 1.0148,\n",
       "                       0.9389, 0.9672, 0.9943, 1.0734, 0.9303, 1.1261, 1.1965, 0.8760, 0.9127,\n",
       "                       1.0333, 0.9715, 0.8898, 0.9864, 0.9687, 1.0274, 0.8392, 0.9403, 1.1021,\n",
       "                       0.8805, 0.9313, 0.8912, 0.9837, 0.9411, 0.9828, 1.0088, 1.0653, 1.0191,\n",
       "                       0.9782, 0.9281, 0.9755, 1.1208, 0.9623, 1.0982, 0.9272, 0.8834, 0.9718,\n",
       "                       0.8983, 0.9680, 0.7911, 0.9340, 0.9145, 1.0400, 0.9742, 0.9658, 0.9650,\n",
       "                       0.8959, 0.8861, 1.0475, 0.9423, 0.9390, 1.0824, 1.1093, 0.9431, 0.9750,\n",
       "                       0.9971, 0.8917, 0.8958, 1.1236, 1.0463, 1.0278, 1.0044, 0.8854, 0.9804,\n",
       "                       1.0158, 0.9300, 1.0052, 0.9920, 1.0072, 1.0663, 1.1561, 0.9842, 1.0558,\n",
       "                       0.9364, 0.8963, 1.0349, 1.0252, 0.9782, 0.9104, 0.9210, 0.9312, 0.8376,\n",
       "                       1.0646, 0.9511, 1.0188, 1.0216, 0.9913, 1.1465, 0.9319, 1.1048, 0.9114,\n",
       "                       1.0027, 0.9274, 0.9717, 0.8883, 0.9107, 1.1820, 1.0816, 1.0540, 0.8863,\n",
       "                       1.0786, 0.8461], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-1.8421e-02,  4.5942e-02, -2.3510e-02],\n",
       "                         [-6.7161e-03,  1.6328e-02, -3.3942e-03],\n",
       "                         [-9.5231e-03,  2.2807e-02, -3.2653e-02]],\n",
       "               \n",
       "                        [[ 1.5285e-02,  1.9809e-02, -8.6686e-02],\n",
       "                         [ 5.5677e-03, -1.4408e-02, -2.2742e-02],\n",
       "                         [-2.9976e-02,  5.0147e-02,  1.9476e-03]],\n",
       "               \n",
       "                        [[-1.4431e-02, -8.3362e-02, -2.7975e-02],\n",
       "                         [-4.2717e-02, -5.4659e-02,  9.9725e-03],\n",
       "                         [ 8.8468e-03, -1.9753e-02, -3.9439e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.8055e-03, -3.0024e-02, -2.1962e-02],\n",
       "                         [-3.9601e-02, -5.6776e-02, -2.4262e-02],\n",
       "                         [ 5.6844e-03, -7.5942e-03, -1.8496e-02]],\n",
       "               \n",
       "                        [[-3.3169e-02,  3.0339e-02, -2.5274e-02],\n",
       "                         [-1.9508e-02,  4.1892e-02, -6.8041e-03],\n",
       "                         [-1.9087e-02, -8.0522e-02, -2.3981e-02]],\n",
       "               \n",
       "                        [[-3.4540e-02,  4.6902e-02, -5.1013e-03],\n",
       "                         [ 8.3711e-03,  4.6440e-02,  3.5319e-02],\n",
       "                         [-2.6307e-02,  1.5712e-02,  6.5391e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.4539e-02, -5.2871e-02,  1.0106e-02],\n",
       "                         [ 6.0971e-02,  4.8219e-02, -4.2194e-02],\n",
       "                         [ 3.4482e-03,  1.0782e-02, -3.0432e-02]],\n",
       "               \n",
       "                        [[ 1.1943e-02,  4.7028e-02,  9.9609e-03],\n",
       "                         [-6.1275e-02, -3.0015e-03,  3.8303e-02],\n",
       "                         [-4.2903e-02, -8.4731e-03,  2.2743e-02]],\n",
       "               \n",
       "                        [[ 3.7107e-02,  3.2034e-02, -4.7316e-02],\n",
       "                         [-1.4512e-03, -2.7267e-03, -9.3563e-03],\n",
       "                         [-6.8429e-02, -5.5369e-02, -8.7139e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.7699e-03,  2.2149e-02,  4.5551e-02],\n",
       "                         [-4.4768e-02, -1.9202e-02, -1.6936e-02],\n",
       "                         [-4.7599e-03,  2.3179e-02, -2.4606e-02]],\n",
       "               \n",
       "                        [[ 2.5219e-02, -6.4520e-03, -3.6295e-02],\n",
       "                         [ 2.6069e-02,  8.6340e-02,  3.1366e-02],\n",
       "                         [ 1.3387e-02,  3.9066e-02,  6.3197e-04]],\n",
       "               \n",
       "                        [[ 1.8174e-02, -1.7971e-02,  4.6667e-02],\n",
       "                         [ 5.5754e-02, -1.6730e-02, -5.3859e-03],\n",
       "                         [-2.3827e-02, -4.8496e-02, -2.1431e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.5328e-02, -4.1617e-02, -7.6344e-03],\n",
       "                         [ 2.8011e-02,  3.6458e-03, -5.7427e-02],\n",
       "                         [ 1.1830e-02,  2.7994e-03,  2.8571e-05]],\n",
       "               \n",
       "                        [[-1.5696e-02,  2.1285e-02, -3.9345e-02],\n",
       "                         [-1.8686e-02, -1.5521e-02, -1.2607e-02],\n",
       "                         [-2.9659e-03, -1.6442e-02, -3.0029e-02]],\n",
       "               \n",
       "                        [[-4.4618e-02, -4.1815e-03, -4.9308e-02],\n",
       "                         [-2.3898e-02, -2.7194e-02, -2.2105e-03],\n",
       "                         [ 1.9903e-02,  3.5183e-02,  8.0404e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.3420e-02,  3.9924e-02,  1.5272e-02],\n",
       "                         [-2.0325e-02, -3.7963e-02, -1.1101e-02],\n",
       "                         [-1.7731e-02, -3.2428e-02, -4.6186e-02]],\n",
       "               \n",
       "                        [[ 3.8616e-02,  5.9187e-02,  7.5352e-02],\n",
       "                         [-4.2822e-02,  1.8402e-02,  3.2873e-02],\n",
       "                         [-2.4875e-02,  8.1104e-02,  8.8442e-02]],\n",
       "               \n",
       "                        [[-4.6010e-02, -5.0765e-02, -3.1280e-03],\n",
       "                         [-1.7794e-02,  2.4364e-02,  2.7992e-02],\n",
       "                         [ 3.7922e-02,  7.2111e-04, -9.1824e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.7465e-02,  1.5447e-02,  2.7042e-02],\n",
       "                         [ 4.3345e-02,  2.4691e-02,  1.4037e-02],\n",
       "                         [ 1.5661e-02, -3.6099e-02, -3.7198e-02]],\n",
       "               \n",
       "                        [[-4.0475e-02,  3.0647e-02,  3.6895e-02],\n",
       "                         [-7.0297e-02, -1.1805e-03, -3.8291e-02],\n",
       "                         [-6.6906e-02,  2.2006e-02, -3.3985e-02]],\n",
       "               \n",
       "                        [[ 5.9479e-02,  5.6499e-02,  4.4599e-02],\n",
       "                         [ 2.4324e-02, -5.3477e-02,  2.2036e-02],\n",
       "                         [-2.3248e-02,  8.3573e-03,  2.9598e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2302e-02,  5.9809e-03, -6.1708e-04],\n",
       "                         [ 1.9842e-02, -5.3676e-02,  1.4214e-03],\n",
       "                         [ 5.2017e-03, -3.0237e-03, -2.2536e-02]],\n",
       "               \n",
       "                        [[-3.5783e-02,  4.2528e-03, -1.8091e-02],\n",
       "                         [ 2.1509e-02,  3.1809e-02,  3.8768e-04],\n",
       "                         [-3.8440e-02,  8.7856e-02,  5.2899e-02]],\n",
       "               \n",
       "                        [[-7.3113e-03, -1.7189e-02, -1.5020e-03],\n",
       "                         [-2.2871e-02,  6.1540e-03,  1.0268e-02],\n",
       "                         [-9.6741e-03,  2.8880e-02, -2.4545e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.9379e-02, -1.3735e-02,  4.1459e-02],\n",
       "                         [-2.4380e-02, -1.0832e-02,  7.3639e-02],\n",
       "                         [-5.5167e-02,  2.6256e-02,  1.8037e-02]],\n",
       "               \n",
       "                        [[-1.3582e-02,  3.5898e-02,  1.3156e-02],\n",
       "                         [ 4.0361e-02,  3.6464e-02,  4.1027e-02],\n",
       "                         [ 6.9228e-03,  3.0821e-02, -8.9163e-03]],\n",
       "               \n",
       "                        [[ 9.8708e-03,  6.4792e-02,  1.6493e-02],\n",
       "                         [ 2.9409e-02,  3.7960e-02, -3.5453e-02],\n",
       "                         [-4.7722e-02,  1.2751e-02,  3.5725e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.4790e-02,  2.1301e-02,  5.4987e-02],\n",
       "                         [ 1.5497e-02,  5.6304e-02,  7.1555e-04],\n",
       "                         [ 1.1318e-03, -1.6770e-02, -3.3554e-02]],\n",
       "               \n",
       "                        [[ 1.8123e-02,  8.2089e-03,  1.9303e-02],\n",
       "                         [ 4.3885e-03, -7.1785e-02, -2.5842e-02],\n",
       "                         [ 8.5658e-03, -7.3575e-02, -1.8196e-02]],\n",
       "               \n",
       "                        [[-2.2223e-02, -2.4858e-02, -2.2300e-02],\n",
       "                         [-1.3775e-02,  1.7962e-02, -2.8010e-02],\n",
       "                         [ 5.8130e-03, -3.5711e-02, -3.1801e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.5281e-03, -8.1186e-03,  1.4322e-03],\n",
       "                         [ 4.6392e-02,  3.6115e-02, -3.9125e-02],\n",
       "                         [-8.6523e-03,  4.4971e-02,  1.0613e-02]],\n",
       "               \n",
       "                        [[-5.1405e-02, -2.2719e-02, -6.4671e-02],\n",
       "                         [ 2.3050e-02, -9.3238e-02, -6.6037e-02],\n",
       "                         [-6.9944e-02, -8.3688e-02,  3.1987e-02]],\n",
       "               \n",
       "                        [[-1.7006e-03, -4.4218e-02, -1.3682e-02],\n",
       "                         [ 1.3169e-02, -6.8741e-03, -4.4597e-02],\n",
       "                         [-4.5687e-03, -2.2494e-02,  1.0191e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.3370e-02,  3.6774e-02, -1.4710e-02],\n",
       "                         [ 5.8491e-02,  9.2524e-02, -1.6663e-02],\n",
       "                         [ 3.1014e-02,  3.7351e-02, -1.7413e-02]],\n",
       "               \n",
       "                        [[-2.7905e-03,  9.1050e-03, -5.1178e-02],\n",
       "                         [ 4.0252e-02,  9.8092e-02, -2.5838e-02],\n",
       "                         [-3.3599e-02,  5.3777e-02, -4.7507e-02]],\n",
       "               \n",
       "                        [[-4.4664e-02, -1.0807e-03, -3.4640e-02],\n",
       "                         [-2.5369e-02, -1.1326e-03,  2.8266e-02],\n",
       "                         [ 4.6844e-03, -2.8502e-02, -2.8494e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 1.1382e-05, -4.5943e-06,  5.7271e-06,  1.1232e-05, -8.9155e-07,\n",
       "                        4.4431e-07, -1.7490e-06, -2.0258e-02,  5.8112e-03,  2.8931e-06,\n",
       "                        3.4151e-06,  7.2485e-03,  1.0590e-06, -3.1865e-07,  1.3174e-02,\n",
       "                        1.5370e-06,  3.1869e-03,  5.4187e-06, -9.9829e-03, -6.2200e-03,\n",
       "                       -1.2066e-05,  2.2985e-06,  3.3257e-06,  3.1030e-07,  1.2834e-05,\n",
       "                       -1.3143e-05, -1.4979e-02, -1.6005e-02,  1.0102e-06,  1.4454e-02,\n",
       "                        2.4474e-05, -1.8934e-02,  6.2059e-05,  5.7387e-06, -3.3412e-06,\n",
       "                       -2.3360e-02,  1.6606e-06, -1.1116e-05, -3.2493e-06, -1.8067e-06,\n",
       "                        9.1054e-03, -1.5966e-06,  7.3860e-07, -9.9261e-03, -2.5843e-06,\n",
       "                       -8.8590e-08, -8.3390e-03, -6.6678e-07, -3.0293e-07, -1.6910e-02,\n",
       "                       -4.1479e-06,  1.1259e-02, -2.2842e-02,  1.0694e-02,  4.3638e-05,\n",
       "                        2.6737e-06,  3.9556e-06, -6.1545e-07, -1.1325e-02,  1.4393e-06,\n",
       "                        8.0874e-06, -3.1655e-03, -8.6405e-07,  5.7270e-07, -1.0570e-02,\n",
       "                        5.0633e-06,  2.5320e-05,  1.6618e-05,  1.4064e-06, -1.4775e-05,\n",
       "                        9.1198e-07, -4.5387e-06,  8.6211e-04,  1.9196e-02,  2.5854e-06,\n",
       "                       -6.4328e-06,  3.3842e-06,  1.5396e-06, -2.3023e-02,  6.4919e-04,\n",
       "                       -6.2108e-06, -6.4018e-06, -2.1870e-02, -1.7163e-05,  5.8392e-07,\n",
       "                       -2.5679e-05,  6.6115e-06, -1.1430e-06,  1.7325e-02, -2.3378e-02,\n",
       "                       -2.1954e-05, -1.2214e-06, -2.4139e-08, -1.1735e-02,  5.5014e-07,\n",
       "                        8.8665e-04, -1.1652e-02, -2.3617e-02,  2.5003e-06, -6.3996e-07,\n",
       "                       -1.1153e-05, -1.7030e-06,  1.0040e-02, -2.3995e-06, -4.4477e-07,\n",
       "                       -8.4296e-07, -2.6652e-02, -1.8780e-05, -1.8141e-02, -7.3714e-07,\n",
       "                        1.7054e-08,  1.9071e-05,  1.0863e-06, -9.4584e-07, -1.3249e-05,\n",
       "                       -4.3312e-03,  3.7526e-06,  2.4487e-03,  7.9004e-07,  2.5039e-03,\n",
       "                       -1.8776e-07, -1.0371e-06,  9.9036e-03,  3.6553e-06,  3.2395e-06,\n",
       "                       -1.6698e-06, -7.2019e-07,  5.2926e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.2015, -0.1218, -0.2759, -0.0703, -0.1511, -0.3141, -0.1569, -0.2701,\n",
       "                       -0.2575, -0.2435, -0.2244, -0.1999, -0.2385, -0.2265, -0.2378, -0.1922,\n",
       "                       -0.2688, -0.0977, -0.2173, -0.2438, -0.2526, -0.2129, -0.1149, -0.1012,\n",
       "                       -0.0931, -0.1777, -0.1470, -0.2604, -0.2196, -0.2353, -0.0330, -0.3086,\n",
       "                       -0.1392, -0.1768, -0.2046, -0.2068, -0.0565, -0.0711, -0.1233, -0.3286,\n",
       "                       -0.2192, -0.0640, -0.2406, -0.1225, -0.1955, -0.2254, -0.2424, -0.1137,\n",
       "                       -0.1652, -0.1589, -0.0299, -0.2879, -0.1646, -0.2370, -0.2350, -0.2782,\n",
       "                       -0.1661, -0.1114, -0.2084, -0.2412, -0.2103, -0.3734, -0.2777, -0.1236,\n",
       "                       -0.3020, -0.0903, -0.1360, -0.1417, -0.2040, -0.1682, -0.1410, -0.1899,\n",
       "                       -0.2099, -0.0892, -0.0794, -0.0522, -0.1747, -0.1198, -0.1418, -0.1124,\n",
       "                       -0.2150, -0.2173, -0.1305, -0.0973, -0.2019, -0.2278, -0.0895, -0.2035,\n",
       "                       -0.1717, -0.0678, -0.2740, -0.2780, -0.2097, -0.1894, -0.1023, -0.0955,\n",
       "                       -0.1298, -0.1207, -0.1911, -0.1838, -0.2160, -0.2240, -0.2348, -0.2898,\n",
       "                       -0.2185, -0.2623, -0.2260, -0.2036, -0.1311, -0.1585, -0.1970, -0.2299,\n",
       "                       -0.2714, -0.2245, -0.1908, -0.1289, -0.2010, -0.2610, -0.2529, -0.1962,\n",
       "                       -0.0903, -0.1505, -0.1736,  0.0185, -0.1622, -0.1912, -0.2069, -0.2346],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9763, 1.0567, 0.9885, 0.9791, 0.9500, 1.0135, 1.0237, 1.1473, 1.0523,\n",
       "                       0.8873, 0.9703, 1.0268, 0.9413, 0.9871, 1.0644, 0.9951, 0.9166, 0.9620,\n",
       "                       1.0000, 0.8761, 0.9806, 0.9808, 1.0553, 1.0513, 1.0063, 1.0418, 0.9939,\n",
       "                       1.0377, 0.9643, 0.9765, 0.9746, 1.0559, 0.9852, 0.9932, 0.9239, 0.9852,\n",
       "                       0.9859, 0.9756, 0.9693, 0.9043, 0.9647, 0.9673, 1.0227, 0.9452, 0.9454,\n",
       "                       0.9943, 0.9897, 0.9673, 1.1390, 1.0100, 1.0690, 0.9791, 0.9282, 1.0125,\n",
       "                       0.9424, 0.9808, 0.9561, 0.9878, 1.0512, 0.9884, 1.0529, 1.0070, 0.9755,\n",
       "                       1.0168, 0.8688, 0.9906, 1.0165, 0.9347, 0.9024, 0.9896, 0.9558, 0.8986,\n",
       "                       0.8851, 1.0117, 1.0268, 1.1110, 0.9598, 1.0026, 1.0837, 1.0147, 1.0136,\n",
       "                       0.8957, 1.0099, 0.9645, 1.0407, 0.9371, 0.9914, 0.9231, 0.9756, 1.0149,\n",
       "                       1.0820, 0.8991, 0.9857, 0.9778, 1.0152, 0.9633, 0.9789, 0.9998, 1.0599,\n",
       "                       0.9549, 0.9690, 1.0088, 0.9437, 0.9654, 1.0442, 1.0657, 1.0163, 0.9519,\n",
       "                       0.9648, 1.0051, 0.9827, 0.9202, 1.0086, 1.0133, 0.9250, 1.0273, 0.9032,\n",
       "                       1.0175, 0.9523, 0.9910, 1.0298, 1.0566, 0.9895, 1.0203, 0.9296, 0.9561,\n",
       "                       1.0066, 1.0293], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-0.0157,  0.0616, -0.0112],\n",
       "                         [-0.0413, -0.0080, -0.0121],\n",
       "                         [ 0.0030, -0.0155,  0.0338]],\n",
       "               \n",
       "                        [[ 0.0346,  0.0193,  0.0255],\n",
       "                         [ 0.0044,  0.0410, -0.0082],\n",
       "                         [ 0.0945,  0.0886, -0.0308]],\n",
       "               \n",
       "                        [[-0.0672, -0.1002, -0.0795],\n",
       "                         [ 0.0062,  0.0122, -0.0644],\n",
       "                         [-0.0789, -0.0101, -0.0535]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0166, -0.0310, -0.0814],\n",
       "                         [-0.0061,  0.0159,  0.0115],\n",
       "                         [-0.0017, -0.0120, -0.0174]],\n",
       "               \n",
       "                        [[-0.0311, -0.0423, -0.0094],\n",
       "                         [ 0.0313,  0.0387, -0.0095],\n",
       "                         [ 0.0366,  0.0625,  0.0241]],\n",
       "               \n",
       "                        [[-0.0652,  0.0038,  0.0122],\n",
       "                         [-0.0340, -0.0133, -0.0217],\n",
       "                         [-0.0168, -0.0019,  0.0099]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0634,  0.0639,  0.0532],\n",
       "                         [ 0.0230,  0.0476, -0.0275],\n",
       "                         [ 0.0090, -0.0009,  0.0161]],\n",
       "               \n",
       "                        [[ 0.0026,  0.0238,  0.0209],\n",
       "                         [ 0.0200,  0.0225,  0.0015],\n",
       "                         [-0.0341, -0.0210, -0.0208]],\n",
       "               \n",
       "                        [[-0.0595,  0.0179, -0.0492],\n",
       "                         [-0.0238, -0.0027, -0.0338],\n",
       "                         [-0.0093,  0.0313,  0.0368]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0215,  0.0299, -0.0352],\n",
       "                         [ 0.0231,  0.0326,  0.0267],\n",
       "                         [ 0.0116,  0.0772, -0.0132]],\n",
       "               \n",
       "                        [[ 0.0108, -0.0067, -0.0276],\n",
       "                         [-0.0151,  0.0110, -0.0302],\n",
       "                         [ 0.0122,  0.0450, -0.0370]],\n",
       "               \n",
       "                        [[ 0.1085,  0.0130, -0.0643],\n",
       "                         [ 0.0369,  0.0691,  0.0046],\n",
       "                         [ 0.0250,  0.0331, -0.0500]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0557,  0.0228, -0.0517],\n",
       "                         [-0.0375, -0.0158,  0.0172],\n",
       "                         [-0.0056, -0.0222, -0.0220]],\n",
       "               \n",
       "                        [[ 0.0473,  0.0186,  0.0109],\n",
       "                         [-0.0024,  0.0588,  0.0055],\n",
       "                         [-0.0383,  0.0312, -0.0633]],\n",
       "               \n",
       "                        [[ 0.0028, -0.0396,  0.0442],\n",
       "                         [-0.0253,  0.0609, -0.0244],\n",
       "                         [-0.0242,  0.0254,  0.0417]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0105,  0.0078, -0.0371],\n",
       "                         [-0.0129, -0.0503, -0.0363],\n",
       "                         [-0.0559, -0.0504, -0.0054]],\n",
       "               \n",
       "                        [[ 0.0183,  0.0263,  0.0392],\n",
       "                         [ 0.0705, -0.0236, -0.0101],\n",
       "                         [-0.0033, -0.0496,  0.0440]],\n",
       "               \n",
       "                        [[ 0.0003,  0.0250,  0.0256],\n",
       "                         [ 0.0464,  0.0714,  0.0504],\n",
       "                         [-0.0143,  0.0253,  0.0404]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0204,  0.0279,  0.0044],\n",
       "                         [ 0.0999,  0.0428,  0.0508],\n",
       "                         [ 0.0537,  0.0498,  0.0467]],\n",
       "               \n",
       "                        [[ 0.0344, -0.0076,  0.0319],\n",
       "                         [ 0.0592,  0.0411,  0.0094],\n",
       "                         [ 0.0534,  0.0331,  0.0091]],\n",
       "               \n",
       "                        [[ 0.0034, -0.0259, -0.0630],\n",
       "                         [-0.0358, -0.0446,  0.0114],\n",
       "                         [-0.0466, -0.0449, -0.0589]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0687,  0.0616,  0.0805],\n",
       "                         [ 0.0411, -0.0041,  0.0380],\n",
       "                         [-0.0126, -0.0250,  0.0209]],\n",
       "               \n",
       "                        [[ 0.0383, -0.0342,  0.0347],\n",
       "                         [-0.0192, -0.0134, -0.0214],\n",
       "                         [-0.0290,  0.0137, -0.0222]],\n",
       "               \n",
       "                        [[-0.0388, -0.0329, -0.0574],\n",
       "                         [-0.0379, -0.0306, -0.0739],\n",
       "                         [ 0.0048,  0.0528, -0.0336]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0515,  0.0383, -0.0256],\n",
       "                         [-0.0455, -0.0230, -0.0420],\n",
       "                         [-0.0113,  0.0084, -0.0600]],\n",
       "               \n",
       "                        [[ 0.0049, -0.0075,  0.0039],\n",
       "                         [ 0.0058, -0.0084,  0.0318],\n",
       "                         [-0.0501,  0.0250,  0.0332]],\n",
       "               \n",
       "                        [[ 0.0009,  0.0010, -0.0039],\n",
       "                         [ 0.0286,  0.0336,  0.0226],\n",
       "                         [-0.0290, -0.0365,  0.0300]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0148, -0.0523,  0.0077],\n",
       "                         [-0.0359,  0.0198, -0.0391],\n",
       "                         [-0.0434, -0.0261, -0.0378]],\n",
       "               \n",
       "                        [[ 0.0058,  0.0285,  0.0054],\n",
       "                         [ 0.0591, -0.0232,  0.0430],\n",
       "                         [ 0.0668,  0.0038, -0.0113]],\n",
       "               \n",
       "                        [[ 0.0380,  0.0452, -0.0435],\n",
       "                         [ 0.0094,  0.0675, -0.0375],\n",
       "                         [-0.0029,  0.0338,  0.0334]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0323, -0.0324, -0.0127],\n",
       "                         [-0.0399,  0.0038,  0.0788],\n",
       "                         [-0.0530,  0.0089,  0.0532]],\n",
       "               \n",
       "                        [[ 0.0322,  0.0202,  0.0204],\n",
       "                         [ 0.0082,  0.0127,  0.0249],\n",
       "                         [ 0.0618, -0.0207,  0.0228]],\n",
       "               \n",
       "                        [[-0.0717,  0.0195,  0.0431],\n",
       "                         [-0.0541,  0.0153,  0.0284],\n",
       "                         [-0.0156, -0.0214, -0.0038]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0043, -0.0073, -0.0164],\n",
       "                         [ 0.0155,  0.0308,  0.0171],\n",
       "                         [-0.0079,  0.0229,  0.0092]],\n",
       "               \n",
       "                        [[ 0.0097,  0.0378,  0.0033],\n",
       "                         [-0.0528,  0.0128, -0.0339],\n",
       "                         [-0.0179, -0.0278, -0.0955]],\n",
       "               \n",
       "                        [[-0.0781,  0.0600,  0.0460],\n",
       "                         [-0.0602,  0.0361, -0.0067],\n",
       "                         [ 0.0099, -0.0265, -0.0368]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-2.3046e-03, -2.5249e-04, -1.4723e-03,  7.1516e-04,  1.3261e-04,\n",
       "                        3.8010e-04, -7.8955e-04, -3.3939e-03,  2.5435e-03,  2.9267e-04,\n",
       "                        2.8291e-04,  1.7457e-03, -1.6479e-03, -1.0912e-03, -1.3848e-03,\n",
       "                        2.5230e-03,  5.8655e-04, -1.3235e-03,  3.1187e-04,  6.7531e-04,\n",
       "                       -7.0187e-04,  3.7332e-04,  5.1963e-04, -4.2013e-04, -3.7787e-04,\n",
       "                       -2.3829e-04, -9.3719e-04, -5.8934e-04,  1.3101e-03,  8.8467e-04,\n",
       "                       -6.1098e-06,  1.9108e-03,  1.1241e-03,  1.3391e-04, -1.2726e-03,\n",
       "                        1.6616e-03,  2.4411e-03,  4.2087e-05,  1.7143e-04,  1.3297e-03,\n",
       "                        1.4454e-03, -4.8601e-04, -3.1696e-03, -1.2626e-03,  8.0089e-04,\n",
       "                       -7.1787e-04,  8.0517e-05,  9.4667e-04, -1.3573e-03,  2.9342e-04,\n",
       "                       -1.4236e-03, -3.4078e-04, -1.7589e-03,  5.1067e-04, -1.1210e-03,\n",
       "                       -2.7931e-04,  2.2806e-03,  3.0336e-03, -7.5296e-04,  1.2272e-03,\n",
       "                        1.6127e-03, -1.1285e-04,  3.2594e-03,  5.6391e-04,  3.7492e-04,\n",
       "                        2.5222e-03,  1.8136e-03, -2.7758e-04, -1.0784e-03,  1.0266e-03,\n",
       "                        8.4520e-04, -7.5138e-04,  1.7569e-03,  1.9746e-04, -1.7659e-03,\n",
       "                       -9.3506e-04, -1.4368e-03,  3.7919e-04, -2.5352e-04,  3.1565e-04,\n",
       "                       -1.3708e-04,  1.8593e-03, -3.1605e-03,  7.6628e-04,  6.5034e-04,\n",
       "                       -1.6080e-03, -7.9853e-04,  4.1174e-04,  1.2956e-03,  9.7615e-04,\n",
       "                       -1.6907e-03,  5.7384e-04, -3.3202e-04,  1.4974e-03,  1.4775e-03,\n",
       "                       -1.9730e-03, -9.1590e-04,  3.8159e-04, -7.9417e-04, -1.1647e-04,\n",
       "                        1.4300e-03, -1.6956e-03,  6.7280e-04, -8.5893e-04, -2.0796e-03,\n",
       "                       -1.0968e-03, -1.5779e-03,  1.6891e-03, -6.3326e-04,  1.7794e-03,\n",
       "                        1.1611e-03,  6.1075e-04,  1.2570e-04,  7.5817e-04,  9.9740e-04,\n",
       "                       -1.9159e-04,  7.6739e-04, -1.7871e-03,  2.9501e-03,  1.0350e-03,\n",
       "                        3.9428e-04, -3.2374e-04,  7.4452e-04, -2.3969e-04,  1.3916e-03,\n",
       "                        1.3315e-04, -5.2142e-04,  2.1854e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.3756, -0.3237, -0.2798, -0.4122, -0.2637, -0.3130, -0.2840, -0.2447,\n",
       "                       -0.2887, -0.3200, -0.2671, -0.3487, -0.3692, -0.3083, -0.2962, -0.4168,\n",
       "                       -0.2419, -0.2499, -0.2815, -0.2752, -0.2701, -0.3311, -0.3112, -0.2454,\n",
       "                       -0.2702, -0.3110, -0.2226, -0.1716, -0.3339, -0.2410, -0.3209, -0.2771,\n",
       "                       -0.2722, -0.2223, -0.3260, -0.3925, -0.4392, -0.2568, -0.3359, -0.3062,\n",
       "                       -0.3565, -0.3097, -0.3001, -0.3513, -0.2285, -0.3579, -0.3101, -0.3563,\n",
       "                       -0.3872, -0.3522, -0.2497, -0.3944, -0.2690, -0.3433, -0.2348, -0.3122,\n",
       "                       -0.3750, -0.3254, -0.4001, -0.2660, -0.3172, -0.3594, -0.3169, -0.2919,\n",
       "                       -0.3456, -0.3614, -0.2927, -0.2938, -0.2598, -0.2955, -0.2922, -0.4160,\n",
       "                       -0.3582, -0.3529, -0.3281, -0.3793, -0.3194, -0.3458, -0.3063, -0.3115,\n",
       "                       -0.2442, -0.3460, -0.2608, -0.3273, -0.2441, -0.2639, -0.2233, -0.3309,\n",
       "                       -0.2481, -0.2768, -0.2717, -0.2942, -0.2583, -0.4245, -0.2744, -0.3188,\n",
       "                       -0.3014, -0.2321, -0.3873, -0.4215, -0.1510, -0.3114, -0.2564, -0.3310,\n",
       "                       -0.3042, -0.2856, -0.2723, -0.3240, -0.2902, -0.2337, -0.3435, -0.3052,\n",
       "                       -0.3149, -0.2607, -0.3057, -0.2811, -0.3071, -0.3194, -0.3545, -0.2996,\n",
       "                       -0.2723, -0.2759, -0.3282, -0.2464, -0.2939, -0.3549, -0.3708, -0.3215],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.9894, 0.9471, 0.9050, 0.9458, 0.9574, 0.9092, 0.9082, 1.0137, 0.8966,\n",
       "                       0.9835, 0.9919, 0.9701, 1.0088, 0.9870, 0.8917, 1.1607, 1.0036, 0.8754,\n",
       "                       0.8889, 0.8932, 0.9583, 1.0489, 0.9335, 0.8181, 0.9636, 0.9516, 0.9346,\n",
       "                       0.9113, 1.1329, 0.9409, 0.9946, 0.9382, 1.0107, 0.9990, 1.0838, 1.1238,\n",
       "                       1.1889, 0.9812, 1.0448, 0.9035, 0.8929, 0.9454, 1.0053, 0.7620, 0.8848,\n",
       "                       0.8729, 0.8936, 1.0672, 0.9843, 1.0082, 0.9215, 1.0622, 0.9452, 0.9058,\n",
       "                       0.8533, 0.8372, 1.0192, 0.9414, 1.0729, 0.9815, 0.9925, 1.0734, 0.9374,\n",
       "                       0.8819, 1.0426, 0.8940, 0.9140, 0.8576, 0.8223, 0.9463, 1.0850, 1.0922,\n",
       "                       1.0117, 1.0627, 1.0292, 1.0301, 0.9493, 0.9398, 0.9507, 1.0167, 0.9122,\n",
       "                       1.0712, 0.9734, 1.0670, 0.9444, 1.0151, 0.9211, 0.9005, 1.0136, 0.8621,\n",
       "                       0.9478, 0.9486, 0.9855, 1.1083, 0.8663, 0.8938, 0.8683, 0.8485, 1.0702,\n",
       "                       1.1692, 0.8512, 0.7951, 0.8720, 0.9395, 0.9773, 0.9224, 0.9612, 0.9862,\n",
       "                       0.8156, 0.9286, 1.0340, 0.9514, 1.0000, 1.0010, 1.0999, 0.8128, 0.8226,\n",
       "                       0.8614, 0.9675, 0.9096, 0.9033, 1.0109, 0.8809, 0.8533, 0.9088, 1.0205,\n",
       "                       1.0088, 0.9350], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-0.0173, -0.0006,  0.0013],\n",
       "                         [ 0.0625,  0.0429,  0.0024],\n",
       "                         [-0.0135,  0.0054,  0.0660]],\n",
       "               \n",
       "                        [[-0.0117, -0.0305, -0.0056],\n",
       "                         [-0.0492, -0.0250, -0.0363],\n",
       "                         [-0.0544,  0.0027, -0.0478]],\n",
       "               \n",
       "                        [[ 0.0331, -0.0615,  0.0135],\n",
       "                         [-0.0719, -0.0645,  0.0313],\n",
       "                         [-0.0401,  0.0033, -0.0153]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0405,  0.0422, -0.0089],\n",
       "                         [ 0.0221, -0.0444, -0.0633],\n",
       "                         [-0.0010,  0.0479, -0.0249]],\n",
       "               \n",
       "                        [[-0.0384, -0.0238,  0.0084],\n",
       "                         [-0.0006,  0.0280,  0.0249],\n",
       "                         [ 0.0297, -0.0309, -0.0349]],\n",
       "               \n",
       "                        [[ 0.0259, -0.0432, -0.0156],\n",
       "                         [-0.0394, -0.0232, -0.0391],\n",
       "                         [-0.0394, -0.0793, -0.0420]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0220, -0.0342, -0.0445],\n",
       "                         [ 0.0247, -0.0547, -0.0332],\n",
       "                         [-0.0062, -0.0521, -0.0387]],\n",
       "               \n",
       "                        [[ 0.0053,  0.0263,  0.0169],\n",
       "                         [-0.0366, -0.0139, -0.0816],\n",
       "                         [-0.0640, -0.0057, -0.0099]],\n",
       "               \n",
       "                        [[ 0.0053,  0.0083,  0.0348],\n",
       "                         [ 0.0894,  0.0405,  0.0505],\n",
       "                         [ 0.0061,  0.0384,  0.0798]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0388, -0.0360,  0.0446],\n",
       "                         [-0.0049, -0.0401,  0.0237],\n",
       "                         [ 0.0388,  0.0198, -0.0306]],\n",
       "               \n",
       "                        [[-0.0006,  0.0369, -0.0291],\n",
       "                         [-0.0054,  0.0463, -0.0011],\n",
       "                         [ 0.0538,  0.0337,  0.0364]],\n",
       "               \n",
       "                        [[ 0.0208,  0.0633, -0.0508],\n",
       "                         [-0.0485, -0.0092, -0.0524],\n",
       "                         [-0.0151,  0.0199, -0.0255]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0390,  0.0132,  0.0404],\n",
       "                         [-0.0466, -0.0088,  0.0034],\n",
       "                         [ 0.0176, -0.0274, -0.0562]],\n",
       "               \n",
       "                        [[-0.0141, -0.0277,  0.0242],\n",
       "                         [-0.0403,  0.0147, -0.0113],\n",
       "                         [-0.0159, -0.0474, -0.0431]],\n",
       "               \n",
       "                        [[ 0.0569,  0.0076,  0.0044],\n",
       "                         [ 0.0260,  0.0501, -0.0014],\n",
       "                         [ 0.0707,  0.0758,  0.0122]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0076,  0.0368, -0.0408],\n",
       "                         [ 0.0244, -0.0245, -0.0303],\n",
       "                         [-0.0347, -0.0058,  0.0009]],\n",
       "               \n",
       "                        [[ 0.0454,  0.0299,  0.0227],\n",
       "                         [ 0.0426,  0.0492,  0.0482],\n",
       "                         [-0.0168,  0.0444,  0.0257]],\n",
       "               \n",
       "                        [[-0.0260, -0.0143,  0.0375],\n",
       "                         [ 0.0005, -0.0314, -0.0113],\n",
       "                         [ 0.0267,  0.0006,  0.0108]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0384, -0.0215,  0.0042],\n",
       "                         [-0.0213, -0.0175, -0.0364],\n",
       "                         [-0.0455, -0.0702, -0.0909]],\n",
       "               \n",
       "                        [[-0.0117,  0.0223, -0.0269],\n",
       "                         [ 0.0230, -0.0111, -0.0128],\n",
       "                         [-0.0315, -0.0300, -0.0157]],\n",
       "               \n",
       "                        [[ 0.0587,  0.0791,  0.0299],\n",
       "                         [ 0.0683,  0.0705, -0.0301],\n",
       "                         [ 0.0034,  0.0402,  0.0270]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0229,  0.0355, -0.0692],\n",
       "                         [ 0.0070, -0.0051, -0.0308],\n",
       "                         [-0.0266, -0.0461, -0.0604]],\n",
       "               \n",
       "                        [[-0.0321, -0.0036, -0.0650],\n",
       "                         [-0.0316,  0.0005, -0.0506],\n",
       "                         [-0.0226, -0.0233, -0.0497]],\n",
       "               \n",
       "                        [[-0.0334,  0.0442,  0.0092],\n",
       "                         [-0.0335,  0.0139,  0.0071],\n",
       "                         [-0.0270, -0.0071, -0.0607]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0052,  0.0100, -0.0040],\n",
       "                         [-0.0122, -0.0292,  0.0088],\n",
       "                         [ 0.0316, -0.0265, -0.0284]],\n",
       "               \n",
       "                        [[ 0.0233, -0.0047, -0.0111],\n",
       "                         [ 0.0248,  0.0696,  0.0323],\n",
       "                         [ 0.0126,  0.0310, -0.0464]],\n",
       "               \n",
       "                        [[ 0.0248, -0.0037, -0.0072],\n",
       "                         [-0.0606, -0.0440, -0.0335],\n",
       "                         [ 0.0050,  0.0205, -0.0299]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0203, -0.0466, -0.0031],\n",
       "                         [-0.0010, -0.0500, -0.0155],\n",
       "                         [-0.0164,  0.0376, -0.0525]],\n",
       "               \n",
       "                        [[ 0.0100, -0.0310,  0.0314],\n",
       "                         [-0.0133,  0.0387,  0.0324],\n",
       "                         [ 0.0308, -0.0513,  0.0284]],\n",
       "               \n",
       "                        [[ 0.0734,  0.0596,  0.0057],\n",
       "                         [-0.0181,  0.0446, -0.0420],\n",
       "                         [ 0.0443,  0.0422,  0.0138]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0031, -0.0278, -0.0082],\n",
       "                         [ 0.0809,  0.0789,  0.0138],\n",
       "                         [-0.0060,  0.0113,  0.0512]],\n",
       "               \n",
       "                        [[-0.0145, -0.0241,  0.0123],\n",
       "                         [-0.0222, -0.0481, -0.0124],\n",
       "                         [-0.0095, -0.0053, -0.0126]],\n",
       "               \n",
       "                        [[ 0.0278,  0.0096, -0.0164],\n",
       "                         [-0.0833, -0.0284, -0.0360],\n",
       "                         [-0.0227, -0.0706,  0.0118]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0381,  0.0005,  0.0738],\n",
       "                         [ 0.0377, -0.0390,  0.0569],\n",
       "                         [-0.0230, -0.0083,  0.0020]],\n",
       "               \n",
       "                        [[-0.0334, -0.0171,  0.0031],\n",
       "                         [ 0.0083, -0.0587, -0.0359],\n",
       "                         [-0.0658, -0.0258, -0.0712]],\n",
       "               \n",
       "                        [[ 0.0206,  0.0431,  0.0278],\n",
       "                         [ 0.0410,  0.0729,  0.0047],\n",
       "                         [-0.0125, -0.0044, -0.0294]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.5671, -0.5700,  0.5674,  0.5685,  0.5722, -0.5683, -0.5712, -0.5677,\n",
       "                       -0.5737, -0.5728, -0.5638,  0.5636,  0.5705, -0.5675, -0.5704, -0.5643,\n",
       "                       -0.5694,  0.5672, -0.5728, -0.5740,  0.5715,  0.5652, -0.5715, -0.5728,\n",
       "                        0.5670, -0.5677, -0.5718,  0.5708,  0.5687, -0.5732,  0.5664, -0.5723,\n",
       "                       -0.5666, -0.5689, -0.5673, -0.5679,  0.5606, -0.5711,  0.5744,  0.5668,\n",
       "                       -0.5707,  0.5647, -0.5748, -0.5708, -0.5722, -0.5630,  0.5679, -0.5720,\n",
       "                       -0.5735, -0.5674,  0.5672,  0.5706,  0.5705, -0.5684, -0.5722, -0.5662,\n",
       "                       -0.5705, -0.5657, -0.5690,  0.5723, -0.5707, -0.5686, -0.5686,  0.5670,\n",
       "                        0.5693, -0.5685, -0.5650, -0.5692, -0.5687,  0.5677,  0.5695,  0.5717,\n",
       "                        0.5621, -0.5684, -0.5742, -0.5724, -0.5736,  0.5709, -0.5735,  0.5731,\n",
       "                       -0.5691, -0.5671,  0.5684, -0.5700, -0.5663, -0.5654,  0.5734,  0.5695,\n",
       "                        0.5717,  0.5744, -0.5689,  0.5689,  0.5718,  0.5638, -0.5713, -0.5745,\n",
       "                       -0.5746,  0.5680, -0.5730,  0.5706, -0.5689, -0.5699,  0.5717,  0.5659,\n",
       "                        0.5707, -0.5729, -0.5660,  0.5679, -0.5729,  0.5727,  0.5654, -0.5697,\n",
       "                        0.5717,  0.5682,  0.5732, -0.5720, -0.5749, -0.5763, -0.5693, -0.5680,\n",
       "                        0.5664, -0.5683, -0.5712, -0.5731, -0.5667,  0.5736, -0.5743, -0.5697],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.1046, -0.2133, -0.1855, -0.1379, -0.2127, -0.2435, -0.2385, -0.1352,\n",
       "                       -0.2101, -0.1068, -0.1901, -0.1737, -0.0785, -0.1995, -0.2251, -0.1660,\n",
       "                       -0.0363, -0.1622, -0.1393, -0.2418, -0.1717, -0.1952, -0.1570, -0.1441,\n",
       "                       -0.2014, -0.2224, -0.1994, -0.0737, -0.2011, -0.1323, -0.2119, -0.1263,\n",
       "                       -0.1093, -0.2176, -0.0929, -0.2066, -0.2371, -0.1872, -0.1094, -0.1591,\n",
       "                       -0.1466, -0.0834, -0.2246, -0.2266, -0.2319, -0.2126, -0.2430, -0.1801,\n",
       "                       -0.0753, -0.0351, -0.0938, -0.2273, -0.2213, -0.2038, -0.1797, -0.2522,\n",
       "                       -0.2614, -0.1097, -0.1879, -0.2102, -0.2378, -0.2036, -0.1559, -0.1753,\n",
       "                       -0.2412, -0.1972, -0.2565, -0.2325, -0.0857, -0.1662, -0.0515, -0.2176,\n",
       "                       -0.1319, -0.1172, -0.1774, -0.2193, -0.1492, -0.1409, -0.1317, -0.1953,\n",
       "                       -0.0860, -0.1596, -0.0691, -0.2163, -0.1577, -0.2345, -0.1570, -0.2016,\n",
       "                       -0.0932, -0.1812, -0.0145, -0.1387, -0.1499, -0.2275, -0.2687, -0.2396,\n",
       "                       -0.2510, -0.1395, -0.2574, -0.0958, -0.1754, -0.1908, -0.2499, -0.2645,\n",
       "                       -0.1774, -0.0955, -0.2087, -0.0937, -0.1647, -0.1642, -0.1425, -0.0561,\n",
       "                       -0.2103, -0.1594, -0.1940, -0.1583, -0.1430, -0.1561, -0.2422, -0.2122,\n",
       "                       -0.2006, -0.1015, -0.2438, -0.1364, -0.1262, -0.2692, -0.0839, -0.0605],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.7978, 0.7939, 0.7605, 0.9164, 0.7195, 0.6898, 0.7615, 0.8119, 0.7665,\n",
       "                       0.7671, 0.7631, 0.8314, 0.8001, 0.8565, 0.8722, 0.9119, 0.8772, 0.8931,\n",
       "                       0.7548, 0.6996, 0.7407, 0.9855, 0.7500, 0.7893, 0.7432, 0.8248, 0.8640,\n",
       "                       0.9039, 0.8990, 0.7649, 0.9997, 0.7678, 0.7478, 0.8878, 0.8065, 0.9500,\n",
       "                       0.6937, 0.8854, 0.8641, 0.7863, 0.7877, 0.8179, 0.7851, 0.7659, 0.7092,\n",
       "                       0.8739, 0.6648, 0.8882, 0.8520, 0.7940, 0.7648, 0.9699, 0.7583, 0.7743,\n",
       "                       0.9575, 0.7285, 0.8852, 0.7569, 0.8602, 0.9757, 0.8849, 0.7473, 0.9293,\n",
       "                       0.9811, 0.7066, 0.7757, 0.7372, 0.8012, 0.8407, 0.9515, 0.7765, 0.7432,\n",
       "                       0.8269, 0.7889, 0.7752, 0.6797, 0.8516, 0.8125, 0.9834, 0.7012, 0.8150,\n",
       "                       1.0062, 0.7819, 0.7434, 0.7825, 0.7764, 0.9317, 0.7773, 0.7809, 1.0025,\n",
       "                       0.9247, 1.0204, 0.8638, 0.7078, 0.7869, 0.7933, 0.6736, 0.8995, 0.8487,\n",
       "                       0.7663, 0.9773, 0.6930, 0.7109, 0.8594, 0.7340, 0.8198, 0.8048, 0.7740,\n",
       "                       0.9438, 0.8348, 0.8110, 0.8309, 0.7886, 0.7191, 0.8284, 0.8423, 0.7618,\n",
       "                       0.8983, 0.8596, 0.8199, 0.9363, 0.8216, 0.7052, 0.8035, 0.8503, 0.6580,\n",
       "                       0.8762, 0.8596], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0038,  0.0088,  0.0031,  ..., -0.0049,  0.0102, -0.0232],\n",
       "                       [-0.0263, -0.0177,  0.0071,  ..., -0.0124,  0.0097, -0.0241],\n",
       "                       [ 0.0072, -0.0065,  0.0020,  ..., -0.0063,  0.0101, -0.0055],\n",
       "                       [ 0.0079, -0.0043,  0.0204,  ...,  0.0026, -0.0082, -0.0033],\n",
       "                       [-0.0124, -0.0048,  0.0072,  ..., -0.0103,  0.0072,  0.0085]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0010, -0.0034,  0.0043, -0.0018, -0.0022], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[ 1.9450e-01, -9.9795e-02,  5.8899e-02,  2.0421e-01, -6.8798e-02,\n",
       "                        -2.0007e-01, -1.7882e-01,  1.9901e-01,  7.3716e-02, -7.7455e-02,\n",
       "                        -1.7553e-01,  1.2926e-01, -2.1960e-01, -5.1855e-02,  1.6968e-01,\n",
       "                        -7.0568e-02, -1.7667e-01,  8.5367e-02, -1.8126e-01, -4.0493e-02],\n",
       "                       [ 3.8980e-01, -2.4708e-01,  3.8164e-01, -2.9687e-01,  4.5349e-01,\n",
       "                        -3.8652e-02,  3.3299e-01,  3.0432e-01,  6.7535e-01, -1.2620e-01,\n",
       "                        -6.3609e-01, -8.6019e-02, -5.0681e-01, -3.7749e-02, -2.9866e-01,\n",
       "                        -1.2184e-01, -6.8388e-01, -3.9214e-01, -8.0084e-01, -4.2827e-01],\n",
       "                       [-1.6620e-01, -8.1471e-03, -4.4961e-02,  8.5881e-03,  1.1564e-01,\n",
       "                        -1.6309e-01, -6.2042e-02, -8.4404e-03,  1.4042e-01,  2.6008e-02,\n",
       "                         3.1990e-02,  1.9294e-01,  1.6990e-01,  1.4820e-01,  1.9681e-01,\n",
       "                        -1.3945e-01,  1.1138e-01,  1.8941e-01,  6.5732e-03, -1.8138e-01],\n",
       "                       [-8.6432e-02, -8.4771e-02, -1.6149e-02, -1.0261e-01, -1.2665e-01,\n",
       "                        -6.0045e-02,  7.8200e-02,  8.3162e-02,  1.6663e-01,  1.4832e-01,\n",
       "                        -6.2887e-02, -7.0228e-02, -7.1047e-02, -5.1418e-02, -1.4043e-01,\n",
       "                        -1.6729e-01,  1.6258e-01,  1.0347e-01, -7.5450e-02, -2.5517e-05],\n",
       "                       [-5.7176e-01, -5.3567e-01,  2.5546e-01, -2.1685e-01,  1.5066e-01,\n",
       "                        -5.3538e-01,  2.5594e-01,  1.8560e-01, -4.1911e-01, -4.9700e-01,\n",
       "                         2.0255e-01, -2.6155e-01, -2.3279e-01, -3.0105e-01, -3.5505e-02,\n",
       "                        -3.5435e-01, -8.3221e-02, -2.4268e-01,  9.1219e-01, -4.1578e-01],\n",
       "                       [-1.9996e-01, -4.1678e-01,  4.0942e-01, -5.2367e-01,  6.5971e-01,\n",
       "                        -4.6258e-01,  7.0568e-01,  1.4866e-01,  2.6635e-01, -6.8733e-01,\n",
       "                         2.1990e-01, -4.2533e-01, -5.9473e-01, -5.9796e-01, -3.8962e-01,\n",
       "                        -4.8641e-01, -3.7637e-01, -6.1603e-01, -2.0829e-01, -4.6343e-01],\n",
       "                       [-4.3167e-01, -2.7574e-01,  4.1411e-01, -3.8236e-01,  5.3511e-01,\n",
       "                        -5.1694e-01,  5.2726e-01, -1.8161e-01, -1.2616e-01, -5.4819e-01,\n",
       "                         1.7054e-01, -1.6569e-01, -8.9271e-02, -3.8799e-01, -4.4435e-01,\n",
       "                        -4.5438e-01, -7.4693e-03, -4.2712e-01,  5.5863e-01, -3.6690e-01],\n",
       "                       [-2.1779e-01,  1.4848e-01, -1.7760e-01, -1.5083e-02,  1.6466e-01,\n",
       "                         1.9651e-01, -2.1856e-02, -1.2311e-02, -5.9592e-03, -1.5788e-01,\n",
       "                        -1.5053e-01, -1.8217e-01,  1.2928e-01, -4.6376e-02,  2.1403e-01,\n",
       "                         1.6271e-01, -2.1682e-01,  3.1186e-02, -1.7407e-01,  5.3682e-02],\n",
       "                       [-1.8789e-01, -2.6455e-01,  2.4643e-01, -3.7812e-01,  2.8643e-01,\n",
       "                        -4.0444e-01,  2.4196e-01, -1.2720e-01, -3.4686e-01, -3.7747e-01,\n",
       "                         2.1578e-01, -5.6660e-02, -2.4622e-01, -1.8555e-01,  1.2737e-01,\n",
       "                        -3.7837e-01,  1.1596e-01, -1.3072e-01,  8.1301e-01, -5.7052e-02],\n",
       "                       [-8.1927e-03, -1.9930e-01, -1.8574e-01, -6.5261e-02,  1.2612e-02,\n",
       "                        -5.7991e-02, -1.5564e-01,  6.4414e-02, -9.2935e-02,  2.0700e-01,\n",
       "                         3.9932e-02,  8.1564e-02, -1.6845e-01, -7.8590e-04, -1.0365e-01,\n",
       "                        -9.2567e-03,  1.4797e-02, -6.9942e-02, -4.7156e-02,  1.9742e-01],\n",
       "                       [-5.2067e-01, -3.2243e-01,  4.1729e-01, -4.3652e-02,  2.9763e-01,\n",
       "                        -2.5148e-01,  1.5672e-01,  2.3519e-02, -1.3659e-01, -2.4649e-01,\n",
       "                         2.8707e-01, -2.3514e-01,  4.6663e-02, -1.9362e-01,  1.9682e-01,\n",
       "                        -2.8972e-01,  3.1460e-01, -2.0496e-01,  4.5684e-01, -2.4085e-01],\n",
       "                       [ 9.0016e-02, -1.0440e-01, -7.2647e-02,  3.1704e-01,  9.2110e-02,\n",
       "                         1.0998e-01, -2.6964e-01,  2.0037e-01, -2.8880e-02,  3.1490e-03,\n",
       "                         1.1870e-01,  1.2041e-01,  2.3588e-01, -7.4556e-02,  2.2500e-01,\n",
       "                         1.9153e-02,  2.1070e-01,  3.6588e-02,  9.8119e-02,  8.2447e-02],\n",
       "                       [ 2.2102e-01, -1.1770e-01, -1.9069e-03,  1.4257e-01,  1.8480e-01,\n",
       "                         5.4289e-02, -1.9983e-01,  2.4026e-02,  1.2194e-01,  4.9479e-02,\n",
       "                         9.9185e-02,  2.4027e-01,  9.5005e-02,  1.1797e-01,  1.1141e-01,\n",
       "                         1.8811e-01, -9.3484e-02,  1.9488e-01,  1.4801e-01, -1.0379e-03],\n",
       "                       [-4.5869e-02, -7.2122e-02, -1.6090e-01,  2.1132e-01, -1.0666e-01,\n",
       "                         1.4656e-01,  3.7628e-02, -2.0761e-01,  1.4668e-01,  1.4304e-01,\n",
       "                         2.1277e-01,  5.8399e-02,  1.9407e-01,  1.8373e-01, -1.8475e-01,\n",
       "                         2.0963e-01,  7.9984e-02, -1.2375e-01, -4.4160e-02, -2.1078e-01],\n",
       "                       [-4.0172e-01, -3.4385e-01,  2.4891e-01, -9.2865e-02, -7.8467e-02,\n",
       "                        -3.9211e-01,  3.2992e-01, -6.3181e-03, -5.9432e-01, -2.4161e-01,\n",
       "                         1.8651e-01, -3.8065e-02,  7.9387e-02, -2.6197e-01, -5.0401e-02,\n",
       "                        -1.9614e-01,  2.6322e-01, -3.9110e-01,  5.7222e-01, -3.1454e-01],\n",
       "                       [ 6.2884e-02,  1.5268e-01, -9.8051e-02, -7.9451e-02,  5.4087e-02,\n",
       "                        -8.8575e-02, -2.0254e-01,  7.8244e-03, -2.1483e-01,  1.6323e-01,\n",
       "                         5.5022e-02,  1.6895e-01,  1.5126e-01, -4.8578e-04, -1.3752e-01,\n",
       "                         1.3657e-02,  8.7678e-02,  7.2777e-02, -2.0556e-01,  1.6001e-01],\n",
       "                       [-1.7184e-01,  1.5199e-01, -1.4167e-01,  4.3341e-02, -1.1432e-01,\n",
       "                         1.8257e-02,  1.5661e-01,  6.9146e-03,  1.4682e-01,  1.5712e-01,\n",
       "                        -1.8079e-01, -2.4925e-02,  9.8675e-02,  1.8632e-01,  7.6713e-02,\n",
       "                        -7.8911e-02, -9.5569e-02, -1.8848e-01,  2.3741e-02, -1.2041e-01],\n",
       "                       [-1.6276e-01, -4.5067e-01,  6.8388e-01, -5.0697e-01,  7.1938e-01,\n",
       "                        -3.9967e-01,  6.2084e-01,  8.8683e-01,  2.9116e-01, -3.3880e-01,\n",
       "                        -1.1797e-03, -3.9034e-01, -7.1274e-01, -3.8924e-01, -8.9630e-01,\n",
       "                        -6.1500e-01, -8.4075e-01, -3.2942e-01, -3.4429e-01, -6.2866e-01],\n",
       "                       [-3.7177e-01, -3.7319e-01,  1.2635e-01, -5.4267e-01,  1.3889e-01,\n",
       "                        -4.5140e-01,  3.2392e-01,  1.0258e-01, -1.4233e-01, -2.6357e-01,\n",
       "                         1.7592e-01, -5.2133e-01, -3.3430e-02, -5.0358e-01, -3.1022e-01,\n",
       "                        -5.3176e-01, -2.5664e-01, -5.2543e-01,  3.8923e-01, -5.2397e-01],\n",
       "                       [ 7.0586e-02, -1.8415e-01,  4.1921e-02, -1.8028e-01,  2.6406e-01,\n",
       "                        -2.6472e-01,  3.8520e-01,  4.9044e-01,  8.8484e-02, -2.0624e-01,\n",
       "                        -1.1292e-01,  5.3118e-02, -5.3980e-01, -2.5811e-01, -4.4602e-01,\n",
       "                        -3.2130e-01, -3.7673e-01, -1.8490e-03, -6.5664e-01, -2.7760e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([-0.1579,  0.3907, -0.2297, -0.1645,  0.1794,  0.6311,  0.4431,  0.0164,\n",
       "                        0.1715,  0.1361,  0.2913,  0.1168,  0.1037, -0.1589,  0.2151,  0.1238,\n",
       "                        0.1505,  0.6952,  0.3778,  0.1268], device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 1.2625e-01, -1.1142e-01,  1.5654e-01, -8.9212e-02, -2.3575e-01,\n",
       "                        -3.3273e-01, -4.9206e-01,  1.5559e-01, -2.6902e-01, -2.1596e-02,\n",
       "                        -2.0657e-01,  2.0025e-01,  2.1064e-01,  1.2310e-01, -1.0050e-01,\n",
       "                         3.2825e-02,  7.7861e-02, -2.5794e-01, -6.2703e-02, -3.1445e-01],\n",
       "                       [ 9.4664e-02, -1.3031e-01, -1.4995e-01,  1.2825e-01,  1.9603e-01,\n",
       "                         2.3480e-01,  1.0646e-01, -1.3566e-01,  2.5350e-01,  1.7717e-01,\n",
       "                         2.1791e-02,  9.2639e-03,  1.0955e-01,  9.6875e-02,  2.7011e-01,\n",
       "                        -9.5869e-02, -1.5447e-01,  2.5802e-01,  1.3808e-01, -9.6510e-02],\n",
       "                       [-1.2312e-01, -3.0094e-01, -1.3213e-01,  1.0311e-01, -4.4095e-01,\n",
       "                        -1.3131e-01, -5.0038e-01, -1.6387e-01, -3.4844e-01, -1.1402e-02,\n",
       "                        -2.6386e-01,  1.5359e-02,  2.9411e-02,  8.3758e-03, -4.7741e-01,\n",
       "                         9.3342e-02,  7.1448e-02, -5.1055e-01, -3.8752e-01, -5.1403e-01],\n",
       "                       [ 2.1371e-01,  6.2856e-03,  1.4731e-01,  1.9034e-01, -3.4510e-01,\n",
       "                        -1.8583e-01, -1.5590e-01,  2.6205e-02, -2.9098e-01,  1.7085e-01,\n",
       "                        -2.3675e-01, -1.2414e-01, -4.0816e-03,  7.6588e-03, -3.2411e-01,\n",
       "                         6.1526e-02, -1.8303e-01, -9.5315e-02, -1.3271e-01,  3.5093e-02],\n",
       "                       [-1.6823e-01,  3.4460e-01, -1.2423e-01,  1.6210e-01, -1.0299e-01,\n",
       "                         2.3330e-01,  3.1879e-01,  2.0271e-01, -1.2630e-02, -1.3443e-01,\n",
       "                         1.6393e-01,  1.2077e-01, -1.4421e-01, -8.4100e-02,  4.6720e-02,\n",
       "                         2.3076e-02,  9.0937e-02, -4.9595e-02,  2.6614e-01, -7.0200e-02],\n",
       "                       [ 1.3204e-01, -3.6840e-04,  1.7486e-01,  8.3363e-02,  2.1467e-01,\n",
       "                         1.1595e-01, -1.0595e-01,  2.6531e-02, -3.0941e-03, -1.5437e-01,\n",
       "                        -1.5091e-01,  1.6651e-01,  6.5514e-02,  6.0086e-02,  8.6567e-02,\n",
       "                        -6.1640e-02,  2.0749e-01,  3.8836e-01, -1.0199e-01, -3.3233e-02],\n",
       "                       [ 1.8598e-02,  8.2672e-01, -5.6475e-02,  7.2102e-02,  9.0358e-01,\n",
       "                         9.2427e-01,  9.8408e-01, -1.3016e-01,  5.7577e-01,  6.3274e-02,\n",
       "                         6.4313e-01, -2.9636e-02,  6.4393e-02, -1.4676e-01,  6.2358e-01,\n",
       "                         3.4168e-02,  1.9005e-01,  6.3749e-01,  9.4827e-01,  4.1781e-01],\n",
       "                       [-3.4839e-02, -2.1596e-01, -2.0602e-01,  3.9176e-02,  3.0882e-03,\n",
       "                        -2.1684e-01, -2.2468e-01,  2.2352e-01,  8.0894e-02,  2.1470e-01,\n",
       "                         7.6647e-02, -1.4804e-01,  1.2350e-01,  1.0633e-01, -1.9979e-01,\n",
       "                        -2.1084e-01,  8.7749e-02, -4.0896e-01, -3.0455e-01, -9.9467e-02],\n",
       "                       [-6.7627e-02, -2.8169e-01, -1.8843e-01,  2.0364e-01,  3.4602e-01,\n",
       "                        -7.6296e-02,  2.2534e-01, -1.7646e-01,  2.0878e-01,  1.7681e-01,\n",
       "                         2.3399e-01, -1.1961e-01, -1.1331e-01, -5.0474e-02,  2.6882e-01,\n",
       "                        -1.0091e-01,  8.8520e-02, -1.8803e-01,  5.1278e-02, -3.9060e-01],\n",
       "                       [ 1.8689e-02, -5.0139e-01,  1.7061e-02,  6.4536e-02, -3.8319e-01,\n",
       "                        -2.4184e-01, -3.8859e-01,  6.2210e-02, -4.3627e-01,  1.2259e-01,\n",
       "                        -4.1052e-01,  4.8389e-02, -1.1025e-02,  9.5937e-02, -1.6831e-01,\n",
       "                        -9.4036e-03,  2.1143e-01, -5.9704e-01, -1.7414e-01, -3.8511e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.3261,  0.1651, -0.3542, -0.1349,  0.0623,  0.0221,  0.6558, -0.2301,\n",
       "                        0.1458, -0.0976], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.4871257066726684,\n",
       "   1.359152992606163,\n",
       "   1.313425171136856,\n",
       "   1.2633237639665604,\n",
       "   1.210182501077652,\n",
       "   1.1633008378744125,\n",
       "   1.1187003617286682,\n",
       "   1.0831409302949906,\n",
       "   1.0483278150558473,\n",
       "   1.0349365949630738,\n",
       "   0.9947823992967606,\n",
       "   0.985070841550827,\n",
       "   0.9751642806529999,\n",
       "   0.9426674112081528,\n",
       "   0.9370706976652146,\n",
       "   0.9273006557226181,\n",
       "   0.9087767043113708,\n",
       "   0.9018122251033783,\n",
       "   0.8896550332903862,\n",
       "   0.874093946814537,\n",
       "   0.8666872065067291,\n",
       "   0.8362258025407792,\n",
       "   0.8329485636353493,\n",
       "   0.8336436698436737,\n",
       "   0.822275983273983,\n",
       "   0.8112739166617393,\n",
       "   0.7926360340714454,\n",
       "   0.7874330879449845,\n",
       "   0.7817245162129403,\n",
       "   0.790426372051239,\n",
       "   0.7654561707973481,\n",
       "   0.7642998648285866,\n",
       "   0.7548812957406044,\n",
       "   0.73447911465168,\n",
       "   0.7435128963589668,\n",
       "   0.719028260409832,\n",
       "   0.7200626616477966,\n",
       "   0.711368779540062,\n",
       "   0.7100827465057373,\n",
       "   0.6863856623768807,\n",
       "   0.7036727345585823,\n",
       "   0.6851320981383323,\n",
       "   0.6786301241517066,\n",
       "   0.6840163297057151,\n",
       "   0.6699509848952293,\n",
       "   0.6670484399795532,\n",
       "   0.665900018632412,\n",
       "   0.6527955750226975,\n",
       "   0.6432089362740516,\n",
       "   0.6481548866629601,\n",
       "   0.6461113160848617,\n",
       "   0.6263603148460388,\n",
       "   0.6280675618648529,\n",
       "   0.6145265136957169,\n",
       "   0.6261562059521675,\n",
       "   0.6171774310469628,\n",
       "   0.6066627011895179,\n",
       "   0.6094745087623596,\n",
       "   0.5989947299659252,\n",
       "   0.6007179483771324,\n",
       "   0.5967502174973488,\n",
       "   0.5897222097516059,\n",
       "   0.5754554066061973,\n",
       "   0.5731471665799618,\n",
       "   0.5772148718833924,\n",
       "   0.5709423180222511,\n",
       "   0.5694349449276924,\n",
       "   0.5458618040084839,\n",
       "   0.5487817575335503,\n",
       "   0.5582499881386757,\n",
       "   0.5427864486575127,\n",
       "   0.5432042125463485,\n",
       "   0.5484354943037033,\n",
       "   0.5476589985489845,\n",
       "   0.5294758928716182,\n",
       "   0.5263957353234291,\n",
       "   0.5289667768478393,\n",
       "   0.5309377280473709,\n",
       "   0.5156142988801002,\n",
       "   0.5196091137826443,\n",
       "   0.5132117956578731,\n",
       "   0.5104401254355907,\n",
       "   0.5018985850512981,\n",
       "   0.5002960314750672,\n",
       "   0.4877698335647583,\n",
       "   0.49126826027035714,\n",
       "   0.4859432065784931,\n",
       "   0.4886163456737995,\n",
       "   0.47646064215898515,\n",
       "   0.4706421595811844,\n",
       "   0.4677057972848415,\n",
       "   0.476171452999115,\n",
       "   0.4561540378332138,\n",
       "   0.4543369657099247,\n",
       "   0.4621051945388317,\n",
       "   0.4743588656485081,\n",
       "   0.45534216791391374,\n",
       "   0.4567204810976982,\n",
       "   0.4403771677315235],\n",
       "  'train_loss_std': [0.13731290177262045,\n",
       "   0.11454586168840486,\n",
       "   0.13391340388326245,\n",
       "   0.1222508088116535,\n",
       "   0.13041187363139164,\n",
       "   0.1294216169641068,\n",
       "   0.13829303890271025,\n",
       "   0.13110796899467483,\n",
       "   0.13717727923348128,\n",
       "   0.13203898632940417,\n",
       "   0.13616550984310216,\n",
       "   0.13995072235013994,\n",
       "   0.12951958688496926,\n",
       "   0.1398780901468476,\n",
       "   0.13675323181859828,\n",
       "   0.14043908350573414,\n",
       "   0.1313411159610808,\n",
       "   0.13825687971967737,\n",
       "   0.14341020645895233,\n",
       "   0.14762322620463275,\n",
       "   0.14099473911655594,\n",
       "   0.1330234290423144,\n",
       "   0.14334070673991073,\n",
       "   0.14539862274177695,\n",
       "   0.13993486215683582,\n",
       "   0.1458377463534045,\n",
       "   0.13961118320443447,\n",
       "   0.13714950942080692,\n",
       "   0.13647807616551633,\n",
       "   0.15235799686663304,\n",
       "   0.1366022840069871,\n",
       "   0.14339636805390918,\n",
       "   0.14322873966567903,\n",
       "   0.13990120973293801,\n",
       "   0.14431159859180964,\n",
       "   0.1485068047132956,\n",
       "   0.13469091592024227,\n",
       "   0.14410104803501472,\n",
       "   0.14264167462626556,\n",
       "   0.14381486155314702,\n",
       "   0.14079543842221656,\n",
       "   0.14118180839601904,\n",
       "   0.1367096659455097,\n",
       "   0.14528363721955143,\n",
       "   0.14094355175217294,\n",
       "   0.1437882787571438,\n",
       "   0.14890047460852976,\n",
       "   0.14091821277988126,\n",
       "   0.14252491481486299,\n",
       "   0.14007700607615112,\n",
       "   0.14012684516484186,\n",
       "   0.1350529684246839,\n",
       "   0.13898368554623916,\n",
       "   0.14191017117813326,\n",
       "   0.15031062243535634,\n",
       "   0.13824202606003197,\n",
       "   0.1344333224202206,\n",
       "   0.14479523756481014,\n",
       "   0.13568800515872426,\n",
       "   0.13697264438985968,\n",
       "   0.14329715876181687,\n",
       "   0.14219304309132305,\n",
       "   0.13701499241606846,\n",
       "   0.13607650216731357,\n",
       "   0.13617030774410932,\n",
       "   0.13804648616256665,\n",
       "   0.13896769180483512,\n",
       "   0.13936669542502858,\n",
       "   0.12699238951692124,\n",
       "   0.13907523565432006,\n",
       "   0.12442608969267008,\n",
       "   0.13000945567505245,\n",
       "   0.12874101395494153,\n",
       "   0.13469713500002367,\n",
       "   0.12949520334543319,\n",
       "   0.12656616579645735,\n",
       "   0.131958149869709,\n",
       "   0.1310794097470331,\n",
       "   0.12714573232840387,\n",
       "   0.13365367292390717,\n",
       "   0.13046751271695958,\n",
       "   0.12837538341044458,\n",
       "   0.1321137661678798,\n",
       "   0.1289909687481939,\n",
       "   0.13203554084240413,\n",
       "   0.12411366489142588,\n",
       "   0.12742925522320514,\n",
       "   0.13036289361863182,\n",
       "   0.13000045922873316,\n",
       "   0.12114032222515425,\n",
       "   0.12964152745801452,\n",
       "   0.12922663747780147,\n",
       "   0.12187368821641195,\n",
       "   0.12525680956897212,\n",
       "   0.13196454174466496,\n",
       "   0.14003444590164657,\n",
       "   0.11933920035080466,\n",
       "   0.12678942920103303,\n",
       "   0.12100549187764872],\n",
       "  'train_accuracy_mean': [0.36761333429813386,\n",
       "   0.44044000101089475,\n",
       "   0.4646266667842865,\n",
       "   0.4884133324623108,\n",
       "   0.515213333427906,\n",
       "   0.5414533317089081,\n",
       "   0.5594799993634224,\n",
       "   0.5766799983382225,\n",
       "   0.5930399990677834,\n",
       "   0.5992533323764802,\n",
       "   0.6185466645956039,\n",
       "   0.6212666652798653,\n",
       "   0.6252133328318595,\n",
       "   0.6413066662549972,\n",
       "   0.6418666663765907,\n",
       "   0.6469600002765655,\n",
       "   0.6537333312630653,\n",
       "   0.6573066664934158,\n",
       "   0.6638533336520195,\n",
       "   0.6704666649103165,\n",
       "   0.6692800003886222,\n",
       "   0.6844266678094864,\n",
       "   0.6856266664266586,\n",
       "   0.6851600005030632,\n",
       "   0.6906533334851265,\n",
       "   0.6946133325099945,\n",
       "   0.7023733330965042,\n",
       "   0.7022933328151703,\n",
       "   0.7067866666316986,\n",
       "   0.703879998922348,\n",
       "   0.7117599995732308,\n",
       "   0.7141733343601226,\n",
       "   0.7185466657876969,\n",
       "   0.7256399992704391,\n",
       "   0.7209333332777024,\n",
       "   0.7332266664505005,\n",
       "   0.729573333978653,\n",
       "   0.7337866657972336,\n",
       "   0.7364266661405563,\n",
       "   0.7434666666984558,\n",
       "   0.7379999980926514,\n",
       "   0.7458666676282882,\n",
       "   0.7476399998664855,\n",
       "   0.7461733335256576,\n",
       "   0.7521733335256576,\n",
       "   0.7512133316993713,\n",
       "   0.751706666469574,\n",
       "   0.7586133323907852,\n",
       "   0.7601466674804688,\n",
       "   0.7596799999475479,\n",
       "   0.7588400000333786,\n",
       "   0.7678533339500427,\n",
       "   0.7669866664409637,\n",
       "   0.7726799993515014,\n",
       "   0.7682399995326996,\n",
       "   0.7716933336257935,\n",
       "   0.7755333325862884,\n",
       "   0.7746533329486847,\n",
       "   0.779346668124199,\n",
       "   0.777933333158493,\n",
       "   0.7790000015497207,\n",
       "   0.7812000011205673,\n",
       "   0.7874266664981842,\n",
       "   0.7885599988698959,\n",
       "   0.7861199995279312,\n",
       "   0.7886666655540466,\n",
       "   0.7890400011539459,\n",
       "   0.7989333329200745,\n",
       "   0.7969600007534027,\n",
       "   0.7942799994945526,\n",
       "   0.7995600000619888,\n",
       "   0.7995999985933304,\n",
       "   0.7975066673755645,\n",
       "   0.798,\n",
       "   0.8051599993705749,\n",
       "   0.8057866666316986,\n",
       "   0.8041999998092652,\n",
       "   0.8020933334827423,\n",
       "   0.8099066661596298,\n",
       "   0.8076533335447311,\n",
       "   0.8107733316421509,\n",
       "   0.8111999992132187,\n",
       "   0.8181333330869675,\n",
       "   0.8168799992799759,\n",
       "   0.8216266663074493,\n",
       "   0.8184399991035461,\n",
       "   0.8220400000810624,\n",
       "   0.8216666668653488,\n",
       "   0.8249733327627182,\n",
       "   0.8269200019836426,\n",
       "   0.8281333328485488,\n",
       "   0.8244799998998642,\n",
       "   0.8321866674423217,\n",
       "   0.8334133330583572,\n",
       "   0.8294133343696595,\n",
       "   0.8261599998474121,\n",
       "   0.8320533348321915,\n",
       "   0.8308399999141693,\n",
       "   0.8403466674089431],\n",
       "  'train_accuracy_std': [0.07517264161009087,\n",
       "   0.06470914164845684,\n",
       "   0.07209264516642666,\n",
       "   0.06580825856765521,\n",
       "   0.06850611435634496,\n",
       "   0.06865128419916422,\n",
       "   0.0706074946056965,\n",
       "   0.06628490519920642,\n",
       "   0.06957715405709171,\n",
       "   0.06602607373541702,\n",
       "   0.06983742251040316,\n",
       "   0.07068235661595393,\n",
       "   0.06567072023849245,\n",
       "   0.06846299591106884,\n",
       "   0.06744532063846242,\n",
       "   0.06786852179458815,\n",
       "   0.06497876731029269,\n",
       "   0.06635687720736938,\n",
       "   0.06837800675171049,\n",
       "   0.07089306667668809,\n",
       "   0.0669136557892947,\n",
       "   0.06309467550975216,\n",
       "   0.06617406929532575,\n",
       "   0.06702136357921372,\n",
       "   0.06482537001236753,\n",
       "   0.06513955598214899,\n",
       "   0.06450676588115131,\n",
       "   0.06392414324128087,\n",
       "   0.06101864049431423,\n",
       "   0.06826330576880006,\n",
       "   0.06243513981884328,\n",
       "   0.06608079998720835,\n",
       "   0.06385068584582812,\n",
       "   0.06462688921502023,\n",
       "   0.06582549293873956,\n",
       "   0.06772878693451799,\n",
       "   0.062041528947728346,\n",
       "   0.06406363961718893,\n",
       "   0.0651496400475622,\n",
       "   0.06468577010307516,\n",
       "   0.06409922725428818,\n",
       "   0.061866379464618496,\n",
       "   0.06142047534750493,\n",
       "   0.06485968512866158,\n",
       "   0.06288039004273305,\n",
       "   0.06459063476312106,\n",
       "   0.0645891179098121,\n",
       "   0.06223423113968634,\n",
       "   0.06465275288534388,\n",
       "   0.06087863916624068,\n",
       "   0.06284362495329679,\n",
       "   0.057802082807196495,\n",
       "   0.06018552702500816,\n",
       "   0.060406178669346215,\n",
       "   0.06485686949211465,\n",
       "   0.05977494188316725,\n",
       "   0.058568515965236545,\n",
       "   0.06322685716895993,\n",
       "   0.05921876472252802,\n",
       "   0.058288896538878296,\n",
       "   0.06300176485073324,\n",
       "   0.06257425738078942,\n",
       "   0.059361419310624976,\n",
       "   0.05822383708372633,\n",
       "   0.058283322031925384,\n",
       "   0.060688641550627226,\n",
       "   0.060866800131636804,\n",
       "   0.05990600137810344,\n",
       "   0.057248023427390056,\n",
       "   0.06026232832307045,\n",
       "   0.055693463539186465,\n",
       "   0.05470583972244415,\n",
       "   0.05522886663363608,\n",
       "   0.05856430402049349,\n",
       "   0.05578009463089291,\n",
       "   0.05550678784130429,\n",
       "   0.05760289420643055,\n",
       "   0.05729743247777395,\n",
       "   0.05452412430472498,\n",
       "   0.05609123418569232,\n",
       "   0.05678205711247417,\n",
       "   0.054758298051680615,\n",
       "   0.05587510008129771,\n",
       "   0.05528470861168596,\n",
       "   0.05603985484538768,\n",
       "   0.0525286143681646,\n",
       "   0.05319497322124318,\n",
       "   0.05470384980065993,\n",
       "   0.05379859079629046,\n",
       "   0.05342016154645089,\n",
       "   0.05292829697231113,\n",
       "   0.05589689625467829,\n",
       "   0.05324092783295258,\n",
       "   0.05338491557842544,\n",
       "   0.055043521597710116,\n",
       "   0.057570913691220395,\n",
       "   0.05045179822950518,\n",
       "   0.054141430577563286,\n",
       "   0.05161106717832437],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.46935511191686,\n",
       "   1.4300840616226196,\n",
       "   1.3963250994682312,\n",
       "   1.3509969162940978,\n",
       "   1.310428602695465,\n",
       "   1.2754932888348898,\n",
       "   1.2356616510947545,\n",
       "   1.2111576122045518,\n",
       "   1.181922315955162,\n",
       "   1.171851439078649,\n",
       "   1.1423675505320232,\n",
       "   1.1350313812494277,\n",
       "   1.1164341245094935,\n",
       "   1.1049268050988514,\n",
       "   1.0961804558833441,\n",
       "   1.081214776833852,\n",
       "   1.081943629582723,\n",
       "   1.06333356042703,\n",
       "   1.0503953011830647,\n",
       "   1.042465033729871,\n",
       "   1.035974028110504,\n",
       "   1.015799675186475,\n",
       "   1.0096267813444137,\n",
       "   1.0322985096772512,\n",
       "   0.999878652493159,\n",
       "   0.9975027994314829,\n",
       "   0.9904993323485056,\n",
       "   0.9987314331531525,\n",
       "   0.9664807424942652,\n",
       "   0.9632408189773559,\n",
       "   0.9608525425195694,\n",
       "   0.9627982209126155,\n",
       "   0.9577448266744614,\n",
       "   0.9628533341487249,\n",
       "   0.9400929375489553,\n",
       "   0.9528935805956522,\n",
       "   0.9372499561309815,\n",
       "   0.9319053496917089,\n",
       "   0.9338617310921351,\n",
       "   0.9265615902344386,\n",
       "   0.9296671299139658,\n",
       "   0.9345942481358847,\n",
       "   0.9242303311824799,\n",
       "   0.9116130499045054,\n",
       "   0.9083692864576975,\n",
       "   0.900348424911499,\n",
       "   0.9139749908447266,\n",
       "   0.9033569832642873,\n",
       "   0.9463233004013697,\n",
       "   0.9035892383257548,\n",
       "   0.9001660935084025,\n",
       "   0.8903394794464111,\n",
       "   0.8895667397975922,\n",
       "   0.8954772108793259,\n",
       "   0.8828357960780462,\n",
       "   0.8836998590826988,\n",
       "   0.8752675493558247,\n",
       "   0.8763476675748825,\n",
       "   0.8748166652520498,\n",
       "   0.8842395502328873,\n",
       "   0.8866802632808686,\n",
       "   0.8669396205743154,\n",
       "   0.9265838448206584,\n",
       "   0.8657766083876292,\n",
       "   0.853567409714063,\n",
       "   0.862093137105306,\n",
       "   0.8744269633293151,\n",
       "   0.9034813511371612,\n",
       "   0.862430646220843,\n",
       "   0.8738753980398178,\n",
       "   0.8653603557745616,\n",
       "   0.8495526681343715,\n",
       "   0.8618825481335322,\n",
       "   0.8485959915320078,\n",
       "   0.8583070681492487,\n",
       "   0.8650303359826406,\n",
       "   0.8598885363340378,\n",
       "   0.8595727930466334,\n",
       "   0.8689931893348694,\n",
       "   0.8783900417884191,\n",
       "   0.8531057612101237,\n",
       "   0.8572001379728317,\n",
       "   0.8421337072054546,\n",
       "   0.8559975334008535,\n",
       "   0.8747907696167628,\n",
       "   0.8664210850000381,\n",
       "   0.8697675200303395,\n",
       "   0.8646516068776449,\n",
       "   0.8647469643751781,\n",
       "   0.8629891965786616,\n",
       "   0.8712407847245535,\n",
       "   0.8804804213841756,\n",
       "   0.8649602417151133,\n",
       "   0.8834961102406184,\n",
       "   0.8416663418213527,\n",
       "   0.8401535036166509,\n",
       "   0.861534652709961,\n",
       "   0.8476466010014216,\n",
       "   0.8576469026009241],\n",
       "  'val_loss_std': [0.08742828939625255,\n",
       "   0.09367099901544768,\n",
       "   0.10221104866175312,\n",
       "   0.11044747142929941,\n",
       "   0.11523824211072928,\n",
       "   0.11236824043716169,\n",
       "   0.11990674119513496,\n",
       "   0.12160265103107,\n",
       "   0.12588741250059468,\n",
       "   0.1297824280121948,\n",
       "   0.12784638937148182,\n",
       "   0.13384388486502705,\n",
       "   0.13315616926400373,\n",
       "   0.13471458769459996,\n",
       "   0.1309786025819578,\n",
       "   0.1316387428450882,\n",
       "   0.13680420919424433,\n",
       "   0.13748668626451063,\n",
       "   0.13083795720852073,\n",
       "   0.13722731182694864,\n",
       "   0.13452159244681525,\n",
       "   0.13674252263593847,\n",
       "   0.13837174677663944,\n",
       "   0.13592522339855656,\n",
       "   0.1387768130303455,\n",
       "   0.13972111769553525,\n",
       "   0.1408333700218511,\n",
       "   0.13788866140922584,\n",
       "   0.14242562532556644,\n",
       "   0.13550135088046672,\n",
       "   0.14310135639506305,\n",
       "   0.14222685270140917,\n",
       "   0.13851950702831678,\n",
       "   0.13694835521641943,\n",
       "   0.14639548008042758,\n",
       "   0.14426544339467653,\n",
       "   0.1435225053774618,\n",
       "   0.14981887867852461,\n",
       "   0.13799379749855833,\n",
       "   0.1463891245118974,\n",
       "   0.13928760984192665,\n",
       "   0.14346856128517715,\n",
       "   0.14308565007324633,\n",
       "   0.13978522152291736,\n",
       "   0.14204728426629595,\n",
       "   0.14123728058431628,\n",
       "   0.14215900639329107,\n",
       "   0.14255265790372168,\n",
       "   0.14648053978186926,\n",
       "   0.14045681268297736,\n",
       "   0.144636375591787,\n",
       "   0.1439295957669356,\n",
       "   0.14603822908993863,\n",
       "   0.14112692204318916,\n",
       "   0.14175911169552532,\n",
       "   0.14096504636893284,\n",
       "   0.13595531164584856,\n",
       "   0.14026063691663213,\n",
       "   0.1401443068990694,\n",
       "   0.14700166990790062,\n",
       "   0.15086048428461465,\n",
       "   0.13905219389491924,\n",
       "   0.14434558774787778,\n",
       "   0.1409929069790176,\n",
       "   0.14429633542273892,\n",
       "   0.14570954841008807,\n",
       "   0.150214634642469,\n",
       "   0.15392399764184644,\n",
       "   0.14290461929702908,\n",
       "   0.14987037390976668,\n",
       "   0.14416456585970283,\n",
       "   0.14107517899693747,\n",
       "   0.14145672516760954,\n",
       "   0.1418445846097686,\n",
       "   0.14460051752031083,\n",
       "   0.14476402136362476,\n",
       "   0.1415180026151796,\n",
       "   0.14795976059012472,\n",
       "   0.14691737757291215,\n",
       "   0.14742225287586316,\n",
       "   0.1440677151204381,\n",
       "   0.1475962715191048,\n",
       "   0.14490483501782417,\n",
       "   0.1493337112377051,\n",
       "   0.14778278214544946,\n",
       "   0.15433280812106462,\n",
       "   0.15342207725417986,\n",
       "   0.15358044577072547,\n",
       "   0.15250888163934778,\n",
       "   0.15331022098918082,\n",
       "   0.14973830550921396,\n",
       "   0.15100688571480625,\n",
       "   0.15470710598554832,\n",
       "   0.1603487039679827,\n",
       "   0.15178530292006917,\n",
       "   0.1491020481620154,\n",
       "   0.15288468829212928,\n",
       "   0.1478796544769707,\n",
       "   0.15381602206314668],\n",
       "  'val_accuracy_mean': [0.37611111213763554,\n",
       "   0.4023111122349898,\n",
       "   0.4212222212056319,\n",
       "   0.4462000005443891,\n",
       "   0.468444444835186,\n",
       "   0.4825111120939255,\n",
       "   0.5026888887087504,\n",
       "   0.5147333339850108,\n",
       "   0.5281777766346931,\n",
       "   0.5329111117124558,\n",
       "   0.5455333328247071,\n",
       "   0.5485555542508761,\n",
       "   0.5599999985098839,\n",
       "   0.566444443166256,\n",
       "   0.5693111110726993,\n",
       "   0.5765111099680265,\n",
       "   0.5762222217520078,\n",
       "   0.580444443722566,\n",
       "   0.586622222562631,\n",
       "   0.5926444418231647,\n",
       "   0.595422222216924,\n",
       "   0.6057999989390374,\n",
       "   0.6080666651328405,\n",
       "   0.5976222225030263,\n",
       "   0.6100222221016884,\n",
       "   0.6125999988118808,\n",
       "   0.6146444432934125,\n",
       "   0.6145333325862885,\n",
       "   0.6249555552005768,\n",
       "   0.6264666669567426,\n",
       "   0.6298888885974884,\n",
       "   0.6277111112078031,\n",
       "   0.6289777777592341,\n",
       "   0.6281555566191673,\n",
       "   0.6362444437543551,\n",
       "   0.6315111108620961,\n",
       "   0.6385111106435458,\n",
       "   0.642088887989521,\n",
       "   0.6401999989151954,\n",
       "   0.6423111128807067,\n",
       "   0.6427111104130745,\n",
       "   0.640888888835907,\n",
       "   0.6457111105322838,\n",
       "   0.6498666658004125,\n",
       "   0.6512888882557551,\n",
       "   0.6516222217679024,\n",
       "   0.6472666674852371,\n",
       "   0.6525999994079272,\n",
       "   0.6361999987562498,\n",
       "   0.6511111137270927,\n",
       "   0.6530666665236156,\n",
       "   0.6577111103137334,\n",
       "   0.6591111107667287,\n",
       "   0.6552222230037054,\n",
       "   0.6624444443980853,\n",
       "   0.659155554274718,\n",
       "   0.663777777949969,\n",
       "   0.6661555546522141,\n",
       "   0.6654000013073286,\n",
       "   0.6618222211798032,\n",
       "   0.6599333323041598,\n",
       "   0.6692444444696108,\n",
       "   0.6433777767419815,\n",
       "   0.6686444433530172,\n",
       "   0.674866666396459,\n",
       "   0.670333333214124,\n",
       "   0.6666222230593364,\n",
       "   0.6582444440325101,\n",
       "   0.670444443821907,\n",
       "   0.6663333324591318,\n",
       "   0.6699777791897455,\n",
       "   0.6746666658918062,\n",
       "   0.6719777784744898,\n",
       "   0.6763555556535721,\n",
       "   0.6706888887286186,\n",
       "   0.6691555559635163,\n",
       "   0.6693555561701456,\n",
       "   0.6717555562655131,\n",
       "   0.6679777796069781,\n",
       "   0.6652222222089768,\n",
       "   0.6722666657964389,\n",
       "   0.6749777768055598,\n",
       "   0.6800888884067535,\n",
       "   0.6746222213904063,\n",
       "   0.6683111109336217,\n",
       "   0.6691777769724528,\n",
       "   0.6678888875246048,\n",
       "   0.672488888502121,\n",
       "   0.6740222219626109,\n",
       "   0.6729111108183861,\n",
       "   0.670199998319149,\n",
       "   0.666733333269755,\n",
       "   0.6709777778387069,\n",
       "   0.6677555555105209,\n",
       "   0.6783111099402109,\n",
       "   0.6804888878266017,\n",
       "   0.6720444458723068,\n",
       "   0.6790888901551565,\n",
       "   0.6778888903061548],\n",
       "  'val_accuracy_std': [0.05492744923155846,\n",
       "   0.05420393208492005,\n",
       "   0.05734226844969452,\n",
       "   0.06056221783976439,\n",
       "   0.05957462407962807,\n",
       "   0.060817194965760824,\n",
       "   0.06121267736736936,\n",
       "   0.06275144340034473,\n",
       "   0.06394509889652561,\n",
       "   0.06539882942687392,\n",
       "   0.06304651870194429,\n",
       "   0.06827929235522612,\n",
       "   0.06402082870140965,\n",
       "   0.06355361220678313,\n",
       "   0.06431150656841922,\n",
       "   0.06291303358964855,\n",
       "   0.06143670726054731,\n",
       "   0.06574098156995073,\n",
       "   0.06249620571382885,\n",
       "   0.06298947227094807,\n",
       "   0.06184634055056076,\n",
       "   0.0633437770604199,\n",
       "   0.06131614446419022,\n",
       "   0.06197829565956332,\n",
       "   0.06230509960999539,\n",
       "   0.0636254908537577,\n",
       "   0.06344364329790378,\n",
       "   0.06401189802532384,\n",
       "   0.06370910563202466,\n",
       "   0.0630140842112261,\n",
       "   0.06498195345460098,\n",
       "   0.06322872902443394,\n",
       "   0.06253170739518646,\n",
       "   0.06284612265222293,\n",
       "   0.06442658393505157,\n",
       "   0.06661507793820806,\n",
       "   0.06372166109158685,\n",
       "   0.06656720116030689,\n",
       "   0.06358515049918573,\n",
       "   0.06185280916166752,\n",
       "   0.06212101373298848,\n",
       "   0.06553558975619835,\n",
       "   0.065227623055849,\n",
       "   0.0650548269123059,\n",
       "   0.06361200367016627,\n",
       "   0.06228635917187665,\n",
       "   0.06376643802860245,\n",
       "   0.0643570710406048,\n",
       "   0.06403879658468731,\n",
       "   0.062242855504873545,\n",
       "   0.06462142890332746,\n",
       "   0.0615499040065614,\n",
       "   0.06296932401767025,\n",
       "   0.06453585542003201,\n",
       "   0.06313497302735932,\n",
       "   0.06338733130302601,\n",
       "   0.0613667399975418,\n",
       "   0.06207908583349773,\n",
       "   0.06293814511742186,\n",
       "   0.06286680316277918,\n",
       "   0.06465059505806899,\n",
       "   0.06244066440612822,\n",
       "   0.06433821157026275,\n",
       "   0.06154143106887721,\n",
       "   0.06388778531664496,\n",
       "   0.06282751253225706,\n",
       "   0.06295911369119532,\n",
       "   0.064901290059545,\n",
       "   0.06226308379975616,\n",
       "   0.06393312550215524,\n",
       "   0.061230587513150136,\n",
       "   0.06220634829837815,\n",
       "   0.06215464223504056,\n",
       "   0.06136031686328819,\n",
       "   0.06280811226607692,\n",
       "   0.06154093741443849,\n",
       "   0.0632832514976096,\n",
       "   0.06222563320260837,\n",
       "   0.06116328832554499,\n",
       "   0.0623739233571983,\n",
       "   0.0632435879162728,\n",
       "   0.060007094703622844,\n",
       "   0.06243212262226288,\n",
       "   0.06106315846299092,\n",
       "   0.06276558043156305,\n",
       "   0.06171432017128471,\n",
       "   0.06267956832975695,\n",
       "   0.06301462228544438,\n",
       "   0.061667924763994506,\n",
       "   0.06232031632405127,\n",
       "   0.06212142227699969,\n",
       "   0.06177494478118117,\n",
       "   0.06413937119983815,\n",
       "   0.06359710878297109,\n",
       "   0.06165945619321875,\n",
       "   0.06037952536275093,\n",
       "   0.06171954285628396,\n",
       "   0.06012997868930779,\n",
       "   0.05991959842385007],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2ada4",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f54da463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAMLFewShotClassifier(\n",
       "  (classifier): VGGReLUNormNetwork(\n",
       "    (layer_dict): ModuleDict(\n",
       "      (conv0): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (linear): MetaLinearLayer()\n",
       "    )\n",
       "  )\n",
       "  (inner_loop_optimizer): GradientDescentLearningRule()\n",
       "  (arbiter): Arbiter(\n",
       "    (linear1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation1): ReLU(inplace=True)\n",
       "    (linear2): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (activation2): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "maml_system.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbfddb9",
   "metadata": {},
   "source": [
    "# 2. Samlpe 별 loss 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b81648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the loss of 1\n",
      "Calculating the loss of 2\n",
      "Calculating the loss of 3\n",
      "Calculating the loss of 4\n",
      "Calculating the loss of 5\n",
      "Calculating the loss of 6\n",
      "Calculating the loss of 7\n",
      "Calculating the loss of 8\n",
      "Calculating the loss of 9\n",
      "Calculating the loss of 10\n",
      "Calculating the loss of 11\n",
      "Calculating the loss of 12\n",
      "Calculating the loss of 13\n",
      "Calculating the loss of 14\n",
      "Calculating the loss of 15\n",
      "Calculating the loss of 16\n",
      "Calculating the loss of 17\n",
      "Calculating the loss of 18\n",
      "Calculating the loss of 19\n",
      "Calculating the loss of 20\n",
      "Calculating the loss of 21\n",
      "Calculating the loss of 22\n",
      "Calculating the loss of 23\n",
      "Calculating the loss of 24\n",
      "Calculating the loss of 25\n",
      "Calculating the loss of 26\n",
      "Calculating the loss of 27\n",
      "Calculating the loss of 28\n",
      "Calculating the loss of 29\n",
      "Calculating the loss of 30\n",
      "Calculating the loss of 31\n",
      "Calculating the loss of 32\n",
      "Calculating the loss of 33\n",
      "Calculating the loss of 34\n",
      "Calculating the loss of 35\n",
      "Calculating the loss of 36\n",
      "Calculating the loss of 37\n",
      "Calculating the loss of 38\n",
      "Calculating the loss of 39\n",
      "Calculating the loss of 40\n",
      "Calculating the loss of 41\n",
      "Calculating the loss of 42\n",
      "Calculating the loss of 43\n",
      "Calculating the loss of 44\n",
      "Calculating the loss of 45\n",
      "Calculating the loss of 46\n",
      "Calculating the loss of 47\n",
      "Calculating the loss of 48\n",
      "Calculating the loss of 49\n",
      "Calculating the loss of 50\n",
      "Calculating the loss of 51\n",
      "Calculating the loss of 52\n",
      "Calculating the loss of 53\n",
      "Calculating the loss of 54\n",
      "Calculating the loss of 55\n",
      "Calculating the loss of 56\n",
      "Calculating the loss of 57\n",
      "Calculating the loss of 58\n",
      "Calculating the loss of 59\n",
      "Calculating the loss of 60\n",
      "Calculating the loss of 61\n",
      "Calculating the loss of 62\n",
      "Calculating the loss of 63\n",
      "Calculating the loss of 64\n",
      "Calculating the loss of 65\n",
      "Calculating the loss of 66\n",
      "Calculating the loss of 67\n",
      "Calculating the loss of 68\n",
      "Calculating the loss of 69\n",
      "Calculating the loss of 70\n",
      "Calculating the loss of 71\n",
      "Calculating the loss of 72\n",
      "Calculating the loss of 73\n",
      "Calculating the loss of 74\n",
      "Calculating the loss of 75\n",
      "Calculating the loss of 76\n",
      "Calculating the loss of 77\n",
      "Calculating the loss of 78\n",
      "Calculating the loss of 79\n",
      "Calculating the loss of 80\n",
      "Calculating the loss of 81\n",
      "Calculating the loss of 82\n",
      "Calculating the loss of 83\n",
      "Calculating the loss of 84\n",
      "Calculating the loss of 85\n",
      "Calculating the loss of 86\n",
      "Calculating the loss of 87\n",
      "Calculating the loss of 88\n",
      "Calculating the loss of 89\n",
      "Calculating the loss of 90\n",
      "Calculating the loss of 91\n",
      "Calculating the loss of 92\n",
      "Calculating the loss of 93\n",
      "Calculating the loss of 94\n",
      "Calculating the loss of 95\n",
      "Calculating the loss of 96\n",
      "Calculating the loss of 97\n",
      "Calculating the loss of 98\n",
      "Calculating the loss of 99\n",
      "Calculating the loss of 100\n",
      "Calculating the loss of 101\n",
      "Calculating the loss of 102\n",
      "Calculating the loss of 103\n",
      "Calculating the loss of 104\n",
      "Calculating the loss of 105\n",
      "Calculating the loss of 106\n",
      "Calculating the loss of 107\n",
      "Calculating the loss of 108\n",
      "Calculating the loss of 109\n",
      "Calculating the loss of 110\n",
      "Calculating the loss of 111\n",
      "Calculating the loss of 112\n",
      "Calculating the loss of 113\n",
      "Calculating the loss of 114\n",
      "Calculating the loss of 115\n",
      "Calculating the loss of 116\n",
      "Calculating the loss of 117\n",
      "Calculating the loss of 118\n",
      "Calculating the loss of 119\n",
      "Calculating the loss of 120\n",
      "Calculating the loss of 121\n",
      "Calculating the loss of 122\n",
      "Calculating the loss of 123\n",
      "Calculating the loss of 124\n",
      "Calculating the loss of 125\n",
      "Calculating the loss of 126\n",
      "Calculating the loss of 127\n",
      "Calculating the loss of 128\n",
      "Calculating the loss of 129\n",
      "Calculating the loss of 130\n",
      "Calculating the loss of 131\n",
      "Calculating the loss of 132\n",
      "Calculating the loss of 133\n",
      "Calculating the loss of 134\n",
      "Calculating the loss of 135\n",
      "Calculating the loss of 136\n",
      "Calculating the loss of 137\n",
      "Calculating the loss of 138\n",
      "Calculating the loss of 139\n",
      "Calculating the loss of 140\n",
      "Calculating the loss of 141\n",
      "Calculating the loss of 142\n",
      "Calculating the loss of 143\n",
      "Calculating the loss of 144\n",
      "Calculating the loss of 145\n",
      "Calculating the loss of 146\n",
      "Calculating the loss of 147\n",
      "Calculating the loss of 148\n",
      "Calculating the loss of 149\n",
      "Calculating the loss of 150\n",
      "Calculating the loss of 151\n",
      "Calculating the loss of 152\n",
      "Calculating the loss of 153\n",
      "Calculating the loss of 154\n",
      "Calculating the loss of 155\n",
      "Calculating the loss of 156\n",
      "Calculating the loss of 157\n",
      "Calculating the loss of 158\n",
      "Calculating the loss of 159\n",
      "Calculating the loss of 160\n",
      "Calculating the loss of 161\n",
      "Calculating the loss of 162\n",
      "Calculating the loss of 163\n",
      "Calculating the loss of 164\n",
      "Calculating the loss of 165\n",
      "Calculating the loss of 166\n",
      "Calculating the loss of 167\n",
      "Calculating the loss of 168\n",
      "Calculating the loss of 169\n",
      "Calculating the loss of 170\n",
      "Calculating the loss of 171\n",
      "Calculating the loss of 172\n",
      "Calculating the loss of 173\n",
      "Calculating the loss of 174\n",
      "Calculating the loss of 175\n",
      "Calculating the loss of 176\n",
      "Calculating the loss of 177\n",
      "Calculating the loss of 178\n",
      "Calculating the loss of 179\n",
      "Calculating the loss of 180\n",
      "Calculating the loss of 181\n",
      "Calculating the loss of 182\n",
      "Calculating the loss of 183\n",
      "Calculating the loss of 184\n",
      "Calculating the loss of 185\n",
      "Calculating the loss of 186\n",
      "Calculating the loss of 187\n",
      "Calculating the loss of 188\n",
      "Calculating the loss of 189\n",
      "Calculating the loss of 190\n",
      "Calculating the loss of 191\n",
      "Calculating the loss of 192\n",
      "Calculating the loss of 193\n",
      "Calculating the loss of 194\n",
      "Calculating the loss of 195\n",
      "Calculating the loss of 196\n",
      "Calculating the loss of 197\n",
      "Calculating the loss of 198\n",
      "Calculating the loss of 199\n",
      "Calculating the loss of 200\n",
      "Calculating the loss of 201\n",
      "Calculating the loss of 202\n",
      "Calculating the loss of 203\n",
      "Calculating the loss of 204\n",
      "Calculating the loss of 205\n",
      "Calculating the loss of 206\n",
      "Calculating the loss of 207\n",
      "Calculating the loss of 208\n",
      "Calculating the loss of 209\n",
      "Calculating the loss of 210\n",
      "Calculating the loss of 211\n",
      "Calculating the loss of 212\n",
      "Calculating the loss of 213\n",
      "Calculating the loss of 214\n",
      "Calculating the loss of 215\n",
      "Calculating the loss of 216\n",
      "Calculating the loss of 217\n",
      "Calculating the loss of 218\n",
      "Calculating the loss of 219\n",
      "Calculating the loss of 220\n",
      "Calculating the loss of 221\n",
      "Calculating the loss of 222\n",
      "Calculating the loss of 223\n",
      "Calculating the loss of 224\n",
      "Calculating the loss of 225\n",
      "Calculating the loss of 226\n",
      "Calculating the loss of 227\n",
      "Calculating the loss of 228\n",
      "Calculating the loss of 229\n",
      "Calculating the loss of 230\n",
      "Calculating the loss of 231\n",
      "Calculating the loss of 232\n",
      "Calculating the loss of 233\n",
      "Calculating the loss of 234\n",
      "Calculating the loss of 235\n",
      "Calculating the loss of 236\n",
      "Calculating the loss of 237\n",
      "Calculating the loss of 238\n",
      "Calculating the loss of 239\n",
      "Calculating the loss of 240\n",
      "Calculating the loss of 241\n",
      "Calculating the loss of 242\n",
      "Calculating the loss of 243\n",
      "Calculating the loss of 244\n",
      "Calculating the loss of 245\n",
      "Calculating the loss of 246\n",
      "Calculating the loss of 247\n",
      "Calculating the loss of 248\n",
      "Calculating the loss of 249\n",
      "Calculating the loss of 250\n",
      "Calculating the loss of 251\n",
      "Calculating the loss of 252\n",
      "Calculating the loss of 253\n",
      "Calculating the loss of 254\n",
      "Calculating the loss of 255\n",
      "Calculating the loss of 256\n",
      "Calculating the loss of 257\n",
      "Calculating the loss of 258\n",
      "Calculating the loss of 259\n",
      "Calculating the loss of 260\n",
      "Calculating the loss of 261\n",
      "Calculating the loss of 262\n",
      "Calculating the loss of 263\n",
      "Calculating the loss of 264\n",
      "Calculating the loss of 265\n",
      "Calculating the loss of 266\n",
      "Calculating the loss of 267\n",
      "Calculating the loss of 268\n",
      "Calculating the loss of 269\n",
      "Calculating the loss of 270\n",
      "Calculating the loss of 271\n",
      "Calculating the loss of 272\n",
      "Calculating the loss of 273\n",
      "Calculating the loss of 274\n",
      "Calculating the loss of 275\n",
      "Calculating the loss of 276\n",
      "Calculating the loss of 277\n",
      "Calculating the loss of 278\n",
      "Calculating the loss of 279\n",
      "Calculating the loss of 280\n",
      "Calculating the loss of 281\n",
      "Calculating the loss of 282\n",
      "Calculating the loss of 283\n",
      "Calculating the loss of 284\n",
      "Calculating the loss of 285\n",
      "Calculating the loss of 286\n",
      "Calculating the loss of 287\n",
      "Calculating the loss of 288\n",
      "Calculating the loss of 289\n",
      "Calculating the loss of 290\n",
      "Calculating the loss of 291\n",
      "Calculating the loss of 292\n",
      "Calculating the loss of 293\n",
      "Calculating the loss of 294\n",
      "Calculating the loss of 295\n",
      "Calculating the loss of 296\n",
      "Calculating the loss of 297\n",
      "Calculating the loss of 298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the loss of 299\n",
      "Calculating the loss of 300\n",
      "Calculating the loss of 301\n",
      "Calculating the loss of 302\n",
      "Calculating the loss of 303\n",
      "Calculating the loss of 304\n",
      "Calculating the loss of 305\n",
      "Calculating the loss of 306\n",
      "Calculating the loss of 307\n",
      "Calculating the loss of 308\n",
      "Calculating the loss of 309\n",
      "Calculating the loss of 310\n",
      "Calculating the loss of 311\n",
      "Calculating the loss of 312\n",
      "Calculating the loss of 313\n",
      "Calculating the loss of 314\n",
      "Calculating the loss of 315\n",
      "Calculating the loss of 316\n",
      "Calculating the loss of 317\n",
      "Calculating the loss of 318\n",
      "Calculating the loss of 319\n",
      "Calculating the loss of 320\n",
      "Calculating the loss of 321\n",
      "Calculating the loss of 322\n",
      "Calculating the loss of 323\n",
      "Calculating the loss of 324\n",
      "Calculating the loss of 325\n",
      "Calculating the loss of 326\n",
      "Calculating the loss of 327\n",
      "Calculating the loss of 328\n",
      "Calculating the loss of 329\n",
      "Calculating the loss of 330\n",
      "Calculating the loss of 331\n",
      "Calculating the loss of 332\n",
      "Calculating the loss of 333\n",
      "Calculating the loss of 334\n",
      "Calculating the loss of 335\n",
      "Calculating the loss of 336\n",
      "Calculating the loss of 337\n",
      "Calculating the loss of 338\n",
      "Calculating the loss of 339\n",
      "Calculating the loss of 340\n",
      "Calculating the loss of 341\n",
      "Calculating the loss of 342\n",
      "Calculating the loss of 343\n",
      "Calculating the loss of 344\n",
      "Calculating the loss of 345\n",
      "Calculating the loss of 346\n",
      "Calculating the loss of 347\n",
      "Calculating the loss of 348\n",
      "Calculating the loss of 349\n",
      "Calculating the loss of 350\n",
      "Calculating the loss of 351\n",
      "Calculating the loss of 352\n",
      "Calculating the loss of 353\n",
      "Calculating the loss of 354\n",
      "Calculating the loss of 355\n",
      "Calculating the loss of 356\n",
      "Calculating the loss of 357\n",
      "Calculating the loss of 358\n",
      "Calculating the loss of 359\n",
      "Calculating the loss of 360\n",
      "Calculating the loss of 361\n",
      "Calculating the loss of 362\n",
      "Calculating the loss of 363\n",
      "Calculating the loss of 364\n",
      "Calculating the loss of 365\n",
      "Calculating the loss of 366\n",
      "Calculating the loss of 367\n",
      "Calculating the loss of 368\n",
      "Calculating the loss of 369\n",
      "Calculating the loss of 370\n",
      "Calculating the loss of 371\n",
      "Calculating the loss of 372\n",
      "Calculating the loss of 373\n",
      "Calculating the loss of 374\n",
      "Calculating the loss of 375\n",
      "Calculating the loss of 376\n",
      "Calculating the loss of 377\n",
      "Calculating the loss of 378\n",
      "Calculating the loss of 379\n",
      "Calculating the loss of 380\n",
      "Calculating the loss of 381\n",
      "Calculating the loss of 382\n",
      "Calculating the loss of 383\n",
      "Calculating the loss of 384\n",
      "Calculating the loss of 385\n",
      "Calculating the loss of 386\n",
      "Calculating the loss of 387\n",
      "Calculating the loss of 388\n",
      "Calculating the loss of 389\n",
      "Calculating the loss of 390\n",
      "Calculating the loss of 391\n",
      "Calculating the loss of 392\n",
      "Calculating the loss of 393\n",
      "Calculating the loss of 394\n",
      "Calculating the loss of 395\n",
      "Calculating the loss of 396\n",
      "Calculating the loss of 397\n",
      "Calculating the loss of 398\n",
      "Calculating the loss of 399\n",
      "Calculating the loss of 400\n",
      "Calculating the loss of 401\n",
      "Calculating the loss of 402\n",
      "Calculating the loss of 403\n",
      "Calculating the loss of 404\n",
      "Calculating the loss of 405\n",
      "Calculating the loss of 406\n",
      "Calculating the loss of 407\n",
      "Calculating the loss of 408\n",
      "Calculating the loss of 409\n",
      "Calculating the loss of 410\n",
      "Calculating the loss of 411\n",
      "Calculating the loss of 412\n",
      "Calculating the loss of 413\n",
      "Calculating the loss of 414\n",
      "Calculating the loss of 415\n",
      "Calculating the loss of 416\n",
      "Calculating the loss of 417\n",
      "Calculating the loss of 418\n",
      "Calculating the loss of 419\n",
      "Calculating the loss of 420\n",
      "Calculating the loss of 421\n",
      "Calculating the loss of 422\n",
      "Calculating the loss of 423\n",
      "Calculating the loss of 424\n",
      "Calculating the loss of 425\n",
      "Calculating the loss of 426\n",
      "Calculating the loss of 427\n",
      "Calculating the loss of 428\n",
      "Calculating the loss of 429\n",
      "Calculating the loss of 430\n",
      "Calculating the loss of 431\n",
      "Calculating the loss of 432\n",
      "Calculating the loss of 433\n",
      "Calculating the loss of 434\n",
      "Calculating the loss of 435\n",
      "Calculating the loss of 436\n",
      "Calculating the loss of 437\n",
      "Calculating the loss of 438\n",
      "Calculating the loss of 439\n",
      "Calculating the loss of 440\n",
      "Calculating the loss of 441\n",
      "Calculating the loss of 442\n",
      "Calculating the loss of 443\n",
      "Calculating the loss of 444\n",
      "Calculating the loss of 445\n",
      "Calculating the loss of 446\n",
      "Calculating the loss of 447\n",
      "Calculating the loss of 448\n",
      "Calculating the loss of 449\n",
      "Calculating the loss of 450\n",
      "Calculating the loss of 451\n",
      "Calculating the loss of 452\n",
      "Calculating the loss of 453\n",
      "Calculating the loss of 454\n",
      "Calculating the loss of 455\n",
      "Calculating the loss of 456\n",
      "Calculating the loss of 457\n",
      "Calculating the loss of 458\n",
      "Calculating the loss of 459\n",
      "Calculating the loss of 460\n",
      "Calculating the loss of 461\n",
      "Calculating the loss of 462\n",
      "Calculating the loss of 463\n",
      "Calculating the loss of 464\n",
      "Calculating the loss of 465\n",
      "Calculating the loss of 466\n",
      "Calculating the loss of 467\n",
      "Calculating the loss of 468\n",
      "Calculating the loss of 469\n",
      "Calculating the loss of 470\n",
      "Calculating the loss of 471\n",
      "Calculating the loss of 472\n",
      "Calculating the loss of 473\n",
      "Calculating the loss of 474\n",
      "Calculating the loss of 475\n",
      "Calculating the loss of 476\n",
      "Calculating the loss of 477\n",
      "Calculating the loss of 478\n",
      "Calculating the loss of 479\n",
      "Calculating the loss of 480\n",
      "Calculating the loss of 481\n",
      "Calculating the loss of 482\n",
      "Calculating the loss of 483\n",
      "Calculating the loss of 484\n",
      "Calculating the loss of 485\n",
      "Calculating the loss of 486\n",
      "Calculating the loss of 487\n",
      "Calculating the loss of 488\n",
      "Calculating the loss of 489\n",
      "Calculating the loss of 490\n",
      "Calculating the loss of 491\n",
      "Calculating the loss of 492\n",
      "Calculating the loss of 493\n",
      "Calculating the loss of 494\n",
      "Calculating the loss of 495\n",
      "Calculating the loss of 496\n",
      "Calculating the loss of 497\n",
      "Calculating the loss of 498\n",
      "Calculating the loss of 499\n",
      "Calculating the loss of 500\n",
      "Calculating the loss of 501\n",
      "Calculating the loss of 502\n",
      "Calculating the loss of 503\n",
      "Calculating the loss of 504\n",
      "Calculating the loss of 505\n",
      "Calculating the loss of 506\n",
      "Calculating the loss of 507\n",
      "Calculating the loss of 508\n",
      "Calculating the loss of 509\n",
      "Calculating the loss of 510\n",
      "Calculating the loss of 511\n",
      "Calculating the loss of 512\n",
      "Calculating the loss of 513\n",
      "Calculating the loss of 514\n",
      "Calculating the loss of 515\n",
      "Calculating the loss of 516\n",
      "Calculating the loss of 517\n",
      "Calculating the loss of 518\n",
      "Calculating the loss of 519\n",
      "Calculating the loss of 520\n",
      "Calculating the loss of 521\n",
      "Calculating the loss of 522\n",
      "Calculating the loss of 523\n",
      "Calculating the loss of 524\n",
      "Calculating the loss of 525\n",
      "Calculating the loss of 526\n",
      "Calculating the loss of 527\n",
      "Calculating the loss of 528\n",
      "Calculating the loss of 529\n",
      "Calculating the loss of 530\n",
      "Calculating the loss of 531\n",
      "Calculating the loss of 532\n",
      "Calculating the loss of 533\n",
      "Calculating the loss of 534\n",
      "Calculating the loss of 535\n",
      "Calculating the loss of 536\n",
      "Calculating the loss of 537\n",
      "Calculating the loss of 538\n",
      "Calculating the loss of 539\n",
      "Calculating the loss of 540\n",
      "Calculating the loss of 541\n",
      "Calculating the loss of 542\n",
      "Calculating the loss of 543\n",
      "Calculating the loss of 544\n",
      "Calculating the loss of 545\n",
      "Calculating the loss of 546\n",
      "Calculating the loss of 547\n",
      "Calculating the loss of 548\n",
      "Calculating the loss of 549\n",
      "Calculating the loss of 550\n",
      "Calculating the loss of 551\n",
      "Calculating the loss of 552\n",
      "Calculating the loss of 553\n",
      "Calculating the loss of 554\n",
      "Calculating the loss of 555\n",
      "Calculating the loss of 556\n",
      "Calculating the loss of 557\n",
      "Calculating the loss of 558\n",
      "Calculating the loss of 559\n",
      "Calculating the loss of 560\n",
      "Calculating the loss of 561\n",
      "Calculating the loss of 562\n",
      "Calculating the loss of 563\n",
      "Calculating the loss of 564\n",
      "Calculating the loss of 565\n",
      "Calculating the loss of 566\n",
      "Calculating the loss of 567\n",
      "Calculating the loss of 568\n",
      "Calculating the loss of 569\n",
      "Calculating the loss of 570\n",
      "Calculating the loss of 571\n",
      "Calculating the loss of 572\n",
      "Calculating the loss of 573\n",
      "Calculating the loss of 574\n",
      "Calculating the loss of 575\n",
      "Calculating the loss of 576\n",
      "Calculating the loss of 577\n",
      "Calculating the loss of 578\n",
      "Calculating the loss of 579\n",
      "Calculating the loss of 580\n",
      "Calculating the loss of 581\n",
      "Calculating the loss of 582\n",
      "Calculating the loss of 583\n",
      "Calculating the loss of 584\n",
      "Calculating the loss of 585\n",
      "Calculating the loss of 586\n",
      "Calculating the loss of 587\n",
      "Calculating the loss of 588\n",
      "Calculating the loss of 589\n",
      "Calculating the loss of 590\n",
      "Calculating the loss of 591\n",
      "Calculating the loss of 592\n",
      "Calculating the loss of 593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the loss of 594\n",
      "Calculating the loss of 595\n",
      "Calculating the loss of 596\n",
      "Calculating the loss of 597\n",
      "Calculating the loss of 598\n",
      "Calculating the loss of 599\n",
      "Calculating the loss of 600\n",
      "Calculating the loss of 601\n",
      "Calculating the loss of 602\n",
      "Calculating the loss of 603\n",
      "Calculating the loss of 604\n",
      "Calculating the loss of 605\n",
      "Calculating the loss of 606\n",
      "Calculating the loss of 607\n",
      "Calculating the loss of 608\n",
      "Calculating the loss of 609\n",
      "Calculating the loss of 610\n",
      "Calculating the loss of 611\n",
      "Calculating the loss of 612\n",
      "Calculating the loss of 613\n",
      "Calculating the loss of 614\n",
      "Calculating the loss of 615\n",
      "Calculating the loss of 616\n",
      "Calculating the loss of 617\n",
      "Calculating the loss of 618\n",
      "Calculating the loss of 619\n",
      "Calculating the loss of 620\n",
      "Calculating the loss of 621\n",
      "Calculating the loss of 622\n",
      "Calculating the loss of 623\n",
      "Calculating the loss of 624\n",
      "Calculating the loss of 625\n",
      "Calculating the loss of 626\n",
      "Calculating the loss of 627\n",
      "Calculating the loss of 628\n",
      "Calculating the loss of 629\n",
      "Calculating the loss of 630\n",
      "Calculating the loss of 631\n",
      "Calculating the loss of 632\n",
      "Calculating the loss of 633\n",
      "Calculating the loss of 634\n",
      "Calculating the loss of 635\n",
      "Calculating the loss of 636\n",
      "Calculating the loss of 637\n",
      "Calculating the loss of 638\n",
      "Calculating the loss of 639\n",
      "Calculating the loss of 640\n",
      "Calculating the loss of 641\n",
      "Calculating the loss of 642\n",
      "Calculating the loss of 643\n",
      "Calculating the loss of 644\n",
      "Calculating the loss of 645\n",
      "Calculating the loss of 646\n",
      "Calculating the loss of 647\n",
      "Calculating the loss of 648\n",
      "Calculating the loss of 649\n",
      "Calculating the loss of 650\n",
      "Calculating the loss of 651\n",
      "Calculating the loss of 652\n",
      "Calculating the loss of 653\n",
      "Calculating the loss of 654\n",
      "Calculating the loss of 655\n",
      "Calculating the loss of 656\n",
      "Calculating the loss of 657\n",
      "Calculating the loss of 658\n",
      "Calculating the loss of 659\n",
      "Calculating the loss of 660\n",
      "Calculating the loss of 661\n",
      "Calculating the loss of 662\n",
      "Calculating the loss of 663\n",
      "Calculating the loss of 664\n",
      "Calculating the loss of 665\n",
      "Calculating the loss of 666\n",
      "Calculating the loss of 667\n",
      "Calculating the loss of 668\n",
      "Calculating the loss of 669\n",
      "Calculating the loss of 670\n",
      "Calculating the loss of 671\n",
      "Calculating the loss of 672\n",
      "Calculating the loss of 673\n",
      "Calculating the loss of 674\n",
      "Calculating the loss of 675\n",
      "Calculating the loss of 676\n",
      "Calculating the loss of 677\n",
      "Calculating the loss of 678\n",
      "Calculating the loss of 679\n",
      "Calculating the loss of 680\n",
      "Calculating the loss of 681\n",
      "Calculating the loss of 682\n",
      "Calculating the loss of 683\n",
      "Calculating the loss of 684\n",
      "Calculating the loss of 685\n",
      "Calculating the loss of 686\n",
      "Calculating the loss of 687\n",
      "Calculating the loss of 688\n",
      "Calculating the loss of 689\n",
      "Calculating the loss of 690\n",
      "Calculating the loss of 691\n",
      "Calculating the loss of 692\n",
      "Calculating the loss of 693\n",
      "Calculating the loss of 694\n",
      "Calculating the loss of 695\n",
      "Calculating the loss of 696\n",
      "Calculating the loss of 697\n",
      "Calculating the loss of 698\n",
      "Calculating the loss of 699\n",
      "Calculating the loss of 700\n",
      "Calculating the loss of 701\n",
      "Calculating the loss of 702\n",
      "Calculating the loss of 703\n",
      "Calculating the loss of 704\n",
      "Calculating the loss of 705\n",
      "Calculating the loss of 706\n",
      "Calculating the loss of 707\n",
      "Calculating the loss of 708\n",
      "Calculating the loss of 709\n",
      "Calculating the loss of 710\n",
      "Calculating the loss of 711\n",
      "Calculating the loss of 712\n",
      "Calculating the loss of 713\n",
      "Calculating the loss of 714\n",
      "Calculating the loss of 715\n",
      "Calculating the loss of 716\n",
      "Calculating the loss of 717\n",
      "Calculating the loss of 718\n",
      "Calculating the loss of 719\n",
      "Calculating the loss of 720\n",
      "Calculating the loss of 721\n",
      "Calculating the loss of 722\n",
      "Calculating the loss of 723\n",
      "Calculating the loss of 724\n",
      "Calculating the loss of 725\n",
      "Calculating the loss of 726\n",
      "Calculating the loss of 727\n",
      "Calculating the loss of 728\n",
      "Calculating the loss of 729\n",
      "Calculating the loss of 730\n",
      "Calculating the loss of 731\n",
      "Calculating the loss of 732\n",
      "Calculating the loss of 733\n",
      "Calculating the loss of 734\n",
      "Calculating the loss of 735\n",
      "Calculating the loss of 736\n",
      "Calculating the loss of 737\n",
      "Calculating the loss of 738\n",
      "Calculating the loss of 739\n",
      "Calculating the loss of 740\n",
      "Calculating the loss of 741\n",
      "Calculating the loss of 742\n",
      "Calculating the loss of 743\n",
      "Calculating the loss of 744\n",
      "Calculating the loss of 745\n",
      "Calculating the loss of 746\n",
      "Calculating the loss of 747\n",
      "Calculating the loss of 748\n",
      "Calculating the loss of 749\n",
      "Calculating the loss of 750\n",
      "Calculating the loss of 751\n",
      "Calculating the loss of 752\n",
      "Calculating the loss of 753\n",
      "Calculating the loss of 754\n",
      "Calculating the loss of 755\n",
      "Calculating the loss of 756\n",
      "Calculating the loss of 757\n",
      "Calculating the loss of 758\n",
      "Calculating the loss of 759\n",
      "Calculating the loss of 760\n",
      "Calculating the loss of 761\n",
      "Calculating the loss of 762\n",
      "Calculating the loss of 763\n",
      "Calculating the loss of 764\n",
      "Calculating the loss of 765\n",
      "Calculating the loss of 766\n",
      "Calculating the loss of 767\n",
      "Calculating the loss of 768\n",
      "Calculating the loss of 769\n",
      "Calculating the loss of 770\n",
      "Calculating the loss of 771\n",
      "Calculating the loss of 772\n",
      "Calculating the loss of 773\n",
      "Calculating the loss of 774\n",
      "Calculating the loss of 775\n",
      "Calculating the loss of 776\n",
      "Calculating the loss of 777\n",
      "Calculating the loss of 778\n",
      "Calculating the loss of 779\n",
      "Calculating the loss of 780\n",
      "Calculating the loss of 781\n",
      "Calculating the loss of 782\n",
      "Calculating the loss of 783\n",
      "Calculating the loss of 784\n",
      "Calculating the loss of 785\n",
      "Calculating the loss of 786\n",
      "Calculating the loss of 787\n",
      "Calculating the loss of 788\n",
      "Calculating the loss of 789\n",
      "Calculating the loss of 790\n",
      "Calculating the loss of 791\n",
      "Calculating the loss of 792\n",
      "Calculating the loss of 793\n",
      "Calculating the loss of 794\n",
      "Calculating the loss of 795\n",
      "Calculating the loss of 796\n",
      "Calculating the loss of 797\n",
      "Calculating the loss of 798\n",
      "Calculating the loss of 799\n",
      "Calculating the loss of 800\n",
      "Calculating the loss of 801\n",
      "Calculating the loss of 802\n",
      "Calculating the loss of 803\n",
      "Calculating the loss of 804\n",
      "Calculating the loss of 805\n",
      "Calculating the loss of 806\n",
      "Calculating the loss of 807\n",
      "Calculating the loss of 808\n",
      "Calculating the loss of 809\n",
      "Calculating the loss of 810\n",
      "Calculating the loss of 811\n",
      "Calculating the loss of 812\n",
      "Calculating the loss of 813\n",
      "Calculating the loss of 814\n",
      "Calculating the loss of 815\n",
      "Calculating the loss of 816\n",
      "Calculating the loss of 817\n",
      "Calculating the loss of 818\n",
      "Calculating the loss of 819\n",
      "Calculating the loss of 820\n",
      "Calculating the loss of 821\n",
      "Calculating the loss of 822\n",
      "Calculating the loss of 823\n",
      "Calculating the loss of 824\n",
      "Calculating the loss of 825\n",
      "Calculating the loss of 826\n",
      "Calculating the loss of 827\n",
      "Calculating the loss of 828\n",
      "Calculating the loss of 829\n",
      "Calculating the loss of 830\n",
      "Calculating the loss of 831\n",
      "Calculating the loss of 832\n",
      "Calculating the loss of 833\n",
      "Calculating the loss of 834\n",
      "Calculating the loss of 835\n",
      "Calculating the loss of 836\n",
      "Calculating the loss of 837\n",
      "Calculating the loss of 838\n",
      "Calculating the loss of 839\n",
      "Calculating the loss of 840\n",
      "Calculating the loss of 841\n",
      "Calculating the loss of 842\n",
      "Calculating the loss of 843\n",
      "Calculating the loss of 844\n",
      "Calculating the loss of 845\n",
      "Calculating the loss of 846\n",
      "Calculating the loss of 847\n",
      "Calculating the loss of 848\n",
      "Calculating the loss of 849\n",
      "Calculating the loss of 850\n",
      "Calculating the loss of 851\n",
      "Calculating the loss of 852\n",
      "Calculating the loss of 853\n",
      "Calculating the loss of 854\n",
      "Calculating the loss of 855\n",
      "Calculating the loss of 856\n",
      "Calculating the loss of 857\n",
      "Calculating the loss of 858\n",
      "Calculating the loss of 859\n",
      "Calculating the loss of 860\n",
      "Calculating the loss of 861\n",
      "Calculating the loss of 862\n",
      "Calculating the loss of 863\n",
      "Calculating the loss of 864\n",
      "Calculating the loss of 865\n",
      "Calculating the loss of 866\n",
      "Calculating the loss of 867\n",
      "Calculating the loss of 868\n",
      "Calculating the loss of 869\n",
      "Calculating the loss of 870\n",
      "Calculating the loss of 871\n",
      "Calculating the loss of 872\n",
      "Calculating the loss of 873\n",
      "Calculating the loss of 874\n",
      "Calculating the loss of 875\n",
      "Calculating the loss of 876\n",
      "Calculating the loss of 877\n",
      "Calculating the loss of 878\n",
      "Calculating the loss of 879\n",
      "Calculating the loss of 880\n",
      "Calculating the loss of 881\n",
      "Calculating the loss of 882\n",
      "Calculating the loss of 883\n",
      "Calculating the loss of 884\n",
      "Calculating the loss of 885\n",
      "Calculating the loss of 886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the loss of 887\n",
      "Calculating the loss of 888\n",
      "Calculating the loss of 889\n",
      "Calculating the loss of 890\n",
      "Calculating the loss of 891\n",
      "Calculating the loss of 892\n",
      "Calculating the loss of 893\n",
      "Calculating the loss of 894\n",
      "Calculating the loss of 895\n",
      "Calculating the loss of 896\n",
      "Calculating the loss of 897\n",
      "Calculating the loss of 898\n",
      "Calculating the loss of 899\n",
      "Calculating the loss of 900\n",
      "Calculating the loss of 901\n",
      "Calculating the loss of 902\n",
      "Calculating the loss of 903\n",
      "Calculating the loss of 904\n",
      "Calculating the loss of 905\n",
      "Calculating the loss of 906\n",
      "Calculating the loss of 907\n",
      "Calculating the loss of 908\n",
      "Calculating the loss of 909\n",
      "Calculating the loss of 910\n",
      "Calculating the loss of 911\n",
      "Calculating the loss of 912\n",
      "Calculating the loss of 913\n",
      "Calculating the loss of 914\n",
      "Calculating the loss of 915\n",
      "Calculating the loss of 916\n",
      "Calculating the loss of 917\n",
      "Calculating the loss of 918\n",
      "Calculating the loss of 919\n",
      "Calculating the loss of 920\n",
      "Calculating the loss of 921\n",
      "Calculating the loss of 922\n",
      "Calculating the loss of 923\n",
      "Calculating the loss of 924\n",
      "Calculating the loss of 925\n",
      "Calculating the loss of 926\n",
      "Calculating the loss of 927\n",
      "Calculating the loss of 928\n",
      "Calculating the loss of 929\n",
      "Calculating the loss of 930\n",
      "Calculating the loss of 931\n",
      "Calculating the loss of 932\n",
      "Calculating the loss of 933\n",
      "Calculating the loss of 934\n",
      "Calculating the loss of 935\n",
      "Calculating the loss of 936\n",
      "Calculating the loss of 937\n",
      "Calculating the loss of 938\n",
      "Calculating the loss of 939\n",
      "Calculating the loss of 940\n",
      "Calculating the loss of 941\n",
      "Calculating the loss of 942\n",
      "Calculating the loss of 943\n",
      "Calculating the loss of 944\n",
      "Calculating the loss of 945\n",
      "Calculating the loss of 946\n",
      "Calculating the loss of 947\n",
      "Calculating the loss of 948\n",
      "Calculating the loss of 949\n",
      "Calculating the loss of 950\n",
      "Calculating the loss of 951\n",
      "Calculating the loss of 952\n",
      "Calculating the loss of 953\n",
      "Calculating the loss of 954\n",
      "Calculating the loss of 955\n",
      "Calculating the loss of 956\n",
      "Calculating the loss of 957\n",
      "Calculating the loss of 958\n",
      "Calculating the loss of 959\n",
      "Calculating the loss of 960\n",
      "Calculating the loss of 961\n",
      "Calculating the loss of 962\n",
      "Calculating the loss of 963\n",
      "Calculating the loss of 964\n",
      "Calculating the loss of 965\n",
      "Calculating the loss of 966\n",
      "Calculating the loss of 967\n",
      "Calculating the loss of 968\n",
      "Calculating the loss of 969\n",
      "Calculating the loss of 970\n",
      "Calculating the loss of 971\n",
      "Calculating the loss of 972\n",
      "Calculating the loss of 973\n",
      "Calculating the loss of 974\n",
      "Calculating the loss of 975\n",
      "Calculating the loss of 976\n",
      "Calculating the loss of 977\n",
      "Calculating the loss of 978\n",
      "Calculating the loss of 979\n",
      "Calculating the loss of 980\n",
      "Calculating the loss of 981\n",
      "Calculating the loss of 982\n",
      "Calculating the loss of 983\n",
      "Calculating the loss of 984\n",
      "Calculating the loss of 985\n",
      "Calculating the loss of 986\n",
      "Calculating the loss of 987\n",
      "Calculating the loss of 988\n",
      "Calculating the loss of 989\n",
      "Calculating the loss of 990\n",
      "Calculating the loss of 991\n",
      "Calculating the loss of 992\n",
      "Calculating the loss of 993\n",
      "Calculating the loss of 994\n",
      "Calculating the loss of 995\n",
      "Calculating the loss of 996\n",
      "Calculating the loss of 997\n",
      "Calculating the loss of 998\n",
      "Calculating the loss of 999\n",
      "Calculating the loss of 1000\n",
      "Calculating the loss of 1001\n",
      "Calculating the loss of 1002\n",
      "Calculating the loss of 1003\n",
      "Calculating the loss of 1004\n",
      "Calculating the loss of 1005\n",
      "Calculating the loss of 1006\n",
      "Calculating the loss of 1007\n",
      "Calculating the loss of 1008\n",
      "Calculating the loss of 1009\n",
      "Calculating the loss of 1010\n",
      "Calculating the loss of 1011\n",
      "Calculating the loss of 1012\n",
      "Calculating the loss of 1013\n",
      "Calculating the loss of 1014\n",
      "Calculating the loss of 1015\n",
      "Calculating the loss of 1016\n",
      "Calculating the loss of 1017\n",
      "Calculating the loss of 1018\n",
      "Calculating the loss of 1019\n",
      "Calculating the loss of 1020\n",
      "Calculating the loss of 1021\n",
      "Calculating the loss of 1022\n",
      "Calculating the loss of 1023\n",
      "Calculating the loss of 1024\n",
      "Calculating the loss of 1025\n",
      "Calculating the loss of 1026\n",
      "Calculating the loss of 1027\n",
      "Calculating the loss of 1028\n",
      "Calculating the loss of 1029\n",
      "Calculating the loss of 1030\n",
      "Calculating the loss of 1031\n",
      "Calculating the loss of 1032\n",
      "Calculating the loss of 1033\n",
      "Calculating the loss of 1034\n",
      "Calculating the loss of 1035\n",
      "Calculating the loss of 1036\n",
      "Calculating the loss of 1037\n",
      "Calculating the loss of 1038\n",
      "Calculating the loss of 1039\n",
      "Calculating the loss of 1040\n",
      "Calculating the loss of 1041\n",
      "Calculating the loss of 1042\n",
      "Calculating the loss of 1043\n",
      "Calculating the loss of 1044\n",
      "Calculating the loss of 1045\n",
      "Calculating the loss of 1046\n",
      "Calculating the loss of 1047\n",
      "Calculating the loss of 1048\n",
      "Calculating the loss of 1049\n",
      "Calculating the loss of 1050\n",
      "Calculating the loss of 1051\n",
      "Calculating the loss of 1052\n",
      "Calculating the loss of 1053\n",
      "Calculating the loss of 1054\n",
      "Calculating the loss of 1055\n",
      "Calculating the loss of 1056\n",
      "Calculating the loss of 1057\n",
      "Calculating the loss of 1058\n",
      "Calculating the loss of 1059\n",
      "Calculating the loss of 1060\n",
      "Calculating the loss of 1061\n",
      "Calculating the loss of 1062\n",
      "Calculating the loss of 1063\n",
      "Calculating the loss of 1064\n",
      "Calculating the loss of 1065\n",
      "Calculating the loss of 1066\n",
      "Calculating the loss of 1067\n",
      "Calculating the loss of 1068\n",
      "Calculating the loss of 1069\n",
      "Calculating the loss of 1070\n",
      "Calculating the loss of 1071\n",
      "Calculating the loss of 1072\n",
      "Calculating the loss of 1073\n",
      "Calculating the loss of 1074\n",
      "Calculating the loss of 1075\n",
      "Calculating the loss of 1076\n",
      "Calculating the loss of 1077\n",
      "Calculating the loss of 1078\n",
      "Calculating the loss of 1079\n",
      "Calculating the loss of 1080\n",
      "Calculating the loss of 1081\n",
      "Calculating the loss of 1082\n",
      "Calculating the loss of 1083\n",
      "Calculating the loss of 1084\n",
      "Calculating the loss of 1085\n",
      "Calculating the loss of 1086\n",
      "Calculating the loss of 1087\n",
      "Calculating the loss of 1088\n",
      "Calculating the loss of 1089\n",
      "Calculating the loss of 1090\n",
      "Calculating the loss of 1091\n",
      "Calculating the loss of 1092\n",
      "Calculating the loss of 1093\n",
      "Calculating the loss of 1094\n",
      "Calculating the loss of 1095\n",
      "Calculating the loss of 1096\n",
      "Calculating the loss of 1097\n",
      "Calculating the loss of 1098\n",
      "Calculating the loss of 1099\n",
      "Calculating the loss of 1100\n",
      "Calculating the loss of 1101\n",
      "Calculating the loss of 1102\n",
      "Calculating the loss of 1103\n",
      "Calculating the loss of 1104\n",
      "Calculating the loss of 1105\n",
      "Calculating the loss of 1106\n",
      "Calculating the loss of 1107\n",
      "Calculating the loss of 1108\n",
      "Calculating the loss of 1109\n",
      "Calculating the loss of 1110\n",
      "Calculating the loss of 1111\n",
      "Calculating the loss of 1112\n",
      "Calculating the loss of 1113\n",
      "Calculating the loss of 1114\n",
      "Calculating the loss of 1115\n",
      "Calculating the loss of 1116\n",
      "Calculating the loss of 1117\n",
      "Calculating the loss of 1118\n",
      "Calculating the loss of 1119\n",
      "Calculating the loss of 1120\n",
      "Calculating the loss of 1121\n",
      "Calculating the loss of 1122\n",
      "Calculating the loss of 1123\n",
      "Calculating the loss of 1124\n",
      "Calculating the loss of 1125\n",
      "Calculating the loss of 1126\n",
      "Calculating the loss of 1127\n",
      "Calculating the loss of 1128\n",
      "Calculating the loss of 1129\n",
      "Calculating the loss of 1130\n",
      "Calculating the loss of 1131\n",
      "Calculating the loss of 1132\n",
      "Calculating the loss of 1133\n",
      "Calculating the loss of 1134\n",
      "Calculating the loss of 1135\n",
      "Calculating the loss of 1136\n",
      "Calculating the loss of 1137\n",
      "Calculating the loss of 1138\n",
      "Calculating the loss of 1139\n",
      "Calculating the loss of 1140\n",
      "Calculating the loss of 1141\n",
      "Calculating the loss of 1142\n",
      "Calculating the loss of 1143\n",
      "Calculating the loss of 1144\n",
      "Calculating the loss of 1145\n",
      "Calculating the loss of 1146\n",
      "Calculating the loss of 1147\n",
      "Calculating the loss of 1148\n",
      "Calculating the loss of 1149\n",
      "Calculating the loss of 1150\n",
      "Calculating the loss of 1151\n",
      "Calculating the loss of 1152\n",
      "Calculating the loss of 1153\n",
      "Calculating the loss of 1154\n",
      "Calculating the loss of 1155\n",
      "Calculating the loss of 1156\n",
      "Calculating the loss of 1157\n",
      "Calculating the loss of 1158\n",
      "Calculating the loss of 1159\n",
      "Calculating the loss of 1160\n",
      "Calculating the loss of 1161\n",
      "Calculating the loss of 1162\n",
      "Calculating the loss of 1163\n",
      "Calculating the loss of 1164\n",
      "Calculating the loss of 1165\n",
      "Calculating the loss of 1166\n",
      "Calculating the loss of 1167\n",
      "Calculating the loss of 1168\n",
      "Calculating the loss of 1169\n",
      "Calculating the loss of 1170\n",
      "Calculating the loss of 1171\n",
      "Calculating the loss of 1172\n",
      "Calculating the loss of 1173\n",
      "Calculating the loss of 1174\n",
      "Calculating the loss of 1175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the loss of 1176\n",
      "Calculating the loss of 1177\n",
      "Calculating the loss of 1178\n",
      "Calculating the loss of 1179\n",
      "Calculating the loss of 1180\n",
      "Calculating the loss of 1181\n",
      "Calculating the loss of 1182\n",
      "Calculating the loss of 1183\n",
      "Calculating the loss of 1184\n",
      "Calculating the loss of 1185\n",
      "Calculating the loss of 1186\n",
      "Calculating the loss of 1187\n",
      "Calculating the loss of 1188\n",
      "Calculating the loss of 1189\n",
      "Calculating the loss of 1190\n",
      "Calculating the loss of 1191\n",
      "Calculating the loss of 1192\n",
      "Calculating the loss of 1193\n",
      "Calculating the loss of 1194\n",
      "Calculating the loss of 1195\n",
      "Calculating the loss of 1196\n",
      "Calculating the loss of 1197\n",
      "Calculating the loss of 1198\n",
      "Calculating the loss of 1199\n",
      "Calculating the loss of 1200\n",
      "Loss_per_sample_Arbiter End\n"
     ]
    }
   ],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "task_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        task_idx = task_idx + 1\n",
    "        print(f\"Calculating the loss of {task_idx}\")\n",
    "        \n",
    "        num_steps=5\n",
    "        for num_step in range(num_steps):            \n",
    "            support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "                    x=x_support_set_task,\n",
    "                    y=y_support_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    backup_running_statistics=num_step == 0,\n",
    "                    training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase='test',\n",
    "                    epoch=0\n",
    "                )\n",
    "            \n",
    "            \n",
    "            # 데이터프레임 생성\n",
    "            df = pd.DataFrame({\n",
    "                'Task_Idx' : [task_idx] * len(support_loss_seperate),\n",
    "                'Step': [num_step] * len(support_loss_seperate),\n",
    "                'Sample_Index': list(range(len(support_loss_seperate))),\n",
    "                'Loss': support_loss_seperate.detach().cpu().numpy()\n",
    "            })\n",
    "\n",
    "            # 파일이 존재하지 않으면 새로운 파일 생성 (mode='w')\n",
    "            if not os.path.exists(csv_file):\n",
    "                df.to_csv(csv_file, mode='w', index=False, encoding='utf-8')\n",
    "            else:\n",
    "                # 파일이 존재하면 append (mode='a')\n",
    "                df.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8')\n",
    "        \n",
    "            generated_alpha_params = {}\n",
    "\n",
    "            if maml_system.model.args.arbiter:\n",
    "                support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                        retain_graph=True)\n",
    "\n",
    "                names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "                per_step_task_embedding = []\n",
    "\n",
    "                for key, weight in names_weights_copy.items():\n",
    "                    weight_norm = torch.norm(weight, p=2)\n",
    "                    per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "                for key, grad in names_grads_copy.items():\n",
    "                    gradient_l2norm = torch.norm(grad, p=2)\n",
    "                    per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "                per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "                per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                            per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "                generated_gradient_rate = maml_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "                g = 0\n",
    "                for key in names_weights_copy.keys():\n",
    "                    generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "                    g += 1\n",
    "\n",
    "            names_weights_copy, names_grads_copy = maml_system.model.apply_inner_loop_update(\n",
    "                loss=support_loss,\n",
    "                support_loss_seperate=support_loss_seperate,\n",
    "                names_weights_copy=names_weights_copy,\n",
    "                alpha=generated_alpha_params,\n",
    "                use_second_order=args.second_order,\n",
    "                current_step_idx=num_step,\n",
    "                current_iter=maml_system.state['current_iter'],\n",
    "                training_phase='test')\n",
    "                      \n",
    "#             if num_step==4:\n",
    "#                 target_loss, target_preds, _, _ = self.net_forward(x=x_target_set_task,\n",
    "#                                                                  y=y_target_set_task, weights=names_weights_copy,\n",
    "#                                                                  backup_running_statistics=False, training=True,\n",
    "#                                                                  num_step=num_step, training_phase=training_phase,\n",
    "#                                                                  epoch=epoch)\n",
    "\n",
    "        ##########Inner loop 종료 ############\n",
    "\n",
    "print(\"Loss_per_sample_Arbiter End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f62080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
