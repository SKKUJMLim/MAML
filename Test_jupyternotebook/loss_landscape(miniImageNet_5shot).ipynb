{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16249129",
   "metadata": {},
   "source": [
    "## [참고]\n",
    "### https://cocoa-t.tistory.com/entry/PyHessian-Loss-Landscape-%EC%8B%9C%EA%B0%81%ED%99%94-PyHessian-Neural-Networks-Through-the-Lens-of-the-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af476",
   "metadata": {},
   "source": [
    "# 0. Dataset 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7235fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"mini_imagenet_full_size\"\n",
    "# dataset=\"tiered_imagenet\"\n",
    "# dataset=\"CIFAR_FS\"\n",
    "# dataset=\"CUB\"\n",
    "\n",
    "title = 'miniImageNet'\n",
    "# title = 'tieredImageNet'\n",
    "# title = 'CIFAR-FS'\n",
    "# title = 'CUB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005193c",
   "metadata": {},
   "source": [
    "# 1. MAML 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8f0d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_maml = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_filter128\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_maml.im_shape = (2, 3, args_maml.image_height, args_maml.image_width)\n",
    "\n",
    "args_maml.use_cuda = torch.cuda.is_available()\n",
    "args_maml.seed = 104\n",
    "args_maml.reverse_channels=False\n",
    "args_maml.labels_as_int=False\n",
    "args_maml.reset_stored_filepaths=False\n",
    "args_maml.num_of_gpus=1\n",
    "\n",
    "args_maml.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9052a",
   "metadata": {},
   "source": [
    "## 2. Arbiter 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_arbiter = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 150,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_arbiter.im_shape = (2, 3, args_arbiter.image_height, args_arbiter.image_width)\n",
    "\n",
    "args_arbiter.use_cuda = torch.cuda.is_available()\n",
    "args_arbiter.seed = 104\n",
    "args_arbiter.reverse_channels=False\n",
    "args_arbiter.labels_as_int=False\n",
    "args_arbiter.reset_stored_filepaths=False\n",
    "args_arbiter.num_of_gpus=1\n",
    "\n",
    "args_arbiter.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f7d8",
   "metadata": {},
   "source": [
    "## 3. Model 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803156ee",
   "metadata": {},
   "source": [
    "### 3.1. MAML Model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f85286c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_filter128\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_maml = MAMLFewShotClassifier(args=args_maml, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_maml.image_height, args_maml.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model_maml, data=data, args=args_maml, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3acf",
   "metadata": {},
   "source": [
    "### 3.2.  Arbiter 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "25651dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_arbiter = MAMLFewShotClassifier(args=args_arbiter, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_arbiter.image_height, args_arbiter.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "arbiter_system = ExperimentBuilder(model=model_arbiter, data=data, args=args_arbiter, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9a2ff6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6359555553396543,\n",
       " 'best_val_iter': 49000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 98,\n",
       " 'train_loss_mean': 0.6403910377025605,\n",
       " 'train_loss_std': 0.1255260544055785,\n",
       " 'train_accuracy_mean': 0.7639866684675216,\n",
       " 'train_accuracy_std': 0.06118401968549816,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.9505268172423045,\n",
       " 'val_loss_std': 0.15325273037358733,\n",
       " 'val_accuracy_mean': 0.6287999994556109,\n",
       " 'val_accuracy_std': 0.06361374163962585,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-0.0255, -0.0844,  0.0421],\n",
       "                         [-0.0809, -0.0591, -0.0046],\n",
       "                         [-0.0325,  0.1015, -0.0134]],\n",
       "               \n",
       "                        [[ 0.0662, -0.0385,  0.0898],\n",
       "                         [-0.0224,  0.0329,  0.0889],\n",
       "                         [-0.0375,  0.0514, -0.0026]],\n",
       "               \n",
       "                        [[ 0.0667, -0.0065, -0.0467],\n",
       "                         [ 0.0322,  0.0595, -0.0776],\n",
       "                         [-0.0642,  0.0074, -0.0751]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0714,  0.1215,  0.0690],\n",
       "                         [ 0.0657, -0.0497,  0.0189],\n",
       "                         [-0.1058, -0.0985, -0.0763]],\n",
       "               \n",
       "                        [[-0.0225,  0.0162, -0.0007],\n",
       "                         [ 0.0545, -0.0631, -0.0476],\n",
       "                         [ 0.0542, -0.0559,  0.0761]],\n",
       "               \n",
       "                        [[-0.0650,  0.0606, -0.0772],\n",
       "                         [-0.0224,  0.0663,  0.0857],\n",
       "                         [-0.0305, -0.0117, -0.0181]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0324, -0.0386,  0.0566],\n",
       "                         [-0.0143, -0.0344, -0.0204],\n",
       "                         [ 0.0755,  0.0579, -0.0644]],\n",
       "               \n",
       "                        [[-0.0797, -0.0395,  0.0388],\n",
       "                         [ 0.0813, -0.0030,  0.0204],\n",
       "                         [-0.0698, -0.0439,  0.0303]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0722,  0.0254],\n",
       "                         [ 0.0777,  0.0115, -0.0638],\n",
       "                         [-0.0668,  0.0245,  0.0016]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0359, -0.0860, -0.0662],\n",
       "                         [-0.0050, -0.0928,  0.0520],\n",
       "                         [-0.0283,  0.0588, -0.0609]],\n",
       "               \n",
       "                        [[ 0.0648,  0.0180,  0.0776],\n",
       "                         [ 0.0499, -0.0222, -0.0206],\n",
       "                         [ 0.0198,  0.0787,  0.0061]],\n",
       "               \n",
       "                        [[ 0.0096, -0.0855, -0.0410],\n",
       "                         [-0.0569,  0.0049,  0.0344],\n",
       "                         [-0.0491,  0.0319,  0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0077,  0.0151, -0.0113],\n",
       "                         [ 0.0198,  0.0805, -0.0301],\n",
       "                         [ 0.0457,  0.0801, -0.0407]],\n",
       "               \n",
       "                        [[ 0.0811,  0.0580,  0.1031],\n",
       "                         [ 0.0596,  0.0067,  0.0002],\n",
       "                         [-0.0017, -0.0517,  0.0510]],\n",
       "               \n",
       "                        [[-0.0385, -0.0292, -0.0301],\n",
       "                         [-0.0170, -0.0443,  0.0586],\n",
       "                         [-0.0917, -0.0518,  0.0024]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0323,  0.0842, -0.0393],\n",
       "                         [-0.0391,  0.0747,  0.0626],\n",
       "                         [ 0.0786,  0.0831,  0.0922]],\n",
       "               \n",
       "                        [[-0.0269, -0.0427,  0.0134],\n",
       "                         [ 0.0379, -0.0726, -0.0621],\n",
       "                         [-0.0407, -0.0233, -0.0411]],\n",
       "               \n",
       "                        [[ 0.0461, -0.0238,  0.0385],\n",
       "                         [-0.0098, -0.0332, -0.0305],\n",
       "                         [-0.0480, -0.0654, -0.0453]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.0175,  0.0289,  0.0017, -0.0053,  0.0039,  0.0073,  0.0086,  0.0064,\n",
       "                        0.0009, -0.0196,  0.0216,  0.0025, -0.0158,  0.0111,  0.0017,  0.0114,\n",
       "                        0.0064,  0.0143,  0.0169, -0.0116, -0.0180, -0.0081, -0.0033, -0.0074,\n",
       "                       -0.0037,  0.0029,  0.0011,  0.0125,  0.0148,  0.0091,  0.0051, -0.0029,\n",
       "                       -0.0089,  0.0250,  0.0072, -0.0091,  0.0011,  0.0039,  0.0159,  0.0169,\n",
       "                       -0.0124, -0.0267,  0.0020, -0.0068, -0.0071,  0.0006, -0.0248, -0.0077,\n",
       "                        0.0044,  0.0075, -0.0075,  0.0021,  0.0150, -0.0009, -0.0197, -0.0072,\n",
       "                        0.0030,  0.0047, -0.0043, -0.0002, -0.0109, -0.0002,  0.0212,  0.0022,\n",
       "                       -0.0252, -0.0014,  0.0105, -0.0034, -0.0002, -0.0239, -0.0024, -0.0128,\n",
       "                        0.0037,  0.0041,  0.0119, -0.0083,  0.0055, -0.0148,  0.0025, -0.0067,\n",
       "                        0.0161,  0.0038, -0.0007,  0.0080,  0.0148, -0.0060, -0.0045,  0.0282,\n",
       "                       -0.0009, -0.0187, -0.0101, -0.0353,  0.0054,  0.0013, -0.0063, -0.0174,\n",
       "                       -0.0096, -0.0242,  0.0046, -0.0104, -0.0169,  0.0135,  0.0113,  0.0116,\n",
       "                        0.0058, -0.0031, -0.0009, -0.0004,  0.0068, -0.0032, -0.0076,  0.0107,\n",
       "                        0.0072, -0.0025, -0.0163,  0.0089,  0.0125, -0.0108,  0.0024,  0.0226,\n",
       "                       -0.0144,  0.0032, -0.0118, -0.0031,  0.0021, -0.0323,  0.0027, -0.0002],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1631,  0.1944, -0.1138,  0.1166,  0.1172, -0.0240, -0.2111, -0.1388,\n",
       "                       -0.1821, -0.0969,  0.2105, -0.1737,  0.3352, -0.0586,  0.3851,  0.2701,\n",
       "                        0.0005,  0.0484,  0.2549, -0.0579,  0.0446, -0.1371, -0.0903,  0.0135,\n",
       "                        0.0157, -0.0954, -0.0991,  0.3129,  0.2739,  0.1327,  0.0106, -0.0495,\n",
       "                        0.1543, -0.1553, -0.0760, -0.0628,  0.0274,  0.1642, -0.1468, -0.0709,\n",
       "                        0.0016,  0.3324, -0.1130,  0.1172,  0.2649, -0.1730, -0.1429, -0.1181,\n",
       "                       -0.0757,  0.0019,  0.2784, -0.0801,  0.3400,  0.2097, -0.0662, -0.0745,\n",
       "                       -0.0332,  0.2216,  0.0059,  0.3655, -0.1121,  0.2333,  0.0753, -0.1723,\n",
       "                        0.1420, -0.0555,  0.2159, -0.0682,  0.3034, -0.0017,  0.1455,  0.3801,\n",
       "                       -0.0174, -0.1359,  0.2083, -0.1117,  0.0331,  0.3195,  0.1055, -0.1900,\n",
       "                        0.4710, -0.0528, -0.1267, -0.0556,  0.3039,  0.3539,  0.3447,  0.2085,\n",
       "                       -0.1442,  0.1741, -0.1334,  0.0495,  0.0896,  0.1874, -0.0507,  0.2346,\n",
       "                        0.0595,  0.0425, -0.0443, -0.1186,  0.0008,  0.0851, -0.1202, -0.0469,\n",
       "                       -0.0340,  0.0512,  0.1297, -0.1034,  0.0106,  0.2907,  0.2411,  0.4341,\n",
       "                       -0.0890,  0.0216, -0.0916,  0.1807,  0.0130,  0.4608, -0.0359,  0.4015,\n",
       "                       -0.0879, -0.0734, -0.1045, -0.1029,  0.3831, -0.0157, -0.0910, -0.0575],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.9696, 1.1085, 0.8447, 1.1021, 0.9693, 0.9709, 0.9787, 0.9343, 0.8471,\n",
       "                       0.9381, 1.1029, 0.9399, 1.1080, 0.9657, 1.1290, 1.1905, 0.8940, 1.0025,\n",
       "                       0.9887, 0.9973, 0.9491, 0.9175, 0.9180, 0.9997, 0.9615, 0.9512, 0.9187,\n",
       "                       0.9743, 1.0115, 1.0331, 1.0684, 0.9661, 1.1221, 0.9391, 0.9968, 0.9734,\n",
       "                       0.9412, 1.0812, 0.9791, 1.0023, 1.0361, 1.1265, 0.9472, 1.1115, 0.9799,\n",
       "                       0.9660, 0.9552, 0.9922, 0.9178, 1.0445, 0.9584, 0.9479, 1.0931, 0.9649,\n",
       "                       0.9693, 0.9627, 0.9913, 1.0533, 1.0207, 1.0808, 1.0113, 1.0104, 0.9997,\n",
       "                       0.9408, 1.0169, 0.9237, 0.9685, 1.0111, 1.1905, 0.9716, 1.0283, 1.1558,\n",
       "                       0.9849, 0.9728, 0.9640, 0.9509, 1.0301, 1.1699, 1.1360, 0.8957, 1.0791,\n",
       "                       1.0465, 0.9238, 1.0184, 1.0511, 1.1042, 0.9738, 1.0620, 0.8964, 0.9912,\n",
       "                       0.8152, 0.9297, 0.9573, 1.0192, 0.9643, 0.9865, 0.9691, 0.9859, 0.9162,\n",
       "                       1.0091, 0.9118, 1.0460, 0.9582, 1.0327, 0.9797, 1.0288, 1.0011, 0.9599,\n",
       "                       1.0142, 1.0149, 0.9774, 1.1019, 1.0000, 0.9757, 0.9746, 1.1659, 0.9524,\n",
       "                       1.0279, 1.0224, 1.1533, 0.9746, 0.9960, 0.9669, 0.9357, 1.0148, 0.9732,\n",
       "                       0.9627, 0.9500], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 0.0073,  0.0615,  0.0120],\n",
       "                         [-0.0109,  0.0211, -0.0202],\n",
       "                         [ 0.0240,  0.0670, -0.0142]],\n",
       "               \n",
       "                        [[ 0.0408,  0.0463, -0.0002],\n",
       "                         [-0.0267, -0.0258, -0.0401],\n",
       "                         [-0.0322,  0.0412, -0.0157]],\n",
       "               \n",
       "                        [[ 0.0051, -0.0471, -0.0257],\n",
       "                         [-0.0522, -0.0002,  0.0558],\n",
       "                         [ 0.0197,  0.0114, -0.0256]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0189, -0.0542, -0.0090],\n",
       "                         [-0.0584, -0.0753, -0.0285],\n",
       "                         [-0.0501, -0.0588, -0.0454]],\n",
       "               \n",
       "                        [[ 0.0147,  0.0626, -0.0057],\n",
       "                         [-0.0027,  0.0439, -0.0190],\n",
       "                         [ 0.0332, -0.0397,  0.0007]],\n",
       "               \n",
       "                        [[-0.0371,  0.0505, -0.0220],\n",
       "                         [-0.0072,  0.0231,  0.0021],\n",
       "                         [-0.0321, -0.0171,  0.0332]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0149, -0.0397,  0.0341],\n",
       "                         [ 0.0416,  0.0181, -0.0336],\n",
       "                         [ 0.0185,  0.0093,  0.0077]],\n",
       "               \n",
       "                        [[ 0.0276,  0.0237, -0.0054],\n",
       "                         [-0.0764,  0.0186,  0.0145],\n",
       "                         [-0.0378, -0.0316,  0.0317]],\n",
       "               \n",
       "                        [[ 0.0133, -0.0043, -0.0575],\n",
       "                         [-0.0120, -0.0424, -0.0046],\n",
       "                         [-0.0632, -0.0027, -0.0540]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0045, -0.0387, -0.0022],\n",
       "                         [-0.0536, -0.0996, -0.0563],\n",
       "                         [-0.0248,  0.0001, -0.0399]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0114, -0.0219],\n",
       "                         [ 0.0516,  0.0429,  0.0258],\n",
       "                         [ 0.0443, -0.0104,  0.0394]],\n",
       "               \n",
       "                        [[-0.0065, -0.0368,  0.0156],\n",
       "                         [ 0.0126, -0.0725, -0.0404],\n",
       "                         [-0.0613, -0.0766, -0.0209]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0573, -0.0550, -0.0113],\n",
       "                         [-0.0121,  0.0007, -0.0628],\n",
       "                         [ 0.0059,  0.0141, -0.0043]],\n",
       "               \n",
       "                        [[ 0.0305, -0.0125, -0.0485],\n",
       "                         [ 0.0321,  0.0384, -0.0741],\n",
       "                         [ 0.0067,  0.0431, -0.0392]],\n",
       "               \n",
       "                        [[-0.0431,  0.0168, -0.0467],\n",
       "                         [-0.0372,  0.0102,  0.0053],\n",
       "                         [ 0.0024,  0.0143,  0.0398]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0455,  0.0046, -0.0200],\n",
       "                         [-0.0405, -0.0600, -0.0352],\n",
       "                         [-0.0408, -0.0624, -0.0764]],\n",
       "               \n",
       "                        [[ 0.0437,  0.0360,  0.0398],\n",
       "                         [-0.0529, -0.0126, -0.0394],\n",
       "                         [-0.0420,  0.0150, -0.0273]],\n",
       "               \n",
       "                        [[-0.0355, -0.0435,  0.0117],\n",
       "                         [-0.0331,  0.0213,  0.0256],\n",
       "                         [ 0.0059, -0.0061, -0.0219]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0504,  0.0111,  0.0091],\n",
       "                         [ 0.0700,  0.0102, -0.0127],\n",
       "                         [ 0.0472, -0.0750, -0.0749]],\n",
       "               \n",
       "                        [[ 0.0126,  0.0457,  0.0538],\n",
       "                         [-0.0469, -0.0615, -0.0298],\n",
       "                         [-0.0257,  0.0307,  0.0064]],\n",
       "               \n",
       "                        [[ 0.0381,  0.0234,  0.0319],\n",
       "                         [ 0.0427, -0.0436, -0.0046],\n",
       "                         [-0.0133, -0.0470, -0.0157]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0282, -0.0291, -0.0278],\n",
       "                         [ 0.0064, -0.0879, -0.0409],\n",
       "                         [-0.0045, -0.0201, -0.0508]],\n",
       "               \n",
       "                        [[ 0.0012,  0.0317,  0.0562],\n",
       "                         [ 0.0444,  0.0208,  0.0051],\n",
       "                         [-0.0373,  0.0474,  0.0533]],\n",
       "               \n",
       "                        [[-0.0329, -0.0453, -0.0214],\n",
       "                         [-0.0277,  0.0045,  0.0224],\n",
       "                         [-0.0115,  0.0191, -0.0240]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0053, -0.0619,  0.0081],\n",
       "                         [-0.0290, -0.0561,  0.0356],\n",
       "                         [-0.0297,  0.0281,  0.0076]],\n",
       "               \n",
       "                        [[ 0.0019,  0.0412, -0.0531],\n",
       "                         [-0.0054,  0.0489, -0.0376],\n",
       "                         [-0.0378, -0.0377,  0.0578]],\n",
       "               \n",
       "                        [[-0.0227,  0.0314,  0.0516],\n",
       "                         [ 0.0181, -0.0008, -0.0313],\n",
       "                         [-0.0155,  0.0141,  0.0279]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0321, -0.0038,  0.0526],\n",
       "                         [-0.0037,  0.0310, -0.0090],\n",
       "                         [-0.0042, -0.0025, -0.0319]],\n",
       "               \n",
       "                        [[ 0.0066,  0.0332,  0.0456],\n",
       "                         [-0.0186, -0.0539, -0.0037],\n",
       "                         [-0.0198, -0.0442,  0.0125]],\n",
       "               \n",
       "                        [[-0.0379, -0.0151,  0.0192],\n",
       "                         [-0.0437,  0.0384,  0.0344],\n",
       "                         [ 0.0192,  0.0085,  0.0302]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0173, -0.0070, -0.0065],\n",
       "                         [ 0.0314,  0.0438, -0.0579],\n",
       "                         [-0.0199,  0.0290, -0.0254]],\n",
       "               \n",
       "                        [[-0.0348,  0.0351,  0.0423],\n",
       "                         [ 0.0056, -0.0599, -0.0338],\n",
       "                         [-0.0596, -0.0250,  0.0199]],\n",
       "               \n",
       "                        [[-0.0150, -0.0501,  0.0067],\n",
       "                         [-0.0007,  0.0117,  0.0053],\n",
       "                         [ 0.0055, -0.0287,  0.0358]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0159, -0.0148, -0.0628],\n",
       "                         [ 0.0441,  0.0680, -0.0351],\n",
       "                         [ 0.0223,  0.0213, -0.0230]],\n",
       "               \n",
       "                        [[-0.0644, -0.0820, -0.0552],\n",
       "                         [ 0.0076,  0.0235, -0.0232],\n",
       "                         [-0.0340, -0.0002, -0.0556]],\n",
       "               \n",
       "                        [[-0.0215,  0.0316,  0.0109],\n",
       "                         [-0.0233,  0.0090,  0.0530],\n",
       "                         [-0.0053, -0.0364, -0.0194]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-1.6949e-02, -8.4122e-03, -6.4646e-03, -4.2481e-03, -2.1994e-03,\n",
       "                        3.4437e-03, -1.9195e-03,  2.9394e-03, -8.6836e-04,  2.9146e-03,\n",
       "                       -4.8363e-03, -2.9786e-03,  8.2595e-04,  2.0065e-03,  1.7538e-03,\n",
       "                        5.7412e-03, -3.8066e-05,  3.0353e-03, -2.8385e-03,  2.7388e-03,\n",
       "                       -5.7158e-03,  1.9158e-03, -2.4051e-03, -2.9564e-03,  2.5613e-03,\n",
       "                        2.3439e-04, -2.2504e-03,  7.3651e-03, -1.2441e-03, -8.3805e-05,\n",
       "                        8.6782e-03, -3.5325e-03,  3.8182e-03,  1.5983e-03,  1.2204e-03,\n",
       "                       -6.0784e-03, -4.5049e-04,  4.5410e-03,  3.2182e-04, -2.8716e-03,\n",
       "                       -2.2109e-03,  3.7276e-03,  7.1509e-04, -2.3736e-03,  3.0177e-04,\n",
       "                        3.1425e-03,  4.0651e-03, -3.7909e-03, -1.2114e-03, -1.6332e-03,\n",
       "                       -8.7488e-03, -9.9171e-04,  8.2588e-04, -7.7100e-04, -8.6759e-03,\n",
       "                        3.9878e-03, -2.7320e-03, -8.2349e-03,  2.0059e-03,  6.6739e-04,\n",
       "                        2.1313e-03, -2.2211e-03,  1.5898e-03,  6.0315e-04, -7.0056e-03,\n",
       "                        3.8265e-03,  4.4242e-03, -4.5509e-03,  2.2188e-04, -2.7838e-03,\n",
       "                        1.4363e-03,  4.0279e-04, -1.7379e-03, -4.3296e-03,  2.3067e-03,\n",
       "                       -5.1976e-03, -2.9156e-04, -1.3852e-03,  7.8145e-03,  1.6117e-03,\n",
       "                        6.3610e-03, -2.0517e-03, -5.3275e-03,  4.3522e-03, -2.8026e-03,\n",
       "                        3.9732e-03,  5.5070e-03, -7.5503e-05,  1.0788e-03,  2.9781e-03,\n",
       "                       -9.4866e-03,  5.1489e-04, -3.9218e-04, -5.8387e-05, -1.9815e-03,\n",
       "                        6.0945e-04,  1.0402e-03,  9.8662e-04, -5.3993e-04, -2.6299e-04,\n",
       "                        1.4962e-03, -5.3848e-04,  1.7470e-03, -3.8175e-03, -1.7810e-03,\n",
       "                       -1.8436e-03, -2.5974e-03,  2.9317e-03,  2.3702e-03,  2.2829e-04,\n",
       "                        2.0404e-03,  5.7490e-03,  1.5104e-03,  7.5078e-04, -1.6547e-03,\n",
       "                        4.6222e-03,  1.2253e-04,  3.9792e-03,  4.9803e-03,  4.8457e-04,\n",
       "                       -3.8067e-03, -2.3458e-03,  1.2213e-03, -6.6374e-03, -5.1143e-03,\n",
       "                       -1.9425e-03, -1.7261e-03,  2.3845e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0239, -0.1029, -0.1356, -0.1313, -0.1094, -0.1200, -0.0928, -0.1694,\n",
       "                       -0.0216, -0.0378, -0.0961, -0.0686, -0.2043, -0.1061, -0.1571, -0.1857,\n",
       "                       -0.0801,  0.1526, -0.0903, -0.1624, -0.2084, -0.0794, -0.1526, -0.0904,\n",
       "                       -0.1907, -0.0218, -0.1831,  0.0054, -0.0388, -0.1296,  0.0191, -0.1421,\n",
       "                       -0.0195, -0.1232, -0.0756, -0.1430, -0.2357, -0.0668, -0.1038, -0.1295,\n",
       "                       -0.1906, -0.0274, -0.1887, -0.1435, -0.1363, -0.1023, -0.1372, -0.0549,\n",
       "                       -0.0128, -0.1253, -0.1443, -0.0114, -0.0838, -0.1771, -0.0118, -0.0546,\n",
       "                       -0.0105, -0.1224, -0.1915, -0.1396, -0.1079, -0.0798, -0.1026, -0.0963,\n",
       "                       -0.0723, -0.0284, -0.0720, -0.0425, -0.1336, -0.0733, -0.1823, -0.0852,\n",
       "                       -0.0766, -0.0089, -0.0376, -0.1518, -0.0774, -0.1717,  0.0542, -0.1189,\n",
       "                       -0.0779, -0.1253, -0.0342, -0.0763, -0.1555, -0.0973, -0.0166, -0.0855,\n",
       "                       -0.0579, -0.1178, -0.1892, -0.0419, -0.0477, -0.0605, -0.0799, -0.0898,\n",
       "                       -0.0811, -0.0729, -0.1795, -0.1760, -0.1528, -0.1409, -0.1406, -0.1007,\n",
       "                       -0.1355, -0.1034, -0.0196, -0.1602, -0.0788, -0.0995, -0.1055, -0.0866,\n",
       "                       -0.0884, -0.0945, -0.1266, -0.0264, -0.0842, -0.1382, -0.0980, -0.1124,\n",
       "                       -0.1105, -0.0172, -0.1221, -0.0662, -0.0260, -0.0930, -0.0811, -0.1032],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9428, 0.9862, 1.0353, 1.0197, 0.9484, 1.0319, 1.0427, 0.9799, 1.0889,\n",
       "                       0.9699, 1.0409, 0.9796, 0.9760, 1.0362, 1.0216, 1.0216, 1.0361, 0.9781,\n",
       "                       1.0763, 1.0203, 1.0362, 1.0268, 1.0446, 0.9615, 1.0413, 1.0226, 1.0333,\n",
       "                       1.0005, 1.0836, 1.0583, 0.9296, 0.9788, 1.0094, 0.9958, 1.0039, 0.9190,\n",
       "                       1.0555, 0.9978, 0.9221, 0.9902, 1.0340, 0.9762, 1.0726, 1.0702, 0.9993,\n",
       "                       0.9641, 1.0156, 1.0302, 1.0744, 0.9556, 0.9469, 0.9848, 0.9398, 0.9957,\n",
       "                       0.9438, 1.0218, 1.0183, 0.9649, 0.9851, 0.9523, 0.9629, 0.9934, 0.9863,\n",
       "                       0.9597, 0.9350, 1.0053, 1.0129, 1.0250, 0.9279, 1.0033, 0.9282, 0.9721,\n",
       "                       1.0010, 0.9276, 1.0228, 0.9233, 1.0320, 1.0337, 0.9309, 0.9750, 1.0079,\n",
       "                       0.9799, 1.0670, 0.9271, 1.0485, 0.9817, 0.9526, 0.9983, 1.1004, 0.9980,\n",
       "                       1.0839, 1.0081, 0.9850, 1.0833, 0.9840, 0.9629, 0.9918, 0.9759, 1.0162,\n",
       "                       1.0476, 0.9885, 1.0907, 0.9968, 1.0194, 0.9749, 0.9604, 0.9823, 0.9796,\n",
       "                       1.0114, 1.0355, 0.9358, 1.0666, 0.9849, 1.0066, 0.9954, 1.0633, 1.0803,\n",
       "                       0.9390, 0.9113, 1.0046, 0.9884, 1.0092, 1.0470, 0.9643, 0.9522, 1.0208,\n",
       "                       1.0418, 0.9650], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-5.6612e-04,  5.5613e-02, -1.4964e-02],\n",
       "                         [-3.1003e-02, -5.2417e-03, -3.5244e-02],\n",
       "                         [ 1.5153e-02, -2.5045e-02,  8.3373e-03]],\n",
       "               \n",
       "                        [[ 2.6745e-02, -1.6569e-02,  9.6491e-03],\n",
       "                         [-2.3582e-02, -1.7072e-02, -3.0692e-02],\n",
       "                         [ 8.8099e-03,  1.9431e-02, -1.0071e-02]],\n",
       "               \n",
       "                        [[ 2.2886e-02,  1.2657e-02,  2.7444e-02],\n",
       "                         [ 6.7986e-02,  2.3340e-02, -6.3387e-02],\n",
       "                         [-8.0550e-03,  4.3950e-03, -2.6644e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.2784e-02, -4.7080e-02, -5.8843e-02],\n",
       "                         [ 5.8877e-02,  3.3930e-02,  5.3777e-02],\n",
       "                         [-3.0208e-06, -3.6364e-02, -3.0298e-02]],\n",
       "               \n",
       "                        [[-1.8547e-02, -3.3489e-02,  1.4912e-02],\n",
       "                         [ 4.4668e-02,  1.5992e-02, -3.8668e-02],\n",
       "                         [ 1.9676e-02,  3.2789e-02,  3.8013e-02]],\n",
       "               \n",
       "                        [[ 9.6036e-03,  1.6508e-02,  6.3899e-02],\n",
       "                         [-2.2815e-02, -2.2302e-02, -3.1161e-02],\n",
       "                         [ 1.2610e-03,  2.5403e-03,  4.1579e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3809e-03,  3.2775e-03,  3.4006e-02],\n",
       "                         [ 1.2260e-02,  2.4693e-03, -3.2722e-02],\n",
       "                         [-2.4378e-02, -6.5934e-02, -2.4807e-02]],\n",
       "               \n",
       "                        [[-4.2487e-02, -2.5946e-02, -2.8232e-02],\n",
       "                         [-1.6780e-02,  6.6708e-03, -4.2812e-03],\n",
       "                         [-1.7184e-02, -2.3446e-03,  1.5289e-02]],\n",
       "               \n",
       "                        [[ 8.0496e-03,  5.3279e-02,  9.2603e-03],\n",
       "                         [ 3.7587e-02,  3.2692e-02,  6.3311e-03],\n",
       "                         [ 4.8233e-02,  1.8544e-02, -1.1102e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.0004e-02,  3.0527e-02, -4.1664e-02],\n",
       "                         [-5.6780e-02, -4.1942e-02, -5.5847e-02],\n",
       "                         [-4.5592e-02, -2.2262e-02, -7.3276e-02]],\n",
       "               \n",
       "                        [[-2.0432e-02, -1.4052e-02,  1.8676e-03],\n",
       "                         [-9.4970e-02, -1.9388e-02, -1.8318e-02],\n",
       "                         [-6.2985e-02, -3.7837e-02, -3.0712e-02]],\n",
       "               \n",
       "                        [[-9.6093e-03, -8.8685e-02, -7.3937e-02],\n",
       "                         [-2.9326e-02,  7.3677e-04,  8.5064e-03],\n",
       "                         [-4.6906e-02, -1.3926e-02, -2.8744e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.4945e-02, -1.6051e-03, -6.4338e-02],\n",
       "                         [-9.1217e-04, -1.0536e-02,  1.0642e-02],\n",
       "                         [ 5.3609e-02,  2.5719e-02,  1.7490e-02]],\n",
       "               \n",
       "                        [[ 3.2974e-02,  7.0537e-02,  6.3091e-02],\n",
       "                         [-2.3824e-02,  3.8106e-02,  3.0877e-03],\n",
       "                         [-8.1763e-02, -1.9468e-02, -8.9834e-02]],\n",
       "               \n",
       "                        [[ 5.2738e-02,  1.4020e-02,  6.0283e-02],\n",
       "                         [-8.0464e-03,  9.1787e-02,  2.4450e-02],\n",
       "                         [-7.7595e-04,  4.2054e-02, -5.3030e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.1894e-02,  7.4259e-03, -4.8836e-02],\n",
       "                         [-3.9580e-03, -3.1565e-02, -1.5890e-02],\n",
       "                         [-2.4345e-02, -1.7730e-02,  5.6163e-02]],\n",
       "               \n",
       "                        [[-1.3640e-02, -2.9441e-02, -1.1606e-02],\n",
       "                         [ 6.9270e-05, -5.4901e-02, -6.5660e-02],\n",
       "                         [-7.3340e-02, -7.6530e-02, -5.1404e-02]],\n",
       "               \n",
       "                        [[-1.6647e-02,  2.0942e-02,  5.6214e-02],\n",
       "                         [ 1.0635e-02,  3.8857e-02,  2.6654e-02],\n",
       "                         [-1.2568e-02, -2.9424e-04,  1.8442e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.2160e-02, -4.8316e-02, -5.4221e-02],\n",
       "                         [ 3.5441e-02, -3.3133e-02, -1.6483e-02],\n",
       "                         [ 1.4458e-02,  2.3074e-02,  1.1153e-02]],\n",
       "               \n",
       "                        [[-4.4017e-02,  5.9920e-03,  1.8519e-02],\n",
       "                         [-1.0042e-02,  1.2894e-02, -3.1734e-02],\n",
       "                         [-4.1772e-03,  5.4501e-02,  8.8462e-03]],\n",
       "               \n",
       "                        [[-2.9016e-02,  7.5986e-03, -2.8667e-02],\n",
       "                         [ 3.5275e-02,  5.9086e-03,  3.4249e-02],\n",
       "                         [ 6.0786e-02,  2.2129e-02,  1.7514e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.5898e-02,  3.3125e-02,  1.3987e-02],\n",
       "                         [ 1.1372e-02,  4.8750e-03, -5.3042e-03],\n",
       "                         [-1.7770e-02, -3.6008e-02, -4.0320e-02]],\n",
       "               \n",
       "                        [[-8.7624e-03, -1.3168e-02,  6.0018e-02],\n",
       "                         [-6.0902e-03, -3.3698e-03, -2.7294e-02],\n",
       "                         [-3.2872e-02,  3.5987e-02, -7.3005e-02]],\n",
       "               \n",
       "                        [[ 1.0989e-02, -4.6545e-02, -6.0721e-02],\n",
       "                         [-7.2877e-02, -8.1973e-02, -7.7765e-02],\n",
       "                         [-3.0052e-02,  2.0828e-02, -2.5616e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.8925e-02,  4.8917e-02,  2.8418e-02],\n",
       "                         [-2.1285e-02, -2.5500e-02, -7.5975e-03],\n",
       "                         [-2.8078e-03,  2.9815e-02, -1.3520e-02]],\n",
       "               \n",
       "                        [[ 2.1450e-04, -2.8705e-02,  2.4119e-02],\n",
       "                         [-3.6257e-03, -3.2389e-02,  4.0854e-02],\n",
       "                         [-3.2382e-02,  3.5661e-03,  7.8317e-03]],\n",
       "               \n",
       "                        [[-3.0559e-02,  2.0490e-02,  5.0351e-03],\n",
       "                         [ 5.0072e-02,  4.6661e-02, -1.5058e-02],\n",
       "                         [ 1.7608e-02, -1.3727e-02,  2.3573e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.6052e-02, -3.8395e-03,  3.6573e-02],\n",
       "                         [-5.7949e-04,  7.1536e-02, -1.0412e-02],\n",
       "                         [-1.5482e-03,  2.2744e-02, -8.5390e-03]],\n",
       "               \n",
       "                        [[-6.3845e-02, -4.6169e-02, -4.0007e-02],\n",
       "                         [-1.1293e-02, -3.5827e-02,  1.3014e-02],\n",
       "                         [ 5.1841e-02, -1.6890e-02, -7.2922e-02]],\n",
       "               \n",
       "                        [[ 4.1673e-02,  1.7039e-02, -8.0290e-02],\n",
       "                         [-4.1658e-04, -2.3544e-02, -1.0676e-01],\n",
       "                         [ 4.5347e-03, -2.7998e-02, -2.3365e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.9852e-02,  5.9324e-03,  9.6843e-04],\n",
       "                         [-1.5981e-02, -3.1834e-02,  5.2805e-02],\n",
       "                         [-3.6868e-02, -1.7340e-02,  3.9554e-02]],\n",
       "               \n",
       "                        [[ 6.7321e-02,  2.9623e-02,  5.4673e-03],\n",
       "                         [ 2.0529e-02,  6.5971e-03, -2.3391e-02],\n",
       "                         [ 1.7442e-02,  3.5414e-02,  3.6108e-02]],\n",
       "               \n",
       "                        [[-3.3463e-02, -2.1611e-02, -1.9678e-02],\n",
       "                         [-4.8424e-02,  3.5010e-02,  3.9424e-02],\n",
       "                         [-5.9420e-03,  4.3907e-02,  3.3360e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9797e-02,  4.3752e-02,  1.3179e-02],\n",
       "                         [ 4.3918e-02,  3.1366e-02, -2.0022e-02],\n",
       "                         [-3.3028e-02, -2.8256e-02, -3.0205e-02]],\n",
       "               \n",
       "                        [[-1.2938e-02,  1.2830e-02,  2.9501e-02],\n",
       "                         [-1.7726e-02, -7.1185e-03, -3.7811e-02],\n",
       "                         [ 2.8611e-02, -2.0118e-02, -6.1311e-02]],\n",
       "               \n",
       "                        [[-1.0113e-02,  5.4758e-02,  2.1487e-02],\n",
       "                         [ 3.2794e-03,  1.9351e-02, -2.8451e-02],\n",
       "                         [ 8.8019e-02, -1.0072e-02, -3.6928e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 4.6061e-04, -2.6240e-03, -4.8668e-03,  1.3499e-03, -6.7688e-04,\n",
       "                       -1.3051e-03, -3.9467e-04,  1.6521e-03, -7.8187e-03,  4.3409e-03,\n",
       "                       -2.4901e-03, -1.1735e-03, -1.9624e-04, -7.4011e-03,  1.8107e-05,\n",
       "                       -4.4562e-03,  2.1683e-03, -2.7531e-03,  9.8409e-05,  1.1074e-03,\n",
       "                       -5.6326e-03,  6.3480e-03,  5.0846e-03,  1.4609e-03, -3.3035e-03,\n",
       "                       -3.3362e-03,  9.9636e-04, -5.7492e-04, -3.1613e-03, -1.0247e-02,\n",
       "                        4.1812e-03, -1.9299e-03,  4.8126e-03,  6.2209e-03,  9.1613e-03,\n",
       "                       -3.4160e-03,  1.0048e-02, -1.8047e-02,  1.5066e-04, -9.0010e-04,\n",
       "                       -3.3024e-03,  1.1539e-02,  1.5924e-03, -7.8076e-04,  2.7959e-03,\n",
       "                       -3.8437e-03,  5.5420e-04,  5.6630e-03,  1.7700e-03, -1.1816e-03,\n",
       "                       -3.2343e-03, -1.1565e-03,  5.6338e-03, -2.6592e-03, -2.2296e-03,\n",
       "                       -7.1357e-04,  1.0998e-03, -8.9221e-04, -3.3544e-03, -5.9220e-03,\n",
       "                        3.0695e-03, -2.3067e-03,  7.3235e-03,  6.6048e-03,  1.7583e-03,\n",
       "                        1.2300e-03, -8.0600e-04, -1.3774e-02,  2.6487e-03, -2.2824e-03,\n",
       "                       -4.4781e-03, -4.0840e-03, -4.4604e-04, -9.0035e-04,  1.4574e-04,\n",
       "                        2.9544e-03,  1.5378e-03,  3.9039e-03, -1.7558e-03,  3.4783e-03,\n",
       "                       -3.1708e-03,  6.7397e-03,  2.6774e-03, -4.4915e-03,  1.7781e-03,\n",
       "                        1.4992e-03,  2.2443e-03, -9.6971e-04, -1.8576e-03,  2.4945e-03,\n",
       "                        9.1154e-03,  4.9740e-03,  3.8111e-03,  4.0308e-04,  3.2366e-03,\n",
       "                       -1.2821e-03, -6.6860e-04, -6.5602e-03,  2.9548e-03,  3.9707e-03,\n",
       "                        3.1116e-03, -3.0379e-03,  1.4848e-03, -2.9219e-03, -2.7217e-03,\n",
       "                       -3.1725e-03, -2.3843e-03, -2.0663e-04, -3.5932e-03, -2.9701e-03,\n",
       "                        2.3363e-03, -4.0607e-03,  1.1661e-03,  3.8860e-03, -7.5729e-03,\n",
       "                       -2.9014e-03,  7.2572e-04, -6.2508e-03,  1.2011e-04,  2.4776e-03,\n",
       "                        1.8391e-03, -9.3481e-04,  2.6883e-03, -1.7414e-03,  3.9286e-03,\n",
       "                        3.7851e-03,  5.6336e-03,  5.9318e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.1391, -0.2048, -0.1393, -0.1732, -0.1353, -0.1210, -0.1603, -0.2054,\n",
       "                       -0.0353, -0.1253, -0.1125, -0.0369, -0.2840, -0.1378, -0.1706, -0.2121,\n",
       "                       -0.0803, -0.1494, -0.1289, -0.1470, -0.0888, -0.0605, -0.1538, -0.0884,\n",
       "                       -0.0962, -0.0889, -0.1651, -0.1394, -0.2496, -0.1341, -0.1365, -0.1336,\n",
       "                       -0.2978, -0.0906, -0.1493, -0.2138, -0.0082, -0.2035, -0.2011, -0.2006,\n",
       "                       -0.1547, -0.2458, -0.1979, -0.0222, -0.1107, -0.1439, -0.0984, -0.0936,\n",
       "                       -0.1489, -0.1794, -0.1324, -0.1430, -0.2192, -0.1099, -0.1431, -0.1055,\n",
       "                       -0.1645, -0.1377, -0.1245, -0.0560, -0.2493, -0.0998, -0.1132, -0.2081,\n",
       "                       -0.0350, -0.1173, -0.0356, -0.0940, -0.1193, -0.1307, -0.0620, -0.2028,\n",
       "                       -0.1571, -0.1631, -0.0327, -0.1306, -0.0988, -0.1368, -0.0456, -0.0868,\n",
       "                       -0.1458, -0.1827, -0.1800, -0.1469, -0.0483, -0.1266, -0.1537, -0.1395,\n",
       "                       -0.1806, -0.0839, -0.2118, -0.0111, -0.1126, -0.1722, -0.1317, -0.1179,\n",
       "                       -0.1163, -0.1933, -0.1038, -0.1531, -0.1672, -0.1171, -0.0923, -0.0193,\n",
       "                       -0.0547, -0.2908, -0.1291, -0.1831, -0.1212, -0.1551, -0.0190, -0.0806,\n",
       "                       -0.2205, -0.0160,  0.0108, -0.1951, -0.1521, -0.3187, -0.1491, -0.1353,\n",
       "                       -0.0793, -0.1182,  0.0170, -0.1368, -0.1985, -0.1302, -0.1994, -0.1754],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([1.0277, 1.0128, 0.9804, 0.9433, 0.9412, 0.9652, 0.9658, 0.9749, 1.0517,\n",
       "                       1.0016, 0.9643, 1.0066, 1.0405, 0.9912, 0.9841, 0.9218, 1.1151, 1.0733,\n",
       "                       0.9509, 0.9652, 1.0227, 0.9606, 1.0235, 0.9114, 1.0268, 1.0234, 1.0819,\n",
       "                       0.9045, 1.0696, 1.0600, 0.9874, 1.0240, 1.0417, 1.0809, 0.9790, 1.0562,\n",
       "                       0.9363, 1.1194, 0.9735, 0.9849, 0.9253, 1.0497, 1.0019, 0.9294, 0.9515,\n",
       "                       1.0383, 1.0002, 0.9651, 0.9030, 0.9744, 1.0132, 0.9474, 1.0187, 0.9983,\n",
       "                       0.9703, 0.9965, 0.9904, 0.9778, 0.9497, 1.0112, 1.0529, 0.9374, 0.9905,\n",
       "                       1.0483, 1.0328, 1.0214, 0.9460, 0.9351, 0.9422, 0.9163, 0.9332, 1.0385,\n",
       "                       0.9540, 0.9346, 0.9807, 0.9988, 1.0332, 0.9307, 0.9741, 0.9708, 1.0372,\n",
       "                       0.9618, 0.9719, 1.0110, 0.9977, 0.9824, 0.9189, 0.9367, 1.1518, 0.9375,\n",
       "                       1.0127, 0.9777, 0.9833, 1.0089, 0.9567, 0.9944, 0.9539, 0.9537, 0.9933,\n",
       "                       1.0257, 0.9628, 0.9432, 0.9521, 0.9561, 1.0270, 1.0515, 1.0031, 0.9811,\n",
       "                       1.0540, 0.9654, 1.0987, 0.9105, 0.8965, 0.9892, 0.9852, 0.9682, 0.8834,\n",
       "                       1.0933, 1.0380, 0.9390, 0.9505, 1.0529, 1.0772, 1.0426, 0.9539, 1.0445,\n",
       "                       0.9834, 1.0106], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-5.4627e-02,  7.3330e-04,  1.0867e-02],\n",
       "                         [-1.2974e-02,  1.8572e-02, -8.2295e-03],\n",
       "                         [-6.2027e-03,  3.7114e-03,  6.4246e-02]],\n",
       "               \n",
       "                        [[-4.5182e-02, -5.4583e-02, -4.1419e-02],\n",
       "                         [-4.6200e-02, -4.0708e-02, -3.4592e-02],\n",
       "                         [-3.8282e-02,  5.8337e-03, -4.8956e-02]],\n",
       "               \n",
       "                        [[ 6.5746e-02,  1.5244e-02,  8.7147e-02],\n",
       "                         [-3.4472e-02, -2.5536e-02,  2.4144e-02],\n",
       "                         [-3.4479e-02,  4.1504e-02,  2.4551e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.6482e-02, -7.8470e-03, -3.9585e-02],\n",
       "                         [ 2.1164e-02, -1.7489e-02,  1.2096e-02],\n",
       "                         [ 2.4554e-02,  6.7232e-02,  4.6818e-02]],\n",
       "               \n",
       "                        [[-1.3402e-02, -3.8527e-03, -2.4207e-02],\n",
       "                         [ 1.4606e-02, -7.9428e-03, -1.4672e-02],\n",
       "                         [ 2.4145e-02, -2.7772e-02, -3.5707e-02]],\n",
       "               \n",
       "                        [[ 5.1828e-02, -9.1739e-03,  8.9307e-03],\n",
       "                         [ 3.5144e-03, -6.4678e-03, -4.3382e-02],\n",
       "                         [-9.9676e-03, -3.8737e-02, -2.5570e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4494e-03, -8.8308e-03, -2.3047e-02],\n",
       "                         [ 5.3864e-03, -3.2619e-02, -1.8617e-02],\n",
       "                         [ 3.7532e-03, -3.0453e-03, -3.3701e-02]],\n",
       "               \n",
       "                        [[-1.7086e-02, -1.4788e-02, -3.8940e-03],\n",
       "                         [-6.3392e-02, -3.3190e-02, -4.4878e-02],\n",
       "                         [-3.9955e-02,  1.3318e-02,  1.3630e-02]],\n",
       "               \n",
       "                        [[-2.2971e-02,  1.3972e-02,  4.7967e-02],\n",
       "                         [ 2.2433e-02,  3.2503e-03, -3.8789e-04],\n",
       "                         [ 1.0756e-02,  1.0665e-02,  4.0786e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.7207e-02, -3.5612e-02,  3.2990e-02],\n",
       "                         [-1.0289e-02, -5.2001e-02,  2.2325e-02],\n",
       "                         [ 4.3186e-02, -5.0887e-03, -3.1521e-02]],\n",
       "               \n",
       "                        [[-7.2463e-03,  2.2586e-02, -4.4914e-02],\n",
       "                         [-2.0083e-02, -1.9633e-04, -3.0193e-02],\n",
       "                         [ 3.4482e-02, -5.3447e-03,  2.6395e-02]],\n",
       "               \n",
       "                        [[ 2.2625e-02, -2.5691e-03, -5.2997e-02],\n",
       "                         [-3.9718e-02, -5.9920e-02, -7.0250e-02],\n",
       "                         [-3.1720e-02, -5.6156e-03, -3.6102e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.1116e-02,  4.1400e-02,  7.7896e-02],\n",
       "                         [-5.2314e-03,  5.7153e-02,  6.5850e-02],\n",
       "                         [ 6.1248e-02,  3.1912e-02,  8.1214e-03]],\n",
       "               \n",
       "                        [[-1.7594e-02, -2.7667e-02,  6.5020e-03],\n",
       "                         [-2.5013e-02,  3.1521e-02,  1.0809e-02],\n",
       "                         [-4.3111e-03, -8.4143e-05, -1.1235e-02]],\n",
       "               \n",
       "                        [[ 2.0829e-02,  8.6329e-03, -4.2440e-02],\n",
       "                         [-2.5366e-02,  2.7901e-02, -9.4518e-03],\n",
       "                         [ 8.4475e-03,  5.2818e-02, -3.1964e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2820e-02, -1.8504e-02, -6.6513e-02],\n",
       "                         [-1.3898e-03, -2.1544e-02, -1.7567e-02],\n",
       "                         [-5.2538e-02,  1.3536e-02,  1.3664e-03]],\n",
       "               \n",
       "                        [[ 5.7266e-02,  3.3359e-02,  2.5032e-03],\n",
       "                         [ 6.3723e-02,  6.3739e-02,  5.4753e-02],\n",
       "                         [-1.3828e-02,  1.9169e-02,  1.1911e-02]],\n",
       "               \n",
       "                        [[-2.6761e-02,  1.5723e-02, -1.2717e-02],\n",
       "                         [-2.4329e-02,  3.9293e-03, -3.9389e-02],\n",
       "                         [ 1.6449e-02,  1.2233e-02, -1.2991e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0369e-02,  2.0749e-02,  7.4456e-02],\n",
       "                         [ 7.0774e-02,  4.0400e-02,  2.0487e-02],\n",
       "                         [ 3.9115e-02,  3.1699e-02,  2.0401e-02]],\n",
       "               \n",
       "                        [[-2.9395e-02,  2.5912e-02, -1.5670e-02],\n",
       "                         [ 3.6106e-02, -5.6428e-03, -1.2668e-02],\n",
       "                         [ 1.1267e-02, -1.9298e-02,  1.0444e-02]],\n",
       "               \n",
       "                        [[ 6.6017e-02,  9.4842e-02,  1.0661e-01],\n",
       "                         [ 5.6030e-02,  4.9520e-02, -1.6742e-02],\n",
       "                         [ 3.1501e-02,  5.3863e-02,  6.5480e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.5279e-03,  2.6637e-02, -1.5511e-02],\n",
       "                         [ 1.0471e-02, -2.2111e-02, -1.6427e-02],\n",
       "                         [ 6.6035e-02,  2.6296e-02,  9.1037e-04]],\n",
       "               \n",
       "                        [[-1.9806e-02, -5.3623e-03, -2.7325e-02],\n",
       "                         [-2.5083e-02, -1.0296e-02, -1.6746e-02],\n",
       "                         [ 2.9134e-02, -5.8477e-03, -2.6059e-03]],\n",
       "               \n",
       "                        [[-3.5956e-02,  9.9449e-03,  7.3160e-03],\n",
       "                         [-3.3580e-02, -1.2026e-02,  1.8119e-02],\n",
       "                         [-5.9664e-02, -2.7382e-02, -9.3631e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0678e-02,  2.4934e-02,  2.0218e-02],\n",
       "                         [ 4.2124e-02,  6.6200e-03,  6.8485e-02],\n",
       "                         [ 4.8555e-02,  3.4361e-03, -4.3516e-03]],\n",
       "               \n",
       "                        [[ 2.7361e-02, -1.3040e-02,  1.2497e-02],\n",
       "                         [ 2.7245e-02,  7.9802e-03,  6.1445e-02],\n",
       "                         [ 2.1408e-02, -2.5271e-02, -1.4236e-02]],\n",
       "               \n",
       "                        [[ 2.6388e-02,  1.0168e-02,  5.2068e-03],\n",
       "                         [-1.7016e-02, -1.5717e-02, -2.3489e-02],\n",
       "                         [ 2.1516e-03,  2.9778e-02, -4.5113e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0460e-02, -1.1264e-02,  1.8085e-02],\n",
       "                         [ 6.7404e-02, -4.3560e-03,  7.5595e-02],\n",
       "                         [ 3.1166e-02,  8.9611e-02,  2.6672e-02]],\n",
       "               \n",
       "                        [[ 3.6279e-03, -4.4067e-02,  2.6130e-02],\n",
       "                         [-2.2956e-02,  3.1149e-02,  1.0416e-02],\n",
       "                         [ 2.2577e-02, -3.9788e-02,  4.6509e-02]],\n",
       "               \n",
       "                        [[ 1.1394e-02,  2.5146e-03, -3.5653e-02],\n",
       "                         [-4.5091e-02,  1.8573e-02, -5.8179e-02],\n",
       "                         [-2.0710e-02, -1.1970e-02, -3.7716e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.0215e-02, -2.5592e-02, -7.2966e-03],\n",
       "                         [-6.4089e-03,  8.9794e-03,  4.4473e-03],\n",
       "                         [-5.4829e-02, -2.1084e-02,  1.4473e-03]],\n",
       "               \n",
       "                        [[ 5.4302e-02,  6.5507e-02,  8.2479e-02],\n",
       "                         [ 5.4617e-02,  6.6363e-02,  1.0264e-01],\n",
       "                         [ 6.9296e-02,  1.1446e-01,  1.0007e-01]],\n",
       "               \n",
       "                        [[-2.8678e-02, -2.0243e-02, -2.5346e-02],\n",
       "                         [-5.8157e-02, -1.5913e-02, -2.3890e-02],\n",
       "                         [ 6.0083e-03, -2.9671e-02, -3.1314e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5507e-02, -1.0065e-02,  3.0881e-02],\n",
       "                         [ 3.1270e-03, -3.6857e-02,  1.6174e-02],\n",
       "                         [-4.4302e-02, -1.9763e-02, -2.2433e-02]],\n",
       "               \n",
       "                        [[ 4.3493e-02,  8.6395e-02,  3.7174e-02],\n",
       "                         [ 2.9114e-02,  2.4833e-02, -1.4103e-02],\n",
       "                         [ 3.0108e-02,  8.4913e-02,  2.5513e-02]],\n",
       "               \n",
       "                        [[ 5.9403e-03,  9.7578e-03, -2.9943e-02],\n",
       "                         [ 1.4497e-03,  2.5015e-02, -1.4867e-02],\n",
       "                         [ 3.0806e-02,  3.3726e-02,  1.3615e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.0357,  0.0166,  0.0038, -0.0068, -0.0011, -0.0061, -0.0103, -0.0034,\n",
       "                       -0.0166,  0.0006,  0.0040, -0.0227, -0.0109, -0.0218, -0.0041,  0.0159,\n",
       "                        0.0249,  0.0085,  0.0003, -0.0096, -0.0051,  0.0175, -0.0161,  0.0572,\n",
       "                       -0.0335,  0.0107,  0.0247,  0.0042,  0.0377, -0.0171, -0.1015, -0.0228,\n",
       "                       -0.0087, -0.0797, -0.0014,  0.0367, -0.0072, -0.0147,  0.0137, -0.0163,\n",
       "                       -0.0089,  0.0034, -0.0174, -0.0188, -0.0105,  0.0368, -0.0123, -0.0034,\n",
       "                        0.0002,  0.0054, -0.0139,  0.0104, -0.0051, -0.0017,  0.0151, -0.0027,\n",
       "                        0.0474,  0.0014,  0.0009,  0.0025, -0.0011, -0.0130,  0.0087, -0.0031,\n",
       "                       -0.0616, -0.0055, -0.0170, -0.0149, -0.0131, -0.0219, -0.0087,  0.0045,\n",
       "                        0.0055,  0.0209, -0.0233, -0.0038, -0.0206, -0.0121,  0.0124, -0.0020,\n",
       "                       -0.0793,  0.0078,  0.0091, -0.0271,  0.0183, -0.0040, -0.0276, -0.0200,\n",
       "                       -0.0018, -0.0120,  0.0062,  0.0035, -0.0193, -0.0043,  0.0060, -0.0033,\n",
       "                        0.0979,  0.0046,  0.0230, -0.0159,  0.0214,  0.0229, -0.0133,  0.0055,\n",
       "                        0.0117, -0.0413,  0.0519, -0.0069, -0.0121,  0.0104,  0.0603,  0.0050,\n",
       "                       -0.0129, -0.0009,  0.0235,  0.0443, -0.0130, -0.0091, -0.0486,  0.0067,\n",
       "                       -0.0302,  0.0049, -0.0119, -0.0003,  0.0028, -0.0006, -0.0140, -0.0030],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([ 0.0080, -0.1051, -0.0583, -0.2114, -0.0156, -0.3025, -0.1431, -0.1690,\n",
       "                       -0.1414, -0.1600, -0.0668, -0.2989, -0.3124, -0.2248, -0.1399, -0.2922,\n",
       "                       -0.1752, -0.2401, -0.0152, -0.2591, -0.2447, -0.1701, -0.1470, -0.2224,\n",
       "                       -0.0362, -0.2455, -0.1750, -0.0396, -0.2098, -0.1359, -0.3048, -0.0343,\n",
       "                        0.0020, -0.1888, -0.2021, -0.0549, -0.1270, -0.1581, -0.1407, -0.0158,\n",
       "                       -0.0772, -0.0932, -0.1724, -0.2669, -0.1154, -0.2852, -0.1887, -0.0445,\n",
       "                       -0.1951, -0.1330, -0.0634, -0.0719, -0.0925, -0.0487, -0.1158, -0.0116,\n",
       "                       -0.3001, -0.2889, -0.1159, -0.0192, -0.1558, -0.1853, -0.1915, -0.1553,\n",
       "                       -0.1455, -0.2951, -0.2013,  0.0113, -0.0357, -0.2242, -0.0636, -0.3184,\n",
       "                       -0.2581, -0.1903, -0.0680, -0.1121, -0.2960, -0.1810, -0.0241, -0.2858,\n",
       "                       -0.0688, -0.1451, -0.0308, -0.1205, -0.3220, -0.3186, -0.0910, -0.1209,\n",
       "                       -0.0116, -0.1675, -0.0689, -0.1094, -0.1885, -0.1492,  0.0643, -0.2327,\n",
       "                       -0.1868, -0.1117, -0.1298, -0.1076, -0.2204, -0.2048, -0.1689, -0.1174,\n",
       "                       -0.1029, -0.1313, -0.2718, -0.1771, -0.0555, -0.1106, -0.2870, -0.0997,\n",
       "                       -0.1625, -0.1487, -0.3023, -0.2512, -0.2352, -0.0299, -0.0183, -0.0160,\n",
       "                       -0.2046, -0.2446, -0.2953, -0.1546, -0.1913, -0.0767, -0.0154, -0.0886],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([1.3516, 1.1140, 1.1335, 1.0717, 1.3636, 0.7596, 1.1063, 1.2343, 1.0519,\n",
       "                       1.5841, 1.0131, 1.0762, 1.1994, 1.4031, 1.1075, 0.9533, 1.0498, 1.0185,\n",
       "                       1.1805, 1.0876, 0.7843, 1.0940, 1.0523, 0.8384, 1.0683, 0.7602, 0.9357,\n",
       "                       1.2673, 0.9396, 1.0553, 0.9735, 1.3504, 1.2847, 1.0209, 1.1235, 1.0936,\n",
       "                       1.1686, 0.9953, 1.0376, 1.3279, 1.3441, 1.0842, 1.2170, 1.0637, 1.1351,\n",
       "                       1.0148, 1.0655, 1.2049, 1.2501, 1.0279, 1.1874, 1.2345, 1.1056, 1.6950,\n",
       "                       1.0704, 1.3799, 0.9915, 1.0065, 1.1655, 1.3675, 1.2508, 0.9559, 0.8257,\n",
       "                       1.2192, 1.0724, 1.2390, 0.9456, 1.2447, 1.2510, 0.8256, 1.2509, 0.9981,\n",
       "                       1.2127, 1.0307, 1.1309, 1.1541, 1.0804, 1.2754, 1.1647, 0.7533, 1.2607,\n",
       "                       1.0583, 1.2901, 0.9747, 0.7526, 1.0185, 1.1555, 1.1468, 1.3335, 1.1562,\n",
       "                       1.3142, 1.2976, 1.2818, 1.0301, 1.4043, 0.9505, 0.9716, 1.0872, 1.1282,\n",
       "                       1.0112, 0.9712, 0.9378, 1.1418, 1.2569, 1.0719, 1.2371, 0.8701, 1.1466,\n",
       "                       1.2526, 1.2469, 0.7931, 1.2396, 1.0986, 1.1581, 0.7296, 0.8709, 1.0038,\n",
       "                       1.3437, 1.2107, 1.2521, 0.9667, 1.2116, 0.9963, 1.1073, 1.0488, 1.2081,\n",
       "                       1.1719, 1.4677], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-4.9095e-03, -1.0411e-02,  5.6891e-03,  ..., -4.5805e-03,\n",
       "                         3.6054e-03,  7.6953e-04],\n",
       "                       [-1.1041e-02, -4.2250e-03,  1.1841e-02,  ..., -7.0961e-03,\n",
       "                         1.0293e-02, -1.3303e-02],\n",
       "                       [ 1.2086e-02,  3.3581e-03,  1.2667e-05,  ..., -2.6210e-03,\n",
       "                         6.9780e-03, -2.0183e-02],\n",
       "                       [-1.2930e-02, -5.7542e-03,  8.0124e-03,  ..., -9.6367e-03,\n",
       "                         2.3009e-04, -4.6410e-03],\n",
       "                       [-7.4690e-03, -9.8778e-03,  1.0700e-02,  ..., -4.4659e-03,\n",
       "                         9.0888e-03, -5.3043e-03]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0318,  0.0158,  0.0506, -0.1405,  0.1046], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.481154550075531,\n",
       "   1.3577819538116456,\n",
       "   1.3073870079517365,\n",
       "   1.2560416444540023,\n",
       "   1.2139344412088393,\n",
       "   1.1738374639749527,\n",
       "   1.1513489400148391,\n",
       "   1.1156441736221314,\n",
       "   1.1000408185720443,\n",
       "   1.1022559398412703,\n",
       "   1.0600120342969894,\n",
       "   1.0610931652784348,\n",
       "   1.0509943833351136,\n",
       "   1.0297512702941896,\n",
       "   1.0259157729148864,\n",
       "   1.0170646781921386,\n",
       "   1.0025434201955796,\n",
       "   0.9979545515775681,\n",
       "   0.9981109420061112,\n",
       "   0.983209489941597,\n",
       "   0.9851244906187058,\n",
       "   0.9585923129320144,\n",
       "   0.9533718949556351,\n",
       "   0.9537534577846527,\n",
       "   0.9565720618963242,\n",
       "   0.9413936160802842,\n",
       "   0.9310720965862275,\n",
       "   0.9261977025270463,\n",
       "   0.927066831946373,\n",
       "   0.9329067448377609,\n",
       "   0.912413406252861,\n",
       "   0.9153839558362961,\n",
       "   0.8964474000930787,\n",
       "   0.8877046661376953,\n",
       "   0.8930694156885147,\n",
       "   0.8728554663658142,\n",
       "   0.8791505703926087,\n",
       "   0.8727824296951294,\n",
       "   0.876187061548233,\n",
       "   0.8546114403605461,\n",
       "   0.8619972977638245,\n",
       "   0.8517441082596778,\n",
       "   0.8415768438577652,\n",
       "   0.8492840473651886,\n",
       "   0.843542504310608,\n",
       "   0.8249881924390793,\n",
       "   0.8268159502148629,\n",
       "   0.8218312242031097,\n",
       "   0.8166390725374222,\n",
       "   0.8181124683618546,\n",
       "   0.8208748137354851,\n",
       "   0.8045872502923012,\n",
       "   0.8046765422224998,\n",
       "   0.7958034241199493,\n",
       "   0.8077864516973495,\n",
       "   0.7975693554878235,\n",
       "   0.790548490345478,\n",
       "   0.7801121383309364,\n",
       "   0.7803864649534226,\n",
       "   0.7814394291639328,\n",
       "   0.7828030265569687,\n",
       "   0.7666941681504249,\n",
       "   0.7606288375258445,\n",
       "   0.7601050273180008,\n",
       "   0.7594932028055191,\n",
       "   0.7595013257265091,\n",
       "   0.7552100366353989,\n",
       "   0.7360448144674301,\n",
       "   0.7414670512080193,\n",
       "   0.7494295625090599,\n",
       "   0.7371460669636727,\n",
       "   0.7371414548754692,\n",
       "   0.7387767916321755,\n",
       "   0.7346621882319451,\n",
       "   0.729343874335289,\n",
       "   0.7154981326460839,\n",
       "   0.7241853189468384,\n",
       "   0.7200521076917649,\n",
       "   0.7168885003328324,\n",
       "   0.7137106335759162,\n",
       "   0.704644744694233,\n",
       "   0.712933917939663,\n",
       "   0.7101412235498429,\n",
       "   0.6976962233185768,\n",
       "   0.6835261573195457,\n",
       "   0.6959152084589004,\n",
       "   0.6946925252676011,\n",
       "   0.6936626036167145,\n",
       "   0.6855437688231468,\n",
       "   0.6757366685271263,\n",
       "   0.6853866739869118,\n",
       "   0.6831811745166778,\n",
       "   0.669134651184082,\n",
       "   0.674112578690052,\n",
       "   0.6681912115812302,\n",
       "   0.6822096589207649,\n",
       "   0.6632646207809448,\n",
       "   0.6614208935499192,\n",
       "   0.6562968887090683],\n",
       "  'train_loss_std': [0.1826323852296664,\n",
       "   0.1304803816692784,\n",
       "   0.13473082044900705,\n",
       "   0.1264148518281296,\n",
       "   0.1309927541645151,\n",
       "   0.13041765483289183,\n",
       "   0.13595038986421248,\n",
       "   0.1282007766301599,\n",
       "   0.125523515884704,\n",
       "   0.12920474467392193,\n",
       "   0.13149506488701052,\n",
       "   0.14458368235504027,\n",
       "   0.13563396875998307,\n",
       "   0.1340685289431707,\n",
       "   0.1269818281368983,\n",
       "   0.13682861508570374,\n",
       "   0.13270618698630288,\n",
       "   0.13596521509053586,\n",
       "   0.13223582941469023,\n",
       "   0.14225810632161273,\n",
       "   0.13308502067756578,\n",
       "   0.12679021754557895,\n",
       "   0.1413390589588845,\n",
       "   0.1359445235222156,\n",
       "   0.13791269515262905,\n",
       "   0.14443757268196517,\n",
       "   0.13392421180560674,\n",
       "   0.13951165945190877,\n",
       "   0.13877931301643015,\n",
       "   0.1358298267473773,\n",
       "   0.13501797264604687,\n",
       "   0.1349164184967577,\n",
       "   0.13695523673551566,\n",
       "   0.1451483073167463,\n",
       "   0.13982310347214305,\n",
       "   0.14314590149939144,\n",
       "   0.1363050076063552,\n",
       "   0.14002422585038526,\n",
       "   0.14759149039856237,\n",
       "   0.14450472539140388,\n",
       "   0.13188158767524813,\n",
       "   0.1350245778423253,\n",
       "   0.12905263484849128,\n",
       "   0.13764389183831624,\n",
       "   0.13881268250807685,\n",
       "   0.1354287122386498,\n",
       "   0.13985902688178561,\n",
       "   0.13178799312143952,\n",
       "   0.1342801378552925,\n",
       "   0.13451919669575796,\n",
       "   0.12829729237562648,\n",
       "   0.14118260486342377,\n",
       "   0.1308724887873948,\n",
       "   0.13919309404204383,\n",
       "   0.14872840812009974,\n",
       "   0.12976774577852346,\n",
       "   0.12731262007723013,\n",
       "   0.13881869922137072,\n",
       "   0.13462943655004367,\n",
       "   0.13770271167993803,\n",
       "   0.13847298443940023,\n",
       "   0.140252588668378,\n",
       "   0.1382651704000421,\n",
       "   0.13250643508855578,\n",
       "   0.13567018807668704,\n",
       "   0.14114834201387394,\n",
       "   0.13345703945506426,\n",
       "   0.13655771799823885,\n",
       "   0.12798300171630722,\n",
       "   0.1373892975461676,\n",
       "   0.12939987675503944,\n",
       "   0.1371566591935981,\n",
       "   0.1380250530490947,\n",
       "   0.13158649704415568,\n",
       "   0.13477428405938588,\n",
       "   0.13420257553948292,\n",
       "   0.1407853177902386,\n",
       "   0.14061222669207435,\n",
       "   0.13677513237132138,\n",
       "   0.13573551411253723,\n",
       "   0.13190207288199612,\n",
       "   0.132529215541455,\n",
       "   0.13839148092956385,\n",
       "   0.130502342266507,\n",
       "   0.1302888656064939,\n",
       "   0.1315539211756657,\n",
       "   0.12965038840868331,\n",
       "   0.13434221155013185,\n",
       "   0.13612430655629618,\n",
       "   0.13094215138925933,\n",
       "   0.13315066921480567,\n",
       "   0.13470950451381783,\n",
       "   0.1279483889866256,\n",
       "   0.13657829005808028,\n",
       "   0.12884267597012028,\n",
       "   0.14187728639938374,\n",
       "   0.12587805437370733,\n",
       "   0.13249774895751423,\n",
       "   0.13068641180660404],\n",
       "  'train_accuracy_mean': [0.4261333337724209,\n",
       "   0.4493066667318344,\n",
       "   0.4682266671061516,\n",
       "   0.4951066664457321,\n",
       "   0.5127066660523415,\n",
       "   0.5336266663074494,\n",
       "   0.5442933322191238,\n",
       "   0.5627199993133545,\n",
       "   0.5695866670012474,\n",
       "   0.565373331964016,\n",
       "   0.588639999628067,\n",
       "   0.5888799980282784,\n",
       "   0.5933466667532921,\n",
       "   0.601453332722187,\n",
       "   0.6016533324122428,\n",
       "   0.603493331849575,\n",
       "   0.6135866670608521,\n",
       "   0.6145866674780845,\n",
       "   0.6122266656756401,\n",
       "   0.6214533323645591,\n",
       "   0.6191599994301796,\n",
       "   0.6309333310723305,\n",
       "   0.6338266662359238,\n",
       "   0.6329333322644234,\n",
       "   0.6336533327102661,\n",
       "   0.6381600015163421,\n",
       "   0.6439333313703537,\n",
       "   0.6469600001573562,\n",
       "   0.6427999994754792,\n",
       "   0.6396266660690307,\n",
       "   0.6505066667199135,\n",
       "   0.6491199991106987,\n",
       "   0.6559199989438057,\n",
       "   0.6606533325314522,\n",
       "   0.6594800000190735,\n",
       "   0.6673466663360595,\n",
       "   0.6655200003981591,\n",
       "   0.6673333329558373,\n",
       "   0.6650399996638298,\n",
       "   0.6748266662359238,\n",
       "   0.6705600000619888,\n",
       "   0.6772666668891907,\n",
       "   0.6799200012087822,\n",
       "   0.6772000007033349,\n",
       "   0.6801466667056084,\n",
       "   0.6869466667175292,\n",
       "   0.686026665687561,\n",
       "   0.6902533336877823,\n",
       "   0.6920133324265481,\n",
       "   0.6886266642808914,\n",
       "   0.68794666659832,\n",
       "   0.6986266648769379,\n",
       "   0.6940266666412354,\n",
       "   0.699613334774971,\n",
       "   0.6942933332920075,\n",
       "   0.7006933341026306,\n",
       "   0.7033600001335144,\n",
       "   0.7074799988865852,\n",
       "   0.7057733334302903,\n",
       "   0.7049866656064987,\n",
       "   0.7036266678571701,\n",
       "   0.7119333344697952,\n",
       "   0.7138933347463607,\n",
       "   0.715853333234787,\n",
       "   0.7141999994516373,\n",
       "   0.7140400002002716,\n",
       "   0.7168533338308334,\n",
       "   0.725626667380333,\n",
       "   0.7227066665887832,\n",
       "   0.7193200001716614,\n",
       "   0.725413333773613,\n",
       "   0.7234800004959107,\n",
       "   0.723253332734108,\n",
       "   0.7253733327388764,\n",
       "   0.7260933326482772,\n",
       "   0.7316666649580001,\n",
       "   0.7288933338522912,\n",
       "   0.7306133338212967,\n",
       "   0.7303333345651627,\n",
       "   0.7314266653060914,\n",
       "   0.735506667137146,\n",
       "   0.7332399996519089,\n",
       "   0.7349199990034103,\n",
       "   0.7404933340549469,\n",
       "   0.7425599994659424,\n",
       "   0.7415066667795182,\n",
       "   0.7392666659355164,\n",
       "   0.7413466668128967,\n",
       "   0.7460399987697601,\n",
       "   0.746693333029747,\n",
       "   0.7457199994325637,\n",
       "   0.7445866672992706,\n",
       "   0.7506400004625321,\n",
       "   0.7486000003814697,\n",
       "   0.7513066667318344,\n",
       "   0.7477466658353805,\n",
       "   0.7541600004434585,\n",
       "   0.7561466666460037,\n",
       "   0.7561733330488205],\n",
       "  'train_accuracy_std': [0.06591361322123487,\n",
       "   0.06227793756495563,\n",
       "   0.06835308367811276,\n",
       "   0.06538203616534438,\n",
       "   0.06831468476400906,\n",
       "   0.06832798748275302,\n",
       "   0.07193369262052633,\n",
       "   0.06729224365959229,\n",
       "   0.06510100054305314,\n",
       "   0.06479930119070763,\n",
       "   0.06795959040083484,\n",
       "   0.07165900098586855,\n",
       "   0.06741513048744034,\n",
       "   0.0680922502859073,\n",
       "   0.06673413008510871,\n",
       "   0.06553351161599054,\n",
       "   0.06747923416971312,\n",
       "   0.06650268395691467,\n",
       "   0.0683710270020473,\n",
       "   0.07152263884476014,\n",
       "   0.06612652072070295,\n",
       "   0.06206032981381363,\n",
       "   0.0703668251657764,\n",
       "   0.06648789418471994,\n",
       "   0.06728701246625754,\n",
       "   0.07155660559517912,\n",
       "   0.066334473167138,\n",
       "   0.06834229755388486,\n",
       "   0.06930627723079129,\n",
       "   0.06537307174004116,\n",
       "   0.06784433944616584,\n",
       "   0.06691306983620103,\n",
       "   0.0668841311562931,\n",
       "   0.069106165892124,\n",
       "   0.06901591305095696,\n",
       "   0.06915219765764169,\n",
       "   0.0647862887261049,\n",
       "   0.06623258767813986,\n",
       "   0.0718419459488728,\n",
       "   0.07169497800973891,\n",
       "   0.06508761424464551,\n",
       "   0.0657706116016513,\n",
       "   0.06296801911217323,\n",
       "   0.06590080222099384,\n",
       "   0.06840842025300041,\n",
       "   0.06627492772510031,\n",
       "   0.06543352288977157,\n",
       "   0.06403907092916886,\n",
       "   0.06840104002154876,\n",
       "   0.06458743119987016,\n",
       "   0.06211642687526532,\n",
       "   0.06738927239924632,\n",
       "   0.06253929844881151,\n",
       "   0.06659951138714797,\n",
       "   0.07149553758333171,\n",
       "   0.06295719321740238,\n",
       "   0.06128475736175209,\n",
       "   0.06637372883572769,\n",
       "   0.06555880956323262,\n",
       "   0.0679099067738081,\n",
       "   0.06434354386270719,\n",
       "   0.06817295001884771,\n",
       "   0.06506917563911233,\n",
       "   0.061723078532587426,\n",
       "   0.0640697890322431,\n",
       "   0.0658114347887569,\n",
       "   0.06567976711477352,\n",
       "   0.06569548135653552,\n",
       "   0.061942325345967345,\n",
       "   0.06522289835833038,\n",
       "   0.06401411539999399,\n",
       "   0.06662449371062536,\n",
       "   0.06412274824441129,\n",
       "   0.06401123695914206,\n",
       "   0.06404671884218813,\n",
       "   0.06506594910096727,\n",
       "   0.06656206531730184,\n",
       "   0.06710573129074256,\n",
       "   0.06630150024425562,\n",
       "   0.06420529746841241,\n",
       "   0.06424250044563738,\n",
       "   0.06427503455762233,\n",
       "   0.06539770887594931,\n",
       "   0.06324433107517077,\n",
       "   0.06368971393939064,\n",
       "   0.06288010494075127,\n",
       "   0.06242716640396464,\n",
       "   0.06472409529633813,\n",
       "   0.06565402924516457,\n",
       "   0.0637313745094133,\n",
       "   0.06218461308944313,\n",
       "   0.06251565245503438,\n",
       "   0.06131912101622321,\n",
       "   0.06498748440065961,\n",
       "   0.062471889729398905,\n",
       "   0.06593726125482527,\n",
       "   0.059265363121513114,\n",
       "   0.06267656471033763,\n",
       "   0.061985131336572095],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.471176369190216,\n",
       "   1.4203386040528616,\n",
       "   1.3806963209311167,\n",
       "   1.3533262141545614,\n",
       "   1.3228069829940796,\n",
       "   1.2923235789934795,\n",
       "   1.2658109680811565,\n",
       "   1.2479879687229791,\n",
       "   1.22375756641229,\n",
       "   1.2312382558981578,\n",
       "   1.2062380532423655,\n",
       "   1.201440934141477,\n",
       "   1.1956076474984487,\n",
       "   1.193373558918635,\n",
       "   1.1765884820620218,\n",
       "   1.17311465660731,\n",
       "   1.187868253191312,\n",
       "   1.1736191300551098,\n",
       "   1.1559246160586676,\n",
       "   1.1491659792264302,\n",
       "   1.1453562692801158,\n",
       "   1.1545843807856242,\n",
       "   1.1417370017369588,\n",
       "   1.1338486299912134,\n",
       "   1.1240334182977676,\n",
       "   1.1281356239318847,\n",
       "   1.1178909635543823,\n",
       "   1.1150680551926295,\n",
       "   1.103643309076627,\n",
       "   1.1039624404907227,\n",
       "   1.0983565264940263,\n",
       "   1.0944552268584569,\n",
       "   1.0878954009215036,\n",
       "   1.0826534553368887,\n",
       "   1.084435091416041,\n",
       "   1.0796767693758011,\n",
       "   1.0691722273826598,\n",
       "   1.075462252298991,\n",
       "   1.0627307752768198,\n",
       "   1.069868769844373,\n",
       "   1.058972169359525,\n",
       "   1.0591596617301304,\n",
       "   1.056293647289276,\n",
       "   1.0516797292232514,\n",
       "   1.0464338860909144,\n",
       "   1.04096601943175,\n",
       "   1.0390473582347235,\n",
       "   1.0437529691060383,\n",
       "   1.0289837962388992,\n",
       "   1.0326292578379312,\n",
       "   1.0349435208241144,\n",
       "   1.03252683142821,\n",
       "   1.0310525498787562,\n",
       "   1.0104590525229773,\n",
       "   1.0139344509442647,\n",
       "   1.0106956521670023,\n",
       "   1.032102631131808,\n",
       "   1.0090903135140736,\n",
       "   1.0105778209368388,\n",
       "   1.0125532406568527,\n",
       "   1.0054045470555624,\n",
       "   0.9964819796880087,\n",
       "   0.9941920403639476,\n",
       "   0.9932256497939428,\n",
       "   0.9877218504746755,\n",
       "   0.9872963384787241,\n",
       "   0.9900266283750534,\n",
       "   0.9877198950449626,\n",
       "   0.9829322874546051,\n",
       "   0.9856233690182368,\n",
       "   0.9900106473763783,\n",
       "   0.9901331992944081,\n",
       "   0.9795691275596619,\n",
       "   0.9728402330478032,\n",
       "   0.9765372564395268,\n",
       "   0.9615845106045405,\n",
       "   0.9775967270135879,\n",
       "   0.9674115592241287,\n",
       "   0.9670706542332967,\n",
       "   0.9565932681163152,\n",
       "   0.9645783323049545,\n",
       "   0.954222569068273,\n",
       "   0.9548276960849762,\n",
       "   0.9615878440936406,\n",
       "   0.9601044146219889,\n",
       "   0.9499993185202281,\n",
       "   0.951361569960912,\n",
       "   0.9399580576022466,\n",
       "   0.9446669640143712,\n",
       "   0.9545165171225866,\n",
       "   0.9450459994872411,\n",
       "   0.9451093810796738,\n",
       "   0.9568006992340088,\n",
       "   0.9583671281735102,\n",
       "   0.9470258406798044,\n",
       "   0.9410540425777435,\n",
       "   0.9342471953233084,\n",
       "   0.9384942634900411,\n",
       "   0.9439093967278799],\n",
       "  'val_loss_std': [0.11203740130864129,\n",
       "   0.10715929112252705,\n",
       "   0.09827341826141245,\n",
       "   0.10125932640351137,\n",
       "   0.10043295801446125,\n",
       "   0.10740659524183253,\n",
       "   0.11049734687425121,\n",
       "   0.11983441107245077,\n",
       "   0.11930227996812569,\n",
       "   0.11529669617151428,\n",
       "   0.11472055706061447,\n",
       "   0.11461747819445803,\n",
       "   0.1188223925285219,\n",
       "   0.12176546787710572,\n",
       "   0.11885147128588246,\n",
       "   0.11581106846402765,\n",
       "   0.11909995483656824,\n",
       "   0.12196800945397059,\n",
       "   0.12398465967775586,\n",
       "   0.12429102030429559,\n",
       "   0.12626138369322082,\n",
       "   0.12399722233293942,\n",
       "   0.12768290815715796,\n",
       "   0.12657912390755227,\n",
       "   0.1270241844349698,\n",
       "   0.12868818947660365,\n",
       "   0.1277192766153749,\n",
       "   0.1314605584614048,\n",
       "   0.13090726697788443,\n",
       "   0.1326811614541339,\n",
       "   0.12793874582377707,\n",
       "   0.1295033536529318,\n",
       "   0.13508594559952292,\n",
       "   0.1256389745632251,\n",
       "   0.1276599462085666,\n",
       "   0.13297767868085042,\n",
       "   0.13163224099730259,\n",
       "   0.1336244294053933,\n",
       "   0.13202423489457363,\n",
       "   0.1308216289214086,\n",
       "   0.1334729536791329,\n",
       "   0.13540187290704866,\n",
       "   0.1331894646012306,\n",
       "   0.13424498999450782,\n",
       "   0.13375660790981328,\n",
       "   0.13291695676870965,\n",
       "   0.13276573331422253,\n",
       "   0.13617518701132503,\n",
       "   0.13315697343223637,\n",
       "   0.131602270458581,\n",
       "   0.13268724730948828,\n",
       "   0.133248200070377,\n",
       "   0.1334498839268254,\n",
       "   0.13341375022411908,\n",
       "   0.1332544375230059,\n",
       "   0.13522877668616448,\n",
       "   0.13646125097249184,\n",
       "   0.13827714189175325,\n",
       "   0.13632873247995536,\n",
       "   0.1397483090422933,\n",
       "   0.14107534466995766,\n",
       "   0.13191791023486107,\n",
       "   0.13609270302166718,\n",
       "   0.1373436265325103,\n",
       "   0.13782754732955282,\n",
       "   0.13881340803437905,\n",
       "   0.13607888968501025,\n",
       "   0.13811387686912519,\n",
       "   0.13513214094033638,\n",
       "   0.13763618903021407,\n",
       "   0.14120659189166673,\n",
       "   0.13886795139057656,\n",
       "   0.13668628195830615,\n",
       "   0.13594511421080996,\n",
       "   0.1339807457042434,\n",
       "   0.13636625945424435,\n",
       "   0.14274674896351155,\n",
       "   0.13894214011758702,\n",
       "   0.13304951145628094,\n",
       "   0.13509201511513172,\n",
       "   0.14027035249475067,\n",
       "   0.13705290740066123,\n",
       "   0.13767863220396992,\n",
       "   0.1387881382663321,\n",
       "   0.13586516452145064,\n",
       "   0.14269873294848856,\n",
       "   0.13649695558837954,\n",
       "   0.13155420116093455,\n",
       "   0.13704813390960915,\n",
       "   0.14400514639874926,\n",
       "   0.14107233424250729,\n",
       "   0.14060849573919304,\n",
       "   0.14711724698209286,\n",
       "   0.1410907367007103,\n",
       "   0.13930975593546616,\n",
       "   0.14175003598064614,\n",
       "   0.14033359981714238,\n",
       "   0.13476484911151734,\n",
       "   0.13750004130782806],\n",
       "  'val_accuracy_mean': [0.4045111114283403,\n",
       "   0.41722222248713176,\n",
       "   0.4324888893961906,\n",
       "   0.4456444451212883,\n",
       "   0.45973333438237507,\n",
       "   0.47355555643637975,\n",
       "   0.485844445625941,\n",
       "   0.49588888804117837,\n",
       "   0.5084666676322619,\n",
       "   0.5044666656851768,\n",
       "   0.5158888885378837,\n",
       "   0.5172444433967273,\n",
       "   0.5201333321134249,\n",
       "   0.5220222216844559,\n",
       "   0.5298222202062607,\n",
       "   0.53033333192269,\n",
       "   0.5256666652361552,\n",
       "   0.5339777773618698,\n",
       "   0.5385111107428868,\n",
       "   0.5424444432059924,\n",
       "   0.543244443833828,\n",
       "   0.5373111102978388,\n",
       "   0.5459999985496203,\n",
       "   0.550755555431048,\n",
       "   0.553444446225961,\n",
       "   0.5526666683952014,\n",
       "   0.5570666654904683,\n",
       "   0.5590888892610868,\n",
       "   0.5632888871431351,\n",
       "   0.563711110452811,\n",
       "   0.5653777765234311,\n",
       "   0.568422221938769,\n",
       "   0.5713999994595845,\n",
       "   0.5731777773300807,\n",
       "   0.5676222208142281,\n",
       "   0.5737999984622002,\n",
       "   0.5795555543899537,\n",
       "   0.5761999988555908,\n",
       "   0.5801555547118187,\n",
       "   0.5775555542111397,\n",
       "   0.584933332502842,\n",
       "   0.584577779173851,\n",
       "   0.5832666645447413,\n",
       "   0.5870222210884094,\n",
       "   0.5891777774691582,\n",
       "   0.5897111116846403,\n",
       "   0.5926888887087504,\n",
       "   0.5913777764638265,\n",
       "   0.5966444445649782,\n",
       "   0.5947777767976125,\n",
       "   0.593177777826786,\n",
       "   0.5961777770519257,\n",
       "   0.5967333329717318,\n",
       "   0.6039111100633939,\n",
       "   0.5999999997019768,\n",
       "   0.6032444436351458,\n",
       "   0.5947777771949768,\n",
       "   0.6045777769883474,\n",
       "   0.6029333333174388,\n",
       "   0.6017555550734202,\n",
       "   0.6071777763962746,\n",
       "   0.6112444439530372,\n",
       "   0.6100222223997116,\n",
       "   0.6110222214460372,\n",
       "   0.6153333310286204,\n",
       "   0.6140222209692001,\n",
       "   0.6119555546840032,\n",
       "   0.6158888883392016,\n",
       "   0.6154888861378034,\n",
       "   0.6144222230712573,\n",
       "   0.612022221883138,\n",
       "   0.6133555539449056,\n",
       "   0.6153111106157303,\n",
       "   0.6194888902703921,\n",
       "   0.616088885863622,\n",
       "   0.6242444427808126,\n",
       "   0.6175777745246888,\n",
       "   0.6214222213625908,\n",
       "   0.6218444431821505,\n",
       "   0.6247555573781332,\n",
       "   0.622711109717687,\n",
       "   0.6277111102143923,\n",
       "   0.6264444435636203,\n",
       "   0.6237333337465922,\n",
       "   0.6238444447517395,\n",
       "   0.6310888887445132,\n",
       "   0.6279999990264574,\n",
       "   0.6320666670799255,\n",
       "   0.6319333316882452,\n",
       "   0.630444445113341,\n",
       "   0.6305555550257365,\n",
       "   0.6290666659673055,\n",
       "   0.6284222219387691,\n",
       "   0.6274222214023272,\n",
       "   0.6303555554151535,\n",
       "   0.6347555540998777,\n",
       "   0.6359333327412605,\n",
       "   0.6359555553396543,\n",
       "   0.6325333312153816],\n",
       "  'val_accuracy_std': [0.05353799162002322,\n",
       "   0.0524720981914053,\n",
       "   0.05174179064930801,\n",
       "   0.0545594439699906,\n",
       "   0.05402814078556591,\n",
       "   0.05838780569726321,\n",
       "   0.058177808417806676,\n",
       "   0.059355695041157855,\n",
       "   0.061534849233953345,\n",
       "   0.05942979616153935,\n",
       "   0.06035072230962436,\n",
       "   0.057592111503469996,\n",
       "   0.05651335922892677,\n",
       "   0.06100806981501532,\n",
       "   0.06108741345146006,\n",
       "   0.05955669406457232,\n",
       "   0.058520333169778685,\n",
       "   0.06090453432575346,\n",
       "   0.06073382762738213,\n",
       "   0.0630527898408561,\n",
       "   0.06264413916069356,\n",
       "   0.0613480611002943,\n",
       "   0.06201911090369558,\n",
       "   0.0629110434808522,\n",
       "   0.062440392068660824,\n",
       "   0.06351319518242336,\n",
       "   0.06256483259522035,\n",
       "   0.06216751088082387,\n",
       "   0.06503216901734982,\n",
       "   0.063530729283033,\n",
       "   0.06337751299008496,\n",
       "   0.06272361094956662,\n",
       "   0.06447941994903779,\n",
       "   0.06387816796777328,\n",
       "   0.06152727250719634,\n",
       "   0.06408735869920205,\n",
       "   0.06256571892982157,\n",
       "   0.06175642809294948,\n",
       "   0.06404262086135958,\n",
       "   0.06290223682003454,\n",
       "   0.0639579955819006,\n",
       "   0.06262474690427144,\n",
       "   0.06504126291026241,\n",
       "   0.06563552119824073,\n",
       "   0.06647131003645682,\n",
       "   0.06358946703232453,\n",
       "   0.06259830473072665,\n",
       "   0.06436952090747526,\n",
       "   0.06365764528204357,\n",
       "   0.06382953621904496,\n",
       "   0.06245690339964687,\n",
       "   0.06149903542250107,\n",
       "   0.06350382894868392,\n",
       "   0.064204242829744,\n",
       "   0.06414104917256028,\n",
       "   0.06333795125922861,\n",
       "   0.06269611338101945,\n",
       "   0.06368271627047645,\n",
       "   0.06318921597259596,\n",
       "   0.0649081370904879,\n",
       "   0.0637927615753104,\n",
       "   0.06103617205479102,\n",
       "   0.06279065355775032,\n",
       "   0.06318924604310566,\n",
       "   0.06395484523229095,\n",
       "   0.06394186781302506,\n",
       "   0.06258764194025813,\n",
       "   0.06308949158158543,\n",
       "   0.06251712203552993,\n",
       "   0.06230509942086057,\n",
       "   0.06541423204172483,\n",
       "   0.061592445558287426,\n",
       "   0.06380363086714479,\n",
       "   0.06302408409497127,\n",
       "   0.06400549391932762,\n",
       "   0.06282414847505599,\n",
       "   0.0629419452437504,\n",
       "   0.06382191338091253,\n",
       "   0.061903194602919105,\n",
       "   0.061412204716973555,\n",
       "   0.06305833634416222,\n",
       "   0.06295163999330895,\n",
       "   0.06317485019100758,\n",
       "   0.06443362498009035,\n",
       "   0.06333829041430022,\n",
       "   0.06522325974022641,\n",
       "   0.06261463607075052,\n",
       "   0.060682747250622435,\n",
       "   0.06417772774643522,\n",
       "   0.06386289312281059,\n",
       "   0.06522998233880463,\n",
       "   0.06278136106601999,\n",
       "   0.06413203697901643,\n",
       "   0.06247979930146581,\n",
       "   0.06308154220281362,\n",
       "   0.06139410959916963,\n",
       "   0.06245279278670863,\n",
       "   0.06198397255791851,\n",
       "   0.06265919627346102],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6818444436788559,\n",
       " 'best_val_iter': 48000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 96,\n",
       " 'train_loss_mean': 0.4526471059322357,\n",
       " 'train_loss_std': 0.12993412983496969,\n",
       " 'train_accuracy_mean': 0.8343333342075347,\n",
       " 'train_accuracy_std': 0.0548158550248203,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.8508474173148474,\n",
       " 'val_loss_std': 0.14179470314582376,\n",
       " 'val_accuracy_mean': 0.6763111112515131,\n",
       " 'val_accuracy_std': 0.05848475377685013,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0115, -0.0778,  0.0524],\n",
       "                         [-0.0305, -0.0349, -0.0020],\n",
       "                         [-0.0158,  0.1034, -0.0275]],\n",
       "               \n",
       "                        [[ 0.0519, -0.0845,  0.0649],\n",
       "                         [-0.0253,  0.0089,  0.0542],\n",
       "                         [-0.0484,  0.0403, -0.0331]],\n",
       "               \n",
       "                        [[ 0.0545, -0.0248, -0.0204],\n",
       "                         [ 0.0301,  0.0602, -0.0543],\n",
       "                         [-0.0532,  0.0362, -0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0481,  0.1008,  0.0398],\n",
       "                         [ 0.0482, -0.0425,  0.0113],\n",
       "                         [-0.0911, -0.0673, -0.0469]],\n",
       "               \n",
       "                        [[-0.0092,  0.0265, -0.0018],\n",
       "                         [ 0.0446, -0.0628, -0.0604],\n",
       "                         [ 0.0496, -0.0558,  0.0711]],\n",
       "               \n",
       "                        [[-0.0320,  0.0804, -0.0688],\n",
       "                         [-0.0160,  0.0558,  0.0703],\n",
       "                         [-0.0326, -0.0326, -0.0280]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0071, -0.0199,  0.0549],\n",
       "                         [-0.0223, -0.0425, -0.0309],\n",
       "                         [ 0.0645,  0.0541, -0.0505]],\n",
       "               \n",
       "                        [[-0.0577, -0.0244,  0.0483],\n",
       "                         [ 0.0787, -0.0048,  0.0167],\n",
       "                         [-0.0691, -0.0398,  0.0441]],\n",
       "               \n",
       "                        [[ 0.0377, -0.0742,  0.0330],\n",
       "                         [ 0.0672,  0.0122, -0.0616],\n",
       "                         [-0.0667,  0.0326,  0.0182]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0424, -0.0622, -0.0432],\n",
       "                         [-0.0184, -0.0636,  0.0860],\n",
       "                         [-0.0358,  0.0483, -0.0402]],\n",
       "               \n",
       "                        [[ 0.0383,  0.0228,  0.0691],\n",
       "                         [ 0.0292, -0.0177, -0.0221],\n",
       "                         [ 0.0055,  0.0494, -0.0077]],\n",
       "               \n",
       "                        [[-0.0062, -0.0760, -0.0401],\n",
       "                         [-0.0594,  0.0223,  0.0496],\n",
       "                         [-0.0420,  0.0299,  0.0492]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0190, -0.0077, -0.0359],\n",
       "                         [ 0.0073,  0.0689, -0.0534],\n",
       "                         [ 0.0444,  0.0745, -0.0492]],\n",
       "               \n",
       "                        [[ 0.0429,  0.0223,  0.0724],\n",
       "                         [ 0.0197, -0.0301, -0.0356],\n",
       "                         [-0.0315, -0.0832,  0.0298]],\n",
       "               \n",
       "                        [[-0.0250, -0.0121, -0.0183],\n",
       "                         [-0.0136, -0.0266,  0.0769],\n",
       "                         [-0.0608, -0.0201,  0.0196]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0678,  0.0252, -0.0223],\n",
       "                         [-0.0489,  0.0072,  0.0349],\n",
       "                         [ 0.0568,  0.0435,  0.0784]],\n",
       "               \n",
       "                        [[-0.0682, -0.0912,  0.0311],\n",
       "                         [ 0.0605, -0.0843, -0.0335],\n",
       "                         [ 0.0032,  0.0237,  0.0296]],\n",
       "               \n",
       "                        [[ 0.0041, -0.0576,  0.0435],\n",
       "                         [ 0.0096, -0.0407, -0.0082],\n",
       "                         [-0.0129, -0.0269,  0.0120]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.0765e-03, -8.0369e-04, -1.8382e-03, -1.1685e-04, -1.5081e-04,\n",
       "                        9.4584e-04,  1.9420e-04,  1.0867e-04,  8.7550e-04, -1.9117e-03,\n",
       "                       -3.8360e-04,  1.2847e-04,  2.6250e-03,  1.7375e-03, -2.0646e-03,\n",
       "                        7.6395e-04, -6.3794e-04, -6.1944e-04, -7.8609e-04, -2.0221e-04,\n",
       "                        1.6218e-03, -2.4587e-04,  2.3333e-03,  2.2809e-03,  3.7578e-04,\n",
       "                        2.2750e-05,  6.9353e-04,  7.8915e-04,  1.0253e-03,  9.2878e-04,\n",
       "                        1.5839e-03, -6.1639e-05,  3.5597e-04, -3.5442e-03, -4.6407e-04,\n",
       "                        6.9834e-04,  1.1986e-03,  6.3443e-04, -1.2857e-05, -8.4554e-04,\n",
       "                        2.7600e-04, -3.0869e-03, -1.1622e-03,  2.0713e-04, -3.6686e-04,\n",
       "                       -7.5862e-05,  2.7620e-04, -1.3584e-03, -2.4717e-03, -1.9970e-05,\n",
       "                        1.1759e-03, -8.4054e-04,  1.2545e-03, -2.8829e-03,  1.1732e-03,\n",
       "                        1.6122e-04,  5.0176e-04,  3.3543e-04, -6.3797e-05, -1.7757e-03,\n",
       "                        2.5158e-04,  1.5920e-03, -5.9205e-04,  7.7316e-04,  3.4142e-04,\n",
       "                        7.5710e-04, -6.7782e-04,  1.7540e-03, -8.1071e-04, -5.7699e-04,\n",
       "                        1.4908e-04, -1.1079e-03, -1.0642e-05, -2.4850e-04,  1.3602e-03,\n",
       "                       -5.1398e-04,  8.3879e-04,  8.4650e-04, -1.5811e-04,  3.1155e-04,\n",
       "                        1.4772e-03, -2.1250e-03,  1.2379e-03,  9.4745e-05, -2.6874e-03,\n",
       "                       -8.9463e-04,  1.0693e-03, -1.5491e-03,  1.8957e-04,  2.8946e-03,\n",
       "                        1.1849e-03,  2.1482e-04,  2.0873e-04, -7.8655e-04,  1.2980e-03,\n",
       "                       -2.6352e-03,  1.9492e-03, -1.8718e-03, -1.6439e-03, -3.3162e-04,\n",
       "                        3.8143e-04, -7.6442e-04,  1.1236e-03, -1.1091e-04, -2.0625e-03,\n",
       "                        1.1619e-03, -4.9918e-04,  1.9502e-06, -1.0099e-03, -8.2773e-04,\n",
       "                        1.4017e-03,  1.5289e-03, -1.8522e-03, -2.8503e-04,  3.0935e-04,\n",
       "                        1.6916e-04, -2.2714e-04, -1.6422e-03,  4.0964e-04, -2.4318e-04,\n",
       "                       -6.1455e-05,  9.1467e-05,  9.6760e-04,  4.0069e-04,  1.5191e-03,\n",
       "                        1.3727e-04,  8.4586e-04, -1.2893e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1601, -0.0269,  0.2146, -0.1223, -0.1164,  0.0143, -0.1938, -0.2474,\n",
       "                       -0.1682, -0.1138, -0.1828, -0.1827,  0.0486, -0.1432, -0.0289, -0.1198,\n",
       "                        0.0172, -0.1315, -0.0307, -0.0695, -0.1784, -0.0808, -0.1548,  0.0150,\n",
       "                       -0.0257, -0.1688, -0.0726, -0.0996, -0.0490,  0.0397, -0.1129, -0.1187,\n",
       "                       -0.1021, -0.1569, -0.1652, -0.2654, -0.1424, -0.0779, -0.1773, -0.1431,\n",
       "                       -0.0401,  0.1688, -0.1125, -0.0631, -0.0056, -0.1462, -0.1993, -0.2671,\n",
       "                        0.1400, -0.1451,  0.0623, -0.2133,  0.2319,  0.0191, -0.0483, -0.0743,\n",
       "                       -0.2520, -0.0044, -0.1727,  0.1165, -0.1224,  0.2687, -0.0037, -0.2654,\n",
       "                       -0.1504, -0.2399, -0.1681, -0.1425, -0.0929, -0.0455,  0.0674, -0.0151,\n",
       "                       -0.1439, -0.2008, -0.0738, -0.0622, -0.1589, -0.0268, -0.0954, -0.1174,\n",
       "                        0.0008, -0.1366, -0.2038, -0.2098, -0.0300,  0.1649, -0.0133, -0.1394,\n",
       "                       -0.1591,  0.0642, -0.0768, -0.1414,  0.0458, -0.0098, -0.0927,  0.0210,\n",
       "                        0.1638,  0.0666, -0.1137, -0.1096, -0.2115, -0.0579,  0.0081,  0.0286,\n",
       "                        0.0164, -0.1907, -0.0867, -0.1441, -0.0205, -0.0744, -0.0806, -0.1129,\n",
       "                       -0.0395, -0.1456, -0.0760, -0.0752, -0.0798,  0.1828, -0.0423,  0.0638,\n",
       "                       -0.1928, -0.2187, -0.1924, -0.1746,  0.1367, -0.0291, -0.2259, -0.1710],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.0777, 1.0709, 1.0801, 0.8988, 0.9437, 1.0149, 0.9128, 0.8826, 0.9809,\n",
       "                       0.9149, 0.9107, 0.9115, 1.0526, 0.8928, 1.0546, 1.0420, 1.0268, 0.9431,\n",
       "                       0.9681, 0.9646, 0.9394, 0.8664, 0.9175, 1.0001, 0.9011, 0.8854, 0.9461,\n",
       "                       0.9155, 0.9748, 0.9375, 1.0159, 0.9061, 1.0190, 1.2193, 0.9277, 0.9384,\n",
       "                       0.9600, 0.9206, 0.9119, 0.9612, 0.9499, 1.0274, 0.9329, 0.9598, 1.0849,\n",
       "                       1.0767, 0.9834, 0.9287, 1.1041, 0.9253, 1.0169, 0.9134, 1.0635, 1.0854,\n",
       "                       1.0065, 0.9070, 1.1188, 1.1279, 0.9739, 1.0818, 0.9082, 1.0035, 0.9521,\n",
       "                       0.9313, 0.9319, 0.8501, 0.8868, 0.9298, 1.0671, 0.9991, 0.9329, 0.9704,\n",
       "                       0.8722, 0.8916, 0.9964, 0.8846, 0.9151, 1.0916, 1.0985, 1.0259, 1.0244,\n",
       "                       0.9868, 0.8907, 0.9007, 1.1403, 1.1089, 1.0677, 1.0212, 0.9175, 0.9868,\n",
       "                       1.0047, 0.9299, 1.0141, 1.0329, 0.9991, 1.0203, 1.1726, 0.9629, 0.9522,\n",
       "                       0.9195, 0.8685, 1.0409, 1.1071, 0.9493, 0.9616, 0.9556, 0.9301, 0.8482,\n",
       "                       1.0131, 0.9587, 1.0176, 1.0275, 0.9890, 1.1160, 0.9377, 1.0863, 0.9138,\n",
       "                       1.0243, 0.9302, 0.9634, 0.8600, 0.8900, 1.2006, 1.0526, 1.0483, 0.9347,\n",
       "                       0.9339, 0.8952], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-3.1068e-02,  1.4934e-02, -2.4708e-02],\n",
       "                         [-4.9840e-02, -2.8395e-02, -7.1771e-02],\n",
       "                         [-2.5563e-02,  2.5884e-02, -1.4416e-02]],\n",
       "               \n",
       "                        [[ 7.1535e-02,  3.6242e-02, -8.3086e-02],\n",
       "                         [-5.5571e-02, -6.0254e-03, -3.9204e-02],\n",
       "                         [-2.6580e-02,  7.6422e-02,  2.6242e-02]],\n",
       "               \n",
       "                        [[ 7.9697e-02,  1.2020e-02, -8.9087e-04],\n",
       "                         [-2.4565e-02,  6.1961e-02,  8.5690e-02],\n",
       "                         [ 1.0954e-02,  5.8338e-02,  8.1456e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3707e-04, -3.0362e-02, -3.2934e-02],\n",
       "                         [-5.1523e-02, -6.6658e-02, -3.6795e-02],\n",
       "                         [ 7.4546e-03,  1.6017e-03, -1.5633e-02]],\n",
       "               \n",
       "                        [[ 1.1219e-02,  3.6779e-02, -7.3502e-03],\n",
       "                         [-9.8202e-03,  2.9635e-02, -1.4038e-02],\n",
       "                         [ 3.2854e-02, -5.6267e-02, -1.8176e-02]],\n",
       "               \n",
       "                        [[-1.0652e-02,  4.8453e-02, -3.1925e-02],\n",
       "                         [-2.8970e-02,  1.5542e-02, -4.6588e-03],\n",
       "                         [-4.3812e-02, -2.1815e-02,  4.8127e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4941e-02, -3.0805e-02,  3.8058e-02],\n",
       "                         [ 3.1757e-02,  3.3032e-02, -4.8846e-02],\n",
       "                         [ 3.0079e-02, -1.2053e-03, -3.8244e-02]],\n",
       "               \n",
       "                        [[ 3.1452e-02,  6.4167e-02, -2.9219e-03],\n",
       "                         [-4.8711e-02,  4.5315e-04,  2.8953e-02],\n",
       "                         [-1.8084e-02, -3.7616e-02,  6.2568e-03]],\n",
       "               \n",
       "                        [[ 6.1893e-02,  2.1873e-02, -4.9432e-02],\n",
       "                         [-1.2321e-02, -1.1952e-03, -2.0872e-02],\n",
       "                         [-2.9053e-02, -1.8110e-02, -4.7141e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1403e-02,  5.6295e-03,  4.0007e-02],\n",
       "                         [-4.6823e-02, -2.6105e-02, -2.8252e-02],\n",
       "                         [-1.2773e-02,  1.2540e-02, -1.2665e-02]],\n",
       "               \n",
       "                        [[-2.8528e-02, -7.3125e-02, -6.5685e-02],\n",
       "                         [ 1.5529e-02,  4.3157e-03,  4.4955e-03],\n",
       "                         [ 1.7821e-02, -4.4237e-02,  1.1965e-02]],\n",
       "               \n",
       "                        [[-1.4344e-02, -6.4940e-02,  1.5923e-02],\n",
       "                         [ 2.7153e-02, -2.8147e-02, -7.4200e-04],\n",
       "                         [-1.2954e-02, -3.2091e-02,  2.8604e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.7807e-02, -2.8579e-02, -4.7183e-02],\n",
       "                         [-2.5427e-03, -3.0682e-03, -4.7967e-02],\n",
       "                         [ 3.1416e-02, -1.7803e-02, -1.8947e-03]],\n",
       "               \n",
       "                        [[-3.5335e-03,  2.9050e-02, -5.2048e-02],\n",
       "                         [ 2.8188e-02,  1.9930e-02, -2.0456e-02],\n",
       "                         [ 3.0739e-02,  4.1309e-02, -4.7110e-02]],\n",
       "               \n",
       "                        [[-3.3564e-02, -4.0410e-02, -8.6391e-02],\n",
       "                         [-2.9952e-02, -1.4456e-02,  2.4784e-02],\n",
       "                         [-4.7339e-02, -4.5708e-02,  7.1623e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.2363e-02,  1.3501e-02, -1.5803e-04],\n",
       "                         [-1.9702e-02, -4.1223e-02, -1.4040e-02],\n",
       "                         [-2.1804e-02, -3.4117e-02, -4.1499e-02]],\n",
       "               \n",
       "                        [[ 5.0175e-02,  4.8530e-02,  6.1841e-02],\n",
       "                         [-5.2386e-02, -3.6055e-02, -2.1579e-02],\n",
       "                         [-6.1513e-02, -3.6027e-03, -4.1555e-02]],\n",
       "               \n",
       "                        [[-3.5838e-02, -6.2272e-02, -8.0042e-03],\n",
       "                         [-3.8722e-02,  7.4427e-03,  1.1058e-02],\n",
       "                         [-1.4307e-02, -3.2467e-02, -3.4531e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3628e-02,  2.7285e-03,  1.5521e-02],\n",
       "                         [ 6.4739e-02, -1.6291e-02,  2.1665e-02],\n",
       "                         [ 6.1771e-02, -5.1059e-02, -4.8901e-02]],\n",
       "               \n",
       "                        [[-2.6963e-02,  1.9296e-02,  5.3938e-02],\n",
       "                         [-5.3194e-02, -1.1404e-02,  1.8829e-02],\n",
       "                         [-7.9276e-02, -2.1864e-02, -3.8196e-02]],\n",
       "               \n",
       "                        [[ 8.4808e-02,  6.7533e-02,  6.2033e-02],\n",
       "                         [ 6.5936e-02, -2.6663e-02,  7.5278e-03],\n",
       "                         [ 2.4147e-02, -4.6637e-02, -4.0440e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.3273e-03,  1.0368e-02,  2.9113e-02],\n",
       "                         [ 4.7940e-02, -2.6338e-02,  3.7946e-02],\n",
       "                         [ 3.2950e-02,  4.6247e-02,  2.5710e-02]],\n",
       "               \n",
       "                        [[-9.7459e-03,  2.0474e-02,  4.2825e-02],\n",
       "                         [ 1.6884e-02,  4.6780e-03, -1.5560e-02],\n",
       "                         [-6.3945e-02,  6.8826e-03,  3.1620e-02]],\n",
       "               \n",
       "                        [[ 1.1923e-02, -2.2587e-02, -1.0285e-03],\n",
       "                         [-6.0087e-03,  9.3974e-05,  8.8612e-03],\n",
       "                         [ 8.7517e-03,  4.7585e-03, -3.5720e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.3675e-02, -4.8650e-02,  1.3389e-02],\n",
       "                         [-4.8180e-02, -1.0838e-01, -1.8075e-02],\n",
       "                         [-6.6564e-02, -4.1220e-02, -3.8812e-02]],\n",
       "               \n",
       "                        [[ 4.3953e-03,  5.0149e-02,  6.3318e-02],\n",
       "                         [ 4.8209e-02,  2.4266e-02,  2.1014e-02],\n",
       "                         [ 1.6416e-02, -3.3700e-02, -4.2817e-02]],\n",
       "               \n",
       "                        [[-2.8837e-02,  2.7409e-03,  2.8331e-02],\n",
       "                         [-5.9724e-03, -5.8118e-02, -7.0318e-02],\n",
       "                         [-6.0833e-02, -5.2656e-02, -5.2772e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.0133e-02,  3.9366e-03,  4.7938e-02],\n",
       "                         [-1.2157e-02,  1.9478e-02, -2.6585e-02],\n",
       "                         [-2.0719e-02, -2.5532e-02, -4.1482e-02]],\n",
       "               \n",
       "                        [[ 2.7662e-02,  5.0188e-02,  5.1921e-02],\n",
       "                         [-7.6517e-03, -5.1657e-02, -2.7490e-02],\n",
       "                         [-2.9292e-02, -3.8551e-02, -2.9929e-03]],\n",
       "               \n",
       "                        [[-2.6770e-02, -3.3468e-02, -3.3695e-02],\n",
       "                         [-5.5176e-02, -2.4818e-02, -5.8880e-02],\n",
       "                         [-1.5056e-02, -6.2207e-02, -4.8363e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.1598e-03, -4.3907e-02,  1.5628e-02],\n",
       "                         [ 1.6393e-02,  3.9933e-02, -4.7980e-03],\n",
       "                         [-9.2348e-03,  8.1859e-02,  3.9698e-02]],\n",
       "               \n",
       "                        [[-3.7246e-02,  1.7502e-02,  3.7163e-02],\n",
       "                         [ 5.7703e-02, -2.4650e-02, -5.0248e-03],\n",
       "                         [-2.8848e-02, -1.8945e-02,  4.7855e-02]],\n",
       "               \n",
       "                        [[ 1.3576e-03, -4.7517e-02,  3.5737e-02],\n",
       "                         [ 1.5863e-02,  3.9620e-02,  9.4381e-03],\n",
       "                         [ 5.0647e-04, -2.3891e-02,  3.4428e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9621e-02, -5.0091e-03, -5.7347e-02],\n",
       "                         [ 6.7124e-02,  8.4320e-02, -2.3866e-02],\n",
       "                         [ 1.6802e-02,  4.7377e-02,  3.4077e-03]],\n",
       "               \n",
       "                        [[-5.5406e-02, -5.9962e-02, -3.6674e-02],\n",
       "                         [ 3.1510e-02,  4.7255e-02,  1.2530e-03],\n",
       "                         [-8.1551e-03,  1.8949e-02, -5.1895e-02]],\n",
       "               \n",
       "                        [[-1.0334e-02,  2.3689e-02, -2.3868e-02],\n",
       "                         [-4.3318e-03,  2.3800e-02,  3.7886e-02],\n",
       "                         [ 2.9135e-02, -6.1549e-03, -1.9589e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 5.9933e-08, -6.1479e-05,  1.1136e-06,  1.1191e-06,  2.5666e-07,\n",
       "                        1.5369e-03, -1.1775e-06, -6.2422e-07, -1.1607e-06, -8.1503e-04,\n",
       "                       -1.8291e-06, -2.3342e-06, -2.2181e-04,  6.1000e-07, -2.3925e-06,\n",
       "                       -3.0629e-07, -3.1087e-06,  8.1563e-07,  9.3452e-08, -5.7189e-04,\n",
       "                       -3.2095e-07, -9.1869e-09,  4.8346e-06, -1.7036e-05,  1.3274e-02,\n",
       "                       -1.3903e-05,  1.6652e-05, -2.7412e-05, -3.4676e-03,  5.0787e-06,\n",
       "                        2.6765e-06,  4.6576e-07,  4.0706e-06,  1.7675e-05, -8.4356e-07,\n",
       "                        2.6364e-06,  4.4227e-09, -1.2458e-05,  2.1915e-05,  4.9682e-07,\n",
       "                        7.0539e-07, -1.6904e-06,  2.0231e-05, -5.5559e-06,  7.0082e-07,\n",
       "                       -3.0102e-06, -1.5954e-04, -3.8790e-06, -1.2362e-06, -3.6231e-06,\n",
       "                        7.5977e-07, -4.2942e-07,  1.9629e-06, -1.0215e-02, -4.5511e-07,\n",
       "                        9.4783e-03, -3.0247e-03, -1.9373e-05,  5.5702e-05, -1.3319e-02,\n",
       "                       -9.0035e-07, -1.1355e-06,  3.0399e-06,  2.3563e-03,  3.7166e-07,\n",
       "                        1.0655e-02,  7.9245e-06,  3.9385e-07, -2.3954e-06,  6.9563e-07,\n",
       "                       -9.1358e-07,  1.7789e-06, -1.0133e-06, -9.4920e-05, -3.5892e-05,\n",
       "                        2.6434e-06, -3.6071e-06,  2.4756e-06,  2.6890e-06,  7.2722e-05,\n",
       "                        2.2625e-07, -9.3507e-07,  2.5611e-06,  1.9175e-06, -6.8006e-06,\n",
       "                        3.2866e-07,  3.1739e-06, -7.8642e-03, -4.1726e-06,  4.7427e-07,\n",
       "                        3.0530e-06,  1.0925e-02,  2.6354e-07,  1.7425e-06, -1.6997e-06,\n",
       "                       -2.4809e-06, -7.8697e-07,  1.2117e-02, -1.6563e-06,  1.2712e-06,\n",
       "                        4.4987e-07,  5.1062e-06,  1.0757e-02, -1.8630e-06,  7.0586e-08,\n",
       "                        5.5715e-07, -1.2244e-04,  1.5775e-05,  1.1858e-02,  9.6294e-03,\n",
       "                       -1.0860e-06,  2.7695e-05, -3.0826e-05, -1.0798e-02,  3.5214e-08,\n",
       "                       -4.0514e-07,  1.3645e-02,  2.0043e-05,  1.9055e-04, -8.7982e-03,\n",
       "                       -1.0212e-06, -1.3086e-05, -2.2918e-08, -2.2165e-05, -1.8928e-06,\n",
       "                       -1.2180e-05, -1.3773e-02, -7.5184e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0872, -0.1572, -0.1664, -0.2343, -0.1549, -0.1571, -0.1078, -0.1666,\n",
       "                       -0.1076, -0.1716, -0.3466, -0.2546, -0.1173, -0.2540, -0.1237, -0.1846,\n",
       "                       -0.2605, -0.0961, -0.0841, -0.2577, -0.1466, -0.1738, -0.2186, -0.2139,\n",
       "                       -0.2324, -0.1326, -0.1120, -0.1670, -0.1783, -0.2671, -0.0948, -0.3858,\n",
       "                       -0.2320, -0.0944, -0.2653, -0.2115, -0.2098, -0.0639, -0.3093, -0.2235,\n",
       "                       -0.1407, -0.1924, -0.2923, -0.1656, -0.2135, -0.2605, -0.1603, -0.1156,\n",
       "                       -0.1902, -0.2313, -0.0166, -0.1515, -0.3205, -0.2022, -0.0888, -0.1692,\n",
       "                       -0.2692, -0.1670, -0.2828, -0.2521, -0.0130, -0.2669, -0.1723, -0.2151,\n",
       "                       -0.1083, -0.0798, -0.1164, -0.2391, -0.1477, -0.1890, -0.2052, -0.0476,\n",
       "                       -0.1590, -0.1589, -0.2383, -0.0808, -0.2007, -0.1882, -0.1486, -0.1086,\n",
       "                       -0.1875, -0.2214, -0.0991,  0.0091, -0.2162, -0.2209, -0.0630, -0.2359,\n",
       "                       -0.1235, -0.1990, -0.2295, -0.2036, -0.2071, -0.2148, -0.1414, -0.0935,\n",
       "                       -0.1874, -0.1651, -0.1903, -0.1393, -0.1805, -0.1963, -0.1423, -0.2635,\n",
       "                       -0.2242, -0.2068, -0.1134, -0.2680, -0.1862, -0.1696, -0.2529, -0.2078,\n",
       "                       -0.1584, -0.2204, -0.2384, -0.1047, -0.1335, -0.2205, -0.1528, -0.2487,\n",
       "                       -0.1355, -0.1146, -0.1673, -0.0136, -0.1558, -0.2546, -0.1811, -0.1617],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.0451, 0.9745, 1.0224, 0.9569, 0.9861, 1.0045, 1.0278, 1.0634, 1.0389,\n",
       "                       1.0216, 0.9842, 1.1099, 1.0157, 0.9824, 1.0965, 1.0307, 0.8674, 0.9835,\n",
       "                       1.0076, 0.9665, 0.9861, 0.9705, 1.0266, 1.0917, 0.9736, 1.0308, 0.9726,\n",
       "                       0.9838, 0.9616, 1.0102, 0.9881, 1.0250, 0.9429, 0.9924, 0.9120, 1.0300,\n",
       "                       0.9982, 1.0233, 0.8428, 0.9264, 0.9836, 0.9378, 1.0010, 0.9307, 1.0222,\n",
       "                       0.9822, 1.0209, 0.9528, 1.0514, 0.9630, 1.0344, 1.0062, 0.9767, 0.9748,\n",
       "                       0.9827, 0.9348, 0.9054, 1.0206, 1.0095, 1.0173, 1.0312, 1.0670, 0.9866,\n",
       "                       0.9704, 0.9777, 0.9320, 0.9356, 0.9721, 0.9230, 1.0510, 0.9052, 1.0517,\n",
       "                       0.9739, 0.9824, 1.0422, 1.0663, 0.9386, 0.9604, 1.0494, 0.9874, 1.0360,\n",
       "                       0.9525, 1.0250, 0.9620, 0.9574, 1.0049, 1.0482, 0.9686, 0.9571, 0.9879,\n",
       "                       0.9952, 0.9969, 0.9531, 0.9737, 0.9241, 1.0071, 0.9846, 0.9989, 1.0562,\n",
       "                       0.9546, 0.9243, 1.0043, 0.9716, 1.0233, 1.0340, 1.0055, 0.9923, 0.9698,\n",
       "                       1.0110, 0.9383, 0.9961, 1.0366, 0.9825, 1.0127, 0.8616, 0.9989, 1.0212,\n",
       "                       0.9751, 0.9882, 1.0225, 0.9810, 1.0087, 0.9795, 1.0189, 0.9698, 1.0015,\n",
       "                       1.0015, 1.0593], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-3.5635e-02,  6.6883e-02,  1.6516e-02],\n",
       "                         [-2.0717e-02,  4.5892e-03, -7.0361e-03],\n",
       "                         [-4.0502e-03, -5.3581e-02, -6.3822e-03]],\n",
       "               \n",
       "                        [[ 2.4649e-03, -2.7850e-02,  9.0671e-03],\n",
       "                         [-4.3892e-02,  2.0201e-02, -2.6540e-02],\n",
       "                         [-6.6467e-04,  1.1619e-02, -5.7028e-02]],\n",
       "               \n",
       "                        [[ 4.4089e-03, -2.2262e-02,  1.7012e-02],\n",
       "                         [ 6.2686e-02,  2.4853e-02, -3.6876e-02],\n",
       "                         [-1.6284e-02,  2.1929e-02, -1.3400e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5534e-02, -3.8601e-02, -6.5782e-02],\n",
       "                         [ 2.6658e-02,  4.9235e-03,  4.0978e-02],\n",
       "                         [ 1.1825e-03, -2.5821e-02, -1.8333e-02]],\n",
       "               \n",
       "                        [[-2.6654e-02, -5.5350e-02,  6.5079e-03],\n",
       "                         [-6.1993e-04, -2.8721e-02, -2.6483e-02],\n",
       "                         [ 2.8866e-02,  1.8139e-02,  4.3261e-03]],\n",
       "               \n",
       "                        [[-3.5204e-03,  5.2470e-02,  3.2148e-02],\n",
       "                         [ 1.7759e-02,  1.0482e-02, -2.4699e-02],\n",
       "                         [ 1.1681e-02,  3.0295e-02,  2.6309e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.6161e-02,  1.5252e-02, -2.2819e-02],\n",
       "                         [ 1.0151e-02, -4.9394e-03, -3.1928e-02],\n",
       "                         [-7.3092e-03, -1.6657e-02,  1.1107e-02]],\n",
       "               \n",
       "                        [[-3.2528e-02,  3.4879e-02,  5.5768e-02],\n",
       "                         [ 2.8912e-02,  8.1838e-02,  7.0347e-02],\n",
       "                         [-5.9509e-02, -3.2777e-02,  1.6557e-03]],\n",
       "               \n",
       "                        [[ 1.0216e-02,  4.4529e-02, -8.3797e-02],\n",
       "                         [ 2.7144e-02, -1.2044e-02, -7.0979e-02],\n",
       "                         [ 2.4772e-02, -3.1583e-02, -4.7915e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.8065e-03,  5.4751e-02, -9.6700e-03],\n",
       "                         [-3.1591e-03, -3.3694e-02, -7.3251e-02],\n",
       "                         [ 7.2923e-05,  4.5793e-03, -5.0973e-02]],\n",
       "               \n",
       "                        [[ 5.8034e-03, -1.8566e-02,  9.1351e-03],\n",
       "                         [-5.1339e-02, -2.9027e-02, -1.2111e-02],\n",
       "                         [-2.9039e-02,  1.3079e-02,  3.4591e-02]],\n",
       "               \n",
       "                        [[ 6.6866e-02, -2.1931e-02, -4.6208e-02],\n",
       "                         [ 1.5051e-02,  3.2506e-02, -1.3522e-02],\n",
       "                         [-1.3544e-02,  3.2223e-02, -5.6732e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.1565e-03, -1.2166e-02, -6.4260e-02],\n",
       "                         [-4.6180e-02, -8.3588e-02, -3.3062e-02],\n",
       "                         [-3.1706e-02, -5.0750e-02, -5.2977e-02]],\n",
       "               \n",
       "                        [[ 1.6955e-03,  1.4892e-03, -2.2856e-02],\n",
       "                         [ 3.9290e-03,  6.5597e-02, -3.6062e-02],\n",
       "                         [-3.7413e-02,  3.7076e-02, -3.6557e-02]],\n",
       "               \n",
       "                        [[ 9.4688e-02, -1.3011e-02,  7.1782e-02],\n",
       "                         [ 1.4776e-02,  7.5849e-02, -1.0182e-02],\n",
       "                         [-5.7237e-03,  2.7586e-02,  4.5924e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.3358e-03,  4.7901e-02, -1.3634e-02],\n",
       "                         [ 2.1187e-02, -3.5819e-02, -3.5869e-02],\n",
       "                         [ 1.4592e-02,  1.6706e-02,  4.2851e-02]],\n",
       "               \n",
       "                        [[-2.2170e-02, -5.4084e-02, -2.6914e-04],\n",
       "                         [ 3.5542e-02, -5.5808e-02, -6.2875e-02],\n",
       "                         [-2.2901e-03, -3.7785e-02,  2.0417e-02]],\n",
       "               \n",
       "                        [[ 1.4934e-02,  2.7684e-02,  8.1969e-02],\n",
       "                         [ 6.0429e-02,  6.1587e-02,  3.9178e-02],\n",
       "                         [-1.8586e-03, -2.7665e-03, -1.9067e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1825e-02,  4.8099e-03, -5.5348e-02],\n",
       "                         [ 5.5376e-02, -2.2714e-02, -1.8907e-02],\n",
       "                         [ 9.4701e-03,  1.6956e-02,  2.8504e-02]],\n",
       "               \n",
       "                        [[-8.7255e-03, -1.3725e-02,  3.0419e-02],\n",
       "                         [ 2.3330e-02,  3.1619e-02, -3.6503e-02],\n",
       "                         [ 1.7630e-02,  1.8091e-02, -5.2045e-03]],\n",
       "               \n",
       "                        [[ 2.6194e-02,  2.7558e-02, -5.5006e-02],\n",
       "                         [-1.1049e-02, -1.7657e-02, -2.4263e-03],\n",
       "                         [-7.5835e-03, -1.4259e-02, -4.7830e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.5579e-02,  5.8151e-02, -2.5747e-02],\n",
       "                         [ 9.2701e-02,  6.5077e-02,  3.7771e-02],\n",
       "                         [ 4.0023e-02,  1.2615e-02, -1.5215e-02]],\n",
       "               \n",
       "                        [[ 3.5414e-02, -3.3109e-02,  3.7701e-02],\n",
       "                         [-2.7612e-02, -2.7967e-02, -5.2714e-02],\n",
       "                         [-3.7227e-02,  3.3004e-02, -8.0880e-02]],\n",
       "               \n",
       "                        [[-1.6259e-02,  4.5828e-03, -7.5952e-03],\n",
       "                         [-4.2413e-02, -9.9031e-03, -3.3439e-02],\n",
       "                         [-4.7989e-02,  6.6052e-02, -1.1792e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.2100e-02,  6.1930e-02, -8.5012e-03],\n",
       "                         [-4.9399e-02, -2.1669e-02, -2.1487e-02],\n",
       "                         [-9.4657e-03,  3.6917e-02, -3.9345e-02]],\n",
       "               \n",
       "                        [[ 1.1505e-02, -3.4558e-02,  3.1325e-02],\n",
       "                         [-1.0512e-04, -2.2429e-02,  3.3865e-02],\n",
       "                         [-1.2944e-02,  3.7145e-02,  1.9752e-02]],\n",
       "               \n",
       "                        [[ 2.9329e-03,  1.7710e-02,  6.5504e-03],\n",
       "                         [ 2.7545e-02,  2.8484e-02,  2.6443e-02],\n",
       "                         [-4.8555e-03, -4.9553e-02,  1.7128e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.9563e-03, -1.2934e-02,  1.1113e-02],\n",
       "                         [-1.8983e-02,  4.7031e-02, -4.0295e-02],\n",
       "                         [-1.3619e-02, -1.9917e-02, -3.2294e-02]],\n",
       "               \n",
       "                        [[-3.2172e-02, -2.8210e-02, -4.2929e-02],\n",
       "                         [ 4.4360e-03, -5.5791e-02,  1.2082e-02],\n",
       "                         [ 4.9856e-02, -1.0001e-02, -4.5965e-02]],\n",
       "               \n",
       "                        [[ 1.4553e-02,  6.3308e-02, -2.4518e-02],\n",
       "                         [-1.3177e-03,  5.6378e-02, -6.8796e-03],\n",
       "                         [-7.8446e-03,  2.0849e-02,  1.5414e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.5729e-02, -3.0278e-02,  3.3063e-04],\n",
       "                         [-3.3246e-02, -2.2090e-02,  2.8049e-02],\n",
       "                         [-9.2174e-02, -6.1077e-02, -5.3846e-03]],\n",
       "               \n",
       "                        [[ 1.3817e-02,  1.4932e-02, -3.2316e-02],\n",
       "                         [ 2.9696e-02,  1.5589e-02, -4.2233e-02],\n",
       "                         [ 3.3876e-02,  2.2564e-02,  1.0178e-02]],\n",
       "               \n",
       "                        [[-1.2799e-02, -1.3293e-02, -3.9625e-02],\n",
       "                         [-4.9859e-02,  3.1562e-02, -1.0173e-02],\n",
       "                         [-4.0080e-02, -3.2473e-02, -3.6179e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 5.4078e-02,  7.9191e-02,  1.7750e-03],\n",
       "                         [ 4.3058e-02,  4.6749e-02, -3.7460e-02],\n",
       "                         [ 1.1480e-03, -1.3536e-02, -6.2119e-02]],\n",
       "               \n",
       "                        [[ 3.5932e-02,  4.2279e-02,  7.7663e-03],\n",
       "                         [-3.6863e-02, -1.1133e-02, -4.4179e-02],\n",
       "                         [-5.6386e-03, -1.8559e-02, -5.7912e-02]],\n",
       "               \n",
       "                        [[-7.8392e-02,  3.8048e-02,  3.1213e-02],\n",
       "                         [-5.2759e-02,  8.1980e-03, -3.1568e-02],\n",
       "                         [-7.3119e-03, -2.0015e-02, -3.6982e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-2.1351e-03,  3.4871e-03,  1.7402e-03,  3.3579e-03,  1.0868e-03,\n",
       "                        7.4822e-04,  3.1768e-03,  6.2910e-04,  6.6134e-04, -1.6108e-03,\n",
       "                       -2.7385e-03,  6.7116e-04, -9.6336e-04,  3.2656e-03,  1.8089e-03,\n",
       "                        3.7092e-04,  2.3694e-03, -1.4622e-03,  1.2253e-03,  1.7384e-03,\n",
       "                        8.2374e-04,  1.4852e-03, -1.9792e-04, -6.7408e-04,  2.5066e-03,\n",
       "                       -1.6606e-03, -7.2716e-04,  2.5755e-03, -1.5390e-03,  1.5680e-03,\n",
       "                       -1.4368e-03,  6.3476e-05, -1.6477e-03,  8.5359e-04, -1.1303e-03,\n",
       "                        1.8436e-03,  2.7225e-03, -9.7316e-04,  3.3027e-03, -4.5836e-03,\n",
       "                        1.0710e-03,  2.4210e-03,  9.4007e-04,  2.7616e-04, -2.6665e-03,\n",
       "                        1.0428e-03, -4.5181e-03, -1.0973e-03,  2.9153e-03,  1.1452e-03,\n",
       "                       -2.6050e-04,  1.7071e-04,  2.0001e-03,  1.2810e-04, -1.9158e-04,\n",
       "                       -4.1856e-03, -3.1744e-03, -7.9482e-04, -3.0404e-04,  3.5577e-03,\n",
       "                        8.4278e-04,  1.1236e-03, -5.2809e-03,  1.9397e-03, -1.0979e-03,\n",
       "                        1.9149e-03, -3.6256e-03,  1.5284e-03, -4.2810e-04, -4.3189e-03,\n",
       "                        7.6343e-04, -1.4412e-03, -1.3658e-03,  2.3325e-03,  2.2334e-03,\n",
       "                        1.4130e-03,  3.4702e-03,  8.2384e-04, -5.8385e-04, -2.3952e-03,\n",
       "                        3.0020e-03, -2.1871e-03, -2.9691e-03, -7.2833e-04, -3.2114e-06,\n",
       "                       -4.5352e-04,  1.9773e-03, -1.5028e-03,  4.9686e-04, -5.7484e-04,\n",
       "                        4.2452e-03,  1.2303e-03, -1.5338e-03, -7.8657e-04,  2.1939e-03,\n",
       "                        3.3835e-03,  3.8203e-03,  3.1000e-03, -3.5266e-03,  2.6436e-03,\n",
       "                        9.1690e-04, -3.8383e-03,  5.2958e-04, -6.9434e-05,  1.7816e-03,\n",
       "                        4.8468e-03,  6.3447e-04,  2.1172e-03,  1.7666e-03, -2.1690e-03,\n",
       "                       -2.0607e-03, -3.0691e-03,  4.9682e-04, -5.3011e-04, -3.4542e-03,\n",
       "                       -1.0074e-03,  2.9607e-04, -2.5081e-03, -1.1018e-03, -3.7656e-04,\n",
       "                       -3.8288e-04,  8.6251e-04, -4.0439e-04,  7.3346e-04, -1.4695e-03,\n",
       "                       -5.3210e-05,  2.3575e-03, -2.5863e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.2745, -0.3432, -0.3225, -0.3161, -0.3458, -0.2965, -0.2656, -0.4619,\n",
       "                       -0.3528, -0.3486, -0.3127, -0.4352, -0.2927, -0.3775, -0.3179, -0.2020,\n",
       "                       -0.2585, -0.2389, -0.4363, -0.3397, -0.2641, -0.3822, -0.2232, -0.3648,\n",
       "                       -0.2792, -0.3051, -0.2956, -0.2948, -0.2251, -0.2499, -0.3535, -0.2657,\n",
       "                       -0.3528, -0.2497, -0.2430, -0.2566, -0.4218, -0.2970, -0.2469, -0.2932,\n",
       "                       -0.2682, -0.2900, -0.2373, -0.2746, -0.2579, -0.3706, -0.2705, -0.2992,\n",
       "                       -0.2342, -0.4011, -0.3201, -0.3366, -0.2704, -0.3693, -0.3214, -0.2929,\n",
       "                       -0.3103, -0.2991, -0.3957, -0.3168, -0.2977, -0.3472, -0.2490, -0.3007,\n",
       "                       -0.2593, -0.2996, -0.3261, -0.2612, -0.2965, -0.3719, -0.3670, -0.3033,\n",
       "                       -0.3026, -0.3622, -0.3271, -0.3308, -0.3027, -0.3465, -0.2958, -0.3535,\n",
       "                       -0.2374, -0.3401, -0.2038, -0.3748, -0.2901, -0.2589, -0.3164, -0.2999,\n",
       "                       -0.2608, -0.2464, -0.2377, -0.2692, -0.3142, -0.3819, -0.3478, -0.3193,\n",
       "                       -0.3010, -0.2188, -0.2880, -0.3890, -0.2799, -0.2614, -0.2786, -0.3609,\n",
       "                       -0.3335, -0.3494, -0.2471, -0.3684, -0.2899, -0.3292, -0.3916, -0.2884,\n",
       "                       -0.2767, -0.3124, -0.2516, -0.3787, -0.2954, -0.3044, -0.3517, -0.3989,\n",
       "                       -0.2850, -0.3227, -0.3362, -0.2426, -0.3302, -0.2835, -0.2408, -0.2675],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.8677, 0.9911, 0.9240, 0.8862, 0.9875, 0.9502, 0.9117, 1.0771, 0.9400,\n",
       "                       0.9616, 0.9689, 1.0767, 0.8404, 1.1268, 0.9214, 0.9644, 1.1083, 0.8781,\n",
       "                       1.0999, 0.9300, 0.8585, 0.9549, 0.8933, 0.9367, 0.9463, 0.9033, 0.9782,\n",
       "                       0.9583, 0.9087, 0.9267, 1.1003, 0.9585, 0.9880, 0.9270, 1.0583, 1.0271,\n",
       "                       1.0955, 0.9693, 0.9092, 0.9284, 0.8522, 0.9795, 0.9794, 0.8672, 0.9084,\n",
       "                       0.9751, 0.8588, 0.8368, 0.9086, 0.9760, 0.9658, 1.0371, 0.9347, 0.9943,\n",
       "                       0.9089, 0.9417, 0.8640, 0.9460, 0.9710, 0.8725, 0.9926, 1.0481, 1.1232,\n",
       "                       0.9393, 0.9935, 0.9556, 0.8583, 0.8728, 0.8861, 1.0107, 1.2039, 0.9133,\n",
       "                       0.8829, 1.0437, 0.9347, 0.9958, 1.0021, 0.8523, 0.9745, 0.9859, 0.9213,\n",
       "                       0.9626, 1.0658, 1.0581, 0.8953, 0.9729, 0.9728, 0.8852, 1.1389, 0.9146,\n",
       "                       0.9648, 0.8962, 1.0027, 1.0481, 0.9987, 0.9533, 0.9388, 0.9414, 0.9178,\n",
       "                       1.0637, 0.9441, 0.9439, 0.8276, 0.9136, 0.8780, 0.8991, 0.8927, 0.9721,\n",
       "                       0.9828, 0.9578, 0.8569, 1.0319, 0.9744, 1.0446, 0.9831, 0.9474, 0.9019,\n",
       "                       1.0095, 0.9606, 1.0142, 0.9484, 1.1066, 1.0178, 0.9520, 0.8968, 0.9898,\n",
       "                       0.9332, 0.8357], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-4.3702e-02, -1.3627e-02, -3.3736e-02],\n",
       "                         [ 5.9619e-02,  1.6159e-02, -2.8567e-02],\n",
       "                         [ 2.7072e-02,  1.9254e-02,  3.9468e-02]],\n",
       "               \n",
       "                        [[ 1.8669e-02, -3.7957e-02,  9.8068e-03],\n",
       "                         [ 4.1648e-03, -3.6611e-02, -4.8710e-02],\n",
       "                         [ 3.7416e-04,  2.0676e-03, -2.5078e-02]],\n",
       "               \n",
       "                        [[ 5.8284e-02,  5.1901e-03,  5.3009e-02],\n",
       "                         [ 7.2872e-04, -1.6713e-02,  3.5374e-02],\n",
       "                         [-1.0131e-02,  5.0062e-02, -1.9479e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.3767e-03, -1.2034e-03, -4.2708e-02],\n",
       "                         [ 3.1162e-02, -5.4115e-02, -1.8587e-02],\n",
       "                         [-3.1021e-04,  2.3716e-02, -1.8927e-02]],\n",
       "               \n",
       "                        [[-2.0851e-02, -4.9040e-02, -1.3251e-02],\n",
       "                         [ 1.1836e-02,  8.5989e-03,  5.9320e-02],\n",
       "                         [-1.5005e-02, -3.3563e-02, -3.0291e-02]],\n",
       "               \n",
       "                        [[ 3.2447e-02, -4.5090e-02,  3.4413e-02],\n",
       "                         [ 1.4186e-02, -8.0078e-03, -5.5526e-02],\n",
       "                         [ 2.3710e-02, -1.7888e-02, -2.6468e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3535e-03, -4.6939e-02, -3.8000e-02],\n",
       "                         [ 4.5195e-02, -6.4326e-02,  7.6383e-03],\n",
       "                         [ 7.2541e-03, -7.9630e-02, -4.7468e-02]],\n",
       "               \n",
       "                        [[ 2.0901e-02,  1.5754e-02,  2.3625e-02],\n",
       "                         [-5.4572e-02, -4.4321e-02, -4.3351e-02],\n",
       "                         [-5.6763e-02,  5.0064e-02, -1.2850e-02]],\n",
       "               \n",
       "                        [[-1.3336e-02,  3.8217e-02,  2.6076e-02],\n",
       "                         [ 3.3369e-02,  2.9154e-02,  2.0140e-02],\n",
       "                         [-2.7863e-03,  4.9059e-02,  2.7505e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.4442e-04, -6.9905e-03,  5.1513e-02],\n",
       "                         [-2.9548e-02, -4.9088e-02,  3.1979e-02],\n",
       "                         [ 3.9791e-02,  1.4876e-02, -1.2177e-02]],\n",
       "               \n",
       "                        [[-8.2850e-04,  5.1148e-02, -1.5545e-02],\n",
       "                         [-1.2785e-02,  7.3915e-02,  5.4139e-02],\n",
       "                         [ 9.3461e-02,  4.4485e-02,  3.1602e-02]],\n",
       "               \n",
       "                        [[-2.1151e-03, -4.5330e-04, -4.4565e-02],\n",
       "                         [-1.0282e-01, -4.2757e-02, -7.6484e-02],\n",
       "                         [-1.8677e-02,  1.2074e-02, -3.7740e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.0233e-03,  3.4157e-02,  4.8908e-02],\n",
       "                         [-1.3790e-02,  3.5846e-02,  5.0121e-02],\n",
       "                         [ 1.1362e-02, -1.8063e-02, -1.4542e-02]],\n",
       "               \n",
       "                        [[ 4.2676e-03, -1.4356e-02,  1.2788e-02],\n",
       "                         [-1.6214e-02,  8.6472e-03,  1.4520e-03],\n",
       "                         [ 1.4571e-02, -2.7827e-02, -3.9042e-02]],\n",
       "               \n",
       "                        [[ 4.9498e-02,  5.5938e-03, -5.5657e-03],\n",
       "                         [-5.8689e-03,  5.9269e-02,  1.8440e-02],\n",
       "                         [ 1.4232e-02,  5.8087e-02,  1.3712e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.9255e-02,  3.7683e-02, -3.5907e-02],\n",
       "                         [ 2.3164e-02, -2.1429e-02, -1.3765e-02],\n",
       "                         [ 4.2311e-03,  5.1843e-02,  4.9206e-02]],\n",
       "               \n",
       "                        [[ 7.8560e-02,  6.9009e-02,  5.1072e-02],\n",
       "                         [ 5.7993e-02,  9.7662e-02,  7.6212e-02],\n",
       "                         [-1.2178e-02,  5.2104e-02,  3.1011e-02]],\n",
       "               \n",
       "                        [[-2.1720e-02,  6.2952e-04,  3.7310e-02],\n",
       "                         [-3.9092e-03, -3.1375e-02, -3.3830e-02],\n",
       "                         [-2.1882e-02,  6.3913e-03,  2.2926e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.4090e-02, -4.3765e-02, -3.4845e-03],\n",
       "                         [ 4.8406e-02,  8.7374e-03, -6.2583e-03],\n",
       "                         [-5.6868e-02, -6.9677e-02, -8.0262e-02]],\n",
       "               \n",
       "                        [[-4.3687e-02,  2.0018e-02, -3.3008e-02],\n",
       "                         [ 1.4243e-02, -4.5496e-02, -2.0843e-02],\n",
       "                         [-3.3093e-02, -2.0732e-02,  1.6793e-02]],\n",
       "               \n",
       "                        [[-1.1730e-02,  2.7783e-02,  5.2310e-02],\n",
       "                         [ 2.8892e-02,  4.5672e-02, -3.0685e-02],\n",
       "                         [-5.3753e-02,  1.4569e-03, -7.2215e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2346e-03,  3.3736e-03, -5.3751e-02],\n",
       "                         [ 8.0512e-03, -4.7253e-03, -2.7187e-02],\n",
       "                         [ 4.6004e-02,  6.3030e-03, -1.2130e-02]],\n",
       "               \n",
       "                        [[-2.0353e-02,  9.5935e-02,  1.7028e-02],\n",
       "                         [ 6.2968e-04,  7.9693e-02, -1.6896e-03],\n",
       "                         [-4.4462e-02,  8.0178e-03, -1.1550e-04]],\n",
       "               \n",
       "                        [[-5.9322e-02,  1.6931e-02, -3.5418e-02],\n",
       "                         [-2.4703e-02,  1.8146e-03,  2.5648e-02],\n",
       "                         [-2.5140e-02, -2.5331e-02, -5.2890e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.6826e-02,  8.7188e-03, -1.4631e-02],\n",
       "                         [-4.2427e-03,  3.1592e-03,  3.4011e-02],\n",
       "                         [ 2.4498e-02, -4.3636e-03,  1.3631e-02]],\n",
       "               \n",
       "                        [[-4.8765e-03, -2.5093e-02, -4.6500e-02],\n",
       "                         [ 1.4969e-02,  4.3267e-02,  4.7689e-02],\n",
       "                         [ 3.1678e-02, -2.9005e-03,  1.8742e-02]],\n",
       "               \n",
       "                        [[-1.6282e-04,  3.7926e-02, -3.3435e-02],\n",
       "                         [-3.7151e-02, -5.9865e-03,  1.1217e-02],\n",
       "                         [-2.7224e-02,  4.4045e-02,  2.3198e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.8922e-02, -5.0440e-02, -2.4127e-02],\n",
       "                         [ 1.3196e-03, -3.4582e-02, -6.3706e-03],\n",
       "                         [-2.4766e-02,  2.4784e-02, -5.1412e-02]],\n",
       "               \n",
       "                        [[-8.5140e-02, -1.1747e-01, -1.0574e-02],\n",
       "                         [-7.6822e-02, -1.8023e-02, -1.2739e-02],\n",
       "                         [-2.4550e-02, -9.7639e-02,  5.0995e-02]],\n",
       "               \n",
       "                        [[ 1.0049e-02,  7.0355e-03, -2.8812e-02],\n",
       "                         [-6.1743e-02,  2.9907e-02, -4.9510e-02],\n",
       "                         [-1.2106e-02, -1.6349e-02, -4.7471e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.3281e-02, -5.9659e-02, -7.4845e-03],\n",
       "                         [ 3.9666e-03, -2.4579e-02, -1.6912e-02],\n",
       "                         [-3.0060e-02, -5.2557e-02, -3.4569e-02]],\n",
       "               \n",
       "                        [[ 1.1601e-02,  2.0093e-02,  1.9330e-02],\n",
       "                         [ 1.1518e-02, -3.4297e-03, -3.9855e-03],\n",
       "                         [ 1.1395e-02,  3.4817e-02,  6.0226e-03]],\n",
       "               \n",
       "                        [[ 3.0721e-02, -1.4320e-02, -2.4393e-02],\n",
       "                         [-7.5937e-02, -3.5532e-02, -2.3386e-02],\n",
       "                         [-4.9772e-02, -6.8097e-02, -2.4774e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7394e-05, -1.8980e-02,  4.6820e-02],\n",
       "                         [ 3.1611e-02, -5.9833e-02,  3.1288e-02],\n",
       "                         [-4.7890e-02, -1.8718e-02, -2.2998e-02]],\n",
       "               \n",
       "                        [[ 1.0739e-02,  4.0940e-02,  4.8134e-02],\n",
       "                         [ 4.7797e-02,  4.2867e-02, -1.0555e-02],\n",
       "                         [ 4.3696e-02,  1.1575e-01,  2.4784e-02]],\n",
       "               \n",
       "                        [[ 2.0929e-02,  5.0720e-02, -3.8524e-03],\n",
       "                         [ 6.9852e-03,  5.0116e-02, -2.8043e-02],\n",
       "                         [-1.1385e-02,  6.5274e-03,  1.4342e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.5599, -0.5552, -0.5566,  0.5535,  0.5584,  0.5569, -0.5565,  0.5590,\n",
       "                       -0.5581, -0.5547,  0.5560, -0.5532, -0.5587,  0.5597, -0.5554, -0.5580,\n",
       "                        0.5588,  0.5554, -0.5597, -0.5538,  0.5582, -0.5558, -0.5580,  0.5537,\n",
       "                        0.5585, -0.5426,  0.5531,  0.5594, -0.5537, -0.5554, -0.5546,  0.5546,\n",
       "                       -0.5588,  0.5417,  0.5531, -0.5571, -0.5582, -0.5558, -0.5579,  0.5563,\n",
       "                       -0.5596, -0.5546,  0.5582, -0.5543, -0.5566,  0.5547, -0.5583, -0.5600,\n",
       "                        0.5525,  0.5590,  0.5591, -0.5529,  0.5544, -0.5542, -0.5560, -0.5583,\n",
       "                        0.5550, -0.5568, -0.5573, -0.5581,  0.5572,  0.5543,  0.5578, -0.5564,\n",
       "                        0.5584,  0.5558,  0.5579,  0.5603,  0.5548,  0.5582,  0.5539,  0.5539,\n",
       "                       -0.5558,  0.5583, -0.5625, -0.5550,  0.5596, -0.5543,  0.5566,  0.5571,\n",
       "                       -0.5600, -0.5554, -0.5594,  0.5592,  0.5555,  0.5557,  0.5542, -0.5608,\n",
       "                       -0.5536,  0.5579, -0.5575,  0.5527, -0.5549,  0.5600,  0.5552,  0.5595,\n",
       "                        0.5599,  0.5586,  0.5556, -0.5573, -0.5529,  0.5576,  0.5554, -0.5570,\n",
       "                       -0.5573, -0.5582, -0.5583, -0.5566, -0.5555, -0.5548, -0.5574, -0.5599,\n",
       "                        0.5589,  0.5589,  0.5558,  0.5593, -0.5566,  0.5566,  0.5548,  0.5579,\n",
       "                       -0.5469, -0.5545, -0.5583,  0.5559, -0.5517,  0.5581, -0.5545,  0.5590],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.0752, -0.2222, -0.2209, -0.0301, -0.1385, -0.2287, -0.1965, -0.1863,\n",
       "                       -0.1171, -0.1848, -0.1981, -0.2096, -0.0811, -0.1206, -0.1761, -0.1511,\n",
       "                       -0.0310, -0.1785, -0.1354, -0.2051, -0.1303, -0.1971, -0.1930, -0.1128,\n",
       "                       -0.2056, -0.1850, -0.2187, -0.0820, -0.1456, -0.1416, -0.1590, -0.0678,\n",
       "                       -0.1887, -0.1743, -0.1259, -0.2086, -0.1691, -0.2446, -0.0669, -0.1646,\n",
       "                       -0.1385, -0.1979, -0.1650, -0.2105, -0.2164, -0.1499, -0.2251, -0.1195,\n",
       "                       -0.0877, -0.0550, -0.1372, -0.1770, -0.2247, -0.2134, -0.1610, -0.1654,\n",
       "                       -0.2468, -0.1107, -0.1712, -0.1072, -0.2446, -0.1986, -0.1468, -0.1368,\n",
       "                       -0.1576, -0.1597, -0.2342, -0.2002, -0.1029, -0.1339, -0.0791, -0.1650,\n",
       "                       -0.1137, -0.1226, -0.2262, -0.1769, -0.1857, -0.1967, -0.1823, -0.1826,\n",
       "                       -0.0848, -0.2136, -0.0045, -0.2291, -0.1357, -0.2389, -0.1607, -0.1593,\n",
       "                       -0.0241, -0.1573, -0.0325, -0.1927, -0.1623, -0.1636, -0.2368, -0.2161,\n",
       "                       -0.1971, -0.1735, -0.1392, -0.1891, -0.2072, -0.1061, -0.2262, -0.1545,\n",
       "                       -0.2028, -0.0613, -0.1243, -0.0735, -0.1948, -0.1561, -0.1174, -0.0502,\n",
       "                       -0.1899, -0.1496, -0.1963, -0.1685, -0.2141, -0.0210, -0.2279, -0.1681,\n",
       "                       -0.2047, -0.1384, -0.1491, -0.2190, -0.0969, -0.2133, -0.0907, -0.1880],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.8093, 0.7495, 0.7997, 0.8392, 0.8219, 0.7918, 0.8508, 0.7993, 0.9625,\n",
       "                       0.7940, 0.7750, 0.7383, 0.8417, 0.7778, 0.7483, 0.9617, 0.8718, 0.8679,\n",
       "                       0.8584, 0.6964, 0.7606, 0.7884, 0.8327, 0.7895, 0.7564, 0.7520, 0.8513,\n",
       "                       0.7525, 0.7667, 0.8095, 1.0495, 0.8087, 0.6924, 0.7637, 0.8062, 0.9847,\n",
       "                       0.8062, 0.8280, 0.9108, 1.0359, 0.7953, 0.7623, 0.7348, 0.7962, 0.7166,\n",
       "                       0.9111, 0.7272, 0.7804, 0.8488, 0.8290, 0.9651, 0.9863, 0.7369, 0.7757,\n",
       "                       0.9778, 0.7277, 0.8836, 0.7600, 0.7946, 0.8953, 0.8346, 0.7383, 0.9778,\n",
       "                       0.8786, 0.8494, 0.8047, 0.8066, 0.8885, 0.8185, 0.7463, 0.7989, 0.7729,\n",
       "                       0.8339, 0.9020, 0.7465, 0.7171, 0.7810, 0.8230, 0.9260, 0.7882, 0.8264,\n",
       "                       0.9635, 0.8743, 0.7630, 0.7732, 0.7391, 0.8376, 0.7785, 0.8861, 1.0193,\n",
       "                       0.8233, 0.9787, 0.8576, 0.8626, 0.8288, 0.8182, 0.7184, 0.7724, 0.8488,\n",
       "                       0.7623, 0.7973, 0.8505, 0.7446, 1.0277, 0.8610, 0.8332, 0.8319, 0.8343,\n",
       "                       0.9999, 0.9363, 0.8064, 0.8397, 0.7709, 0.7834, 0.7463, 0.8550, 0.7467,\n",
       "                       0.8445, 0.8815, 0.7924, 0.7761, 0.9897, 0.7584, 0.7370, 0.8763, 0.6919,\n",
       "                       0.7355, 0.7686], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0053,  0.0019,  0.0046,  ..., -0.0081,  0.0156, -0.0086],\n",
       "                       [-0.0151, -0.0111,  0.0108,  ..., -0.0063,  0.0078, -0.0164],\n",
       "                       [ 0.0050, -0.0080,  0.0010,  ..., -0.0224,  0.0091, -0.0180],\n",
       "                       [ 0.0016, -0.0087,  0.0101,  ...,  0.0078, -0.0055,  0.0038],\n",
       "                       [-0.0126,  0.0035,  0.0124,  ..., -0.0014,  0.0023, -0.0030]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0076,  0.0032,  0.0015, -0.0034, -0.0013], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[ 1.9450e-01, -9.9795e-02,  5.8899e-02,  2.0421e-01, -6.8798e-02,\n",
       "                        -2.0007e-01, -1.7882e-01,  1.9901e-01,  7.3716e-02, -7.7455e-02,\n",
       "                        -1.7553e-01,  1.2926e-01, -2.1960e-01, -5.1855e-02,  1.6968e-01,\n",
       "                        -7.0568e-02, -1.7667e-01,  8.5367e-02, -1.8126e-01, -4.0493e-02],\n",
       "                       [ 3.1346e-01, -2.4672e-01,  3.6882e-01, -2.9854e-01,  4.4060e-01,\n",
       "                        -3.9375e-02,  3.2006e-01,  2.4884e-01,  6.6769e-01, -1.2758e-01,\n",
       "                        -5.8387e-01, -8.7315e-02, -4.9148e-01, -3.9044e-02, -2.8058e-01,\n",
       "                        -1.2313e-01, -6.4514e-01, -3.9344e-01, -7.0918e-01, -4.2825e-01],\n",
       "                       [-1.6792e-01, -9.5964e-03, -4.3481e-02,  6.7062e-03,  1.1711e-01,\n",
       "                        -1.6487e-01, -6.0568e-02, -1.0674e-02,  1.3948e-01,  2.4193e-02,\n",
       "                         3.2932e-02,  1.9114e-01,  1.7074e-01,  1.4639e-01,  1.9790e-01,\n",
       "                        -1.4125e-01,  1.1538e-01,  1.8761e-01,  8.7872e-03, -1.8317e-01],\n",
       "                       [-8.6637e-02, -8.4453e-02, -1.6488e-02, -1.0229e-01, -1.2699e-01,\n",
       "                        -5.9725e-02,  7.7862e-02,  8.2837e-02,  1.6639e-01,  1.4864e-01,\n",
       "                        -6.2536e-02, -6.9908e-02, -7.0712e-02, -5.1099e-02, -1.4010e-01,\n",
       "                        -1.6697e-01,  1.6291e-01,  1.0379e-01, -7.5108e-02,  2.9448e-04],\n",
       "                       [-5.8330e-01, -5.2382e-01,  2.3097e-01, -2.0290e-01,  1.2620e-01,\n",
       "                        -5.2184e-01,  2.3124e-01,  1.6222e-01, -4.6191e-01, -4.8259e-01,\n",
       "                         2.0806e-01, -2.4699e-01, -2.1455e-01, -2.8649e-01, -5.9960e-03,\n",
       "                        -3.3979e-01, -7.1683e-02, -2.2812e-01,  9.2595e-01, -4.0021e-01],\n",
       "                       [-2.2718e-01, -3.8532e-01,  3.6588e-01, -4.9189e-01,  6.1618e-01,\n",
       "                        -4.3076e-01,  6.6215e-01,  1.3517e-01,  2.0993e-01, -6.5571e-01,\n",
       "                         2.4551e-01, -3.9373e-01, -5.2038e-01, -5.6636e-01, -3.1780e-01,\n",
       "                        -4.5481e-01, -3.0907e-01, -5.8442e-01, -1.5079e-01, -4.3089e-01],\n",
       "                       [-4.3577e-01, -2.5230e-01,  3.8415e-01, -3.5953e-01,  5.0516e-01,\n",
       "                        -4.9321e-01,  4.9733e-01, -2.0980e-01, -1.5045e-01, -5.2452e-01,\n",
       "                         1.5987e-01, -1.4204e-01, -7.4178e-02, -3.6433e-01, -4.2967e-01,\n",
       "                        -4.3072e-01, -4.1603e-03, -4.0346e-01,  5.7543e-01, -3.4250e-01],\n",
       "                       [-2.1789e-01,  1.4847e-01, -1.7759e-01, -1.5096e-02,  1.6466e-01,\n",
       "                         1.9650e-01, -2.1852e-02, -1.2478e-02, -1.0809e-02, -1.5789e-01,\n",
       "                        -1.5054e-01, -1.8219e-01,  1.2927e-01, -4.6389e-02,  2.1401e-01,\n",
       "                         1.6270e-01, -2.1683e-01,  3.1173e-02, -1.7408e-01,  5.3669e-02],\n",
       "                       [-2.0245e-01, -2.5325e-01,  2.2297e-01, -3.6453e-01,  2.6300e-01,\n",
       "                        -3.9136e-01,  2.1834e-01, -1.5191e-01, -3.9730e-01, -3.6379e-01,\n",
       "                         2.3120e-01, -4.2832e-02, -2.1437e-01, -1.7172e-01,  1.6827e-01,\n",
       "                        -3.6454e-01,  1.3989e-01, -1.1689e-01,  8.3335e-01, -4.2437e-02],\n",
       "                       [-4.6539e-01, -4.4326e-01, -1.8556e-02, -3.0206e-01,  1.8009e-01,\n",
       "                        -2.9671e-01,  9.9821e-03,  1.4084e-01, -5.4199e-01, -2.9132e-02,\n",
       "                         5.4752e-01, -1.5445e-01, -2.9636e-01, -2.3680e-01, -1.8175e-01,\n",
       "                        -2.4527e-01, -2.0243e-02, -3.0596e-01,  4.4147e-01, -3.4644e-02],\n",
       "                       [-5.1960e-01, -3.0793e-01,  3.9824e-01, -2.8498e-02,  2.7860e-01,\n",
       "                        -2.3621e-01,  1.3768e-01, -1.1015e-03, -1.6295e-01, -2.3110e-01,\n",
       "                         2.8005e-01, -2.1967e-01,  6.6372e-02, -1.7816e-01,  2.1774e-01,\n",
       "                        -2.7426e-01,  3.1498e-01, -1.8950e-01,  4.5967e-01, -2.2480e-01],\n",
       "                       [ 8.6240e-02, -1.0932e-01, -6.6633e-02,  3.1452e-01,  9.8137e-02,\n",
       "                         1.0588e-01, -2.6359e-01,  1.9449e-01, -3.9574e-02, -9.9108e-04,\n",
       "                         1.2918e-01,  1.1637e-01,  2.6810e-01, -7.8607e-02,  2.6053e-01,\n",
       "                         1.5103e-02,  2.4161e-01,  3.2538e-02,  9.5170e-02,  7.7577e-02],\n",
       "                       [ 2.2077e-01, -1.1893e-01,  3.3556e-03,  1.4105e-01,  1.9006e-01,\n",
       "                         5.2740e-02, -1.9452e-01,  2.2726e-02,  1.2223e-01,  4.7893e-02,\n",
       "                         1.0646e-01,  2.3871e-01,  9.1381e-02,  1.1641e-01,  1.0854e-01,\n",
       "                         1.8654e-01, -1.0111e-01,  1.9331e-01,  1.4477e-01, -3.2333e-03],\n",
       "                       [-4.7303e-02, -7.3556e-02, -1.5947e-01,  2.0988e-01, -1.0523e-01,\n",
       "                         1.4511e-01,  3.9058e-02, -2.0894e-01,  1.4523e-01,  1.4160e-01,\n",
       "                         2.1420e-01,  5.6961e-02,  1.9555e-01,  1.8229e-01, -1.8977e-01,\n",
       "                         2.0819e-01,  7.4866e-02, -1.2519e-01, -4.2634e-02, -2.1221e-01],\n",
       "                       [-4.0376e-01, -3.3445e-01,  2.3031e-01, -8.1199e-02, -9.7037e-02,\n",
       "                        -3.8110e-01,  3.1125e-01, -3.0648e-02, -6.1843e-01, -2.2994e-01,\n",
       "                         1.8667e-01, -2.6246e-02,  1.0519e-01, -2.5016e-01, -2.1890e-02,\n",
       "                        -1.8432e-01,  2.6919e-01, -3.7928e-01,  5.7429e-01, -3.0198e-01],\n",
       "                       [ 6.2884e-02,  1.5268e-01, -9.8051e-02, -7.9451e-02,  5.4087e-02,\n",
       "                        -8.8575e-02, -2.0254e-01,  7.8244e-03, -2.1483e-01,  1.6323e-01,\n",
       "                         5.5022e-02,  1.6895e-01,  1.5126e-01, -4.8578e-04, -1.3752e-01,\n",
       "                         1.3657e-02,  8.7678e-02,  7.2777e-02, -2.0556e-01,  1.6001e-01],\n",
       "                       [-1.7184e-01,  1.5199e-01, -1.4167e-01,  4.3341e-02, -1.1432e-01,\n",
       "                         1.8257e-02,  1.5661e-01,  6.9146e-03,  1.4682e-01,  1.5712e-01,\n",
       "                        -1.8079e-01, -2.4925e-02,  9.8675e-02,  1.8632e-01,  7.6713e-02,\n",
       "                        -7.8911e-02, -9.5569e-02, -1.8848e-01,  2.3741e-02, -1.2041e-01],\n",
       "                       [-3.4018e-01, -5.4116e-01,  7.2154e-01, -5.9619e-01,  7.5661e-01,\n",
       "                        -4.8814e-01,  6.5774e-01,  3.5671e-01,  4.0474e-01, -4.2754e-01,\n",
       "                         1.1906e-01, -4.7903e-01, -7.5758e-01, -4.7793e-01, -9.1343e-01,\n",
       "                        -7.0370e-01, -7.4674e-01, -4.1812e-01, -1.0789e-01, -7.1230e-01],\n",
       "                       [-3.7166e-01, -3.5114e-01,  9.7455e-02, -5.1995e-01,  1.1000e-01,\n",
       "                        -4.2867e-01,  2.9503e-01,  8.6841e-02, -1.5219e-01, -2.4102e-01,\n",
       "                         1.7106e-01, -4.9874e-01,  3.9382e-03, -4.8100e-01, -2.7536e-01,\n",
       "                        -5.0917e-01, -2.2812e-01, -5.0284e-01,  4.0104e-01, -5.0062e-01],\n",
       "                       [ 5.7188e-02, -1.6221e-01,  8.6147e-03, -1.6054e-01,  2.3078e-01,\n",
       "                        -2.4389e-01,  3.5177e-01,  4.9625e-01,  8.3814e-02, -1.8604e-01,\n",
       "                        -8.6181e-02,  7.3271e-02, -4.9693e-01, -2.3796e-01, -3.8974e-01,\n",
       "                        -3.0115e-01, -3.0443e-01,  1.8303e-02, -6.3854e-01, -2.5684e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([-0.1579,  0.3830, -0.2280, -0.1649,  0.1597,  0.5916,  0.4159,  0.0164,\n",
       "                        0.1514,  0.3201,  0.2737,  0.1193,  0.1049, -0.1574,  0.2005,  0.1238,\n",
       "                        0.1505,  0.7526,  0.3516,  0.0970], device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 0.1263, -0.0772,  0.1627, -0.0859, -0.2019, -0.2602, -0.4369,  0.1521,\n",
       "                        -0.2212, -0.2299, -0.1524,  0.1932,  0.2122,  0.1275, -0.0518,  0.0328,\n",
       "                         0.0779, -0.4234, -0.0063, -0.1765],\n",
       "                       [ 0.0947, -0.0592, -0.1505,  0.1282,  0.2700,  0.3092,  0.1813, -0.1357,\n",
       "                         0.3183,  0.2896,  0.0864,  0.0237,  0.1133,  0.0974,  0.3301, -0.0959,\n",
       "                        -0.1545,  0.3448,  0.2064, -0.0190],\n",
       "                       [-0.1231, -0.2893, -0.1265,  0.1026, -0.4462, -0.1169, -0.4914, -0.1678,\n",
       "                        -0.3340, -0.3942, -0.2506,  0.0273,  0.0290,  0.0147, -0.4609,  0.0933,\n",
       "                         0.0714, -0.5103, -0.3784, -0.4865],\n",
       "                       [ 0.2137,  0.0238,  0.1494,  0.1903, -0.3170, -0.1248, -0.1031,  0.0262,\n",
       "                        -0.2517,  0.1737, -0.1939, -0.1401, -0.0070,  0.0077, -0.2880,  0.0615,\n",
       "                        -0.1830, -0.2516, -0.0809,  0.1210],\n",
       "                       [-0.1682,  0.3614, -0.1215,  0.1635, -0.1256,  0.2145,  0.3066,  0.2003,\n",
       "                        -0.0317, -0.3850,  0.1504,  0.1634, -0.1465, -0.0818,  0.0308,  0.0231,\n",
       "                         0.0909,  0.0798,  0.2517, -0.0961],\n",
       "                       [ 0.1320,  0.0388,  0.1741,  0.0834,  0.2778,  0.1680, -0.0524,  0.0265,\n",
       "                         0.0478, -0.0274, -0.0982,  0.1646,  0.0666,  0.0608,  0.1382, -0.0616,\n",
       "                         0.2075,  0.3890, -0.0512,  0.0220],\n",
       "                       [ 0.0186,  0.8560, -0.0559,  0.0717,  0.8958,  0.9087,  0.9736, -0.1300,\n",
       "                         0.5675,  0.3810,  0.6347,  0.0049,  0.0567, -0.1425,  0.6163,  0.0342,\n",
       "                         0.1900,  0.8056,  0.9387,  0.3749],\n",
       "                       [-0.0348, -0.0985, -0.2037,  0.0392,  0.0375, -0.1490, -0.1671,  0.2235,\n",
       "                         0.1203,  0.1824,  0.1228, -0.1389,  0.1232,  0.1064, -0.1620, -0.2108,\n",
       "                         0.0877, -0.3298, -0.2546, -0.0387],\n",
       "                       [-0.0676, -0.2835, -0.1881,  0.2059,  0.3445, -0.0828,  0.2131, -0.1769,\n",
       "                         0.2083,  0.2633,  0.2230, -0.0921, -0.1128, -0.0414,  0.2611, -0.1009,\n",
       "                         0.0885, -0.1704,  0.0455, -0.4391],\n",
       "                       [ 0.0187, -0.5333,  0.0174,  0.0651, -0.3656, -0.2181, -0.3764,  0.0620,\n",
       "                        -0.4032,  0.0820, -0.3893,  0.0414, -0.0107,  0.0959, -0.1434, -0.0094,\n",
       "                         0.2114, -0.6247, -0.1515, -0.3881]], device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.2632,  0.2419, -0.3432, -0.0988,  0.0510,  0.0767,  0.6427, -0.1952,\n",
       "                        0.1331, -0.0769], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.483476383447647,\n",
       "   1.3545481526851655,\n",
       "   1.3098033878803252,\n",
       "   1.2686262041330338,\n",
       "   1.218484871506691,\n",
       "   1.1743631842136384,\n",
       "   1.1345976806879043,\n",
       "   1.086325322985649,\n",
       "   1.0543912591934204,\n",
       "   1.0350105865001678,\n",
       "   0.9866085749864578,\n",
       "   0.9846874786615372,\n",
       "   0.9660252624750137,\n",
       "   0.9358155431747437,\n",
       "   0.937087476849556,\n",
       "   0.915649830698967,\n",
       "   0.8967641706466675,\n",
       "   0.8855581641197204,\n",
       "   0.8825522440671921,\n",
       "   0.8663041898012162,\n",
       "   0.8615279206037522,\n",
       "   0.8308090027570725,\n",
       "   0.8249965822696685,\n",
       "   0.8191860826015472,\n",
       "   0.8189492141604423,\n",
       "   0.8036693303585053,\n",
       "   0.7832032409310341,\n",
       "   0.775857702255249,\n",
       "   0.7779861376881599,\n",
       "   0.7800523315668106,\n",
       "   0.7531858184337616,\n",
       "   0.7541596565246582,\n",
       "   0.7361088641285897,\n",
       "   0.7286121693253517,\n",
       "   0.7292022127509117,\n",
       "   0.7095676583051681,\n",
       "   0.7120725163817405,\n",
       "   0.7002220395207405,\n",
       "   0.7061740514039994,\n",
       "   0.6800183079242706,\n",
       "   0.6920564432740212,\n",
       "   0.6727037338614463,\n",
       "   0.6708960717916489,\n",
       "   0.6698056691288948,\n",
       "   0.6596473541855812,\n",
       "   0.6498868864178657,\n",
       "   0.655269834458828,\n",
       "   0.6354625440835953,\n",
       "   0.6332014302015304,\n",
       "   0.6399552510380745,\n",
       "   0.636753729403019,\n",
       "   0.6139424102902412,\n",
       "   0.6182934874892235,\n",
       "   0.6212592880129815,\n",
       "   0.6169081155657768,\n",
       "   0.6051740693747998,\n",
       "   0.6106658101081848,\n",
       "   0.5890947519540787,\n",
       "   0.5899993230104447,\n",
       "   0.5940517761707306,\n",
       "   0.5941763424873352,\n",
       "   0.5757712825536728,\n",
       "   0.5736726365685463,\n",
       "   0.6003453350663185,\n",
       "   0.5694817636013031,\n",
       "   0.5650899789333343,\n",
       "   0.562277569591999,\n",
       "   0.5423234815001488,\n",
       "   0.5434539637863636,\n",
       "   0.5505697911977768,\n",
       "   0.5450181565880775,\n",
       "   0.5424525443017483,\n",
       "   0.5502477190196514,\n",
       "   0.5398116151988507,\n",
       "   0.5268495962321759,\n",
       "   0.5161014630198478,\n",
       "   0.5290606241226197,\n",
       "   0.5290622363686561,\n",
       "   0.5131781410574913,\n",
       "   0.513524123877287,\n",
       "   0.4992736192941666,\n",
       "   0.5125679462850093,\n",
       "   0.5042422689497471,\n",
       "   0.49598163238167764,\n",
       "   0.48699826806783675,\n",
       "   0.4952307816147804,\n",
       "   0.48522328546643256,\n",
       "   0.48399221505224704,\n",
       "   0.4750914240181446,\n",
       "   0.46963882228732107,\n",
       "   0.4760855414271355,\n",
       "   0.4710879309475422,\n",
       "   0.4571245724260807,\n",
       "   0.4625591897964478,\n",
       "   0.4592583292722702,\n",
       "   0.4662577235996723,\n",
       "   0.44587932297587396,\n",
       "   0.4526464014351368,\n",
       "   0.4377901410162449],\n",
       "  'train_loss_std': [0.14395432451489668,\n",
       "   0.11333838373880073,\n",
       "   0.12711974579300753,\n",
       "   0.12105659789091615,\n",
       "   0.13434592199589895,\n",
       "   0.1313440359206019,\n",
       "   0.13860421011488458,\n",
       "   0.13037191019217056,\n",
       "   0.1272481605095346,\n",
       "   0.12974667647832655,\n",
       "   0.13741041014183927,\n",
       "   0.15123350618087675,\n",
       "   0.1418155604692662,\n",
       "   0.1374293410478573,\n",
       "   0.13186008332118565,\n",
       "   0.140305291953603,\n",
       "   0.13605381381361312,\n",
       "   0.13910498653681638,\n",
       "   0.13889903648615332,\n",
       "   0.1434268096153552,\n",
       "   0.13734807948094938,\n",
       "   0.13614374847329086,\n",
       "   0.14652969026418683,\n",
       "   0.14201605577733833,\n",
       "   0.14244231613074756,\n",
       "   0.15172665964938425,\n",
       "   0.1386274556381588,\n",
       "   0.1448365387598719,\n",
       "   0.1387978411864321,\n",
       "   0.1392369955539013,\n",
       "   0.13945563215547616,\n",
       "   0.13514215764376322,\n",
       "   0.14039674919546777,\n",
       "   0.14985599857866935,\n",
       "   0.13834088182754622,\n",
       "   0.14977615913270442,\n",
       "   0.13755955071196108,\n",
       "   0.1426919142423052,\n",
       "   0.14972353920124865,\n",
       "   0.14629833840481885,\n",
       "   0.13974395437761303,\n",
       "   0.13765699316149732,\n",
       "   0.13424311337994166,\n",
       "   0.13918151588458477,\n",
       "   0.1359302521562211,\n",
       "   0.13696987917735345,\n",
       "   0.1381445758943189,\n",
       "   0.13321522538615158,\n",
       "   0.13803707231606446,\n",
       "   0.13306890934295446,\n",
       "   0.1344519116346583,\n",
       "   0.1382546355147701,\n",
       "   0.13382131089253216,\n",
       "   0.1375218830216236,\n",
       "   0.14794151804726205,\n",
       "   0.1303917236588258,\n",
       "   0.13799221257661803,\n",
       "   0.13942923150945669,\n",
       "   0.13231009475445332,\n",
       "   0.13433800276299657,\n",
       "   0.13837884123711383,\n",
       "   0.13936341481167122,\n",
       "   0.14135101436191946,\n",
       "   0.14932812710107446,\n",
       "   0.1299503415927997,\n",
       "   0.1355631097071234,\n",
       "   0.13544314982727282,\n",
       "   0.1397812187860245,\n",
       "   0.1285458448637036,\n",
       "   0.1353943106281065,\n",
       "   0.13053567723681753,\n",
       "   0.13188190368150735,\n",
       "   0.14005317621076105,\n",
       "   0.13371949564274818,\n",
       "   0.13816014847329,\n",
       "   0.1314713657649213,\n",
       "   0.13112349419041336,\n",
       "   0.13471776218016887,\n",
       "   0.13027689970342018,\n",
       "   0.13290112830775985,\n",
       "   0.12413065756293956,\n",
       "   0.1321072014420857,\n",
       "   0.1399392415305865,\n",
       "   0.13140487575785323,\n",
       "   0.13105213940689583,\n",
       "   0.1271608603606777,\n",
       "   0.12232877988984227,\n",
       "   0.1290914488083553,\n",
       "   0.12357111193000987,\n",
       "   0.1265193672330218,\n",
       "   0.12458573172082225,\n",
       "   0.13162288135038194,\n",
       "   0.11534974463449765,\n",
       "   0.12952297854521969,\n",
       "   0.12435834453104028,\n",
       "   0.1389928684385691,\n",
       "   0.12120323266319391,\n",
       "   0.1277518306115412,\n",
       "   0.1216219440238964],\n",
       "  'train_accuracy_mean': [0.37069333359599116,\n",
       "   0.4429600010514259,\n",
       "   0.4647066667675972,\n",
       "   0.4883599992990494,\n",
       "   0.5131999998092651,\n",
       "   0.5351200005412102,\n",
       "   0.5534133326411247,\n",
       "   0.5779733316302299,\n",
       "   0.5907733336687088,\n",
       "   0.5979866657853127,\n",
       "   0.6220266659855842,\n",
       "   0.6233199979066849,\n",
       "   0.6316133319735527,\n",
       "   0.6442533332705498,\n",
       "   0.6442933332920074,\n",
       "   0.6498933331370353,\n",
       "   0.659346665263176,\n",
       "   0.6675466662049293,\n",
       "   0.6657066665887833,\n",
       "   0.6716933337450027,\n",
       "   0.6720133323669434,\n",
       "   0.6882933328151702,\n",
       "   0.6891733328104019,\n",
       "   0.6909999992251397,\n",
       "   0.6918533331155777,\n",
       "   0.6961466667056083,\n",
       "   0.705066666841507,\n",
       "   0.708159999191761,\n",
       "   0.7087200011014938,\n",
       "   0.7072133328318596,\n",
       "   0.717360000371933,\n",
       "   0.7175333334207534,\n",
       "   0.7243199995756149,\n",
       "   0.7285599997043609,\n",
       "   0.7260400002002716,\n",
       "   0.7356933336853981,\n",
       "   0.7333333344459534,\n",
       "   0.7377333332896232,\n",
       "   0.7363199999332428,\n",
       "   0.7475466676354409,\n",
       "   0.7430000010728836,\n",
       "   0.7492400013208389,\n",
       "   0.7498533320426941,\n",
       "   0.7495199999809266,\n",
       "   0.7541600006818772,\n",
       "   0.7580000001192093,\n",
       "   0.7559466677904129,\n",
       "   0.7636399995088577,\n",
       "   0.7658799993991852,\n",
       "   0.7625333334207535,\n",
       "   0.7626399983167649,\n",
       "   0.7732266647815704,\n",
       "   0.7706933327913285,\n",
       "   0.7705733337402344,\n",
       "   0.7721866673231125,\n",
       "   0.7775466668605805,\n",
       "   0.7746533321142197,\n",
       "   0.7821866668462754,\n",
       "   0.7803599991798401,\n",
       "   0.7778133322000503,\n",
       "   0.7796400007009506,\n",
       "   0.7875466661453248,\n",
       "   0.7886400009393693,\n",
       "   0.7777466655969619,\n",
       "   0.7903733334541321,\n",
       "   0.7915733312368393,\n",
       "   0.7917733327150345,\n",
       "   0.7990533335208893,\n",
       "   0.7999999995231628,\n",
       "   0.7966133326292038,\n",
       "   0.7994533331394196,\n",
       "   0.7988000001907348,\n",
       "   0.7957999991178513,\n",
       "   0.8017200000286102,\n",
       "   0.8063333348035813,\n",
       "   0.8085466663837433,\n",
       "   0.804853335261345,\n",
       "   0.8050666663646698,\n",
       "   0.8098933339118958,\n",
       "   0.8110666654109955,\n",
       "   0.8160133337974549,\n",
       "   0.8106666659116745,\n",
       "   0.8153466678857804,\n",
       "   0.817799998998642,\n",
       "   0.821519998908043,\n",
       "   0.8179866662025451,\n",
       "   0.8210000007152557,\n",
       "   0.8225199991464615,\n",
       "   0.8258133324384689,\n",
       "   0.8262800006866455,\n",
       "   0.8266933342218399,\n",
       "   0.8276533333063125,\n",
       "   0.833626668214798,\n",
       "   0.8289600015878678,\n",
       "   0.8305066677331925,\n",
       "   0.8271066660881042,\n",
       "   0.8359733339548111,\n",
       "   0.8337599998712539,\n",
       "   0.8393333332538605],\n",
       "  'train_accuracy_std': [0.0766484566360368,\n",
       "   0.06375887423490642,\n",
       "   0.06983140344978796,\n",
       "   0.06645081097227663,\n",
       "   0.06909561705264296,\n",
       "   0.06746593476190425,\n",
       "   0.07170428686099195,\n",
       "   0.06530189113446383,\n",
       "   0.06611657844767474,\n",
       "   0.0655488958249352,\n",
       "   0.06948943519117827,\n",
       "   0.07238477309039977,\n",
       "   0.06905824857850075,\n",
       "   0.06624096003845513,\n",
       "   0.06401016017503583,\n",
       "   0.06567317884517705,\n",
       "   0.06622533740727694,\n",
       "   0.06528929708091179,\n",
       "   0.06665708840864606,\n",
       "   0.0709408149481394,\n",
       "   0.06510941014946067,\n",
       "   0.061867409215896334,\n",
       "   0.06982092257944582,\n",
       "   0.063668411695221,\n",
       "   0.06475688423526006,\n",
       "   0.06988114262025241,\n",
       "   0.06372105914721576,\n",
       "   0.0669732372169136,\n",
       "   0.06379311623137775,\n",
       "   0.0649701734573289,\n",
       "   0.06280523295637327,\n",
       "   0.06085797585225891,\n",
       "   0.06363789743475273,\n",
       "   0.0669033610935633,\n",
       "   0.06302492042454541,\n",
       "   0.06611662585150434,\n",
       "   0.06259002349539497,\n",
       "   0.062139770798644786,\n",
       "   0.06652628540686055,\n",
       "   0.06532060176314308,\n",
       "   0.06242346371620217,\n",
       "   0.060841872811850016,\n",
       "   0.05806873802044365,\n",
       "   0.06142920633719081,\n",
       "   0.06116521208393779,\n",
       "   0.060884588818522274,\n",
       "   0.060173946170244186,\n",
       "   0.058278978144496695,\n",
       "   0.06011399276989966,\n",
       "   0.05876642937223986,\n",
       "   0.05848729440810904,\n",
       "   0.06153436154620754,\n",
       "   0.05852983513984785,\n",
       "   0.06068593123072759,\n",
       "   0.06457877700587399,\n",
       "   0.0558046494432728,\n",
       "   0.05948026564097738,\n",
       "   0.05974181981651452,\n",
       "   0.05926178628905173,\n",
       "   0.059203759905654935,\n",
       "   0.060292281796880745,\n",
       "   0.05929908234629067,\n",
       "   0.05990376867607915,\n",
       "   0.06180246448648162,\n",
       "   0.056648179731089446,\n",
       "   0.059240492050440206,\n",
       "   0.059980457648278536,\n",
       "   0.05917726179649516,\n",
       "   0.05534859438199707,\n",
       "   0.05959788838774204,\n",
       "   0.056826939943452844,\n",
       "   0.05730836646921678,\n",
       "   0.06117555891421147,\n",
       "   0.05631160789883762,\n",
       "   0.060449979888514506,\n",
       "   0.05730172560835069,\n",
       "   0.05648127201393746,\n",
       "   0.056781217680472944,\n",
       "   0.056481164532692286,\n",
       "   0.056896162071701076,\n",
       "   0.053301822981599106,\n",
       "   0.05656854241906197,\n",
       "   0.057846846322377785,\n",
       "   0.056290161973020725,\n",
       "   0.054676427920292967,\n",
       "   0.05465804694123846,\n",
       "   0.052474545056419096,\n",
       "   0.0540415962790607,\n",
       "   0.05391994874793915,\n",
       "   0.05351578745123087,\n",
       "   0.05204954302900933,\n",
       "   0.05573353128008745,\n",
       "   0.049162797646936346,\n",
       "   0.054824839077500014,\n",
       "   0.05387401950640552,\n",
       "   0.05937064315086252,\n",
       "   0.052577640343184684,\n",
       "   0.05322651995237746,\n",
       "   0.05125362104696331],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.4665240701039632,\n",
       "   1.4171582623322805,\n",
       "   1.3929247081279754,\n",
       "   1.3521079965432485,\n",
       "   1.3207840005556741,\n",
       "   1.2823985087871552,\n",
       "   1.2357146400213241,\n",
       "   1.2074770802259445,\n",
       "   1.16887910703818,\n",
       "   1.1530817288160324,\n",
       "   1.1405859416723252,\n",
       "   1.1206016393502554,\n",
       "   1.1152420143286388,\n",
       "   1.1028757802645366,\n",
       "   1.0881827984253565,\n",
       "   1.0752600787083308,\n",
       "   1.0674547817309696,\n",
       "   1.0507617690165838,\n",
       "   1.0455724114179612,\n",
       "   1.029528054992358,\n",
       "   1.0347131176789601,\n",
       "   1.0308612650632858,\n",
       "   1.0205002764860789,\n",
       "   1.0117877606550854,\n",
       "   0.9991415997346242,\n",
       "   0.9970713939269383,\n",
       "   0.9776921087503433,\n",
       "   0.9844942214091619,\n",
       "   0.971526497801145,\n",
       "   0.9629404856761297,\n",
       "   0.948158764441808,\n",
       "   0.9565264155467351,\n",
       "   0.9395616263151169,\n",
       "   0.9322134967645009,\n",
       "   0.9252809810638428,\n",
       "   0.9278887983163198,\n",
       "   0.9269906002283096,\n",
       "   0.9079518123467764,\n",
       "   0.9162568575143815,\n",
       "   0.9227782398462295,\n",
       "   0.9155866428216298,\n",
       "   0.9006667524576187,\n",
       "   0.9016995388269424,\n",
       "   0.9051644812027614,\n",
       "   0.9001533788442612,\n",
       "   0.9028488105535507,\n",
       "   0.9130249049266179,\n",
       "   0.9138602681954702,\n",
       "   0.8877618835369746,\n",
       "   0.8777974281708399,\n",
       "   0.9076733261346817,\n",
       "   0.8865848875045776,\n",
       "   0.881484682559967,\n",
       "   0.885455636382103,\n",
       "   0.8732743002971013,\n",
       "   0.8691663382450739,\n",
       "   0.8906439789136251,\n",
       "   0.8844351375102997,\n",
       "   0.8689276750882466,\n",
       "   0.8560870597759883,\n",
       "   0.8502876708904902,\n",
       "   0.8479099889596303,\n",
       "   0.8629307077328364,\n",
       "   0.8676115584373474,\n",
       "   0.850867133140564,\n",
       "   0.8470532782872517,\n",
       "   0.8576970545450846,\n",
       "   0.8460821300745011,\n",
       "   0.8614348691701889,\n",
       "   0.8581943772236507,\n",
       "   0.8761467144886652,\n",
       "   0.8548586076498031,\n",
       "   0.8423090946674346,\n",
       "   0.8468090269962947,\n",
       "   0.8592915604511897,\n",
       "   0.846453515291214,\n",
       "   0.8574012688795726,\n",
       "   0.8604395176966985,\n",
       "   0.8488959797223409,\n",
       "   0.8528792436917623,\n",
       "   0.8373437591393789,\n",
       "   0.8498614275455475,\n",
       "   0.8510965873797735,\n",
       "   0.8445712782939275,\n",
       "   0.8561002175013225,\n",
       "   0.8459126494328181,\n",
       "   0.847146147886912,\n",
       "   0.8611347327629725,\n",
       "   0.8484471168120702,\n",
       "   0.8389656205972036,\n",
       "   0.8527462327480316,\n",
       "   0.8448436069488525,\n",
       "   0.8654573482275009,\n",
       "   0.8528101329008738,\n",
       "   0.847724948724111,\n",
       "   0.8371095420916875,\n",
       "   0.853082467118899,\n",
       "   0.8638810996214549,\n",
       "   0.8563886060317357],\n",
       "  'val_loss_std': [0.08925934824660577,\n",
       "   0.0962115106736041,\n",
       "   0.09785377350198164,\n",
       "   0.10308777117396366,\n",
       "   0.10379286137959226,\n",
       "   0.1100652211658379,\n",
       "   0.10810219226848666,\n",
       "   0.115063940231856,\n",
       "   0.11814714321202346,\n",
       "   0.12110769536321783,\n",
       "   0.12240733687056339,\n",
       "   0.12659264634392622,\n",
       "   0.12382647519402466,\n",
       "   0.12968275305346055,\n",
       "   0.12844136788866767,\n",
       "   0.12429290625371377,\n",
       "   0.12871664938497682,\n",
       "   0.1266904178306537,\n",
       "   0.12727888312397284,\n",
       "   0.12889609389107845,\n",
       "   0.1313175871417351,\n",
       "   0.13201137032978313,\n",
       "   0.1259526668741782,\n",
       "   0.1296961244546495,\n",
       "   0.13350331956566044,\n",
       "   0.13289292460184882,\n",
       "   0.13145396794711273,\n",
       "   0.13060179318986692,\n",
       "   0.13357975411905498,\n",
       "   0.1334472919842112,\n",
       "   0.13183806199079628,\n",
       "   0.13489732834192797,\n",
       "   0.13689987353636482,\n",
       "   0.1342893066494463,\n",
       "   0.13439243955069743,\n",
       "   0.135543301869013,\n",
       "   0.13674488682062708,\n",
       "   0.1358373287169887,\n",
       "   0.13662543119044604,\n",
       "   0.1332164788220565,\n",
       "   0.14243757029091045,\n",
       "   0.13396603707063587,\n",
       "   0.1320232131485934,\n",
       "   0.1328392030775204,\n",
       "   0.13551865885521033,\n",
       "   0.13844065492774665,\n",
       "   0.1375059595211697,\n",
       "   0.1386343597346623,\n",
       "   0.13876031742746722,\n",
       "   0.13199150121379633,\n",
       "   0.141161827694547,\n",
       "   0.1368554390194922,\n",
       "   0.14136913080192484,\n",
       "   0.12493857512129981,\n",
       "   0.13582857057996295,\n",
       "   0.13535902917639603,\n",
       "   0.14323822959728608,\n",
       "   0.13862600489213434,\n",
       "   0.13144245184657638,\n",
       "   0.13920814706771145,\n",
       "   0.13557899393781472,\n",
       "   0.13331776433182943,\n",
       "   0.13177768400348586,\n",
       "   0.140342397160721,\n",
       "   0.14008246389525553,\n",
       "   0.13495934427253403,\n",
       "   0.13348690431098875,\n",
       "   0.13508018210927444,\n",
       "   0.1364814989770138,\n",
       "   0.12875634567051716,\n",
       "   0.13803099216237788,\n",
       "   0.13945810556226512,\n",
       "   0.1347022238355996,\n",
       "   0.13466460868966482,\n",
       "   0.1309065270280309,\n",
       "   0.1371298473825938,\n",
       "   0.13512415833957742,\n",
       "   0.13527794369829224,\n",
       "   0.1410121492868729,\n",
       "   0.13858548135255375,\n",
       "   0.13391902094933214,\n",
       "   0.13720964319277953,\n",
       "   0.14410937150890238,\n",
       "   0.1348865303337384,\n",
       "   0.1388979985155829,\n",
       "   0.14354996131970943,\n",
       "   0.13804031083444798,\n",
       "   0.14501041672857284,\n",
       "   0.13926559769320654,\n",
       "   0.133890478818652,\n",
       "   0.14732048299364692,\n",
       "   0.14037577380632232,\n",
       "   0.14494928544447935,\n",
       "   0.1362168295471893,\n",
       "   0.13970073765015756,\n",
       "   0.14332653753978378,\n",
       "   0.146693152060341,\n",
       "   0.14264648011756165,\n",
       "   0.14517821504828038],\n",
       "  'val_accuracy_mean': [0.38444444532195726,\n",
       "   0.4115555561085542,\n",
       "   0.42495555559794107,\n",
       "   0.44615555594364803,\n",
       "   0.4644888890782992,\n",
       "   0.4791555555661519,\n",
       "   0.5022000006834666,\n",
       "   0.5149777763088544,\n",
       "   0.5347777769962947,\n",
       "   0.540666664938132,\n",
       "   0.5482444435358047,\n",
       "   0.5573999987045923,\n",
       "   0.5586222209533056,\n",
       "   0.5657111112276713,\n",
       "   0.5733999998370807,\n",
       "   0.5777333334088326,\n",
       "   0.5819333322842916,\n",
       "   0.5869555561741193,\n",
       "   0.591244444946448,\n",
       "   0.6012444436550141,\n",
       "   0.5970666656891505,\n",
       "   0.6003111113111178,\n",
       "   0.6002888866265614,\n",
       "   0.6054222213228544,\n",
       "   0.6103999996185303,\n",
       "   0.6132666664322217,\n",
       "   0.6210222199559212,\n",
       "   0.6195777772863706,\n",
       "   0.6230222220222156,\n",
       "   0.6269333319862683,\n",
       "   0.6322222207983335,\n",
       "   0.6310666671395302,\n",
       "   0.6359333338340124,\n",
       "   0.6402666673064232,\n",
       "   0.6431333323319753,\n",
       "   0.6444222223758698,\n",
       "   0.6442222221692403,\n",
       "   0.6508888866504033,\n",
       "   0.6474444450934728,\n",
       "   0.6436666671435038,\n",
       "   0.6492888904611269,\n",
       "   0.6522444445888201,\n",
       "   0.6544888892769813,\n",
       "   0.6525555551052094,\n",
       "   0.6522444439927737,\n",
       "   0.6543333328763644,\n",
       "   0.6491777767737706,\n",
       "   0.6498666656017303,\n",
       "   0.6586222206552823,\n",
       "   0.6625555568933487,\n",
       "   0.6531777779261271,\n",
       "   0.6570888869961102,\n",
       "   0.6623777764042219,\n",
       "   0.6622444446881612,\n",
       "   0.6637999998529752,\n",
       "   0.6656444448232651,\n",
       "   0.6576666649182638,\n",
       "   0.6585555551449458,\n",
       "   0.6678666667143504,\n",
       "   0.6745333335796992,\n",
       "   0.6744000005722046,\n",
       "   0.6759333332379659,\n",
       "   0.6685333351294199,\n",
       "   0.6675333335002264,\n",
       "   0.6761333312590917,\n",
       "   0.6757333340247472,\n",
       "   0.670711112121741,\n",
       "   0.6745999991893769,\n",
       "   0.6738444425662359,\n",
       "   0.670933333337307,\n",
       "   0.6647111114859581,\n",
       "   0.6749999994039535,\n",
       "   0.678466666340828,\n",
       "   0.6754666682084401,\n",
       "   0.6697111110885938,\n",
       "   0.6778888899087906,\n",
       "   0.6735333324472109,\n",
       "   0.6703333316246668,\n",
       "   0.674511108994484,\n",
       "   0.6746444447835287,\n",
       "   0.6797777791817983,\n",
       "   0.6743111101786295,\n",
       "   0.6767111106713612,\n",
       "   0.6751333336035411,\n",
       "   0.6725333332022031,\n",
       "   0.6786888877550761,\n",
       "   0.6757999990383784,\n",
       "   0.673799999554952,\n",
       "   0.6739555535713831,\n",
       "   0.6764444426695506,\n",
       "   0.6765333332618078,\n",
       "   0.6791555551687877,\n",
       "   0.6720222214857737,\n",
       "   0.6716888894637426,\n",
       "   0.6760888901352883,\n",
       "   0.6818444436788559,\n",
       "   0.6746666649977366,\n",
       "   0.6732000005245209,\n",
       "   0.6749777751167615],\n",
       "  'val_accuracy_std': [0.05025957484042379,\n",
       "   0.05491092657106505,\n",
       "   0.056014860953133024,\n",
       "   0.057303915876824554,\n",
       "   0.05489921667889171,\n",
       "   0.0579440170032123,\n",
       "   0.05834803579157738,\n",
       "   0.061105550215058924,\n",
       "   0.06031634521410712,\n",
       "   0.061762986879083576,\n",
       "   0.061663598869231936,\n",
       "   0.06523958087297703,\n",
       "   0.0606033678292508,\n",
       "   0.06229301844736194,\n",
       "   0.06340577131165628,\n",
       "   0.06056685322641931,\n",
       "   0.06327104455941784,\n",
       "   0.06075626592125194,\n",
       "   0.06262711152273724,\n",
       "   0.06248383168348306,\n",
       "   0.06393969275864483,\n",
       "   0.06281701437949051,\n",
       "   0.059931363273559926,\n",
       "   0.06202454358468791,\n",
       "   0.06329229072301316,\n",
       "   0.061935416068123285,\n",
       "   0.05996165125627951,\n",
       "   0.060748173305084176,\n",
       "   0.06150501102389181,\n",
       "   0.060213841196828814,\n",
       "   0.06051833851282325,\n",
       "   0.0621864815082362,\n",
       "   0.06237683816852382,\n",
       "   0.06194085544109535,\n",
       "   0.06175546846432876,\n",
       "   0.059390728577605835,\n",
       "   0.0591716491127654,\n",
       "   0.06141741551392941,\n",
       "   0.06180365114969122,\n",
       "   0.05990888315942995,\n",
       "   0.06136905750596059,\n",
       "   0.060035789344127136,\n",
       "   0.05939820142683592,\n",
       "   0.058374165846856924,\n",
       "   0.05840217808412295,\n",
       "   0.06079260435417133,\n",
       "   0.06010908336176229,\n",
       "   0.05962212130719612,\n",
       "   0.059612559863349374,\n",
       "   0.05809432565964007,\n",
       "   0.05699231617511584,\n",
       "   0.05953623024313608,\n",
       "   0.06010957450016752,\n",
       "   0.058450356632963406,\n",
       "   0.060971797022165326,\n",
       "   0.0598503654293905,\n",
       "   0.06133846862200954,\n",
       "   0.0601441699666946,\n",
       "   0.05833781553150552,\n",
       "   0.05941228396370613,\n",
       "   0.05977528211285417,\n",
       "   0.06057425206481595,\n",
       "   0.060714853963637336,\n",
       "   0.06090664298512532,\n",
       "   0.05838198749400176,\n",
       "   0.0596547109791299,\n",
       "   0.05985858400613019,\n",
       "   0.0599631613697989,\n",
       "   0.05993976845198127,\n",
       "   0.059609990782477584,\n",
       "   0.0596237427018188,\n",
       "   0.05967846078212273,\n",
       "   0.0571043630562775,\n",
       "   0.059368629623608796,\n",
       "   0.05840654132492468,\n",
       "   0.05799989487210741,\n",
       "   0.057538047456175696,\n",
       "   0.05779946214836733,\n",
       "   0.0607674827004244,\n",
       "   0.05679514491519087,\n",
       "   0.05904633261034272,\n",
       "   0.0600537363777726,\n",
       "   0.06022302652991291,\n",
       "   0.05911085845608882,\n",
       "   0.05932669264004693,\n",
       "   0.056531075242787124,\n",
       "   0.06114212978223932,\n",
       "   0.0599154102371408,\n",
       "   0.05677687006701778,\n",
       "   0.059882189065888866,\n",
       "   0.05776694063981755,\n",
       "   0.05748454031729291,\n",
       "   0.05736355312258769,\n",
       "   0.0584817127366227,\n",
       "   0.060124250617929295,\n",
       "   0.05574018951959973,\n",
       "   0.0604844655835651,\n",
       "   0.05885560704646691,\n",
       "   0.0588831216038151],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb0c68",
   "metadata": {},
   "source": [
    "### 1.1 MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     #print(key)\n",
    "#     if value.requires_grad:\n",
    "#         print(key)\n",
    "#         print(value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a599c8",
   "metadata": {},
   "source": [
    "### 1.2 Arbiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9ebc67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = arbiter_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = arbiter_system.state['best_epoch']\n",
    "\n",
    "state = arbiter_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "arbiter_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "569eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0531d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a86b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "\n",
    "x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "\n",
    "\n",
    "x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task = next(zip(x_support_set,y_support_set,x_target_set, y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cdeb442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "647183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbiter_x_support_set, arbiter_x_target_set, arbiter_y_support_set, arbiter_y_target_set, seed = train_sample\n",
    "\n",
    "arbiter_x_support_set = torch.Tensor(arbiter_x_support_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_x_target_set = torch.Tensor(arbiter_x_target_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_y_support_set = torch.Tensor(arbiter_y_support_set).long().to(device=arbiter_system.model.device)\n",
    "arbiter_y_target_set = torch.Tensor(arbiter_y_target_set).long().to(device=arbiter_system.model.device)\n",
    "\n",
    "\n",
    "arbiter_x_support_set_task, arbiter_y_support_set_task, arbiter_x_target_set_task, arbiter_y_target_set_task = next(zip(arbiter_x_support_set,arbiter_y_support_set,arbiter_x_target_set, arbiter_y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ce1c0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fd4d6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = arbiter_system.model.get_inner_loop_parameter_dict(arbiter_system.model.classifier.named_parameters())\n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = arbiter_x_target_set_task.shape\n",
    "\n",
    "arbiter_x_support_set_task = arbiter_x_support_set_task.view(-1, c, h, w)\n",
    "arbiter_y_support_set_task = arbiter_y_support_set_task.view(-1)\n",
    "arbiter_x_target_set_task = arbiter_x_target_set_task.view(-1, c, h, w)\n",
    "arbiter_y_target_set_task = arbiter_y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = arbiter_system.model.net_forward(\n",
    "            x=arbiter_x_support_set_task,\n",
    "            y=arbiter_y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "    \n",
    "    if arbiter_system.model.args.arbiter:\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                retain_graph=True)\n",
    "\n",
    "        names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "\n",
    "        for key, weight in names_weights_copy.items():\n",
    "            weight_norm = torch.norm(weight, p=2)\n",
    "            per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "        for key, grad in names_grads_copy.items():\n",
    "            gradient_l2norm = torch.norm(grad, p=2)\n",
    "            per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "        per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                    per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        generated_gradient_rate = arbiter_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "            g += 1\n",
    "\n",
    "    names_weights_copy,names_grads_copy = arbiter_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                      support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_arbiter.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=arbiter_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in arbiter_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d16650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "y_support_set_task = y_support_set_task.view(-1)\n",
    "x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "y_target_set_task = y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "            x=x_support_set_task,\n",
    "            y=y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "\n",
    "\n",
    "    names_weights_copy,names_grads_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                   support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_maml.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=maml_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575454f0",
   "metadata": {},
   "source": [
    "## landscape 함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aec9618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape==  torch.Size([25, 3, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "tensor([[[[-1.1935e-04,  2.6568e-03,  6.7283e-03],\n",
      "          [-7.2114e-05,  1.0739e-03,  4.9787e-03],\n",
      "          [-8.9697e-04,  1.1596e-03,  3.2536e-03]],\n",
      "\n",
      "         [[-1.4466e-03,  7.7378e-04,  3.5977e-03],\n",
      "          [-1.7130e-03, -1.1535e-03,  1.4853e-03],\n",
      "          [-3.0795e-03, -1.7881e-03, -6.2774e-04]],\n",
      "\n",
      "         [[ 2.4235e-03,  4.0640e-03,  6.5466e-03],\n",
      "          [ 1.4213e-03,  1.7149e-03,  4.3617e-03],\n",
      "          [-7.5947e-04,  2.3111e-04,  1.2487e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.8490e-03, -7.4451e-03, -5.3934e-03],\n",
      "          [-6.7272e-03, -7.8326e-03, -5.3003e-03],\n",
      "          [-5.4965e-03, -6.9102e-03, -4.7017e-03]],\n",
      "\n",
      "         [[-5.1551e-03, -4.8080e-03, -2.4748e-03],\n",
      "          [-3.1338e-03, -4.1795e-03, -1.4726e-03],\n",
      "          [-1.4338e-03, -2.7918e-03, -6.6917e-04]],\n",
      "\n",
      "         [[-3.4027e-03, -3.1511e-03, -5.9323e-04],\n",
      "          [-1.2544e-03, -2.5572e-03,  6.0006e-04],\n",
      "          [ 4.4349e-04, -8.3890e-04,  1.6769e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0733e-04,  2.9998e-04,  2.1565e-03],\n",
      "          [-4.6949e-04, -3.4708e-04,  1.6355e-03],\n",
      "          [-2.1775e-05,  5.1924e-04,  1.8155e-03]],\n",
      "\n",
      "         [[-6.7213e-04, -1.2363e-03, -1.3433e-04],\n",
      "          [-1.3796e-03, -1.7155e-03, -3.7246e-04],\n",
      "          [-7.1789e-04, -7.7283e-04,  4.5494e-05]],\n",
      "\n",
      "         [[-3.9461e-04, -1.1879e-03, -4.8695e-05],\n",
      "          [-1.4153e-03, -1.8496e-03, -5.2787e-04],\n",
      "          [-8.7737e-04, -9.9142e-04, -9.8369e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.9095e-03, -1.9484e-03, -2.6305e-03],\n",
      "          [-2.5051e-03, -1.9853e-03, -2.0366e-03],\n",
      "          [-1.3016e-03, -1.8473e-03, -1.4471e-03]],\n",
      "\n",
      "         [[-2.5799e-03, -1.4853e-03, -2.1397e-03],\n",
      "          [-2.3458e-03, -1.6473e-03, -1.6353e-03],\n",
      "          [-1.0855e-03, -1.6824e-03, -1.2251e-03]],\n",
      "\n",
      "         [[-3.8061e-04,  4.7501e-04, -2.5654e-04],\n",
      "          [-9.7418e-05,  3.6347e-04,  2.9214e-04],\n",
      "          [ 9.4925e-04,  1.8877e-04,  5.1095e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.6122e-03, -1.4724e-03, -1.8810e-03],\n",
      "          [-2.0968e-03, -1.7925e-03, -2.2866e-03],\n",
      "          [-2.9276e-03, -2.4243e-03, -2.3928e-03]],\n",
      "\n",
      "         [[-9.1566e-04, -9.2189e-04, -1.3242e-03],\n",
      "          [-1.4513e-03, -1.3022e-03, -1.8205e-03],\n",
      "          [-2.1901e-03, -1.8818e-03, -2.0159e-03]],\n",
      "\n",
      "         [[-2.2946e-03, -2.2608e-03, -2.6888e-03],\n",
      "          [-3.0678e-03, -2.8090e-03, -3.2719e-03],\n",
      "          [-3.9149e-03, -3.5322e-03, -3.6355e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.8487e-03,  9.5015e-03,  7.2266e-03],\n",
      "          [ 7.5697e-03,  8.4372e-03,  6.8185e-03],\n",
      "          [ 6.7829e-03,  6.4869e-03,  5.2952e-03]],\n",
      "\n",
      "         [[ 8.2855e-03,  8.3001e-03,  6.0868e-03],\n",
      "          [ 7.1049e-03,  7.3330e-03,  5.7230e-03],\n",
      "          [ 6.4244e-03,  5.6718e-03,  4.6582e-03]],\n",
      "\n",
      "         [[ 7.3324e-03,  7.6966e-03,  5.7333e-03],\n",
      "          [ 6.4466e-03,  6.9278e-03,  5.5162e-03],\n",
      "          [ 6.0005e-03,  5.3876e-03,  4.5291e-03]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-9.3406e-07, -1.7126e-04,  6.4003e-04, -3.8635e-04, -3.5806e-04,\n",
      "         5.4498e-04,  4.8724e-04,  5.0076e-04,  9.4905e-05,  2.9777e-04,\n",
      "         1.7165e-04, -2.0534e-05,  8.5453e-05,  7.5451e-05,  2.4412e-05,\n",
      "        -9.8417e-05,  8.6334e-06, -3.3284e-04, -1.2595e-04,  4.5707e-04,\n",
      "         1.9822e-04, -9.3923e-04, -9.3909e-04,  3.4663e-04, -6.7359e-04,\n",
      "         1.8530e-05,  7.1742e-04,  9.1744e-05, -1.2655e-05, -2.3304e-04,\n",
      "        -6.8834e-04, -4.0007e-04, -3.0558e-05, -7.6544e-06, -3.0251e-04,\n",
      "         5.4258e-04,  4.5590e-04,  2.1105e-04,  3.3281e-04,  8.0815e-04,\n",
      "        -3.5272e-04, -2.4683e-04, -5.4023e-04, -1.2037e-04, -1.0900e-05,\n",
      "         1.8581e-05,  9.0205e-04,  7.4599e-04,  7.5132e-04,  3.0600e-04,\n",
      "        -1.2084e-04, -4.6243e-05, -2.3046e-04,  1.5719e-04, -3.1027e-04,\n",
      "         9.6865e-05,  7.7092e-04, -3.8463e-05,  4.2974e-04, -2.8418e-04,\n",
      "        -3.9559e-04, -3.7995e-04, -2.2686e-04, -1.4631e-04, -1.3893e-04,\n",
      "        -8.5224e-04, -9.1852e-06, -1.6645e-05, -1.4689e-04, -1.8687e-04,\n",
      "        -3.4804e-04, -2.0704e-04, -4.2381e-04, -5.3477e-04,  2.1641e-04,\n",
      "        -4.5214e-04,  1.6711e-04,  2.8719e-04,  1.4174e-04,  9.4971e-04,\n",
      "        -5.6962e-05,  2.4545e-04, -8.2422e-05, -5.7047e-04, -1.3996e-04,\n",
      "         6.0696e-06, -7.4447e-05,  5.4960e-05,  2.5499e-04,  1.6214e-04,\n",
      "        -8.8527e-05,  6.4002e-05,  1.4085e-04,  7.0210e-05,  4.3608e-04,\n",
      "        -1.3346e-04,  1.7802e-04,  3.1540e-04,  4.2129e-04, -2.7795e-04,\n",
      "         2.8980e-04,  3.0626e-04, -3.0016e-04, -3.8370e-04, -3.4443e-06,\n",
      "        -5.9550e-05, -2.1842e-04, -2.0821e-04,  1.5902e-04, -2.2108e-05,\n",
      "         1.4829e-05,  6.7043e-05,  2.0034e-04,  1.5962e-04,  4.0541e-04,\n",
      "         2.5020e-04, -2.2336e-05, -6.4953e-05, -1.7961e-04,  7.3111e-05,\n",
      "         1.7084e-04,  9.9732e-05,  3.2229e-04, -2.9357e-04, -4.3950e-07,\n",
      "        -5.4591e-04,  3.3344e-04,  1.5028e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 6.5646e-05,  3.8280e-04,  7.3964e-04, -3.0436e-04, -3.3497e-04,\n",
      "         4.9033e-04,  1.1702e-03,  5.0320e-04, -8.5898e-05, -2.0191e-04,\n",
      "         7.6018e-04,  1.8100e-04,  2.4746e-04, -1.4853e-04, -3.8946e-04,\n",
      "        -2.7403e-04, -6.3039e-05, -2.9671e-05, -4.3404e-04,  1.3676e-03,\n",
      "         1.1235e-04, -1.4742e-03, -3.8636e-04,  9.9251e-05, -2.3958e-04,\n",
      "         2.8529e-05, -4.3344e-05,  6.9144e-04,  4.0417e-04,  4.7744e-05,\n",
      "        -8.1817e-04, -5.1475e-04, -1.4978e-04,  6.1485e-05, -1.0359e-04,\n",
      "         1.4358e-04,  3.9497e-04, -4.8374e-04,  6.6243e-04, -4.3458e-05,\n",
      "        -3.2822e-04, -3.0883e-04, -5.0870e-04,  7.8944e-04,  1.1409e-03,\n",
      "        -1.9348e-04,  1.3302e-03,  8.7805e-04,  6.1906e-04,  3.5091e-04,\n",
      "        -1.3117e-04,  1.5726e-04,  4.9871e-04,  8.2839e-04, -6.4686e-04,\n",
      "        -9.6903e-04, -3.4771e-03,  5.1692e-04,  2.0821e-04, -5.0037e-04,\n",
      "        -3.6426e-04, -3.8532e-04, -4.5678e-04,  5.5491e-04, -7.5117e-06,\n",
      "        -8.1423e-04,  3.0352e-04,  2.8521e-04, -3.1962e-04, -1.8795e-04,\n",
      "         1.4126e-04, -4.4414e-04, -2.5811e-04, -1.9735e-04,  6.5872e-04,\n",
      "        -9.7845e-04, -1.0783e-04, -6.2453e-05, -2.9829e-04,  1.2506e-03,\n",
      "        -5.1097e-04,  5.4077e-04,  1.1009e-04, -4.6656e-04,  4.0190e-04,\n",
      "         7.6259e-04,  1.5423e-04,  1.6381e-04, -3.0126e-04,  4.6841e-04,\n",
      "         1.6122e-04,  1.0211e-03,  8.5081e-05,  4.3690e-04, -5.6005e-06,\n",
      "        -7.1175e-04,  9.4570e-05,  7.0217e-05,  4.3100e-04, -1.5230e-04,\n",
      "        -1.5099e-04,  1.5267e-03, -5.5507e-04, -6.6435e-04, -2.2451e-04,\n",
      "        -4.8828e-04,  7.4771e-05, -4.9570e-04,  6.5554e-04, -5.6098e-04,\n",
      "         4.7200e-04, -6.7656e-04,  2.7927e-04,  8.9913e-04,  8.0474e-04,\n",
      "        -6.6225e-04,  3.8311e-04, -4.4061e-04, -4.2602e-04, -6.0362e-04,\n",
      "        -6.9188e-04,  4.3393e-05, -1.3220e-05, -2.9149e-05,  1.3551e-04,\n",
      "        -9.7395e-04, -5.4868e-04,  1.1532e-03], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-3.2194e-04, -2.6646e-05, -1.3238e-04],\n",
      "          [ 6.3793e-05, -1.7857e-04, -2.4006e-04],\n",
      "          [ 2.4965e-05, -2.1460e-04,  4.7595e-05]],\n",
      "\n",
      "         [[-1.2528e-04,  4.7706e-04,  4.9490e-04],\n",
      "          [ 6.8115e-05, -1.2800e-04,  3.6378e-05],\n",
      "          [-1.2525e-04, -9.6783e-05, -3.3102e-04]],\n",
      "\n",
      "         [[ 1.4551e-04, -1.1727e-04, -4.8655e-05],\n",
      "          [-2.1029e-04, -7.4443e-05, -1.8236e-04],\n",
      "          [-1.5645e-04, -1.4793e-04, -1.0274e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4103e-04, -3.3594e-04, -2.8615e-04],\n",
      "          [-1.5347e-04, -1.5656e-04, -5.7820e-05],\n",
      "          [-2.4090e-04, -3.7660e-05, -6.1921e-05]],\n",
      "\n",
      "         [[-7.0660e-05,  1.5312e-05,  9.7002e-05],\n",
      "          [-2.0310e-04, -1.9940e-04, -1.1672e-04],\n",
      "          [-2.1599e-04, -1.3472e-04, -3.6025e-05]],\n",
      "\n",
      "         [[-1.1432e-05,  1.2774e-04,  3.4125e-04],\n",
      "          [-7.8690e-05,  1.5348e-04,  3.5585e-04],\n",
      "          [-3.3340e-04, -1.3455e-04,  1.6373e-04]]],\n",
      "\n",
      "\n",
      "        [[[-6.8588e-06, -1.1915e-04, -5.4013e-05],\n",
      "          [ 2.2338e-05, -2.3495e-04, -1.2070e-05],\n",
      "          [-8.1719e-05, -1.5257e-04,  1.0704e-05]],\n",
      "\n",
      "         [[ 1.2985e-04,  3.8697e-04,  6.0379e-04],\n",
      "          [ 1.7844e-04,  2.7370e-04,  3.6019e-04],\n",
      "          [ 2.7224e-04,  2.9175e-05, -8.7445e-05]],\n",
      "\n",
      "         [[-4.3152e-04, -3.8425e-04, -4.1394e-04],\n",
      "          [-4.3174e-04, -3.9517e-04, -3.4794e-04],\n",
      "          [-4.9078e-04, -4.7638e-04, -3.5126e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8587e-04, -9.0344e-07, -4.4166e-05],\n",
      "          [ 2.6255e-05,  3.9744e-05,  1.4675e-04],\n",
      "          [ 1.2775e-04,  2.0826e-04,  2.5102e-04]],\n",
      "\n",
      "         [[-5.1870e-04, -5.0336e-04, -5.0452e-04],\n",
      "          [-5.2264e-04, -5.2794e-04, -5.3372e-04],\n",
      "          [-5.2005e-04, -5.3528e-04, -5.8853e-04]],\n",
      "\n",
      "         [[-1.5680e-04, -1.9858e-04, -1.7990e-04],\n",
      "          [-1.9610e-04, -1.9942e-04, -1.8551e-04],\n",
      "          [-1.4656e-04, -2.1752e-04, -1.5713e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0259e-04,  7.2744e-05,  1.2306e-04],\n",
      "          [ 2.1683e-04,  2.5304e-04,  2.4466e-04],\n",
      "          [ 1.6949e-04,  4.4079e-04,  2.7760e-04]],\n",
      "\n",
      "         [[ 4.5180e-05, -3.4648e-05, -5.8257e-05],\n",
      "          [ 1.9493e-04,  9.4379e-05,  2.0396e-04],\n",
      "          [ 1.9418e-04,  1.7738e-04,  3.8753e-04]],\n",
      "\n",
      "         [[-3.1777e-04, -2.3322e-04, -2.3428e-04],\n",
      "          [-2.7809e-04, -2.6083e-04, -1.4605e-04],\n",
      "          [-3.9182e-04, -2.5396e-04, -2.0798e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7730e-04, -3.1329e-04, -2.6236e-04],\n",
      "          [-1.9767e-04, -1.8317e-04, -2.9413e-04],\n",
      "          [-9.3238e-05, -1.5625e-04, -3.2583e-04]],\n",
      "\n",
      "         [[-5.4491e-04, -5.0778e-04, -4.3126e-04],\n",
      "          [-5.2132e-04, -4.7824e-04, -4.2294e-04],\n",
      "          [-5.5091e-04, -4.7952e-04, -4.1308e-04]],\n",
      "\n",
      "         [[-4.0222e-04, -4.3939e-04, -3.4804e-04],\n",
      "          [-3.6798e-04, -4.3392e-04, -4.1295e-04],\n",
      "          [-3.9330e-04, -4.1486e-04, -4.4762e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7.6440e-04,  6.9994e-04,  6.0703e-04],\n",
      "          [ 8.0905e-04,  6.1073e-04,  6.5024e-04],\n",
      "          [ 8.4215e-04,  6.3041e-04,  5.7830e-04]],\n",
      "\n",
      "         [[ 1.0369e-03,  8.2624e-04,  1.0356e-03],\n",
      "          [ 9.1333e-04,  9.6541e-04,  9.7970e-04],\n",
      "          [ 9.1515e-04,  7.4333e-04,  8.8851e-04]],\n",
      "\n",
      "         [[-3.2216e-04, -1.9085e-04, -1.2645e-04],\n",
      "          [-2.3342e-04, -2.8195e-04, -2.7764e-04],\n",
      "          [-2.6901e-04, -2.6278e-04, -1.8125e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.6266e-04,  9.1135e-04,  8.1106e-04],\n",
      "          [ 9.0539e-04,  6.3897e-04,  7.2629e-04],\n",
      "          [ 8.9883e-04,  8.7332e-04,  9.9688e-04]],\n",
      "\n",
      "         [[ 6.7376e-05,  1.5573e-04,  1.9238e-04],\n",
      "          [ 7.6691e-05,  1.3706e-04,  1.8208e-04],\n",
      "          [ 5.4695e-05,  5.0965e-05,  5.6880e-05]],\n",
      "\n",
      "         [[ 1.2151e-03,  1.3898e-03,  1.3272e-03],\n",
      "          [ 1.3025e-03,  1.4970e-03,  1.4897e-03],\n",
      "          [ 1.3306e-03,  1.4948e-03,  1.5764e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.3242e-04, -3.5305e-04, -4.4408e-04],\n",
      "          [-3.0570e-04, -3.1043e-04, -4.1552e-04],\n",
      "          [-3.2027e-04, -4.8960e-04, -3.1935e-04]],\n",
      "\n",
      "         [[-2.7398e-04, -3.4598e-04, -4.3447e-04],\n",
      "          [-5.4715e-04, -4.9648e-04, -3.7422e-04],\n",
      "          [-5.2855e-04, -4.4240e-04, -4.2289e-04]],\n",
      "\n",
      "         [[-4.1199e-04, -4.9583e-04, -4.3357e-04],\n",
      "          [-3.7591e-04, -4.5203e-04, -5.0566e-04],\n",
      "          [-3.7343e-04, -5.4007e-04, -6.1427e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.4245e-04, -3.3070e-04, -4.1148e-04],\n",
      "          [-2.9229e-04, -3.3025e-04, -4.3912e-04],\n",
      "          [-4.2976e-04, -5.3067e-04, -4.6780e-04]],\n",
      "\n",
      "         [[-5.4238e-04, -5.8202e-04, -5.3641e-04],\n",
      "          [-5.5210e-04, -6.0069e-04, -5.6583e-04],\n",
      "          [-4.9375e-04, -5.8655e-04, -6.4896e-04]],\n",
      "\n",
      "         [[-4.4863e-04, -4.8466e-04, -5.9742e-04],\n",
      "          [-4.6296e-04, -5.0228e-04, -5.7996e-04],\n",
      "          [-4.9279e-04, -5.2286e-04, -5.3555e-04]]],\n",
      "\n",
      "\n",
      "        [[[-2.0839e-04, -3.0537e-04, -8.4181e-05],\n",
      "          [-1.5271e-04, -2.6937e-04, -1.3086e-04],\n",
      "          [-1.1816e-04, -1.2515e-04, -4.3421e-05]],\n",
      "\n",
      "         [[-1.5431e-04,  2.1665e-05,  5.8242e-05],\n",
      "          [ 1.5196e-04,  1.8057e-04,  1.8532e-04],\n",
      "          [ 2.2835e-04,  1.7698e-04,  2.3133e-04]],\n",
      "\n",
      "         [[-3.9255e-04, -4.6626e-04, -4.6418e-04],\n",
      "          [-4.6728e-04, -5.2850e-04, -5.1312e-04],\n",
      "          [-4.3528e-04, -5.0550e-04, -4.7222e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.3040e-05,  7.2641e-05,  9.2814e-05],\n",
      "          [-3.3390e-06,  1.0821e-04,  1.3006e-04],\n",
      "          [ 8.4595e-05,  1.7684e-04,  1.3268e-04]],\n",
      "\n",
      "         [[-4.1011e-04, -4.6376e-04, -4.4955e-04],\n",
      "          [-3.6926e-04, -4.6198e-04, -4.7574e-04],\n",
      "          [-4.1772e-04, -4.9358e-04, -5.0331e-04]],\n",
      "\n",
      "         [[ 3.0608e-05,  5.0601e-05,  4.3456e-05],\n",
      "          [ 9.1551e-06,  2.5570e-05,  4.0645e-05],\n",
      "          [-5.1721e-05, -1.5530e-05,  7.0052e-06]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-1.3097e-10, -2.9104e-11,  1.4552e-11, -2.1828e-11, -8.7311e-11,\n",
      "        -8.7311e-11,  1.4552e-11,  1.0914e-11,  5.0932e-11,  2.9104e-11,\n",
      "         1.3824e-10, -3.6380e-12, -2.1828e-11,  6.5484e-11,  7.2760e-11,\n",
      "        -1.0186e-10,  4.0018e-11,  9.4587e-11, -8.7311e-11,  0.0000e+00,\n",
      "        -2.9104e-11,  1.2369e-10,  5.8208e-11,  2.9104e-11,  5.8208e-11,\n",
      "        -1.4552e-11,  4.3656e-11,  2.5466e-11, -1.6371e-10,  3.2014e-10,\n",
      "         1.1642e-10,  5.8208e-11,  4.0018e-11, -2.6193e-10, -9.8225e-11,\n",
      "         1.0914e-11, -9.4587e-11,  8.7311e-11,  1.6371e-11,  2.3647e-11,\n",
      "        -2.9104e-10, -1.6007e-10,  5.8208e-11, -2.9104e-11,  1.0186e-10,\n",
      "         7.2760e-12,  1.3824e-10,  1.8190e-11,  3.6380e-11,  1.4552e-11,\n",
      "         2.9104e-11, -5.8208e-11,  1.8554e-10,  7.2760e-12,  7.6398e-11,\n",
      "        -2.9104e-11,  6.1846e-11, -8.7311e-11,  7.2760e-12, -1.1642e-10,\n",
      "        -5.8208e-11,  1.4916e-10,  1.4552e-11, -1.0186e-10,  1.3097e-10,\n",
      "        -8.0036e-11, -7.2760e-11, -7.2760e-11, -2.1828e-11, -1.4552e-11,\n",
      "        -5.8208e-11, -8.7311e-11,  1.4552e-11,  8.7311e-11,  0.0000e+00,\n",
      "         1.0186e-10,  1.0914e-11,  8.7311e-11,  1.4552e-11,  5.8208e-11,\n",
      "         1.1642e-10,  0.0000e+00,  1.0914e-10,  6.5484e-11,  2.5466e-11,\n",
      "         1.1642e-10,  1.2369e-10, -5.8208e-11, -8.7311e-11,  0.0000e+00,\n",
      "         8.7311e-11,  1.4188e-10,  1.6007e-10,  5.8208e-11,  3.6380e-12,\n",
      "        -2.1828e-11, -6.5484e-11, -9.0949e-12, -1.7462e-10,  5.8208e-11,\n",
      "         0.0000e+00, -7.2760e-12, -7.2760e-11, -5.8208e-11, -5.0932e-11,\n",
      "        -3.2742e-11,  1.0186e-10, -1.7462e-10,  1.1642e-10, -5.8208e-11,\n",
      "        -3.0013e-11,  3.6380e-10,  3.6380e-11,  0.0000e+00, -2.9104e-11,\n",
      "        -2.0373e-10, -1.1278e-10, -1.0186e-10, -3.4561e-11,  1.1642e-10,\n",
      "         4.0018e-11,  1.0186e-10,  4.3656e-11, -1.3097e-10,  6.9122e-11,\n",
      "        -7.2760e-12, -8.0036e-11, -2.4556e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-6.7944e-04, -7.7178e-04, -7.4499e-04],\n",
      "          [-2.0855e-04, -7.6281e-04, -3.7123e-04],\n",
      "          [ 3.5834e-04, -2.7780e-04, -2.3263e-04]],\n",
      "\n",
      "         [[-6.0250e-04, -3.9308e-04, -5.7454e-04],\n",
      "          [-6.3859e-04, -3.3605e-04, -4.6518e-04],\n",
      "          [-9.1989e-04, -6.9798e-04, -4.7309e-04]],\n",
      "\n",
      "         [[-3.9256e-04, -5.8037e-04, -6.9243e-04],\n",
      "          [-3.5258e-04, -2.9461e-04, -6.0142e-04],\n",
      "          [-3.0404e-04, -3.5996e-04, -2.4177e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.3274e-06,  4.0805e-04,  5.0030e-05],\n",
      "          [-7.9003e-05,  2.6110e-04,  1.7360e-04],\n",
      "          [ 7.8678e-05,  2.4019e-04, -4.1708e-06]],\n",
      "\n",
      "         [[ 1.1260e-04,  3.0016e-04, -1.0145e-04],\n",
      "          [-1.3710e-04, -6.9750e-05, -2.1493e-05],\n",
      "          [-3.0867e-04, -4.4325e-04, -1.5377e-05]],\n",
      "\n",
      "         [[ 8.8776e-04,  4.4695e-04,  2.3770e-04],\n",
      "          [ 9.3153e-04,  6.3459e-04,  3.7563e-04],\n",
      "          [ 7.6661e-04,  6.1934e-04,  3.5046e-04]]],\n",
      "\n",
      "\n",
      "        [[[-5.7107e-04, -4.8496e-04, -3.9078e-04],\n",
      "          [-9.5924e-04, -9.4091e-04, -1.1325e-03],\n",
      "          [-4.1909e-04, -4.2908e-04, -6.9294e-04]],\n",
      "\n",
      "         [[-1.3826e-04, -2.4836e-04, -5.6480e-04],\n",
      "          [ 1.2155e-04,  3.8536e-04,  2.1376e-04],\n",
      "          [-4.1215e-04,  3.2976e-04, -1.4319e-04]],\n",
      "\n",
      "         [[ 5.4627e-04,  4.2063e-04, -1.6823e-04],\n",
      "          [-1.9832e-04, -3.3485e-04,  1.9950e-04],\n",
      "          [-5.3910e-04, -3.6357e-04,  1.9413e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.6419e-04,  2.9743e-04,  6.6454e-04],\n",
      "          [ 8.6472e-04,  6.7698e-04,  6.5035e-04],\n",
      "          [ 7.9440e-04,  6.2562e-04,  7.2437e-04]],\n",
      "\n",
      "         [[-1.4408e-05, -2.6986e-04, -5.2605e-04],\n",
      "          [-1.0844e-04, -2.2451e-04, -1.5675e-04],\n",
      "          [-2.8047e-05, -5.3709e-04, -7.7106e-05]],\n",
      "\n",
      "         [[ 3.6988e-04, -3.2494e-04, -2.1907e-04],\n",
      "          [ 3.1614e-04, -2.8933e-04, -4.7322e-04],\n",
      "          [ 1.8451e-04, -9.9012e-05, -3.5724e-05]]],\n",
      "\n",
      "\n",
      "        [[[-4.4885e-05, -4.8120e-04, -5.3275e-04],\n",
      "          [-1.9739e-05,  9.3131e-05, -3.5119e-04],\n",
      "          [ 3.9650e-04,  3.8038e-04,  4.8294e-04]],\n",
      "\n",
      "         [[-8.9441e-04, -5.1312e-04, -2.8414e-04],\n",
      "          [-6.7774e-04, -2.1244e-04, -1.2747e-04],\n",
      "          [-2.6695e-04, -3.9970e-04, -5.2352e-04]],\n",
      "\n",
      "         [[-7.3357e-04, -2.1035e-05, -6.7208e-04],\n",
      "          [-2.2810e-04, -2.5435e-04, -1.4302e-05],\n",
      "          [-1.8655e-04,  1.0598e-05, -2.6044e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2344e-04,  1.1174e-04, -9.3224e-05],\n",
      "          [-1.0059e-04,  2.6580e-04, -1.0161e-04],\n",
      "          [-1.0126e-04,  4.8683e-05, -2.7251e-04]],\n",
      "\n",
      "         [[-2.8269e-04,  3.2375e-04,  1.4284e-04],\n",
      "          [-2.8018e-04,  1.1899e-04,  4.9128e-04],\n",
      "          [ 1.6554e-04,  2.9978e-04,  8.0672e-05]],\n",
      "\n",
      "         [[ 6.1164e-04,  8.4403e-04,  8.6693e-04],\n",
      "          [ 6.1727e-04,  9.8935e-04,  9.8458e-04],\n",
      "          [ 5.8406e-04,  7.6356e-04,  8.5884e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.0120e-04,  1.4926e-04, -4.1845e-04],\n",
      "          [-9.8435e-05,  5.3253e-04, -2.0937e-06],\n",
      "          [ 9.4106e-05,  6.5548e-04,  3.8798e-04]],\n",
      "\n",
      "         [[ 1.7671e-04, -3.5119e-04,  2.8714e-05],\n",
      "          [ 2.3466e-04, -2.0365e-04, -3.3590e-04],\n",
      "          [ 1.6735e-04,  2.1791e-04, -1.4084e-04]],\n",
      "\n",
      "         [[ 2.8638e-04, -2.8752e-04, -3.0046e-04],\n",
      "          [ 1.0641e-04,  3.4901e-05, -1.7784e-04],\n",
      "          [-2.4458e-04, -1.0712e-04, -1.0693e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0545e-04, -2.3831e-04, -1.0065e-04],\n",
      "          [-1.0019e-04, -1.9421e-04, -1.6710e-04],\n",
      "          [-8.7162e-05, -3.1015e-04,  1.4156e-04]],\n",
      "\n",
      "         [[ 7.5187e-04,  1.4908e-04, -1.1029e-04],\n",
      "          [ 1.4179e-04,  2.8720e-04, -2.9265e-04],\n",
      "          [ 4.0863e-05, -1.2571e-04, -1.0318e-04]],\n",
      "\n",
      "         [[ 1.1167e-04,  1.1726e-04, -1.2645e-04],\n",
      "          [ 3.5442e-04,  4.5857e-04, -3.1023e-05],\n",
      "          [ 1.7722e-04,  4.8318e-04,  3.4841e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4468e-04,  8.0711e-04,  8.1008e-04],\n",
      "          [-2.1087e-04,  3.3646e-04,  3.3233e-04],\n",
      "          [ 2.3245e-04,  2.4013e-04,  6.2800e-04]],\n",
      "\n",
      "         [[-3.5112e-05, -2.5272e-04, -3.3763e-04],\n",
      "          [ 2.9113e-04, -2.4426e-04,  9.4179e-05],\n",
      "          [ 2.3450e-04,  5.7120e-05, -6.6510e-06]],\n",
      "\n",
      "         [[ 3.3620e-04,  1.8891e-04,  2.0295e-04],\n",
      "          [ 3.1343e-04,  1.0306e-04,  3.1810e-04],\n",
      "          [ 3.9344e-04,  5.5095e-04,  3.3410e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.8546e-04,  2.6784e-04,  2.5419e-04],\n",
      "          [ 6.3363e-05,  4.6644e-05,  7.0048e-05],\n",
      "          [ 2.7465e-04,  3.4322e-04,  2.6747e-04]],\n",
      "\n",
      "         [[ 1.8071e-04, -1.9826e-04, -7.2200e-05],\n",
      "          [ 1.6092e-04, -1.5336e-04, -1.9339e-04],\n",
      "          [ 3.6207e-04, -1.0137e-05,  8.5725e-05]],\n",
      "\n",
      "         [[ 1.8571e-04, -7.3573e-06, -2.1198e-04],\n",
      "          [ 1.9123e-04,  1.2916e-04, -5.9314e-05],\n",
      "          [ 2.8621e-04,  2.8899e-04,  1.2503e-04]]],\n",
      "\n",
      "\n",
      "        [[[-9.6220e-05,  1.1416e-04, -1.6325e-04],\n",
      "          [-4.9758e-04, -1.4538e-04, -5.2849e-04],\n",
      "          [-4.0234e-04, -9.1198e-05, -1.3840e-04]],\n",
      "\n",
      "         [[-6.5770e-06, -8.8808e-05,  7.4646e-05],\n",
      "          [-3.0395e-04,  3.7182e-04,  1.2540e-04],\n",
      "          [ 1.6349e-04,  9.0294e-05,  9.8582e-05]],\n",
      "\n",
      "         [[ 2.6852e-04,  1.7149e-04,  5.5912e-05],\n",
      "          [-1.2515e-05,  8.6530e-05,  3.7191e-04],\n",
      "          [ 7.1471e-04, -1.7618e-04,  1.7114e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9439e-04,  1.5653e-04,  4.1415e-04],\n",
      "          [ 2.1185e-04,  4.5227e-05,  1.3260e-04],\n",
      "          [ 1.8321e-04,  1.4437e-05, -1.8866e-04]],\n",
      "\n",
      "         [[ 1.3977e-04, -2.0881e-04, -4.2230e-04],\n",
      "          [-4.0684e-05, -2.0267e-04, -1.1474e-04],\n",
      "          [ 3.5417e-04, -2.8956e-04, -4.1446e-04]],\n",
      "\n",
      "         [[-6.7187e-05, -7.7625e-05, -1.7886e-04],\n",
      "          [-3.2606e-04, -5.2293e-04, -2.1666e-04],\n",
      "          [-5.0503e-04, -6.3208e-04, -2.9566e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-3.5906e-04,  6.3504e-04, -1.2853e-03,  6.0976e-04,  3.2086e-04,\n",
      "        -1.3819e-03,  6.0652e-04,  6.2065e-04, -5.0492e-04, -2.4111e-04,\n",
      "        -2.4346e-04, -4.4932e-04,  2.8472e-04,  1.1644e-03,  3.9135e-04,\n",
      "         5.3769e-04, -6.8305e-04,  6.4240e-04, -1.7558e-05, -9.8168e-05,\n",
      "         3.0562e-04, -2.2512e-04,  4.3681e-04, -1.8166e-03, -5.8786e-04,\n",
      "        -4.1728e-06, -4.0329e-04, -1.1221e-03,  1.4074e-03,  9.8928e-04,\n",
      "         1.7537e-04,  1.0219e-04, -1.5927e-03,  4.5920e-04,  1.1055e-03,\n",
      "        -6.5640e-04, -4.7451e-04,  4.7789e-04, -5.8205e-04, -5.8757e-04,\n",
      "         4.4047e-04,  3.0632e-04,  7.2099e-04, -7.0838e-04, -1.0501e-03,\n",
      "         3.1359e-04,  4.8783e-04,  4.8251e-04,  1.2051e-03, -6.0432e-05,\n",
      "        -6.4430e-05, -9.8923e-05, -1.8177e-03, -3.1639e-04,  5.5863e-04,\n",
      "         1.3104e-04, -8.1192e-05, -3.3809e-04,  9.7162e-04, -2.5291e-04,\n",
      "         2.9986e-03, -5.9817e-04,  2.3293e-04, -1.3847e-04, -5.2016e-04,\n",
      "         1.1394e-03,  6.3605e-04,  1.3569e-04, -4.0756e-04,  2.5749e-04,\n",
      "        -1.1207e-04, -2.3906e-04,  6.7851e-04,  2.8595e-05,  1.3055e-03,\n",
      "         1.0557e-04,  6.1889e-04,  5.7307e-04,  7.3410e-04,  5.4472e-04,\n",
      "         2.4921e-04, -5.2187e-04,  8.8880e-04,  5.3856e-04,  1.6721e-03,\n",
      "         1.5975e-03, -9.2139e-04, -6.0994e-04,  7.2661e-04,  6.9252e-04,\n",
      "         2.4148e-03, -4.8616e-04,  4.5401e-05,  9.0416e-05, -1.4420e-03,\n",
      "        -1.2388e-03,  2.6851e-04, -5.5916e-05, -1.9259e-04,  7.7856e-05,\n",
      "         3.7176e-04,  5.3569e-04, -3.9381e-04, -2.8946e-04, -5.2964e-04,\n",
      "        -4.2467e-04, -1.3996e-03, -1.8156e-04, -5.3896e-04,  1.0227e-03,\n",
      "        -1.1178e-03, -3.3594e-04,  7.1713e-04, -1.0648e-03, -8.1393e-04,\n",
      "        -3.6539e-04,  9.3711e-05, -2.8359e-04, -3.9914e-04,  1.4156e-03,\n",
      "         3.4000e-04,  7.6031e-04, -1.0059e-03,  4.7710e-04, -1.6249e-04,\n",
      "         7.9495e-04,  1.7977e-03,  2.3283e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-1.1440e-04,  2.4969e-03, -1.1457e-03,  1.0975e-03, -3.9937e-04,\n",
      "        -2.4161e-03,  9.6750e-04,  7.4990e-04,  4.9819e-04, -4.6844e-04,\n",
      "        -2.9116e-04,  1.9177e-04,  5.5023e-04,  8.8823e-04,  1.2800e-03,\n",
      "         6.9669e-04, -5.3460e-04,  6.8961e-04,  2.8718e-05,  5.7741e-05,\n",
      "         1.0277e-03, -5.8345e-04,  8.6869e-04, -1.6182e-03, -5.8428e-04,\n",
      "        -1.8652e-04, -5.9423e-04, -2.8904e-03,  5.2535e-04,  1.3216e-03,\n",
      "        -3.9883e-04,  4.3766e-04, -1.5999e-03,  5.1769e-04, -3.3156e-05,\n",
      "        -1.2692e-04, -3.0304e-04, -1.5005e-03, -1.0022e-03, -1.5198e-04,\n",
      "         2.1859e-04, -8.8634e-05,  1.5449e-03, -7.9084e-04, -1.5047e-03,\n",
      "         1.2769e-03,  8.6683e-04,  3.0395e-04,  6.5917e-04, -5.7179e-04,\n",
      "         5.6120e-04, -6.8056e-05, -1.9919e-03, -2.7160e-04,  1.0031e-03,\n",
      "        -1.0730e-04,  4.8014e-04, -4.9825e-05,  4.2921e-04, -4.9380e-04,\n",
      "         5.1302e-03, -1.8149e-04, -1.1731e-03,  1.3502e-04, -1.0594e-04,\n",
      "         8.0831e-04,  1.0770e-05, -9.4321e-04,  5.7181e-04,  4.3493e-04,\n",
      "        -3.1482e-04, -5.1635e-04,  9.5546e-04,  1.0731e-03,  1.3485e-03,\n",
      "        -4.2233e-04,  8.8491e-04,  2.9340e-04,  5.3997e-04, -3.8063e-04,\n",
      "        -2.7896e-04,  7.3598e-05,  1.4882e-03,  4.0197e-04,  1.9735e-04,\n",
      "         2.5048e-03, -1.6159e-03, -1.0292e-03,  3.0933e-04,  9.1559e-04,\n",
      "         2.2409e-03, -9.7444e-05, -2.9811e-04, -4.1099e-04, -1.9838e-03,\n",
      "        -2.4773e-03,  7.0314e-04,  1.0531e-03,  1.8783e-05,  9.1309e-05,\n",
      "         1.0573e-03,  9.6904e-04, -1.0348e-03, -2.1978e-04, -3.2836e-04,\n",
      "        -6.5598e-04, -3.3771e-03,  2.2409e-04, -1.2146e-03, -3.6866e-05,\n",
      "        -7.7471e-04, -4.7005e-04,  1.2479e-03, -3.2850e-03, -8.4898e-04,\n",
      "        -2.4567e-03, -2.7393e-04,  7.7153e-04, -1.5325e-03,  1.5538e-03,\n",
      "         1.7645e-04,  3.6807e-04,  5.5603e-04,  3.8159e-04, -6.4432e-04,\n",
      "         8.9164e-04,  1.9803e-03, -3.7813e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.5818e-06,  6.9003e-07,  1.4954e-05, -2.5770e-06,  2.2499e-07,\n",
      "        -5.2190e-06,  1.4669e-05,  3.5707e-06,  5.9871e-06,  7.8576e-06,\n",
      "         2.2586e-06,  8.5187e-06,  4.3407e-06, -9.2927e-06, -2.5173e-06,\n",
      "         2.6697e-06,  2.8163e-06,  1.0631e-05, -6.0742e-07, -8.6829e-06,\n",
      "         2.3265e-06, -4.8669e-06, -2.1020e-06, -9.1709e-06, -9.1863e-06,\n",
      "        -1.4261e-05, -5.0590e-06, -2.9776e-06, -4.1102e-06, -7.3416e-06,\n",
      "        -5.8609e-06,  5.1562e-06, -1.1074e-05,  8.5053e-06, -1.0351e-05,\n",
      "         1.2303e-05, -6.4173e-06,  1.6817e-05,  1.1594e-05, -9.8496e-06,\n",
      "        -7.2978e-06,  2.5708e-06, -6.5265e-06, -1.1957e-06,  8.7703e-06,\n",
      "        -8.6540e-06, -3.0323e-06,  7.0137e-06,  1.7872e-05, -1.1028e-06,\n",
      "         8.2166e-06, -6.6537e-06,  6.5627e-06,  3.2491e-06,  1.0241e-06,\n",
      "        -6.8898e-06, -1.6842e-05,  3.6464e-06,  9.5761e-06,  6.9629e-06,\n",
      "         1.0244e-05,  6.7453e-06,  6.1995e-06,  1.0857e-05, -8.4974e-07,\n",
      "        -2.9949e-06, -7.0920e-06,  1.1113e-05, -1.5661e-06,  2.0052e-06,\n",
      "        -5.5181e-06, -3.1205e-06, -3.2019e-06, -5.7947e-06,  3.3004e-06,\n",
      "         3.2153e-06, -1.0222e-05,  3.0895e-06,  6.3870e-06, -1.0230e-05,\n",
      "         1.3933e-06, -1.0523e-05,  9.0713e-07, -6.8898e-07, -1.3667e-05,\n",
      "         1.8151e-06, -7.7060e-06,  3.5166e-06,  1.2915e-06,  7.3008e-06,\n",
      "         4.0037e-06, -8.1532e-06,  7.8093e-06,  5.4991e-06, -6.0142e-06,\n",
      "         9.0660e-06,  8.1095e-06,  1.5541e-06,  3.8488e-06,  6.1569e-06,\n",
      "        -7.2633e-06,  5.2390e-07,  5.5160e-06,  1.7048e-06, -1.1247e-06,\n",
      "         9.2525e-06, -4.0364e-06, -5.0590e-06, -7.9028e-06,  4.7598e-06,\n",
      "        -4.4102e-06,  5.3164e-06, -6.3441e-06,  8.8417e-06, -2.5089e-06,\n",
      "        -5.4230e-06, -1.4896e-05,  2.3256e-06, -2.5324e-06, -8.8498e-07,\n",
      "         7.9591e-06, -1.3603e-05, -1.5959e-06,  6.0121e-07,  4.2921e-06,\n",
      "        -1.3317e-06,  2.1293e-05,  6.5279e-06], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 2.2713e-05,  2.2283e-05,  2.3640e-05],\n",
      "          [ 1.7980e-05,  2.1677e-05,  2.5299e-05],\n",
      "          [ 2.0391e-05,  2.1684e-05,  2.5789e-05]],\n",
      "\n",
      "         [[ 8.2632e-06,  1.0886e-05,  4.4581e-06],\n",
      "          [ 8.1552e-06,  6.0798e-06,  7.8559e-06],\n",
      "          [ 1.3780e-05,  9.4498e-06,  1.2339e-05]],\n",
      "\n",
      "         [[ 2.3942e-05,  2.1391e-05,  1.6712e-05],\n",
      "          [ 1.5196e-05,  2.0255e-05,  1.9910e-05],\n",
      "          [ 1.7632e-05,  1.9566e-05,  1.8998e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8248e-05,  1.7473e-05,  1.8140e-05],\n",
      "          [ 1.3972e-05,  2.0648e-05,  1.9134e-05],\n",
      "          [ 1.6327e-05,  2.0019e-05,  1.4998e-05]],\n",
      "\n",
      "         [[ 1.6497e-05,  1.4904e-05,  1.5285e-05],\n",
      "          [ 1.6435e-05,  1.4318e-05,  1.4086e-05],\n",
      "          [ 1.8084e-05,  1.5030e-05,  1.2248e-05]],\n",
      "\n",
      "         [[ 1.0533e-05,  1.0594e-05,  1.1583e-05],\n",
      "          [ 9.6216e-06,  1.0262e-05,  1.0248e-05],\n",
      "          [ 9.3638e-06,  9.7582e-06,  8.6318e-06]]],\n",
      "\n",
      "\n",
      "        [[[-1.3234e-05, -1.7224e-05, -1.9772e-05],\n",
      "          [-1.4434e-05, -1.4481e-05, -1.5606e-05],\n",
      "          [-1.1201e-05, -8.9666e-06, -3.1729e-06]],\n",
      "\n",
      "         [[-8.8344e-06, -5.5327e-06, -2.1975e-06],\n",
      "          [-1.8916e-06, -5.7488e-06, -8.1319e-06],\n",
      "          [ 4.3077e-07,  1.2642e-06, -1.9355e-06]],\n",
      "\n",
      "         [[-7.9383e-06, -7.2938e-06, -1.2197e-05],\n",
      "          [-1.2816e-05, -1.3730e-05, -1.0856e-05],\n",
      "          [-4.1863e-06, -7.1103e-06, -2.9358e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4930e-05, -1.4294e-05, -1.5366e-05],\n",
      "          [-1.4273e-05, -1.4921e-05, -1.0083e-05],\n",
      "          [-1.5682e-05, -1.4268e-05, -9.3246e-06]],\n",
      "\n",
      "         [[-7.8017e-06, -6.3885e-06, -4.6486e-06],\n",
      "          [-5.3710e-06, -3.7845e-06, -4.4066e-06],\n",
      "          [-4.8286e-06, -2.2424e-06, -3.2778e-06]],\n",
      "\n",
      "         [[-7.9081e-06, -7.0870e-06, -8.0774e-06],\n",
      "          [-8.7120e-06, -8.4932e-06, -6.1064e-06],\n",
      "          [-7.3859e-06, -7.5723e-06, -5.2856e-06]]],\n",
      "\n",
      "\n",
      "        [[[-3.2209e-06, -4.0119e-06, -2.6350e-06],\n",
      "          [-2.4927e-06, -7.9814e-07, -2.2633e-06],\n",
      "          [-2.3987e-06, -1.8667e-06, -3.0447e-06]],\n",
      "\n",
      "         [[-6.5627e-06, -5.0471e-06, -3.3104e-06],\n",
      "          [-2.6391e-06, -1.3577e-06, -4.8552e-07],\n",
      "          [-4.6811e-06, -2.6680e-06, -1.6175e-06]],\n",
      "\n",
      "         [[-2.5398e-06, -3.4353e-06, -4.9708e-06],\n",
      "          [-1.0907e-06, -1.8685e-06, -4.2818e-06],\n",
      "          [-3.4034e-06, -3.7696e-06, -5.4528e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4305e-06, -1.5101e-07, -2.1997e-06],\n",
      "          [-1.4162e-06,  4.6630e-07, -2.1628e-06],\n",
      "          [-1.3203e-06, -1.6877e-06, -2.2445e-06]],\n",
      "\n",
      "         [[-7.8975e-06, -7.4978e-06, -8.3180e-06],\n",
      "          [-6.0839e-06, -5.7727e-06, -7.2071e-06],\n",
      "          [-5.5016e-06, -5.5769e-06, -5.0591e-06]],\n",
      "\n",
      "         [[-5.4579e-06, -5.2263e-06, -6.5689e-06],\n",
      "          [-6.3223e-06, -6.5746e-06, -7.1880e-06],\n",
      "          [-6.1453e-06, -6.5076e-06, -6.2726e-06]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.2704e-05,  2.7691e-05,  2.7814e-05],\n",
      "          [ 3.0262e-05,  2.0496e-05,  2.0604e-05],\n",
      "          [ 2.7051e-05,  2.4272e-05,  2.4776e-05]],\n",
      "\n",
      "         [[ 2.0845e-05,  2.4262e-05,  2.2570e-05],\n",
      "          [ 2.0718e-05,  2.5365e-05,  2.3391e-05],\n",
      "          [ 1.4680e-05,  1.0747e-05,  8.6014e-06]],\n",
      "\n",
      "         [[ 2.5679e-05,  3.1219e-05,  3.6124e-05],\n",
      "          [ 2.7599e-05,  2.4197e-05,  2.3699e-05],\n",
      "          [ 2.4477e-05,  2.4366e-05,  2.9104e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4792e-05,  1.0568e-05,  1.2782e-05],\n",
      "          [ 1.2127e-05,  6.4018e-06,  1.2183e-05],\n",
      "          [ 1.7086e-05,  1.3361e-05,  2.0023e-05]],\n",
      "\n",
      "         [[ 1.4176e-05,  1.4813e-05,  1.7037e-05],\n",
      "          [ 1.3182e-05,  1.3096e-05,  1.4934e-05],\n",
      "          [ 8.3027e-06,  7.2806e-06,  9.4215e-06]],\n",
      "\n",
      "         [[ 1.7590e-05,  1.5438e-05,  1.7400e-05],\n",
      "          [ 1.6757e-05,  1.4773e-05,  1.5700e-05],\n",
      "          [ 1.7531e-05,  1.8362e-05,  2.1220e-05]]],\n",
      "\n",
      "\n",
      "        [[[-2.0700e-06, -5.7665e-06, -4.0389e-06],\n",
      "          [-2.1750e-06, -3.7698e-06, -2.0623e-06],\n",
      "          [-1.5744e-06, -2.6474e-06, -1.3644e-06]],\n",
      "\n",
      "         [[-8.2639e-06, -9.9238e-06, -8.4056e-06],\n",
      "          [-4.0007e-06, -4.4123e-06, -4.3415e-06],\n",
      "          [-2.0240e-06, -2.6790e-06, -2.6233e-06]],\n",
      "\n",
      "         [[-2.5020e-06, -6.3160e-06, -4.4221e-06],\n",
      "          [-9.4063e-07, -3.1911e-06, -4.8133e-06],\n",
      "          [-1.2228e-06, -7.7529e-07, -1.3508e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.5065e-07,  3.1600e-07,  7.6745e-07],\n",
      "          [-1.9859e-07, -3.7541e-07,  5.7717e-07],\n",
      "          [ 1.3593e-06,  1.9353e-06,  1.8408e-06]],\n",
      "\n",
      "         [[-1.1424e-06, -7.6732e-07, -1.4855e-07],\n",
      "          [ 2.2625e-07, -2.9470e-07,  5.0053e-07],\n",
      "          [ 1.0718e-06,  7.2440e-07,  1.5074e-06]],\n",
      "\n",
      "         [[-4.8390e-07, -9.9984e-07, -6.6453e-07],\n",
      "          [-1.4832e-06, -1.8333e-06, -1.8549e-06],\n",
      "          [-1.2017e-06, -1.1505e-06, -1.0978e-06]]],\n",
      "\n",
      "\n",
      "        [[[-3.1728e-05, -2.6441e-05, -2.4627e-05],\n",
      "          [-3.2700e-05, -2.9256e-05, -2.6944e-05],\n",
      "          [-2.6331e-05, -3.0914e-05, -2.9234e-05]],\n",
      "\n",
      "         [[-2.3873e-05, -2.2938e-05, -1.2741e-05],\n",
      "          [-1.9809e-05, -1.6213e-05, -1.2909e-05],\n",
      "          [-1.9709e-05, -1.6089e-05, -1.1655e-05]],\n",
      "\n",
      "         [[-2.9407e-05, -2.0490e-05, -1.9237e-05],\n",
      "          [-3.3496e-05, -2.4235e-05, -2.2351e-05],\n",
      "          [-2.5813e-05, -2.9376e-05, -1.9801e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9033e-05, -1.7756e-05, -1.2022e-05],\n",
      "          [-1.7644e-05, -1.7420e-05, -1.2741e-05],\n",
      "          [-1.9988e-05, -2.2235e-05, -1.5811e-05]],\n",
      "\n",
      "         [[-1.9646e-05, -1.6275e-05, -1.6038e-05],\n",
      "          [-1.8468e-05, -1.4449e-05, -1.4556e-05],\n",
      "          [-1.6991e-05, -1.4940e-05, -1.1805e-05]],\n",
      "\n",
      "         [[-1.8077e-05, -1.5772e-05, -1.5289e-05],\n",
      "          [-1.7919e-05, -1.8334e-05, -1.5652e-05],\n",
      "          [-1.8079e-05, -1.9684e-05, -1.7671e-05]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 3.1832e-12,  9.0949e-13, -9.6634e-13,  2.2737e-13,  1.3642e-12,\n",
      "        -1.5916e-12,  9.0949e-13, -1.3642e-12,  0.0000e+00, -1.7053e-12,\n",
      "        -3.4106e-13, -1.5916e-12,  2.3874e-12, -1.1369e-13, -4.5475e-13,\n",
      "         2.1600e-12,  1.9327e-12,  3.5811e-12, -1.1369e-13, -1.2506e-12,\n",
      "        -9.0949e-13,  1.2506e-12,  1.0232e-12, -4.5475e-13, -5.4570e-12,\n",
      "         3.1832e-12,  2.1600e-12,  1.1369e-12,  2.7285e-12, -1.3642e-12,\n",
      "        -1.0800e-12, -2.7853e-12,  1.9327e-12, -2.2737e-13, -2.2737e-12,\n",
      "         1.0232e-12, -1.5916e-12, -1.1369e-12, -1.1369e-12, -2.5011e-12,\n",
      "        -3.4106e-13,  9.0949e-13, -1.7621e-12,  2.2737e-12,  2.2737e-13,\n",
      "         9.0949e-13,  2.2737e-12,  1.1369e-13,  9.0949e-13, -2.5011e-12,\n",
      "        -1.1369e-13,  1.9895e-12,  1.9327e-12,  0.0000e+00, -9.0949e-13,\n",
      "        -2.2737e-13,  2.2737e-13,  1.1369e-12,  6.8212e-13,  9.0949e-13,\n",
      "        -1.3642e-12, -1.5916e-12, -4.5475e-13,  9.0949e-13, -4.2064e-12,\n",
      "        -8.2423e-13,  2.0464e-12, -1.0232e-12,  0.0000e+00,  2.0464e-12,\n",
      "         2.4443e-12,  1.5916e-12, -2.5011e-12, -6.8212e-13,  1.3074e-12,\n",
      "         2.2737e-12, -4.5475e-13,  2.9559e-12, -5.6843e-14, -1.5916e-12,\n",
      "         2.2737e-13, -1.3642e-12, -1.3642e-12,  1.2506e-12, -1.8190e-12,\n",
      "        -1.3642e-12, -9.0949e-13,  2.6148e-12,  1.9327e-12, -2.5011e-12,\n",
      "        -1.1369e-13,  4.5475e-13, -4.5475e-13, -1.1369e-13, -2.2737e-13,\n",
      "         0.0000e+00,  1.1369e-12,  2.2737e-13,  4.6612e-12, -2.1600e-12,\n",
      "         1.5916e-12,  2.5011e-12,  1.7621e-12,  9.9476e-13, -2.6148e-12,\n",
      "        -3.1264e-13,  0.0000e+00,  1.1369e-13,  4.5475e-13,  1.1369e-12,\n",
      "        -6.8212e-13, -2.3874e-12, -1.3642e-12,  3.4106e-13, -1.3642e-12,\n",
      "        -1.4211e-13,  6.8212e-13,  0.0000e+00,  2.2737e-13, -3.4106e-12,\n",
      "         2.2737e-12,  1.3642e-12, -1.5916e-12,  1.1937e-12,  2.2737e-13,\n",
      "         9.0949e-13,  5.6843e-13, -3.0695e-12], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 3.9680e-06, -2.2134e-06, -9.2152e-07],\n",
      "          [ 1.2769e-06, -8.0702e-06, -2.1302e-06],\n",
      "          [-2.2711e-06, -7.1509e-06, -5.0531e-06]],\n",
      "\n",
      "         [[-2.3161e-06, -1.2084e-05, -7.9676e-06],\n",
      "          [-4.2044e-06, -7.0483e-06,  3.8436e-07],\n",
      "          [-2.6200e-06, -7.5842e-06, -2.8996e-06]],\n",
      "\n",
      "         [[-5.4894e-07, -6.4995e-06, -4.3412e-06],\n",
      "          [-2.9181e-06, -8.1350e-06, -4.8310e-06],\n",
      "          [-3.0320e-06, -5.3032e-06,  1.1768e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7847e-06, -7.2767e-06, -7.7052e-06],\n",
      "          [-2.5712e-06, -5.7801e-06, -5.0112e-06],\n",
      "          [-1.4393e-06, -2.8778e-06, -6.9178e-06]],\n",
      "\n",
      "         [[-4.1171e-06, -8.5248e-06, -8.6816e-06],\n",
      "          [-6.3294e-06, -6.1645e-06, -2.6799e-06],\n",
      "          [-1.4322e-06, -2.5894e-06, -2.7322e-06]],\n",
      "\n",
      "         [[ 7.4429e-07, -6.1598e-06,  4.0265e-06],\n",
      "          [ 5.1613e-06, -3.3709e-06,  3.9154e-06],\n",
      "          [ 4.6435e-06, -4.5072e-07,  5.6744e-06]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1775e-05,  1.1895e-05,  1.2970e-05],\n",
      "          [ 1.2014e-05,  1.7487e-05,  1.1651e-05],\n",
      "          [ 1.5277e-05,  1.4123e-05,  1.1293e-05]],\n",
      "\n",
      "         [[-2.9488e-06, -3.2192e-06, -4.2467e-06],\n",
      "          [-3.6000e-06,  2.0191e-06,  2.6095e-06],\n",
      "          [-5.5102e-06,  1.9573e-07, -1.3311e-06]],\n",
      "\n",
      "         [[-5.1638e-06, -5.2245e-06, -8.0053e-06],\n",
      "          [-5.0049e-06, -2.9754e-06, -6.9025e-06],\n",
      "          [-4.5075e-06, -2.9944e-06, -5.9972e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1611e-07,  1.6508e-06,  7.7132e-07],\n",
      "          [ 2.2112e-06,  3.4445e-06, -2.9501e-06],\n",
      "          [ 3.7938e-07, -3.1061e-06, -4.2059e-06]],\n",
      "\n",
      "         [[ 1.3376e-06,  2.7843e-06, -1.8767e-06],\n",
      "          [ 2.8033e-06,  2.7587e-06, -8.3708e-07],\n",
      "          [ 3.6176e-06,  2.5390e-06, -1.4785e-07]],\n",
      "\n",
      "         [[-6.7581e-06,  3.3539e-06, -1.3401e-07],\n",
      "          [-2.5385e-06,  4.1441e-06,  8.9659e-07],\n",
      "          [-3.9875e-06,  6.5638e-07, -1.5512e-06]]],\n",
      "\n",
      "\n",
      "        [[[-3.3180e-08,  1.9520e-06,  2.2391e-06],\n",
      "          [ 1.5414e-06,  3.1686e-06,  2.6401e-06],\n",
      "          [ 4.0099e-06,  1.8919e-06,  4.9237e-06]],\n",
      "\n",
      "         [[ 2.1686e-06,  3.5232e-06,  3.5710e-06],\n",
      "          [ 2.4084e-06,  4.4161e-06,  4.3075e-06],\n",
      "          [ 5.0068e-06,  6.5360e-06,  4.2891e-06]],\n",
      "\n",
      "         [[ 3.6616e-06,  3.8580e-06,  3.9894e-06],\n",
      "          [ 3.2680e-06,  2.7361e-06,  2.1849e-06],\n",
      "          [ 3.7154e-06,  4.4768e-06,  3.3593e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7529e-06, -3.9286e-06, -3.2237e-06],\n",
      "          [-1.6242e-06, -1.3728e-06, -2.5076e-06],\n",
      "          [-3.2444e-08,  1.1484e-06,  3.2297e-07]],\n",
      "\n",
      "         [[ 2.4078e-06,  1.1036e-06,  1.3726e-06],\n",
      "          [ 3.2594e-06,  3.3848e-06,  2.2392e-06],\n",
      "          [ 4.9119e-06,  4.4741e-06,  3.8539e-06]],\n",
      "\n",
      "         [[ 1.7475e-06,  4.0572e-06,  5.9737e-06],\n",
      "          [ 1.4769e-06,  3.6353e-06,  6.8254e-06],\n",
      "          [ 1.0616e-06,  1.3722e-06,  4.5694e-07]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.5339e-06,  5.7203e-06,  5.2450e-06],\n",
      "          [ 5.6665e-06,  8.1014e-06,  9.0421e-06],\n",
      "          [ 4.7761e-06,  9.7301e-06,  6.4266e-06]],\n",
      "\n",
      "         [[ 1.0214e-07,  2.7718e-06,  1.5463e-06],\n",
      "          [ 5.9035e-06,  5.8717e-06,  5.0599e-06],\n",
      "          [ 3.1731e-06,  3.0180e-06,  1.2363e-06]],\n",
      "\n",
      "         [[-2.8247e-07,  1.4376e-06,  2.1098e-06],\n",
      "          [ 2.6415e-06,  4.9408e-06,  4.8918e-06],\n",
      "          [ 2.7488e-06,  6.2168e-06,  6.0016e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.1568e-06, -2.0170e-07,  2.9527e-06],\n",
      "          [ 3.1948e-06, -3.0596e-06,  1.2663e-06],\n",
      "          [ 3.4963e-06, -1.6309e-06,  1.5089e-06]],\n",
      "\n",
      "         [[ 2.8305e-06,  3.8421e-06,  3.0531e-06],\n",
      "          [ 6.2848e-06,  7.7199e-06,  6.4593e-06],\n",
      "          [ 7.2316e-06,  6.9923e-06,  4.7898e-06]],\n",
      "\n",
      "         [[-7.4496e-06, -3.1714e-06,  3.4061e-06],\n",
      "          [-3.4409e-06, -2.1202e-06,  4.1468e-06],\n",
      "          [-2.2745e-06, -6.9962e-07,  3.5156e-06]]],\n",
      "\n",
      "\n",
      "        [[[-9.5957e-07, -9.9594e-07, -2.3584e-06],\n",
      "          [-3.0490e-06, -3.9894e-07, -1.7835e-06],\n",
      "          [ 1.0783e-06, -1.2904e-06, -6.1191e-06]],\n",
      "\n",
      "         [[ 5.9947e-06,  1.2132e-06,  5.6387e-06],\n",
      "          [ 9.1044e-06,  2.6288e-06,  3.5495e-06],\n",
      "          [ 6.5074e-06,  3.9574e-06,  5.6399e-06]],\n",
      "\n",
      "         [[ 1.8676e-06, -1.6905e-06,  2.6421e-07],\n",
      "          [ 9.5259e-07, -3.3953e-06, -2.5608e-06],\n",
      "          [ 2.9912e-06, -9.3379e-07, -1.9000e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3080e-06, -3.3307e-06,  1.8791e-06],\n",
      "          [-2.3567e-06, -4.0579e-06, -1.3683e-06],\n",
      "          [-2.7689e-07, -7.2343e-06,  1.3329e-06]],\n",
      "\n",
      "         [[-8.2434e-07, -1.8441e-06, -8.6759e-07],\n",
      "          [-7.3837e-09, -4.0142e-06, -4.4522e-06],\n",
      "          [ 8.7032e-07, -4.8477e-06, -3.3828e-06]],\n",
      "\n",
      "         [[ 8.8087e-07,  3.0494e-07,  4.9615e-07],\n",
      "          [-2.1531e-06, -4.8713e-06, -2.5002e-06],\n",
      "          [ 1.6274e-06, -3.5628e-06, -5.9728e-07]]],\n",
      "\n",
      "\n",
      "        [[[-2.7227e-06, -5.0922e-06, -4.7652e-06],\n",
      "          [-2.8022e-06, -3.1778e-06, -1.9109e-06],\n",
      "          [-2.7567e-06, -1.4118e-06,  2.3128e-07]],\n",
      "\n",
      "         [[-3.5512e-06, -1.1946e-06, -2.6336e-06],\n",
      "          [-2.4248e-06, -1.2760e-06, -4.1761e-06],\n",
      "          [-3.2575e-06, -1.2840e-06, -1.7259e-06]],\n",
      "\n",
      "         [[-3.1727e-06, -7.9512e-07, -1.3710e-06],\n",
      "          [-3.1914e-06, -9.4675e-07, -1.3062e-06],\n",
      "          [-2.3608e-06, -5.6565e-07, -7.0382e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8539e-08,  7.8004e-07, -6.8307e-07],\n",
      "          [-4.7262e-07,  2.2473e-06, -9.0647e-07],\n",
      "          [-4.0366e-08,  2.6010e-06,  2.8940e-07]],\n",
      "\n",
      "         [[-1.1264e-06, -1.4724e-06, -9.5067e-07],\n",
      "          [-4.7731e-07, -1.0725e-06, -2.3615e-06],\n",
      "          [ 6.0805e-08,  1.2112e-06,  3.2422e-07]],\n",
      "\n",
      "         [[ 2.0561e-07, -6.6782e-07, -3.7367e-06],\n",
      "          [ 1.6150e-07,  1.9404e-07, -1.1968e-06],\n",
      "          [-1.3280e-06,  2.1045e-06, -1.3944e-07]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-3.7197e-07, -6.1513e-07, -4.2397e-07],\n",
      "          [-1.9137e-07, -3.5915e-07, -3.8855e-07],\n",
      "          [ 7.0832e-07, -7.7618e-07, -1.0595e-06]],\n",
      "\n",
      "         [[-7.6855e-07,  7.7772e-07,  1.0367e-06],\n",
      "          [-1.6425e-07,  8.7682e-07,  8.2601e-07],\n",
      "          [ 3.2572e-07, -6.8654e-07,  7.4551e-07]],\n",
      "\n",
      "         [[ 9.5959e-07,  2.0012e-07, -6.0525e-07],\n",
      "          [-2.7749e-07,  9.9354e-07, -2.9854e-07],\n",
      "          [ 1.7044e-07,  4.2053e-07,  1.7627e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6853e-06, -4.9911e-07, -2.2739e-07],\n",
      "          [-1.6604e-06, -5.2002e-07, -3.7741e-07],\n",
      "          [-7.7185e-07, -1.0803e-06, -1.1104e-06]],\n",
      "\n",
      "         [[-1.7335e-08,  7.0954e-08,  3.6866e-07],\n",
      "          [-2.1943e-07,  2.5566e-07,  3.3748e-07],\n",
      "          [ 4.4701e-07, -4.3406e-07, -7.1854e-08]],\n",
      "\n",
      "         [[-8.4732e-07, -8.1722e-07, -6.1934e-07],\n",
      "          [ 9.2076e-08,  2.3174e-07,  3.3246e-08],\n",
      "          [-5.2697e-07,  4.6721e-07,  3.5141e-07]]],\n",
      "\n",
      "\n",
      "        [[[-1.6552e-07, -3.7155e-08, -1.2529e-07],\n",
      "          [ 2.0993e-07,  1.2891e-07, -5.4402e-08],\n",
      "          [-9.8873e-08, -2.7837e-07, -7.9270e-08]],\n",
      "\n",
      "         [[ 3.8162e-07,  2.1855e-06, -2.0015e-07],\n",
      "          [-2.6089e-07, -2.2000e-07, -3.3982e-07],\n",
      "          [-4.5097e-09, -1.0731e-06, -3.4720e-07]],\n",
      "\n",
      "         [[ 1.4123e-07,  1.1821e-06,  1.2412e-06],\n",
      "          [ 4.6609e-07, -4.8585e-07, -9.8729e-08],\n",
      "          [-3.4781e-07, -3.3961e-07, -4.3968e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7085e-07, -2.5457e-07,  1.0680e-06],\n",
      "          [ 1.8799e-07, -3.8311e-07,  4.7264e-07],\n",
      "          [-1.7543e-07,  2.8925e-07,  8.5881e-07]],\n",
      "\n",
      "         [[ 1.5994e-07, -3.2335e-08,  2.7444e-07],\n",
      "          [-1.5563e-08, -4.9703e-07, -2.1246e-07],\n",
      "          [-4.2565e-07, -2.7494e-07, -2.8655e-08]],\n",
      "\n",
      "         [[-1.0252e-07,  6.0113e-08,  7.8434e-07],\n",
      "          [-2.6739e-08, -6.7686e-07, -3.8945e-08],\n",
      "          [-3.6831e-07, -1.1082e-07, -5.6559e-07]]],\n",
      "\n",
      "\n",
      "        [[[-5.0259e-08,  1.1615e-06,  1.1097e-07],\n",
      "          [-8.0885e-07, -5.9712e-07,  9.9384e-07],\n",
      "          [-5.5555e-07, -3.4681e-07,  4.2075e-07]],\n",
      "\n",
      "         [[-1.8303e-06, -5.6235e-07, -1.6251e-06],\n",
      "          [-1.6357e-06, -1.2390e-06, -7.8665e-07],\n",
      "          [-1.1717e-06, -4.1114e-07, -3.5114e-07]],\n",
      "\n",
      "         [[ 4.7119e-07, -6.5984e-07, -1.1276e-06],\n",
      "          [ 2.2961e-06,  3.1692e-06,  2.9392e-06],\n",
      "          [ 1.1378e-06,  1.7902e-06,  1.6511e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9953e-06, -1.9815e-06, -1.3181e-06],\n",
      "          [-1.4483e-06, -1.3075e-06, -8.9408e-07],\n",
      "          [-2.8307e-06, -7.7884e-07, -1.3805e-07]],\n",
      "\n",
      "         [[-9.8387e-07,  1.7775e-07, -3.4654e-07],\n",
      "          [-7.9047e-07,  6.2418e-07,  3.0944e-07],\n",
      "          [-6.5023e-07,  7.8126e-08, -3.7068e-08]],\n",
      "\n",
      "         [[-7.9282e-07, -5.9649e-09, -3.7151e-07],\n",
      "          [-3.9813e-07,  5.5581e-07,  1.4641e-07],\n",
      "          [ 4.9022e-07, -6.6919e-08,  2.4469e-07]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.8080e-07, -7.7492e-08, -1.7038e-08],\n",
      "          [ 1.6160e-07, -3.4693e-08,  3.5884e-08],\n",
      "          [ 1.5701e-07,  9.3676e-08, -2.1848e-07]],\n",
      "\n",
      "         [[-1.4732e-07, -8.8142e-07,  6.0339e-07],\n",
      "          [ 1.3121e-07, -3.3360e-07, -3.2972e-07],\n",
      "          [-7.6643e-07,  5.1646e-07, -3.5643e-07]],\n",
      "\n",
      "         [[ 2.7296e-07,  1.1035e-06,  1.3543e-06],\n",
      "          [ 5.0382e-07,  1.6975e-06,  3.6395e-07],\n",
      "          [-8.9619e-08,  1.5027e-06,  8.7143e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.8809e-07, -1.0017e-06, -3.4309e-07],\n",
      "          [-5.9321e-07, -1.2823e-06, -5.7539e-07],\n",
      "          [ 3.8982e-07, -8.2001e-08,  2.8497e-07]],\n",
      "\n",
      "         [[-6.8839e-07, -7.4094e-08, -2.3062e-07],\n",
      "          [-5.5295e-07,  4.0469e-07, -1.7794e-07],\n",
      "          [-4.3497e-07, -2.6323e-07,  4.1026e-07]],\n",
      "\n",
      "         [[-4.7629e-07, -4.5087e-07,  2.6630e-07],\n",
      "          [-6.9957e-07, -3.2609e-07,  8.8815e-07],\n",
      "          [-6.4905e-07, -4.9230e-07,  2.7347e-07]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6626e-07,  5.9560e-08,  2.1525e-07],\n",
      "          [ 3.1290e-07,  2.6420e-07,  1.5460e-07],\n",
      "          [ 3.9470e-08,  6.5927e-08,  3.6632e-07]],\n",
      "\n",
      "         [[ 1.2904e-07,  1.1787e-06,  1.0790e-07],\n",
      "          [-7.1627e-08,  8.9914e-07,  2.3632e-07],\n",
      "          [-4.3892e-08,  7.5904e-07, -7.6365e-08]],\n",
      "\n",
      "         [[ 4.5681e-07,  6.5401e-07,  2.3093e-07],\n",
      "          [ 4.7319e-07,  7.3033e-07,  5.6325e-07],\n",
      "          [ 1.5397e-07,  4.7763e-07,  4.3219e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.1412e-09, -9.9290e-08,  4.4978e-07],\n",
      "          [-1.7508e-07, -5.9738e-07, -3.1551e-07],\n",
      "          [ 1.3344e-07,  1.6581e-07,  3.5782e-07]],\n",
      "\n",
      "         [[ 2.0107e-07,  6.6012e-08, -2.7556e-08],\n",
      "          [ 2.2725e-07,  3.3097e-07, -7.9100e-08],\n",
      "          [ 5.3998e-08,  1.7296e-07,  1.0748e-07]],\n",
      "\n",
      "         [[ 1.8136e-07, -4.2725e-07, -2.6101e-07],\n",
      "          [-2.6716e-09, -4.6433e-07,  4.2798e-08],\n",
      "          [ 1.2860e-07, -3.3933e-08, -1.4700e-08]]],\n",
      "\n",
      "\n",
      "        [[[-1.0143e-06, -6.8379e-07, -7.7622e-07],\n",
      "          [-7.5480e-07,  2.2608e-07, -4.5383e-07],\n",
      "          [-9.7365e-07,  2.0021e-07, -3.2348e-07]],\n",
      "\n",
      "         [[-4.4874e-07,  4.1359e-07, -1.1279e-07],\n",
      "          [ 5.1286e-07,  1.0216e-06, -3.1516e-07],\n",
      "          [-4.1999e-07,  1.6941e-06,  3.8911e-07]],\n",
      "\n",
      "         [[-6.6179e-07,  1.1038e-07,  2.2895e-07],\n",
      "          [-1.8449e-06,  3.7396e-07, -2.4361e-07],\n",
      "          [-1.1309e-06, -7.5336e-07, -1.5693e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.8554e-07,  1.5685e-06,  2.6119e-07],\n",
      "          [ 1.0857e-07,  1.2384e-06,  1.1035e-06],\n",
      "          [ 3.4495e-07,  4.0333e-07, -5.5186e-07]],\n",
      "\n",
      "         [[-1.4295e-07,  1.2578e-07,  1.9259e-07],\n",
      "          [-5.3861e-07,  4.5531e-07,  8.7103e-09],\n",
      "          [-7.9023e-07,  4.2154e-07,  2.0059e-07]],\n",
      "\n",
      "         [[-1.0560e-06, -9.3240e-07,  1.4939e-07],\n",
      "          [-9.1658e-07, -1.1289e-06, -8.3183e-07],\n",
      "          [-1.9274e-07,  1.2681e-07, -2.1991e-07]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 4.5475e-13,  0.0000e+00, -1.4211e-12,  3.9790e-13, -4.5475e-13,\n",
      "         1.1369e-13,  6.8212e-13, -7.1054e-14,  5.4570e-12,  1.1369e-13,\n",
      "        -3.4106e-13, -9.0949e-13,  9.0949e-13, -1.1369e-13,  0.0000e+00,\n",
      "        -3.1832e-11,  4.5475e-13, -9.0949e-13, -2.0464e-12,  4.8317e-13,\n",
      "        -2.8422e-14, -2.2737e-13,  1.7053e-13,  1.4211e-13,  2.2737e-13,\n",
      "        -3.9790e-13, -1.2506e-12,  1.7053e-13,  0.0000e+00,  5.6843e-13,\n",
      "         1.0459e-11, -1.1369e-13,  2.2737e-13, -4.5475e-13, -2.2737e-13,\n",
      "        -1.3642e-12, -1.3642e-12, -6.8212e-13, -9.0949e-13, -1.2733e-11,\n",
      "        -6.8212e-13, -4.5475e-13,  1.7053e-13,  2.2737e-13, -5.1159e-13,\n",
      "        -6.8212e-13, -7.9581e-13, -2.2737e-13,  7.6739e-13,  2.2737e-13,\n",
      "         3.1832e-12,  5.4570e-12,  0.0000e+00,  0.0000e+00,  1.8190e-12,\n",
      "         2.2737e-13,  9.0949e-13, -1.1369e-13, -9.0949e-13,  2.2737e-13,\n",
      "         1.1369e-12,  1.1369e-13,  2.2737e-11, -3.1832e-12, -9.0949e-13,\n",
      "         0.0000e+00,  2.2737e-13,  6.5938e-12,  0.0000e+00,  0.0000e+00,\n",
      "         4.5475e-13,  0.0000e+00,  6.8212e-13,  9.0949e-12, -1.4211e-13,\n",
      "        -1.4211e-13, -1.1369e-13,  0.0000e+00,  6.3665e-12,  3.9790e-13,\n",
      "         4.5475e-13, -4.0927e-12,  1.1369e-13, -4.5475e-13, -5.6843e-14,\n",
      "        -1.1369e-13,  3.4106e-13,  1.7053e-13,  2.2737e-13,  9.0949e-13,\n",
      "        -3.9790e-13, -2.7285e-12, -3.1832e-12, -1.3642e-12, -2.8422e-13,\n",
      "        -1.1369e-13,  5.6843e-14, -6.2528e-13,  9.0949e-13,  0.0000e+00,\n",
      "        -4.5475e-13, -1.1369e-12,  0.0000e+00,  3.1832e-12, -6.0822e-12,\n",
      "         0.0000e+00,  1.1369e-13,  0.0000e+00,  6.3665e-12,  1.3642e-12,\n",
      "         3.9790e-13, -5.6843e-13,  1.7053e-13, -6.8212e-13,  3.4106e-13,\n",
      "         2.2737e-12, -1.7053e-13, -1.1369e-13,  6.5370e-13,  2.2737e-13,\n",
      "        -6.2528e-13,  2.7285e-12,  4.5475e-13,  0.0000e+00,  9.0949e-13,\n",
      "         0.0000e+00, -1.7053e-13,  3.4106e-13], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[ 5.2539e-06,  6.1206e-06, -3.4718e-06,  ...,  2.9594e-06,\n",
      "         -4.4063e-06, -3.1721e-06],\n",
      "        [ 3.7035e-06,  1.4131e-05,  1.4371e-05,  ...,  6.6464e-06,\n",
      "          8.9463e-06,  2.2034e-06],\n",
      "        [ 2.1717e-06, -6.0988e-06,  3.2554e-06,  ..., -2.9768e-05,\n",
      "         -2.4477e-05, -3.6086e-06],\n",
      "        [-4.8243e-06, -1.1725e-06,  6.6546e-06,  ...,  1.5919e-05,\n",
      "          7.8286e-06,  1.8584e-07],\n",
      "        [-6.2903e-06, -1.2966e-05, -2.0789e-05,  ...,  4.2416e-06,\n",
      "          1.2110e-05,  4.3951e-06]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([ 9.6097e-07,  1.2640e-05, -2.8977e-05,  1.7210e-05, -1.8162e-06],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = loss_landscape_join.landscape(maml_system.model.classifier, arbiter_system.model.classifier, args_arbiter)\n",
    "ls.show_2djoin(x_support_set_task, y_support_set_task, title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
