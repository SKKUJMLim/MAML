{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16249129",
   "metadata": {},
   "source": [
    "## [참고]\n",
    "### https://cocoa-t.tistory.com/entry/PyHessian-Loss-Landscape-%EC%8B%9C%EA%B0%81%ED%99%94-PyHessian-Neural-Networks-Through-the-Lens-of-the-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af476",
   "metadata": {},
   "source": [
    "# 0. Dataset 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7235fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"mini_imagenet_full_size\"\n",
    "# dataset=\"tiered_imagenet\"\n",
    "# dataset=\"CIFAR_FS\"\n",
    "# dataset=\"CUB\"\n",
    "\n",
    "title = 'miniImageNet'\n",
    "# title = 'tieredImageNet'\n",
    "# title = 'CIFAR-FS'\n",
    "# title = 'CUB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005193c",
   "metadata": {},
   "source": [
    "# 1. MAML 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8f0d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_maml = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_maml.im_shape = (2, 3, args_maml.image_height, args_maml.image_width)\n",
    "\n",
    "args_maml.use_cuda = torch.cuda.is_available()\n",
    "args_maml.seed = 104\n",
    "args_maml.reverse_channels=False\n",
    "args_maml.labels_as_int=False\n",
    "args_maml.reset_stored_filepaths=False\n",
    "args_maml.num_of_gpus=1\n",
    "\n",
    "args_maml.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9052a",
   "metadata": {},
   "source": [
    "## 2. Arbiter 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_arbiter = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 150,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_arbiter.im_shape = (2, 3, args_arbiter.image_height, args_arbiter.image_width)\n",
    "\n",
    "args_arbiter.use_cuda = torch.cuda.is_available()\n",
    "args_arbiter.seed = 104\n",
    "args_arbiter.reverse_channels=False\n",
    "args_arbiter.labels_as_int=False\n",
    "args_arbiter.reset_stored_filepaths=False\n",
    "args_arbiter.num_of_gpus=1\n",
    "\n",
    "args_arbiter.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f7d8",
   "metadata": {},
   "source": [
    "## 3. Model 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803156ee",
   "metadata": {},
   "source": [
    "### 3.1. MAML Model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f85286c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_maml = MAMLFewShotClassifier(args=args_maml, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_maml.image_height, args_maml.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model_maml, data=data, args=args_maml, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3acf",
   "metadata": {},
   "source": [
    "### 3.2.  Arbiter 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "25651dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_arbiter = MAMLFewShotClassifier(args=args_arbiter, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_arbiter.image_height, args_arbiter.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "arbiter_system = ExperimentBuilder(model=model_arbiter, data=data, args=args_arbiter, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9a2ff6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6359555553396543,\n",
       " 'best_val_iter': 49000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 98,\n",
       " 'train_loss_mean': 0.6403910377025605,\n",
       " 'train_loss_std': 0.1255260544055785,\n",
       " 'train_accuracy_mean': 0.7639866684675216,\n",
       " 'train_accuracy_std': 0.06118401968549816,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.9505268172423045,\n",
       " 'val_loss_std': 0.15325273037358733,\n",
       " 'val_accuracy_mean': 0.6287999994556109,\n",
       " 'val_accuracy_std': 0.06361374163962585,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-0.0255, -0.0844,  0.0421],\n",
       "                         [-0.0809, -0.0591, -0.0046],\n",
       "                         [-0.0325,  0.1015, -0.0134]],\n",
       "               \n",
       "                        [[ 0.0662, -0.0385,  0.0898],\n",
       "                         [-0.0224,  0.0329,  0.0889],\n",
       "                         [-0.0375,  0.0514, -0.0026]],\n",
       "               \n",
       "                        [[ 0.0667, -0.0065, -0.0467],\n",
       "                         [ 0.0322,  0.0595, -0.0776],\n",
       "                         [-0.0642,  0.0074, -0.0751]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0714,  0.1215,  0.0690],\n",
       "                         [ 0.0657, -0.0497,  0.0189],\n",
       "                         [-0.1058, -0.0985, -0.0763]],\n",
       "               \n",
       "                        [[-0.0225,  0.0162, -0.0007],\n",
       "                         [ 0.0545, -0.0631, -0.0476],\n",
       "                         [ 0.0542, -0.0559,  0.0761]],\n",
       "               \n",
       "                        [[-0.0650,  0.0606, -0.0772],\n",
       "                         [-0.0224,  0.0663,  0.0857],\n",
       "                         [-0.0305, -0.0117, -0.0181]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0324, -0.0386,  0.0566],\n",
       "                         [-0.0143, -0.0344, -0.0204],\n",
       "                         [ 0.0755,  0.0579, -0.0644]],\n",
       "               \n",
       "                        [[-0.0797, -0.0395,  0.0388],\n",
       "                         [ 0.0813, -0.0030,  0.0204],\n",
       "                         [-0.0698, -0.0439,  0.0303]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0722,  0.0254],\n",
       "                         [ 0.0777,  0.0115, -0.0638],\n",
       "                         [-0.0668,  0.0245,  0.0016]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0359, -0.0860, -0.0662],\n",
       "                         [-0.0050, -0.0928,  0.0520],\n",
       "                         [-0.0283,  0.0588, -0.0609]],\n",
       "               \n",
       "                        [[ 0.0648,  0.0180,  0.0776],\n",
       "                         [ 0.0499, -0.0222, -0.0206],\n",
       "                         [ 0.0198,  0.0787,  0.0061]],\n",
       "               \n",
       "                        [[ 0.0096, -0.0855, -0.0410],\n",
       "                         [-0.0569,  0.0049,  0.0344],\n",
       "                         [-0.0491,  0.0319,  0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0077,  0.0151, -0.0113],\n",
       "                         [ 0.0198,  0.0805, -0.0301],\n",
       "                         [ 0.0457,  0.0801, -0.0407]],\n",
       "               \n",
       "                        [[ 0.0811,  0.0580,  0.1031],\n",
       "                         [ 0.0596,  0.0067,  0.0002],\n",
       "                         [-0.0017, -0.0517,  0.0510]],\n",
       "               \n",
       "                        [[-0.0385, -0.0292, -0.0301],\n",
       "                         [-0.0170, -0.0443,  0.0586],\n",
       "                         [-0.0917, -0.0518,  0.0024]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0323,  0.0842, -0.0393],\n",
       "                         [-0.0391,  0.0747,  0.0626],\n",
       "                         [ 0.0786,  0.0831,  0.0922]],\n",
       "               \n",
       "                        [[-0.0269, -0.0427,  0.0134],\n",
       "                         [ 0.0379, -0.0726, -0.0621],\n",
       "                         [-0.0407, -0.0233, -0.0411]],\n",
       "               \n",
       "                        [[ 0.0461, -0.0238,  0.0385],\n",
       "                         [-0.0098, -0.0332, -0.0305],\n",
       "                         [-0.0480, -0.0654, -0.0453]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.0175,  0.0289,  0.0017, -0.0053,  0.0039,  0.0073,  0.0086,  0.0064,\n",
       "                        0.0009, -0.0196,  0.0216,  0.0025, -0.0158,  0.0111,  0.0017,  0.0114,\n",
       "                        0.0064,  0.0143,  0.0169, -0.0116, -0.0180, -0.0081, -0.0033, -0.0074,\n",
       "                       -0.0037,  0.0029,  0.0011,  0.0125,  0.0148,  0.0091,  0.0051, -0.0029,\n",
       "                       -0.0089,  0.0250,  0.0072, -0.0091,  0.0011,  0.0039,  0.0159,  0.0169,\n",
       "                       -0.0124, -0.0267,  0.0020, -0.0068, -0.0071,  0.0006, -0.0248, -0.0077,\n",
       "                        0.0044,  0.0075, -0.0075,  0.0021,  0.0150, -0.0009, -0.0197, -0.0072,\n",
       "                        0.0030,  0.0047, -0.0043, -0.0002, -0.0109, -0.0002,  0.0212,  0.0022,\n",
       "                       -0.0252, -0.0014,  0.0105, -0.0034, -0.0002, -0.0239, -0.0024, -0.0128,\n",
       "                        0.0037,  0.0041,  0.0119, -0.0083,  0.0055, -0.0148,  0.0025, -0.0067,\n",
       "                        0.0161,  0.0038, -0.0007,  0.0080,  0.0148, -0.0060, -0.0045,  0.0282,\n",
       "                       -0.0009, -0.0187, -0.0101, -0.0353,  0.0054,  0.0013, -0.0063, -0.0174,\n",
       "                       -0.0096, -0.0242,  0.0046, -0.0104, -0.0169,  0.0135,  0.0113,  0.0116,\n",
       "                        0.0058, -0.0031, -0.0009, -0.0004,  0.0068, -0.0032, -0.0076,  0.0107,\n",
       "                        0.0072, -0.0025, -0.0163,  0.0089,  0.0125, -0.0108,  0.0024,  0.0226,\n",
       "                       -0.0144,  0.0032, -0.0118, -0.0031,  0.0021, -0.0323,  0.0027, -0.0002],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1631,  0.1944, -0.1138,  0.1166,  0.1172, -0.0240, -0.2111, -0.1388,\n",
       "                       -0.1821, -0.0969,  0.2105, -0.1737,  0.3352, -0.0586,  0.3851,  0.2701,\n",
       "                        0.0005,  0.0484,  0.2549, -0.0579,  0.0446, -0.1371, -0.0903,  0.0135,\n",
       "                        0.0157, -0.0954, -0.0991,  0.3129,  0.2739,  0.1327,  0.0106, -0.0495,\n",
       "                        0.1543, -0.1553, -0.0760, -0.0628,  0.0274,  0.1642, -0.1468, -0.0709,\n",
       "                        0.0016,  0.3324, -0.1130,  0.1172,  0.2649, -0.1730, -0.1429, -0.1181,\n",
       "                       -0.0757,  0.0019,  0.2784, -0.0801,  0.3400,  0.2097, -0.0662, -0.0745,\n",
       "                       -0.0332,  0.2216,  0.0059,  0.3655, -0.1121,  0.2333,  0.0753, -0.1723,\n",
       "                        0.1420, -0.0555,  0.2159, -0.0682,  0.3034, -0.0017,  0.1455,  0.3801,\n",
       "                       -0.0174, -0.1359,  0.2083, -0.1117,  0.0331,  0.3195,  0.1055, -0.1900,\n",
       "                        0.4710, -0.0528, -0.1267, -0.0556,  0.3039,  0.3539,  0.3447,  0.2085,\n",
       "                       -0.1442,  0.1741, -0.1334,  0.0495,  0.0896,  0.1874, -0.0507,  0.2346,\n",
       "                        0.0595,  0.0425, -0.0443, -0.1186,  0.0008,  0.0851, -0.1202, -0.0469,\n",
       "                       -0.0340,  0.0512,  0.1297, -0.1034,  0.0106,  0.2907,  0.2411,  0.4341,\n",
       "                       -0.0890,  0.0216, -0.0916,  0.1807,  0.0130,  0.4608, -0.0359,  0.4015,\n",
       "                       -0.0879, -0.0734, -0.1045, -0.1029,  0.3831, -0.0157, -0.0910, -0.0575],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.9696, 1.1085, 0.8447, 1.1021, 0.9693, 0.9709, 0.9787, 0.9343, 0.8471,\n",
       "                       0.9381, 1.1029, 0.9399, 1.1080, 0.9657, 1.1290, 1.1905, 0.8940, 1.0025,\n",
       "                       0.9887, 0.9973, 0.9491, 0.9175, 0.9180, 0.9997, 0.9615, 0.9512, 0.9187,\n",
       "                       0.9743, 1.0115, 1.0331, 1.0684, 0.9661, 1.1221, 0.9391, 0.9968, 0.9734,\n",
       "                       0.9412, 1.0812, 0.9791, 1.0023, 1.0361, 1.1265, 0.9472, 1.1115, 0.9799,\n",
       "                       0.9660, 0.9552, 0.9922, 0.9178, 1.0445, 0.9584, 0.9479, 1.0931, 0.9649,\n",
       "                       0.9693, 0.9627, 0.9913, 1.0533, 1.0207, 1.0808, 1.0113, 1.0104, 0.9997,\n",
       "                       0.9408, 1.0169, 0.9237, 0.9685, 1.0111, 1.1905, 0.9716, 1.0283, 1.1558,\n",
       "                       0.9849, 0.9728, 0.9640, 0.9509, 1.0301, 1.1699, 1.1360, 0.8957, 1.0791,\n",
       "                       1.0465, 0.9238, 1.0184, 1.0511, 1.1042, 0.9738, 1.0620, 0.8964, 0.9912,\n",
       "                       0.8152, 0.9297, 0.9573, 1.0192, 0.9643, 0.9865, 0.9691, 0.9859, 0.9162,\n",
       "                       1.0091, 0.9118, 1.0460, 0.9582, 1.0327, 0.9797, 1.0288, 1.0011, 0.9599,\n",
       "                       1.0142, 1.0149, 0.9774, 1.1019, 1.0000, 0.9757, 0.9746, 1.1659, 0.9524,\n",
       "                       1.0279, 1.0224, 1.1533, 0.9746, 0.9960, 0.9669, 0.9357, 1.0148, 0.9732,\n",
       "                       0.9627, 0.9500], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 0.0073,  0.0615,  0.0120],\n",
       "                         [-0.0109,  0.0211, -0.0202],\n",
       "                         [ 0.0240,  0.0670, -0.0142]],\n",
       "               \n",
       "                        [[ 0.0408,  0.0463, -0.0002],\n",
       "                         [-0.0267, -0.0258, -0.0401],\n",
       "                         [-0.0322,  0.0412, -0.0157]],\n",
       "               \n",
       "                        [[ 0.0051, -0.0471, -0.0257],\n",
       "                         [-0.0522, -0.0002,  0.0558],\n",
       "                         [ 0.0197,  0.0114, -0.0256]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0189, -0.0542, -0.0090],\n",
       "                         [-0.0584, -0.0753, -0.0285],\n",
       "                         [-0.0501, -0.0588, -0.0454]],\n",
       "               \n",
       "                        [[ 0.0147,  0.0626, -0.0057],\n",
       "                         [-0.0027,  0.0439, -0.0190],\n",
       "                         [ 0.0332, -0.0397,  0.0007]],\n",
       "               \n",
       "                        [[-0.0371,  0.0505, -0.0220],\n",
       "                         [-0.0072,  0.0231,  0.0021],\n",
       "                         [-0.0321, -0.0171,  0.0332]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0149, -0.0397,  0.0341],\n",
       "                         [ 0.0416,  0.0181, -0.0336],\n",
       "                         [ 0.0185,  0.0093,  0.0077]],\n",
       "               \n",
       "                        [[ 0.0276,  0.0237, -0.0054],\n",
       "                         [-0.0764,  0.0186,  0.0145],\n",
       "                         [-0.0378, -0.0316,  0.0317]],\n",
       "               \n",
       "                        [[ 0.0133, -0.0043, -0.0575],\n",
       "                         [-0.0120, -0.0424, -0.0046],\n",
       "                         [-0.0632, -0.0027, -0.0540]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0045, -0.0387, -0.0022],\n",
       "                         [-0.0536, -0.0996, -0.0563],\n",
       "                         [-0.0248,  0.0001, -0.0399]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0114, -0.0219],\n",
       "                         [ 0.0516,  0.0429,  0.0258],\n",
       "                         [ 0.0443, -0.0104,  0.0394]],\n",
       "               \n",
       "                        [[-0.0065, -0.0368,  0.0156],\n",
       "                         [ 0.0126, -0.0725, -0.0404],\n",
       "                         [-0.0613, -0.0766, -0.0209]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0573, -0.0550, -0.0113],\n",
       "                         [-0.0121,  0.0007, -0.0628],\n",
       "                         [ 0.0059,  0.0141, -0.0043]],\n",
       "               \n",
       "                        [[ 0.0305, -0.0125, -0.0485],\n",
       "                         [ 0.0321,  0.0384, -0.0741],\n",
       "                         [ 0.0067,  0.0431, -0.0392]],\n",
       "               \n",
       "                        [[-0.0431,  0.0168, -0.0467],\n",
       "                         [-0.0372,  0.0102,  0.0053],\n",
       "                         [ 0.0024,  0.0143,  0.0398]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0455,  0.0046, -0.0200],\n",
       "                         [-0.0405, -0.0600, -0.0352],\n",
       "                         [-0.0408, -0.0624, -0.0764]],\n",
       "               \n",
       "                        [[ 0.0437,  0.0360,  0.0398],\n",
       "                         [-0.0529, -0.0126, -0.0394],\n",
       "                         [-0.0420,  0.0150, -0.0273]],\n",
       "               \n",
       "                        [[-0.0355, -0.0435,  0.0117],\n",
       "                         [-0.0331,  0.0213,  0.0256],\n",
       "                         [ 0.0059, -0.0061, -0.0219]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0504,  0.0111,  0.0091],\n",
       "                         [ 0.0700,  0.0102, -0.0127],\n",
       "                         [ 0.0472, -0.0750, -0.0749]],\n",
       "               \n",
       "                        [[ 0.0126,  0.0457,  0.0538],\n",
       "                         [-0.0469, -0.0615, -0.0298],\n",
       "                         [-0.0257,  0.0307,  0.0064]],\n",
       "               \n",
       "                        [[ 0.0381,  0.0234,  0.0319],\n",
       "                         [ 0.0427, -0.0436, -0.0046],\n",
       "                         [-0.0133, -0.0470, -0.0157]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0282, -0.0291, -0.0278],\n",
       "                         [ 0.0064, -0.0879, -0.0409],\n",
       "                         [-0.0045, -0.0201, -0.0508]],\n",
       "               \n",
       "                        [[ 0.0012,  0.0317,  0.0562],\n",
       "                         [ 0.0444,  0.0208,  0.0051],\n",
       "                         [-0.0373,  0.0474,  0.0533]],\n",
       "               \n",
       "                        [[-0.0329, -0.0453, -0.0214],\n",
       "                         [-0.0277,  0.0045,  0.0224],\n",
       "                         [-0.0115,  0.0191, -0.0240]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0053, -0.0619,  0.0081],\n",
       "                         [-0.0290, -0.0561,  0.0356],\n",
       "                         [-0.0297,  0.0281,  0.0076]],\n",
       "               \n",
       "                        [[ 0.0019,  0.0412, -0.0531],\n",
       "                         [-0.0054,  0.0489, -0.0376],\n",
       "                         [-0.0378, -0.0377,  0.0578]],\n",
       "               \n",
       "                        [[-0.0227,  0.0314,  0.0516],\n",
       "                         [ 0.0181, -0.0008, -0.0313],\n",
       "                         [-0.0155,  0.0141,  0.0279]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0321, -0.0038,  0.0526],\n",
       "                         [-0.0037,  0.0310, -0.0090],\n",
       "                         [-0.0042, -0.0025, -0.0319]],\n",
       "               \n",
       "                        [[ 0.0066,  0.0332,  0.0456],\n",
       "                         [-0.0186, -0.0539, -0.0037],\n",
       "                         [-0.0198, -0.0442,  0.0125]],\n",
       "               \n",
       "                        [[-0.0379, -0.0151,  0.0192],\n",
       "                         [-0.0437,  0.0384,  0.0344],\n",
       "                         [ 0.0192,  0.0085,  0.0302]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0173, -0.0070, -0.0065],\n",
       "                         [ 0.0314,  0.0438, -0.0579],\n",
       "                         [-0.0199,  0.0290, -0.0254]],\n",
       "               \n",
       "                        [[-0.0348,  0.0351,  0.0423],\n",
       "                         [ 0.0056, -0.0599, -0.0338],\n",
       "                         [-0.0596, -0.0250,  0.0199]],\n",
       "               \n",
       "                        [[-0.0150, -0.0501,  0.0067],\n",
       "                         [-0.0007,  0.0117,  0.0053],\n",
       "                         [ 0.0055, -0.0287,  0.0358]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0159, -0.0148, -0.0628],\n",
       "                         [ 0.0441,  0.0680, -0.0351],\n",
       "                         [ 0.0223,  0.0213, -0.0230]],\n",
       "               \n",
       "                        [[-0.0644, -0.0820, -0.0552],\n",
       "                         [ 0.0076,  0.0235, -0.0232],\n",
       "                         [-0.0340, -0.0002, -0.0556]],\n",
       "               \n",
       "                        [[-0.0215,  0.0316,  0.0109],\n",
       "                         [-0.0233,  0.0090,  0.0530],\n",
       "                         [-0.0053, -0.0364, -0.0194]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-1.6949e-02, -8.4122e-03, -6.4646e-03, -4.2481e-03, -2.1994e-03,\n",
       "                        3.4437e-03, -1.9195e-03,  2.9394e-03, -8.6836e-04,  2.9146e-03,\n",
       "                       -4.8363e-03, -2.9786e-03,  8.2595e-04,  2.0065e-03,  1.7538e-03,\n",
       "                        5.7412e-03, -3.8066e-05,  3.0353e-03, -2.8385e-03,  2.7388e-03,\n",
       "                       -5.7158e-03,  1.9158e-03, -2.4051e-03, -2.9564e-03,  2.5613e-03,\n",
       "                        2.3439e-04, -2.2504e-03,  7.3651e-03, -1.2441e-03, -8.3805e-05,\n",
       "                        8.6782e-03, -3.5325e-03,  3.8182e-03,  1.5983e-03,  1.2204e-03,\n",
       "                       -6.0784e-03, -4.5049e-04,  4.5410e-03,  3.2182e-04, -2.8716e-03,\n",
       "                       -2.2109e-03,  3.7276e-03,  7.1509e-04, -2.3736e-03,  3.0177e-04,\n",
       "                        3.1425e-03,  4.0651e-03, -3.7909e-03, -1.2114e-03, -1.6332e-03,\n",
       "                       -8.7488e-03, -9.9171e-04,  8.2588e-04, -7.7100e-04, -8.6759e-03,\n",
       "                        3.9878e-03, -2.7320e-03, -8.2349e-03,  2.0059e-03,  6.6739e-04,\n",
       "                        2.1313e-03, -2.2211e-03,  1.5898e-03,  6.0315e-04, -7.0056e-03,\n",
       "                        3.8265e-03,  4.4242e-03, -4.5509e-03,  2.2188e-04, -2.7838e-03,\n",
       "                        1.4363e-03,  4.0279e-04, -1.7379e-03, -4.3296e-03,  2.3067e-03,\n",
       "                       -5.1976e-03, -2.9156e-04, -1.3852e-03,  7.8145e-03,  1.6117e-03,\n",
       "                        6.3610e-03, -2.0517e-03, -5.3275e-03,  4.3522e-03, -2.8026e-03,\n",
       "                        3.9732e-03,  5.5070e-03, -7.5503e-05,  1.0788e-03,  2.9781e-03,\n",
       "                       -9.4866e-03,  5.1489e-04, -3.9218e-04, -5.8387e-05, -1.9815e-03,\n",
       "                        6.0945e-04,  1.0402e-03,  9.8662e-04, -5.3993e-04, -2.6299e-04,\n",
       "                        1.4962e-03, -5.3848e-04,  1.7470e-03, -3.8175e-03, -1.7810e-03,\n",
       "                       -1.8436e-03, -2.5974e-03,  2.9317e-03,  2.3702e-03,  2.2829e-04,\n",
       "                        2.0404e-03,  5.7490e-03,  1.5104e-03,  7.5078e-04, -1.6547e-03,\n",
       "                        4.6222e-03,  1.2253e-04,  3.9792e-03,  4.9803e-03,  4.8457e-04,\n",
       "                       -3.8067e-03, -2.3458e-03,  1.2213e-03, -6.6374e-03, -5.1143e-03,\n",
       "                       -1.9425e-03, -1.7261e-03,  2.3845e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0239, -0.1029, -0.1356, -0.1313, -0.1094, -0.1200, -0.0928, -0.1694,\n",
       "                       -0.0216, -0.0378, -0.0961, -0.0686, -0.2043, -0.1061, -0.1571, -0.1857,\n",
       "                       -0.0801,  0.1526, -0.0903, -0.1624, -0.2084, -0.0794, -0.1526, -0.0904,\n",
       "                       -0.1907, -0.0218, -0.1831,  0.0054, -0.0388, -0.1296,  0.0191, -0.1421,\n",
       "                       -0.0195, -0.1232, -0.0756, -0.1430, -0.2357, -0.0668, -0.1038, -0.1295,\n",
       "                       -0.1906, -0.0274, -0.1887, -0.1435, -0.1363, -0.1023, -0.1372, -0.0549,\n",
       "                       -0.0128, -0.1253, -0.1443, -0.0114, -0.0838, -0.1771, -0.0118, -0.0546,\n",
       "                       -0.0105, -0.1224, -0.1915, -0.1396, -0.1079, -0.0798, -0.1026, -0.0963,\n",
       "                       -0.0723, -0.0284, -0.0720, -0.0425, -0.1336, -0.0733, -0.1823, -0.0852,\n",
       "                       -0.0766, -0.0089, -0.0376, -0.1518, -0.0774, -0.1717,  0.0542, -0.1189,\n",
       "                       -0.0779, -0.1253, -0.0342, -0.0763, -0.1555, -0.0973, -0.0166, -0.0855,\n",
       "                       -0.0579, -0.1178, -0.1892, -0.0419, -0.0477, -0.0605, -0.0799, -0.0898,\n",
       "                       -0.0811, -0.0729, -0.1795, -0.1760, -0.1528, -0.1409, -0.1406, -0.1007,\n",
       "                       -0.1355, -0.1034, -0.0196, -0.1602, -0.0788, -0.0995, -0.1055, -0.0866,\n",
       "                       -0.0884, -0.0945, -0.1266, -0.0264, -0.0842, -0.1382, -0.0980, -0.1124,\n",
       "                       -0.1105, -0.0172, -0.1221, -0.0662, -0.0260, -0.0930, -0.0811, -0.1032],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9428, 0.9862, 1.0353, 1.0197, 0.9484, 1.0319, 1.0427, 0.9799, 1.0889,\n",
       "                       0.9699, 1.0409, 0.9796, 0.9760, 1.0362, 1.0216, 1.0216, 1.0361, 0.9781,\n",
       "                       1.0763, 1.0203, 1.0362, 1.0268, 1.0446, 0.9615, 1.0413, 1.0226, 1.0333,\n",
       "                       1.0005, 1.0836, 1.0583, 0.9296, 0.9788, 1.0094, 0.9958, 1.0039, 0.9190,\n",
       "                       1.0555, 0.9978, 0.9221, 0.9902, 1.0340, 0.9762, 1.0726, 1.0702, 0.9993,\n",
       "                       0.9641, 1.0156, 1.0302, 1.0744, 0.9556, 0.9469, 0.9848, 0.9398, 0.9957,\n",
       "                       0.9438, 1.0218, 1.0183, 0.9649, 0.9851, 0.9523, 0.9629, 0.9934, 0.9863,\n",
       "                       0.9597, 0.9350, 1.0053, 1.0129, 1.0250, 0.9279, 1.0033, 0.9282, 0.9721,\n",
       "                       1.0010, 0.9276, 1.0228, 0.9233, 1.0320, 1.0337, 0.9309, 0.9750, 1.0079,\n",
       "                       0.9799, 1.0670, 0.9271, 1.0485, 0.9817, 0.9526, 0.9983, 1.1004, 0.9980,\n",
       "                       1.0839, 1.0081, 0.9850, 1.0833, 0.9840, 0.9629, 0.9918, 0.9759, 1.0162,\n",
       "                       1.0476, 0.9885, 1.0907, 0.9968, 1.0194, 0.9749, 0.9604, 0.9823, 0.9796,\n",
       "                       1.0114, 1.0355, 0.9358, 1.0666, 0.9849, 1.0066, 0.9954, 1.0633, 1.0803,\n",
       "                       0.9390, 0.9113, 1.0046, 0.9884, 1.0092, 1.0470, 0.9643, 0.9522, 1.0208,\n",
       "                       1.0418, 0.9650], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-5.6612e-04,  5.5613e-02, -1.4964e-02],\n",
       "                         [-3.1003e-02, -5.2417e-03, -3.5244e-02],\n",
       "                         [ 1.5153e-02, -2.5045e-02,  8.3373e-03]],\n",
       "               \n",
       "                        [[ 2.6745e-02, -1.6569e-02,  9.6491e-03],\n",
       "                         [-2.3582e-02, -1.7072e-02, -3.0692e-02],\n",
       "                         [ 8.8099e-03,  1.9431e-02, -1.0071e-02]],\n",
       "               \n",
       "                        [[ 2.2886e-02,  1.2657e-02,  2.7444e-02],\n",
       "                         [ 6.7986e-02,  2.3340e-02, -6.3387e-02],\n",
       "                         [-8.0550e-03,  4.3950e-03, -2.6644e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.2784e-02, -4.7080e-02, -5.8843e-02],\n",
       "                         [ 5.8877e-02,  3.3930e-02,  5.3777e-02],\n",
       "                         [-3.0208e-06, -3.6364e-02, -3.0298e-02]],\n",
       "               \n",
       "                        [[-1.8547e-02, -3.3489e-02,  1.4912e-02],\n",
       "                         [ 4.4668e-02,  1.5992e-02, -3.8668e-02],\n",
       "                         [ 1.9676e-02,  3.2789e-02,  3.8013e-02]],\n",
       "               \n",
       "                        [[ 9.6036e-03,  1.6508e-02,  6.3899e-02],\n",
       "                         [-2.2815e-02, -2.2302e-02, -3.1161e-02],\n",
       "                         [ 1.2610e-03,  2.5403e-03,  4.1579e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3809e-03,  3.2775e-03,  3.4006e-02],\n",
       "                         [ 1.2260e-02,  2.4693e-03, -3.2722e-02],\n",
       "                         [-2.4378e-02, -6.5934e-02, -2.4807e-02]],\n",
       "               \n",
       "                        [[-4.2487e-02, -2.5946e-02, -2.8232e-02],\n",
       "                         [-1.6780e-02,  6.6708e-03, -4.2812e-03],\n",
       "                         [-1.7184e-02, -2.3446e-03,  1.5289e-02]],\n",
       "               \n",
       "                        [[ 8.0496e-03,  5.3279e-02,  9.2603e-03],\n",
       "                         [ 3.7587e-02,  3.2692e-02,  6.3311e-03],\n",
       "                         [ 4.8233e-02,  1.8544e-02, -1.1102e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.0004e-02,  3.0527e-02, -4.1664e-02],\n",
       "                         [-5.6780e-02, -4.1942e-02, -5.5847e-02],\n",
       "                         [-4.5592e-02, -2.2262e-02, -7.3276e-02]],\n",
       "               \n",
       "                        [[-2.0432e-02, -1.4052e-02,  1.8676e-03],\n",
       "                         [-9.4970e-02, -1.9388e-02, -1.8318e-02],\n",
       "                         [-6.2985e-02, -3.7837e-02, -3.0712e-02]],\n",
       "               \n",
       "                        [[-9.6093e-03, -8.8685e-02, -7.3937e-02],\n",
       "                         [-2.9326e-02,  7.3677e-04,  8.5064e-03],\n",
       "                         [-4.6906e-02, -1.3926e-02, -2.8744e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.4945e-02, -1.6051e-03, -6.4338e-02],\n",
       "                         [-9.1217e-04, -1.0536e-02,  1.0642e-02],\n",
       "                         [ 5.3609e-02,  2.5719e-02,  1.7490e-02]],\n",
       "               \n",
       "                        [[ 3.2974e-02,  7.0537e-02,  6.3091e-02],\n",
       "                         [-2.3824e-02,  3.8106e-02,  3.0877e-03],\n",
       "                         [-8.1763e-02, -1.9468e-02, -8.9834e-02]],\n",
       "               \n",
       "                        [[ 5.2738e-02,  1.4020e-02,  6.0283e-02],\n",
       "                         [-8.0464e-03,  9.1787e-02,  2.4450e-02],\n",
       "                         [-7.7595e-04,  4.2054e-02, -5.3030e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.1894e-02,  7.4259e-03, -4.8836e-02],\n",
       "                         [-3.9580e-03, -3.1565e-02, -1.5890e-02],\n",
       "                         [-2.4345e-02, -1.7730e-02,  5.6163e-02]],\n",
       "               \n",
       "                        [[-1.3640e-02, -2.9441e-02, -1.1606e-02],\n",
       "                         [ 6.9270e-05, -5.4901e-02, -6.5660e-02],\n",
       "                         [-7.3340e-02, -7.6530e-02, -5.1404e-02]],\n",
       "               \n",
       "                        [[-1.6647e-02,  2.0942e-02,  5.6214e-02],\n",
       "                         [ 1.0635e-02,  3.8857e-02,  2.6654e-02],\n",
       "                         [-1.2568e-02, -2.9424e-04,  1.8442e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.2160e-02, -4.8316e-02, -5.4221e-02],\n",
       "                         [ 3.5441e-02, -3.3133e-02, -1.6483e-02],\n",
       "                         [ 1.4458e-02,  2.3074e-02,  1.1153e-02]],\n",
       "               \n",
       "                        [[-4.4017e-02,  5.9920e-03,  1.8519e-02],\n",
       "                         [-1.0042e-02,  1.2894e-02, -3.1734e-02],\n",
       "                         [-4.1772e-03,  5.4501e-02,  8.8462e-03]],\n",
       "               \n",
       "                        [[-2.9016e-02,  7.5986e-03, -2.8667e-02],\n",
       "                         [ 3.5275e-02,  5.9086e-03,  3.4249e-02],\n",
       "                         [ 6.0786e-02,  2.2129e-02,  1.7514e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.5898e-02,  3.3125e-02,  1.3987e-02],\n",
       "                         [ 1.1372e-02,  4.8750e-03, -5.3042e-03],\n",
       "                         [-1.7770e-02, -3.6008e-02, -4.0320e-02]],\n",
       "               \n",
       "                        [[-8.7624e-03, -1.3168e-02,  6.0018e-02],\n",
       "                         [-6.0902e-03, -3.3698e-03, -2.7294e-02],\n",
       "                         [-3.2872e-02,  3.5987e-02, -7.3005e-02]],\n",
       "               \n",
       "                        [[ 1.0989e-02, -4.6545e-02, -6.0721e-02],\n",
       "                         [-7.2877e-02, -8.1973e-02, -7.7765e-02],\n",
       "                         [-3.0052e-02,  2.0828e-02, -2.5616e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.8925e-02,  4.8917e-02,  2.8418e-02],\n",
       "                         [-2.1285e-02, -2.5500e-02, -7.5975e-03],\n",
       "                         [-2.8078e-03,  2.9815e-02, -1.3520e-02]],\n",
       "               \n",
       "                        [[ 2.1450e-04, -2.8705e-02,  2.4119e-02],\n",
       "                         [-3.6257e-03, -3.2389e-02,  4.0854e-02],\n",
       "                         [-3.2382e-02,  3.5661e-03,  7.8317e-03]],\n",
       "               \n",
       "                        [[-3.0559e-02,  2.0490e-02,  5.0351e-03],\n",
       "                         [ 5.0072e-02,  4.6661e-02, -1.5058e-02],\n",
       "                         [ 1.7608e-02, -1.3727e-02,  2.3573e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.6052e-02, -3.8395e-03,  3.6573e-02],\n",
       "                         [-5.7949e-04,  7.1536e-02, -1.0412e-02],\n",
       "                         [-1.5482e-03,  2.2744e-02, -8.5390e-03]],\n",
       "               \n",
       "                        [[-6.3845e-02, -4.6169e-02, -4.0007e-02],\n",
       "                         [-1.1293e-02, -3.5827e-02,  1.3014e-02],\n",
       "                         [ 5.1841e-02, -1.6890e-02, -7.2922e-02]],\n",
       "               \n",
       "                        [[ 4.1673e-02,  1.7039e-02, -8.0290e-02],\n",
       "                         [-4.1658e-04, -2.3544e-02, -1.0676e-01],\n",
       "                         [ 4.5347e-03, -2.7998e-02, -2.3365e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.9852e-02,  5.9324e-03,  9.6843e-04],\n",
       "                         [-1.5981e-02, -3.1834e-02,  5.2805e-02],\n",
       "                         [-3.6868e-02, -1.7340e-02,  3.9554e-02]],\n",
       "               \n",
       "                        [[ 6.7321e-02,  2.9623e-02,  5.4673e-03],\n",
       "                         [ 2.0529e-02,  6.5971e-03, -2.3391e-02],\n",
       "                         [ 1.7442e-02,  3.5414e-02,  3.6108e-02]],\n",
       "               \n",
       "                        [[-3.3463e-02, -2.1611e-02, -1.9678e-02],\n",
       "                         [-4.8424e-02,  3.5010e-02,  3.9424e-02],\n",
       "                         [-5.9420e-03,  4.3907e-02,  3.3360e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9797e-02,  4.3752e-02,  1.3179e-02],\n",
       "                         [ 4.3918e-02,  3.1366e-02, -2.0022e-02],\n",
       "                         [-3.3028e-02, -2.8256e-02, -3.0205e-02]],\n",
       "               \n",
       "                        [[-1.2938e-02,  1.2830e-02,  2.9501e-02],\n",
       "                         [-1.7726e-02, -7.1185e-03, -3.7811e-02],\n",
       "                         [ 2.8611e-02, -2.0118e-02, -6.1311e-02]],\n",
       "               \n",
       "                        [[-1.0113e-02,  5.4758e-02,  2.1487e-02],\n",
       "                         [ 3.2794e-03,  1.9351e-02, -2.8451e-02],\n",
       "                         [ 8.8019e-02, -1.0072e-02, -3.6928e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 4.6061e-04, -2.6240e-03, -4.8668e-03,  1.3499e-03, -6.7688e-04,\n",
       "                       -1.3051e-03, -3.9467e-04,  1.6521e-03, -7.8187e-03,  4.3409e-03,\n",
       "                       -2.4901e-03, -1.1735e-03, -1.9624e-04, -7.4011e-03,  1.8107e-05,\n",
       "                       -4.4562e-03,  2.1683e-03, -2.7531e-03,  9.8409e-05,  1.1074e-03,\n",
       "                       -5.6326e-03,  6.3480e-03,  5.0846e-03,  1.4609e-03, -3.3035e-03,\n",
       "                       -3.3362e-03,  9.9636e-04, -5.7492e-04, -3.1613e-03, -1.0247e-02,\n",
       "                        4.1812e-03, -1.9299e-03,  4.8126e-03,  6.2209e-03,  9.1613e-03,\n",
       "                       -3.4160e-03,  1.0048e-02, -1.8047e-02,  1.5066e-04, -9.0010e-04,\n",
       "                       -3.3024e-03,  1.1539e-02,  1.5924e-03, -7.8076e-04,  2.7959e-03,\n",
       "                       -3.8437e-03,  5.5420e-04,  5.6630e-03,  1.7700e-03, -1.1816e-03,\n",
       "                       -3.2343e-03, -1.1565e-03,  5.6338e-03, -2.6592e-03, -2.2296e-03,\n",
       "                       -7.1357e-04,  1.0998e-03, -8.9221e-04, -3.3544e-03, -5.9220e-03,\n",
       "                        3.0695e-03, -2.3067e-03,  7.3235e-03,  6.6048e-03,  1.7583e-03,\n",
       "                        1.2300e-03, -8.0600e-04, -1.3774e-02,  2.6487e-03, -2.2824e-03,\n",
       "                       -4.4781e-03, -4.0840e-03, -4.4604e-04, -9.0035e-04,  1.4574e-04,\n",
       "                        2.9544e-03,  1.5378e-03,  3.9039e-03, -1.7558e-03,  3.4783e-03,\n",
       "                       -3.1708e-03,  6.7397e-03,  2.6774e-03, -4.4915e-03,  1.7781e-03,\n",
       "                        1.4992e-03,  2.2443e-03, -9.6971e-04, -1.8576e-03,  2.4945e-03,\n",
       "                        9.1154e-03,  4.9740e-03,  3.8111e-03,  4.0308e-04,  3.2366e-03,\n",
       "                       -1.2821e-03, -6.6860e-04, -6.5602e-03,  2.9548e-03,  3.9707e-03,\n",
       "                        3.1116e-03, -3.0379e-03,  1.4848e-03, -2.9219e-03, -2.7217e-03,\n",
       "                       -3.1725e-03, -2.3843e-03, -2.0663e-04, -3.5932e-03, -2.9701e-03,\n",
       "                        2.3363e-03, -4.0607e-03,  1.1661e-03,  3.8860e-03, -7.5729e-03,\n",
       "                       -2.9014e-03,  7.2572e-04, -6.2508e-03,  1.2011e-04,  2.4776e-03,\n",
       "                        1.8391e-03, -9.3481e-04,  2.6883e-03, -1.7414e-03,  3.9286e-03,\n",
       "                        3.7851e-03,  5.6336e-03,  5.9318e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.1391, -0.2048, -0.1393, -0.1732, -0.1353, -0.1210, -0.1603, -0.2054,\n",
       "                       -0.0353, -0.1253, -0.1125, -0.0369, -0.2840, -0.1378, -0.1706, -0.2121,\n",
       "                       -0.0803, -0.1494, -0.1289, -0.1470, -0.0888, -0.0605, -0.1538, -0.0884,\n",
       "                       -0.0962, -0.0889, -0.1651, -0.1394, -0.2496, -0.1341, -0.1365, -0.1336,\n",
       "                       -0.2978, -0.0906, -0.1493, -0.2138, -0.0082, -0.2035, -0.2011, -0.2006,\n",
       "                       -0.1547, -0.2458, -0.1979, -0.0222, -0.1107, -0.1439, -0.0984, -0.0936,\n",
       "                       -0.1489, -0.1794, -0.1324, -0.1430, -0.2192, -0.1099, -0.1431, -0.1055,\n",
       "                       -0.1645, -0.1377, -0.1245, -0.0560, -0.2493, -0.0998, -0.1132, -0.2081,\n",
       "                       -0.0350, -0.1173, -0.0356, -0.0940, -0.1193, -0.1307, -0.0620, -0.2028,\n",
       "                       -0.1571, -0.1631, -0.0327, -0.1306, -0.0988, -0.1368, -0.0456, -0.0868,\n",
       "                       -0.1458, -0.1827, -0.1800, -0.1469, -0.0483, -0.1266, -0.1537, -0.1395,\n",
       "                       -0.1806, -0.0839, -0.2118, -0.0111, -0.1126, -0.1722, -0.1317, -0.1179,\n",
       "                       -0.1163, -0.1933, -0.1038, -0.1531, -0.1672, -0.1171, -0.0923, -0.0193,\n",
       "                       -0.0547, -0.2908, -0.1291, -0.1831, -0.1212, -0.1551, -0.0190, -0.0806,\n",
       "                       -0.2205, -0.0160,  0.0108, -0.1951, -0.1521, -0.3187, -0.1491, -0.1353,\n",
       "                       -0.0793, -0.1182,  0.0170, -0.1368, -0.1985, -0.1302, -0.1994, -0.1754],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([1.0277, 1.0128, 0.9804, 0.9433, 0.9412, 0.9652, 0.9658, 0.9749, 1.0517,\n",
       "                       1.0016, 0.9643, 1.0066, 1.0405, 0.9912, 0.9841, 0.9218, 1.1151, 1.0733,\n",
       "                       0.9509, 0.9652, 1.0227, 0.9606, 1.0235, 0.9114, 1.0268, 1.0234, 1.0819,\n",
       "                       0.9045, 1.0696, 1.0600, 0.9874, 1.0240, 1.0417, 1.0809, 0.9790, 1.0562,\n",
       "                       0.9363, 1.1194, 0.9735, 0.9849, 0.9253, 1.0497, 1.0019, 0.9294, 0.9515,\n",
       "                       1.0383, 1.0002, 0.9651, 0.9030, 0.9744, 1.0132, 0.9474, 1.0187, 0.9983,\n",
       "                       0.9703, 0.9965, 0.9904, 0.9778, 0.9497, 1.0112, 1.0529, 0.9374, 0.9905,\n",
       "                       1.0483, 1.0328, 1.0214, 0.9460, 0.9351, 0.9422, 0.9163, 0.9332, 1.0385,\n",
       "                       0.9540, 0.9346, 0.9807, 0.9988, 1.0332, 0.9307, 0.9741, 0.9708, 1.0372,\n",
       "                       0.9618, 0.9719, 1.0110, 0.9977, 0.9824, 0.9189, 0.9367, 1.1518, 0.9375,\n",
       "                       1.0127, 0.9777, 0.9833, 1.0089, 0.9567, 0.9944, 0.9539, 0.9537, 0.9933,\n",
       "                       1.0257, 0.9628, 0.9432, 0.9521, 0.9561, 1.0270, 1.0515, 1.0031, 0.9811,\n",
       "                       1.0540, 0.9654, 1.0987, 0.9105, 0.8965, 0.9892, 0.9852, 0.9682, 0.8834,\n",
       "                       1.0933, 1.0380, 0.9390, 0.9505, 1.0529, 1.0772, 1.0426, 0.9539, 1.0445,\n",
       "                       0.9834, 1.0106], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-5.4627e-02,  7.3330e-04,  1.0867e-02],\n",
       "                         [-1.2974e-02,  1.8572e-02, -8.2295e-03],\n",
       "                         [-6.2027e-03,  3.7114e-03,  6.4246e-02]],\n",
       "               \n",
       "                        [[-4.5182e-02, -5.4583e-02, -4.1419e-02],\n",
       "                         [-4.6200e-02, -4.0708e-02, -3.4592e-02],\n",
       "                         [-3.8282e-02,  5.8337e-03, -4.8956e-02]],\n",
       "               \n",
       "                        [[ 6.5746e-02,  1.5244e-02,  8.7147e-02],\n",
       "                         [-3.4472e-02, -2.5536e-02,  2.4144e-02],\n",
       "                         [-3.4479e-02,  4.1504e-02,  2.4551e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.6482e-02, -7.8470e-03, -3.9585e-02],\n",
       "                         [ 2.1164e-02, -1.7489e-02,  1.2096e-02],\n",
       "                         [ 2.4554e-02,  6.7232e-02,  4.6818e-02]],\n",
       "               \n",
       "                        [[-1.3402e-02, -3.8527e-03, -2.4207e-02],\n",
       "                         [ 1.4606e-02, -7.9428e-03, -1.4672e-02],\n",
       "                         [ 2.4145e-02, -2.7772e-02, -3.5707e-02]],\n",
       "               \n",
       "                        [[ 5.1828e-02, -9.1739e-03,  8.9307e-03],\n",
       "                         [ 3.5144e-03, -6.4678e-03, -4.3382e-02],\n",
       "                         [-9.9676e-03, -3.8737e-02, -2.5570e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4494e-03, -8.8308e-03, -2.3047e-02],\n",
       "                         [ 5.3864e-03, -3.2619e-02, -1.8617e-02],\n",
       "                         [ 3.7532e-03, -3.0453e-03, -3.3701e-02]],\n",
       "               \n",
       "                        [[-1.7086e-02, -1.4788e-02, -3.8940e-03],\n",
       "                         [-6.3392e-02, -3.3190e-02, -4.4878e-02],\n",
       "                         [-3.9955e-02,  1.3318e-02,  1.3630e-02]],\n",
       "               \n",
       "                        [[-2.2971e-02,  1.3972e-02,  4.7967e-02],\n",
       "                         [ 2.2433e-02,  3.2503e-03, -3.8789e-04],\n",
       "                         [ 1.0756e-02,  1.0665e-02,  4.0786e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.7207e-02, -3.5612e-02,  3.2990e-02],\n",
       "                         [-1.0289e-02, -5.2001e-02,  2.2325e-02],\n",
       "                         [ 4.3186e-02, -5.0887e-03, -3.1521e-02]],\n",
       "               \n",
       "                        [[-7.2463e-03,  2.2586e-02, -4.4914e-02],\n",
       "                         [-2.0083e-02, -1.9633e-04, -3.0193e-02],\n",
       "                         [ 3.4482e-02, -5.3447e-03,  2.6395e-02]],\n",
       "               \n",
       "                        [[ 2.2625e-02, -2.5691e-03, -5.2997e-02],\n",
       "                         [-3.9718e-02, -5.9920e-02, -7.0250e-02],\n",
       "                         [-3.1720e-02, -5.6156e-03, -3.6102e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.1116e-02,  4.1400e-02,  7.7896e-02],\n",
       "                         [-5.2314e-03,  5.7153e-02,  6.5850e-02],\n",
       "                         [ 6.1248e-02,  3.1912e-02,  8.1214e-03]],\n",
       "               \n",
       "                        [[-1.7594e-02, -2.7667e-02,  6.5020e-03],\n",
       "                         [-2.5013e-02,  3.1521e-02,  1.0809e-02],\n",
       "                         [-4.3111e-03, -8.4143e-05, -1.1235e-02]],\n",
       "               \n",
       "                        [[ 2.0829e-02,  8.6329e-03, -4.2440e-02],\n",
       "                         [-2.5366e-02,  2.7901e-02, -9.4518e-03],\n",
       "                         [ 8.4475e-03,  5.2818e-02, -3.1964e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2820e-02, -1.8504e-02, -6.6513e-02],\n",
       "                         [-1.3898e-03, -2.1544e-02, -1.7567e-02],\n",
       "                         [-5.2538e-02,  1.3536e-02,  1.3664e-03]],\n",
       "               \n",
       "                        [[ 5.7266e-02,  3.3359e-02,  2.5032e-03],\n",
       "                         [ 6.3723e-02,  6.3739e-02,  5.4753e-02],\n",
       "                         [-1.3828e-02,  1.9169e-02,  1.1911e-02]],\n",
       "               \n",
       "                        [[-2.6761e-02,  1.5723e-02, -1.2717e-02],\n",
       "                         [-2.4329e-02,  3.9293e-03, -3.9389e-02],\n",
       "                         [ 1.6449e-02,  1.2233e-02, -1.2991e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0369e-02,  2.0749e-02,  7.4456e-02],\n",
       "                         [ 7.0774e-02,  4.0400e-02,  2.0487e-02],\n",
       "                         [ 3.9115e-02,  3.1699e-02,  2.0401e-02]],\n",
       "               \n",
       "                        [[-2.9395e-02,  2.5912e-02, -1.5670e-02],\n",
       "                         [ 3.6106e-02, -5.6428e-03, -1.2668e-02],\n",
       "                         [ 1.1267e-02, -1.9298e-02,  1.0444e-02]],\n",
       "               \n",
       "                        [[ 6.6017e-02,  9.4842e-02,  1.0661e-01],\n",
       "                         [ 5.6030e-02,  4.9520e-02, -1.6742e-02],\n",
       "                         [ 3.1501e-02,  5.3863e-02,  6.5480e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.5279e-03,  2.6637e-02, -1.5511e-02],\n",
       "                         [ 1.0471e-02, -2.2111e-02, -1.6427e-02],\n",
       "                         [ 6.6035e-02,  2.6296e-02,  9.1037e-04]],\n",
       "               \n",
       "                        [[-1.9806e-02, -5.3623e-03, -2.7325e-02],\n",
       "                         [-2.5083e-02, -1.0296e-02, -1.6746e-02],\n",
       "                         [ 2.9134e-02, -5.8477e-03, -2.6059e-03]],\n",
       "               \n",
       "                        [[-3.5956e-02,  9.9449e-03,  7.3160e-03],\n",
       "                         [-3.3580e-02, -1.2026e-02,  1.8119e-02],\n",
       "                         [-5.9664e-02, -2.7382e-02, -9.3631e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0678e-02,  2.4934e-02,  2.0218e-02],\n",
       "                         [ 4.2124e-02,  6.6200e-03,  6.8485e-02],\n",
       "                         [ 4.8555e-02,  3.4361e-03, -4.3516e-03]],\n",
       "               \n",
       "                        [[ 2.7361e-02, -1.3040e-02,  1.2497e-02],\n",
       "                         [ 2.7245e-02,  7.9802e-03,  6.1445e-02],\n",
       "                         [ 2.1408e-02, -2.5271e-02, -1.4236e-02]],\n",
       "               \n",
       "                        [[ 2.6388e-02,  1.0168e-02,  5.2068e-03],\n",
       "                         [-1.7016e-02, -1.5717e-02, -2.3489e-02],\n",
       "                         [ 2.1516e-03,  2.9778e-02, -4.5113e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0460e-02, -1.1264e-02,  1.8085e-02],\n",
       "                         [ 6.7404e-02, -4.3560e-03,  7.5595e-02],\n",
       "                         [ 3.1166e-02,  8.9611e-02,  2.6672e-02]],\n",
       "               \n",
       "                        [[ 3.6279e-03, -4.4067e-02,  2.6130e-02],\n",
       "                         [-2.2956e-02,  3.1149e-02,  1.0416e-02],\n",
       "                         [ 2.2577e-02, -3.9788e-02,  4.6509e-02]],\n",
       "               \n",
       "                        [[ 1.1394e-02,  2.5146e-03, -3.5653e-02],\n",
       "                         [-4.5091e-02,  1.8573e-02, -5.8179e-02],\n",
       "                         [-2.0710e-02, -1.1970e-02, -3.7716e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.0215e-02, -2.5592e-02, -7.2966e-03],\n",
       "                         [-6.4089e-03,  8.9794e-03,  4.4473e-03],\n",
       "                         [-5.4829e-02, -2.1084e-02,  1.4473e-03]],\n",
       "               \n",
       "                        [[ 5.4302e-02,  6.5507e-02,  8.2479e-02],\n",
       "                         [ 5.4617e-02,  6.6363e-02,  1.0264e-01],\n",
       "                         [ 6.9296e-02,  1.1446e-01,  1.0007e-01]],\n",
       "               \n",
       "                        [[-2.8678e-02, -2.0243e-02, -2.5346e-02],\n",
       "                         [-5.8157e-02, -1.5913e-02, -2.3890e-02],\n",
       "                         [ 6.0083e-03, -2.9671e-02, -3.1314e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5507e-02, -1.0065e-02,  3.0881e-02],\n",
       "                         [ 3.1270e-03, -3.6857e-02,  1.6174e-02],\n",
       "                         [-4.4302e-02, -1.9763e-02, -2.2433e-02]],\n",
       "               \n",
       "                        [[ 4.3493e-02,  8.6395e-02,  3.7174e-02],\n",
       "                         [ 2.9114e-02,  2.4833e-02, -1.4103e-02],\n",
       "                         [ 3.0108e-02,  8.4913e-02,  2.5513e-02]],\n",
       "               \n",
       "                        [[ 5.9403e-03,  9.7578e-03, -2.9943e-02],\n",
       "                         [ 1.4497e-03,  2.5015e-02, -1.4867e-02],\n",
       "                         [ 3.0806e-02,  3.3726e-02,  1.3615e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.0357,  0.0166,  0.0038, -0.0068, -0.0011, -0.0061, -0.0103, -0.0034,\n",
       "                       -0.0166,  0.0006,  0.0040, -0.0227, -0.0109, -0.0218, -0.0041,  0.0159,\n",
       "                        0.0249,  0.0085,  0.0003, -0.0096, -0.0051,  0.0175, -0.0161,  0.0572,\n",
       "                       -0.0335,  0.0107,  0.0247,  0.0042,  0.0377, -0.0171, -0.1015, -0.0228,\n",
       "                       -0.0087, -0.0797, -0.0014,  0.0367, -0.0072, -0.0147,  0.0137, -0.0163,\n",
       "                       -0.0089,  0.0034, -0.0174, -0.0188, -0.0105,  0.0368, -0.0123, -0.0034,\n",
       "                        0.0002,  0.0054, -0.0139,  0.0104, -0.0051, -0.0017,  0.0151, -0.0027,\n",
       "                        0.0474,  0.0014,  0.0009,  0.0025, -0.0011, -0.0130,  0.0087, -0.0031,\n",
       "                       -0.0616, -0.0055, -0.0170, -0.0149, -0.0131, -0.0219, -0.0087,  0.0045,\n",
       "                        0.0055,  0.0209, -0.0233, -0.0038, -0.0206, -0.0121,  0.0124, -0.0020,\n",
       "                       -0.0793,  0.0078,  0.0091, -0.0271,  0.0183, -0.0040, -0.0276, -0.0200,\n",
       "                       -0.0018, -0.0120,  0.0062,  0.0035, -0.0193, -0.0043,  0.0060, -0.0033,\n",
       "                        0.0979,  0.0046,  0.0230, -0.0159,  0.0214,  0.0229, -0.0133,  0.0055,\n",
       "                        0.0117, -0.0413,  0.0519, -0.0069, -0.0121,  0.0104,  0.0603,  0.0050,\n",
       "                       -0.0129, -0.0009,  0.0235,  0.0443, -0.0130, -0.0091, -0.0486,  0.0067,\n",
       "                       -0.0302,  0.0049, -0.0119, -0.0003,  0.0028, -0.0006, -0.0140, -0.0030],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([ 0.0080, -0.1051, -0.0583, -0.2114, -0.0156, -0.3025, -0.1431, -0.1690,\n",
       "                       -0.1414, -0.1600, -0.0668, -0.2989, -0.3124, -0.2248, -0.1399, -0.2922,\n",
       "                       -0.1752, -0.2401, -0.0152, -0.2591, -0.2447, -0.1701, -0.1470, -0.2224,\n",
       "                       -0.0362, -0.2455, -0.1750, -0.0396, -0.2098, -0.1359, -0.3048, -0.0343,\n",
       "                        0.0020, -0.1888, -0.2021, -0.0549, -0.1270, -0.1581, -0.1407, -0.0158,\n",
       "                       -0.0772, -0.0932, -0.1724, -0.2669, -0.1154, -0.2852, -0.1887, -0.0445,\n",
       "                       -0.1951, -0.1330, -0.0634, -0.0719, -0.0925, -0.0487, -0.1158, -0.0116,\n",
       "                       -0.3001, -0.2889, -0.1159, -0.0192, -0.1558, -0.1853, -0.1915, -0.1553,\n",
       "                       -0.1455, -0.2951, -0.2013,  0.0113, -0.0357, -0.2242, -0.0636, -0.3184,\n",
       "                       -0.2581, -0.1903, -0.0680, -0.1121, -0.2960, -0.1810, -0.0241, -0.2858,\n",
       "                       -0.0688, -0.1451, -0.0308, -0.1205, -0.3220, -0.3186, -0.0910, -0.1209,\n",
       "                       -0.0116, -0.1675, -0.0689, -0.1094, -0.1885, -0.1492,  0.0643, -0.2327,\n",
       "                       -0.1868, -0.1117, -0.1298, -0.1076, -0.2204, -0.2048, -0.1689, -0.1174,\n",
       "                       -0.1029, -0.1313, -0.2718, -0.1771, -0.0555, -0.1106, -0.2870, -0.0997,\n",
       "                       -0.1625, -0.1487, -0.3023, -0.2512, -0.2352, -0.0299, -0.0183, -0.0160,\n",
       "                       -0.2046, -0.2446, -0.2953, -0.1546, -0.1913, -0.0767, -0.0154, -0.0886],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([1.3516, 1.1140, 1.1335, 1.0717, 1.3636, 0.7596, 1.1063, 1.2343, 1.0519,\n",
       "                       1.5841, 1.0131, 1.0762, 1.1994, 1.4031, 1.1075, 0.9533, 1.0498, 1.0185,\n",
       "                       1.1805, 1.0876, 0.7843, 1.0940, 1.0523, 0.8384, 1.0683, 0.7602, 0.9357,\n",
       "                       1.2673, 0.9396, 1.0553, 0.9735, 1.3504, 1.2847, 1.0209, 1.1235, 1.0936,\n",
       "                       1.1686, 0.9953, 1.0376, 1.3279, 1.3441, 1.0842, 1.2170, 1.0637, 1.1351,\n",
       "                       1.0148, 1.0655, 1.2049, 1.2501, 1.0279, 1.1874, 1.2345, 1.1056, 1.6950,\n",
       "                       1.0704, 1.3799, 0.9915, 1.0065, 1.1655, 1.3675, 1.2508, 0.9559, 0.8257,\n",
       "                       1.2192, 1.0724, 1.2390, 0.9456, 1.2447, 1.2510, 0.8256, 1.2509, 0.9981,\n",
       "                       1.2127, 1.0307, 1.1309, 1.1541, 1.0804, 1.2754, 1.1647, 0.7533, 1.2607,\n",
       "                       1.0583, 1.2901, 0.9747, 0.7526, 1.0185, 1.1555, 1.1468, 1.3335, 1.1562,\n",
       "                       1.3142, 1.2976, 1.2818, 1.0301, 1.4043, 0.9505, 0.9716, 1.0872, 1.1282,\n",
       "                       1.0112, 0.9712, 0.9378, 1.1418, 1.2569, 1.0719, 1.2371, 0.8701, 1.1466,\n",
       "                       1.2526, 1.2469, 0.7931, 1.2396, 1.0986, 1.1581, 0.7296, 0.8709, 1.0038,\n",
       "                       1.3437, 1.2107, 1.2521, 0.9667, 1.2116, 0.9963, 1.1073, 1.0488, 1.2081,\n",
       "                       1.1719, 1.4677], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-4.9095e-03, -1.0411e-02,  5.6891e-03,  ..., -4.5805e-03,\n",
       "                         3.6054e-03,  7.6953e-04],\n",
       "                       [-1.1041e-02, -4.2250e-03,  1.1841e-02,  ..., -7.0961e-03,\n",
       "                         1.0293e-02, -1.3303e-02],\n",
       "                       [ 1.2086e-02,  3.3581e-03,  1.2667e-05,  ..., -2.6210e-03,\n",
       "                         6.9780e-03, -2.0183e-02],\n",
       "                       [-1.2930e-02, -5.7542e-03,  8.0124e-03,  ..., -9.6367e-03,\n",
       "                         2.3009e-04, -4.6410e-03],\n",
       "                       [-7.4690e-03, -9.8778e-03,  1.0700e-02,  ..., -4.4659e-03,\n",
       "                         9.0888e-03, -5.3043e-03]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0318,  0.0158,  0.0506, -0.1405,  0.1046], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.481154550075531,\n",
       "   1.3577819538116456,\n",
       "   1.3073870079517365,\n",
       "   1.2560416444540023,\n",
       "   1.2139344412088393,\n",
       "   1.1738374639749527,\n",
       "   1.1513489400148391,\n",
       "   1.1156441736221314,\n",
       "   1.1000408185720443,\n",
       "   1.1022559398412703,\n",
       "   1.0600120342969894,\n",
       "   1.0610931652784348,\n",
       "   1.0509943833351136,\n",
       "   1.0297512702941896,\n",
       "   1.0259157729148864,\n",
       "   1.0170646781921386,\n",
       "   1.0025434201955796,\n",
       "   0.9979545515775681,\n",
       "   0.9981109420061112,\n",
       "   0.983209489941597,\n",
       "   0.9851244906187058,\n",
       "   0.9585923129320144,\n",
       "   0.9533718949556351,\n",
       "   0.9537534577846527,\n",
       "   0.9565720618963242,\n",
       "   0.9413936160802842,\n",
       "   0.9310720965862275,\n",
       "   0.9261977025270463,\n",
       "   0.927066831946373,\n",
       "   0.9329067448377609,\n",
       "   0.912413406252861,\n",
       "   0.9153839558362961,\n",
       "   0.8964474000930787,\n",
       "   0.8877046661376953,\n",
       "   0.8930694156885147,\n",
       "   0.8728554663658142,\n",
       "   0.8791505703926087,\n",
       "   0.8727824296951294,\n",
       "   0.876187061548233,\n",
       "   0.8546114403605461,\n",
       "   0.8619972977638245,\n",
       "   0.8517441082596778,\n",
       "   0.8415768438577652,\n",
       "   0.8492840473651886,\n",
       "   0.843542504310608,\n",
       "   0.8249881924390793,\n",
       "   0.8268159502148629,\n",
       "   0.8218312242031097,\n",
       "   0.8166390725374222,\n",
       "   0.8181124683618546,\n",
       "   0.8208748137354851,\n",
       "   0.8045872502923012,\n",
       "   0.8046765422224998,\n",
       "   0.7958034241199493,\n",
       "   0.8077864516973495,\n",
       "   0.7975693554878235,\n",
       "   0.790548490345478,\n",
       "   0.7801121383309364,\n",
       "   0.7803864649534226,\n",
       "   0.7814394291639328,\n",
       "   0.7828030265569687,\n",
       "   0.7666941681504249,\n",
       "   0.7606288375258445,\n",
       "   0.7601050273180008,\n",
       "   0.7594932028055191,\n",
       "   0.7595013257265091,\n",
       "   0.7552100366353989,\n",
       "   0.7360448144674301,\n",
       "   0.7414670512080193,\n",
       "   0.7494295625090599,\n",
       "   0.7371460669636727,\n",
       "   0.7371414548754692,\n",
       "   0.7387767916321755,\n",
       "   0.7346621882319451,\n",
       "   0.729343874335289,\n",
       "   0.7154981326460839,\n",
       "   0.7241853189468384,\n",
       "   0.7200521076917649,\n",
       "   0.7168885003328324,\n",
       "   0.7137106335759162,\n",
       "   0.704644744694233,\n",
       "   0.712933917939663,\n",
       "   0.7101412235498429,\n",
       "   0.6976962233185768,\n",
       "   0.6835261573195457,\n",
       "   0.6959152084589004,\n",
       "   0.6946925252676011,\n",
       "   0.6936626036167145,\n",
       "   0.6855437688231468,\n",
       "   0.6757366685271263,\n",
       "   0.6853866739869118,\n",
       "   0.6831811745166778,\n",
       "   0.669134651184082,\n",
       "   0.674112578690052,\n",
       "   0.6681912115812302,\n",
       "   0.6822096589207649,\n",
       "   0.6632646207809448,\n",
       "   0.6614208935499192,\n",
       "   0.6562968887090683],\n",
       "  'train_loss_std': [0.1826323852296664,\n",
       "   0.1304803816692784,\n",
       "   0.13473082044900705,\n",
       "   0.1264148518281296,\n",
       "   0.1309927541645151,\n",
       "   0.13041765483289183,\n",
       "   0.13595038986421248,\n",
       "   0.1282007766301599,\n",
       "   0.125523515884704,\n",
       "   0.12920474467392193,\n",
       "   0.13149506488701052,\n",
       "   0.14458368235504027,\n",
       "   0.13563396875998307,\n",
       "   0.1340685289431707,\n",
       "   0.1269818281368983,\n",
       "   0.13682861508570374,\n",
       "   0.13270618698630288,\n",
       "   0.13596521509053586,\n",
       "   0.13223582941469023,\n",
       "   0.14225810632161273,\n",
       "   0.13308502067756578,\n",
       "   0.12679021754557895,\n",
       "   0.1413390589588845,\n",
       "   0.1359445235222156,\n",
       "   0.13791269515262905,\n",
       "   0.14443757268196517,\n",
       "   0.13392421180560674,\n",
       "   0.13951165945190877,\n",
       "   0.13877931301643015,\n",
       "   0.1358298267473773,\n",
       "   0.13501797264604687,\n",
       "   0.1349164184967577,\n",
       "   0.13695523673551566,\n",
       "   0.1451483073167463,\n",
       "   0.13982310347214305,\n",
       "   0.14314590149939144,\n",
       "   0.1363050076063552,\n",
       "   0.14002422585038526,\n",
       "   0.14759149039856237,\n",
       "   0.14450472539140388,\n",
       "   0.13188158767524813,\n",
       "   0.1350245778423253,\n",
       "   0.12905263484849128,\n",
       "   0.13764389183831624,\n",
       "   0.13881268250807685,\n",
       "   0.1354287122386498,\n",
       "   0.13985902688178561,\n",
       "   0.13178799312143952,\n",
       "   0.1342801378552925,\n",
       "   0.13451919669575796,\n",
       "   0.12829729237562648,\n",
       "   0.14118260486342377,\n",
       "   0.1308724887873948,\n",
       "   0.13919309404204383,\n",
       "   0.14872840812009974,\n",
       "   0.12976774577852346,\n",
       "   0.12731262007723013,\n",
       "   0.13881869922137072,\n",
       "   0.13462943655004367,\n",
       "   0.13770271167993803,\n",
       "   0.13847298443940023,\n",
       "   0.140252588668378,\n",
       "   0.1382651704000421,\n",
       "   0.13250643508855578,\n",
       "   0.13567018807668704,\n",
       "   0.14114834201387394,\n",
       "   0.13345703945506426,\n",
       "   0.13655771799823885,\n",
       "   0.12798300171630722,\n",
       "   0.1373892975461676,\n",
       "   0.12939987675503944,\n",
       "   0.1371566591935981,\n",
       "   0.1380250530490947,\n",
       "   0.13158649704415568,\n",
       "   0.13477428405938588,\n",
       "   0.13420257553948292,\n",
       "   0.1407853177902386,\n",
       "   0.14061222669207435,\n",
       "   0.13677513237132138,\n",
       "   0.13573551411253723,\n",
       "   0.13190207288199612,\n",
       "   0.132529215541455,\n",
       "   0.13839148092956385,\n",
       "   0.130502342266507,\n",
       "   0.1302888656064939,\n",
       "   0.1315539211756657,\n",
       "   0.12965038840868331,\n",
       "   0.13434221155013185,\n",
       "   0.13612430655629618,\n",
       "   0.13094215138925933,\n",
       "   0.13315066921480567,\n",
       "   0.13470950451381783,\n",
       "   0.1279483889866256,\n",
       "   0.13657829005808028,\n",
       "   0.12884267597012028,\n",
       "   0.14187728639938374,\n",
       "   0.12587805437370733,\n",
       "   0.13249774895751423,\n",
       "   0.13068641180660404],\n",
       "  'train_accuracy_mean': [0.4261333337724209,\n",
       "   0.4493066667318344,\n",
       "   0.4682266671061516,\n",
       "   0.4951066664457321,\n",
       "   0.5127066660523415,\n",
       "   0.5336266663074494,\n",
       "   0.5442933322191238,\n",
       "   0.5627199993133545,\n",
       "   0.5695866670012474,\n",
       "   0.565373331964016,\n",
       "   0.588639999628067,\n",
       "   0.5888799980282784,\n",
       "   0.5933466667532921,\n",
       "   0.601453332722187,\n",
       "   0.6016533324122428,\n",
       "   0.603493331849575,\n",
       "   0.6135866670608521,\n",
       "   0.6145866674780845,\n",
       "   0.6122266656756401,\n",
       "   0.6214533323645591,\n",
       "   0.6191599994301796,\n",
       "   0.6309333310723305,\n",
       "   0.6338266662359238,\n",
       "   0.6329333322644234,\n",
       "   0.6336533327102661,\n",
       "   0.6381600015163421,\n",
       "   0.6439333313703537,\n",
       "   0.6469600001573562,\n",
       "   0.6427999994754792,\n",
       "   0.6396266660690307,\n",
       "   0.6505066667199135,\n",
       "   0.6491199991106987,\n",
       "   0.6559199989438057,\n",
       "   0.6606533325314522,\n",
       "   0.6594800000190735,\n",
       "   0.6673466663360595,\n",
       "   0.6655200003981591,\n",
       "   0.6673333329558373,\n",
       "   0.6650399996638298,\n",
       "   0.6748266662359238,\n",
       "   0.6705600000619888,\n",
       "   0.6772666668891907,\n",
       "   0.6799200012087822,\n",
       "   0.6772000007033349,\n",
       "   0.6801466667056084,\n",
       "   0.6869466667175292,\n",
       "   0.686026665687561,\n",
       "   0.6902533336877823,\n",
       "   0.6920133324265481,\n",
       "   0.6886266642808914,\n",
       "   0.68794666659832,\n",
       "   0.6986266648769379,\n",
       "   0.6940266666412354,\n",
       "   0.699613334774971,\n",
       "   0.6942933332920075,\n",
       "   0.7006933341026306,\n",
       "   0.7033600001335144,\n",
       "   0.7074799988865852,\n",
       "   0.7057733334302903,\n",
       "   0.7049866656064987,\n",
       "   0.7036266678571701,\n",
       "   0.7119333344697952,\n",
       "   0.7138933347463607,\n",
       "   0.715853333234787,\n",
       "   0.7141999994516373,\n",
       "   0.7140400002002716,\n",
       "   0.7168533338308334,\n",
       "   0.725626667380333,\n",
       "   0.7227066665887832,\n",
       "   0.7193200001716614,\n",
       "   0.725413333773613,\n",
       "   0.7234800004959107,\n",
       "   0.723253332734108,\n",
       "   0.7253733327388764,\n",
       "   0.7260933326482772,\n",
       "   0.7316666649580001,\n",
       "   0.7288933338522912,\n",
       "   0.7306133338212967,\n",
       "   0.7303333345651627,\n",
       "   0.7314266653060914,\n",
       "   0.735506667137146,\n",
       "   0.7332399996519089,\n",
       "   0.7349199990034103,\n",
       "   0.7404933340549469,\n",
       "   0.7425599994659424,\n",
       "   0.7415066667795182,\n",
       "   0.7392666659355164,\n",
       "   0.7413466668128967,\n",
       "   0.7460399987697601,\n",
       "   0.746693333029747,\n",
       "   0.7457199994325637,\n",
       "   0.7445866672992706,\n",
       "   0.7506400004625321,\n",
       "   0.7486000003814697,\n",
       "   0.7513066667318344,\n",
       "   0.7477466658353805,\n",
       "   0.7541600004434585,\n",
       "   0.7561466666460037,\n",
       "   0.7561733330488205],\n",
       "  'train_accuracy_std': [0.06591361322123487,\n",
       "   0.06227793756495563,\n",
       "   0.06835308367811276,\n",
       "   0.06538203616534438,\n",
       "   0.06831468476400906,\n",
       "   0.06832798748275302,\n",
       "   0.07193369262052633,\n",
       "   0.06729224365959229,\n",
       "   0.06510100054305314,\n",
       "   0.06479930119070763,\n",
       "   0.06795959040083484,\n",
       "   0.07165900098586855,\n",
       "   0.06741513048744034,\n",
       "   0.0680922502859073,\n",
       "   0.06673413008510871,\n",
       "   0.06553351161599054,\n",
       "   0.06747923416971312,\n",
       "   0.06650268395691467,\n",
       "   0.0683710270020473,\n",
       "   0.07152263884476014,\n",
       "   0.06612652072070295,\n",
       "   0.06206032981381363,\n",
       "   0.0703668251657764,\n",
       "   0.06648789418471994,\n",
       "   0.06728701246625754,\n",
       "   0.07155660559517912,\n",
       "   0.066334473167138,\n",
       "   0.06834229755388486,\n",
       "   0.06930627723079129,\n",
       "   0.06537307174004116,\n",
       "   0.06784433944616584,\n",
       "   0.06691306983620103,\n",
       "   0.0668841311562931,\n",
       "   0.069106165892124,\n",
       "   0.06901591305095696,\n",
       "   0.06915219765764169,\n",
       "   0.0647862887261049,\n",
       "   0.06623258767813986,\n",
       "   0.0718419459488728,\n",
       "   0.07169497800973891,\n",
       "   0.06508761424464551,\n",
       "   0.0657706116016513,\n",
       "   0.06296801911217323,\n",
       "   0.06590080222099384,\n",
       "   0.06840842025300041,\n",
       "   0.06627492772510031,\n",
       "   0.06543352288977157,\n",
       "   0.06403907092916886,\n",
       "   0.06840104002154876,\n",
       "   0.06458743119987016,\n",
       "   0.06211642687526532,\n",
       "   0.06738927239924632,\n",
       "   0.06253929844881151,\n",
       "   0.06659951138714797,\n",
       "   0.07149553758333171,\n",
       "   0.06295719321740238,\n",
       "   0.06128475736175209,\n",
       "   0.06637372883572769,\n",
       "   0.06555880956323262,\n",
       "   0.0679099067738081,\n",
       "   0.06434354386270719,\n",
       "   0.06817295001884771,\n",
       "   0.06506917563911233,\n",
       "   0.061723078532587426,\n",
       "   0.0640697890322431,\n",
       "   0.0658114347887569,\n",
       "   0.06567976711477352,\n",
       "   0.06569548135653552,\n",
       "   0.061942325345967345,\n",
       "   0.06522289835833038,\n",
       "   0.06401411539999399,\n",
       "   0.06662449371062536,\n",
       "   0.06412274824441129,\n",
       "   0.06401123695914206,\n",
       "   0.06404671884218813,\n",
       "   0.06506594910096727,\n",
       "   0.06656206531730184,\n",
       "   0.06710573129074256,\n",
       "   0.06630150024425562,\n",
       "   0.06420529746841241,\n",
       "   0.06424250044563738,\n",
       "   0.06427503455762233,\n",
       "   0.06539770887594931,\n",
       "   0.06324433107517077,\n",
       "   0.06368971393939064,\n",
       "   0.06288010494075127,\n",
       "   0.06242716640396464,\n",
       "   0.06472409529633813,\n",
       "   0.06565402924516457,\n",
       "   0.0637313745094133,\n",
       "   0.06218461308944313,\n",
       "   0.06251565245503438,\n",
       "   0.06131912101622321,\n",
       "   0.06498748440065961,\n",
       "   0.062471889729398905,\n",
       "   0.06593726125482527,\n",
       "   0.059265363121513114,\n",
       "   0.06267656471033763,\n",
       "   0.061985131336572095],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.471176369190216,\n",
       "   1.4203386040528616,\n",
       "   1.3806963209311167,\n",
       "   1.3533262141545614,\n",
       "   1.3228069829940796,\n",
       "   1.2923235789934795,\n",
       "   1.2658109680811565,\n",
       "   1.2479879687229791,\n",
       "   1.22375756641229,\n",
       "   1.2312382558981578,\n",
       "   1.2062380532423655,\n",
       "   1.201440934141477,\n",
       "   1.1956076474984487,\n",
       "   1.193373558918635,\n",
       "   1.1765884820620218,\n",
       "   1.17311465660731,\n",
       "   1.187868253191312,\n",
       "   1.1736191300551098,\n",
       "   1.1559246160586676,\n",
       "   1.1491659792264302,\n",
       "   1.1453562692801158,\n",
       "   1.1545843807856242,\n",
       "   1.1417370017369588,\n",
       "   1.1338486299912134,\n",
       "   1.1240334182977676,\n",
       "   1.1281356239318847,\n",
       "   1.1178909635543823,\n",
       "   1.1150680551926295,\n",
       "   1.103643309076627,\n",
       "   1.1039624404907227,\n",
       "   1.0983565264940263,\n",
       "   1.0944552268584569,\n",
       "   1.0878954009215036,\n",
       "   1.0826534553368887,\n",
       "   1.084435091416041,\n",
       "   1.0796767693758011,\n",
       "   1.0691722273826598,\n",
       "   1.075462252298991,\n",
       "   1.0627307752768198,\n",
       "   1.069868769844373,\n",
       "   1.058972169359525,\n",
       "   1.0591596617301304,\n",
       "   1.056293647289276,\n",
       "   1.0516797292232514,\n",
       "   1.0464338860909144,\n",
       "   1.04096601943175,\n",
       "   1.0390473582347235,\n",
       "   1.0437529691060383,\n",
       "   1.0289837962388992,\n",
       "   1.0326292578379312,\n",
       "   1.0349435208241144,\n",
       "   1.03252683142821,\n",
       "   1.0310525498787562,\n",
       "   1.0104590525229773,\n",
       "   1.0139344509442647,\n",
       "   1.0106956521670023,\n",
       "   1.032102631131808,\n",
       "   1.0090903135140736,\n",
       "   1.0105778209368388,\n",
       "   1.0125532406568527,\n",
       "   1.0054045470555624,\n",
       "   0.9964819796880087,\n",
       "   0.9941920403639476,\n",
       "   0.9932256497939428,\n",
       "   0.9877218504746755,\n",
       "   0.9872963384787241,\n",
       "   0.9900266283750534,\n",
       "   0.9877198950449626,\n",
       "   0.9829322874546051,\n",
       "   0.9856233690182368,\n",
       "   0.9900106473763783,\n",
       "   0.9901331992944081,\n",
       "   0.9795691275596619,\n",
       "   0.9728402330478032,\n",
       "   0.9765372564395268,\n",
       "   0.9615845106045405,\n",
       "   0.9775967270135879,\n",
       "   0.9674115592241287,\n",
       "   0.9670706542332967,\n",
       "   0.9565932681163152,\n",
       "   0.9645783323049545,\n",
       "   0.954222569068273,\n",
       "   0.9548276960849762,\n",
       "   0.9615878440936406,\n",
       "   0.9601044146219889,\n",
       "   0.9499993185202281,\n",
       "   0.951361569960912,\n",
       "   0.9399580576022466,\n",
       "   0.9446669640143712,\n",
       "   0.9545165171225866,\n",
       "   0.9450459994872411,\n",
       "   0.9451093810796738,\n",
       "   0.9568006992340088,\n",
       "   0.9583671281735102,\n",
       "   0.9470258406798044,\n",
       "   0.9410540425777435,\n",
       "   0.9342471953233084,\n",
       "   0.9384942634900411,\n",
       "   0.9439093967278799],\n",
       "  'val_loss_std': [0.11203740130864129,\n",
       "   0.10715929112252705,\n",
       "   0.09827341826141245,\n",
       "   0.10125932640351137,\n",
       "   0.10043295801446125,\n",
       "   0.10740659524183253,\n",
       "   0.11049734687425121,\n",
       "   0.11983441107245077,\n",
       "   0.11930227996812569,\n",
       "   0.11529669617151428,\n",
       "   0.11472055706061447,\n",
       "   0.11461747819445803,\n",
       "   0.1188223925285219,\n",
       "   0.12176546787710572,\n",
       "   0.11885147128588246,\n",
       "   0.11581106846402765,\n",
       "   0.11909995483656824,\n",
       "   0.12196800945397059,\n",
       "   0.12398465967775586,\n",
       "   0.12429102030429559,\n",
       "   0.12626138369322082,\n",
       "   0.12399722233293942,\n",
       "   0.12768290815715796,\n",
       "   0.12657912390755227,\n",
       "   0.1270241844349698,\n",
       "   0.12868818947660365,\n",
       "   0.1277192766153749,\n",
       "   0.1314605584614048,\n",
       "   0.13090726697788443,\n",
       "   0.1326811614541339,\n",
       "   0.12793874582377707,\n",
       "   0.1295033536529318,\n",
       "   0.13508594559952292,\n",
       "   0.1256389745632251,\n",
       "   0.1276599462085666,\n",
       "   0.13297767868085042,\n",
       "   0.13163224099730259,\n",
       "   0.1336244294053933,\n",
       "   0.13202423489457363,\n",
       "   0.1308216289214086,\n",
       "   0.1334729536791329,\n",
       "   0.13540187290704866,\n",
       "   0.1331894646012306,\n",
       "   0.13424498999450782,\n",
       "   0.13375660790981328,\n",
       "   0.13291695676870965,\n",
       "   0.13276573331422253,\n",
       "   0.13617518701132503,\n",
       "   0.13315697343223637,\n",
       "   0.131602270458581,\n",
       "   0.13268724730948828,\n",
       "   0.133248200070377,\n",
       "   0.1334498839268254,\n",
       "   0.13341375022411908,\n",
       "   0.1332544375230059,\n",
       "   0.13522877668616448,\n",
       "   0.13646125097249184,\n",
       "   0.13827714189175325,\n",
       "   0.13632873247995536,\n",
       "   0.1397483090422933,\n",
       "   0.14107534466995766,\n",
       "   0.13191791023486107,\n",
       "   0.13609270302166718,\n",
       "   0.1373436265325103,\n",
       "   0.13782754732955282,\n",
       "   0.13881340803437905,\n",
       "   0.13607888968501025,\n",
       "   0.13811387686912519,\n",
       "   0.13513214094033638,\n",
       "   0.13763618903021407,\n",
       "   0.14120659189166673,\n",
       "   0.13886795139057656,\n",
       "   0.13668628195830615,\n",
       "   0.13594511421080996,\n",
       "   0.1339807457042434,\n",
       "   0.13636625945424435,\n",
       "   0.14274674896351155,\n",
       "   0.13894214011758702,\n",
       "   0.13304951145628094,\n",
       "   0.13509201511513172,\n",
       "   0.14027035249475067,\n",
       "   0.13705290740066123,\n",
       "   0.13767863220396992,\n",
       "   0.1387881382663321,\n",
       "   0.13586516452145064,\n",
       "   0.14269873294848856,\n",
       "   0.13649695558837954,\n",
       "   0.13155420116093455,\n",
       "   0.13704813390960915,\n",
       "   0.14400514639874926,\n",
       "   0.14107233424250729,\n",
       "   0.14060849573919304,\n",
       "   0.14711724698209286,\n",
       "   0.1410907367007103,\n",
       "   0.13930975593546616,\n",
       "   0.14175003598064614,\n",
       "   0.14033359981714238,\n",
       "   0.13476484911151734,\n",
       "   0.13750004130782806],\n",
       "  'val_accuracy_mean': [0.4045111114283403,\n",
       "   0.41722222248713176,\n",
       "   0.4324888893961906,\n",
       "   0.4456444451212883,\n",
       "   0.45973333438237507,\n",
       "   0.47355555643637975,\n",
       "   0.485844445625941,\n",
       "   0.49588888804117837,\n",
       "   0.5084666676322619,\n",
       "   0.5044666656851768,\n",
       "   0.5158888885378837,\n",
       "   0.5172444433967273,\n",
       "   0.5201333321134249,\n",
       "   0.5220222216844559,\n",
       "   0.5298222202062607,\n",
       "   0.53033333192269,\n",
       "   0.5256666652361552,\n",
       "   0.5339777773618698,\n",
       "   0.5385111107428868,\n",
       "   0.5424444432059924,\n",
       "   0.543244443833828,\n",
       "   0.5373111102978388,\n",
       "   0.5459999985496203,\n",
       "   0.550755555431048,\n",
       "   0.553444446225961,\n",
       "   0.5526666683952014,\n",
       "   0.5570666654904683,\n",
       "   0.5590888892610868,\n",
       "   0.5632888871431351,\n",
       "   0.563711110452811,\n",
       "   0.5653777765234311,\n",
       "   0.568422221938769,\n",
       "   0.5713999994595845,\n",
       "   0.5731777773300807,\n",
       "   0.5676222208142281,\n",
       "   0.5737999984622002,\n",
       "   0.5795555543899537,\n",
       "   0.5761999988555908,\n",
       "   0.5801555547118187,\n",
       "   0.5775555542111397,\n",
       "   0.584933332502842,\n",
       "   0.584577779173851,\n",
       "   0.5832666645447413,\n",
       "   0.5870222210884094,\n",
       "   0.5891777774691582,\n",
       "   0.5897111116846403,\n",
       "   0.5926888887087504,\n",
       "   0.5913777764638265,\n",
       "   0.5966444445649782,\n",
       "   0.5947777767976125,\n",
       "   0.593177777826786,\n",
       "   0.5961777770519257,\n",
       "   0.5967333329717318,\n",
       "   0.6039111100633939,\n",
       "   0.5999999997019768,\n",
       "   0.6032444436351458,\n",
       "   0.5947777771949768,\n",
       "   0.6045777769883474,\n",
       "   0.6029333333174388,\n",
       "   0.6017555550734202,\n",
       "   0.6071777763962746,\n",
       "   0.6112444439530372,\n",
       "   0.6100222223997116,\n",
       "   0.6110222214460372,\n",
       "   0.6153333310286204,\n",
       "   0.6140222209692001,\n",
       "   0.6119555546840032,\n",
       "   0.6158888883392016,\n",
       "   0.6154888861378034,\n",
       "   0.6144222230712573,\n",
       "   0.612022221883138,\n",
       "   0.6133555539449056,\n",
       "   0.6153111106157303,\n",
       "   0.6194888902703921,\n",
       "   0.616088885863622,\n",
       "   0.6242444427808126,\n",
       "   0.6175777745246888,\n",
       "   0.6214222213625908,\n",
       "   0.6218444431821505,\n",
       "   0.6247555573781332,\n",
       "   0.622711109717687,\n",
       "   0.6277111102143923,\n",
       "   0.6264444435636203,\n",
       "   0.6237333337465922,\n",
       "   0.6238444447517395,\n",
       "   0.6310888887445132,\n",
       "   0.6279999990264574,\n",
       "   0.6320666670799255,\n",
       "   0.6319333316882452,\n",
       "   0.630444445113341,\n",
       "   0.6305555550257365,\n",
       "   0.6290666659673055,\n",
       "   0.6284222219387691,\n",
       "   0.6274222214023272,\n",
       "   0.6303555554151535,\n",
       "   0.6347555540998777,\n",
       "   0.6359333327412605,\n",
       "   0.6359555553396543,\n",
       "   0.6325333312153816],\n",
       "  'val_accuracy_std': [0.05353799162002322,\n",
       "   0.0524720981914053,\n",
       "   0.05174179064930801,\n",
       "   0.0545594439699906,\n",
       "   0.05402814078556591,\n",
       "   0.05838780569726321,\n",
       "   0.058177808417806676,\n",
       "   0.059355695041157855,\n",
       "   0.061534849233953345,\n",
       "   0.05942979616153935,\n",
       "   0.06035072230962436,\n",
       "   0.057592111503469996,\n",
       "   0.05651335922892677,\n",
       "   0.06100806981501532,\n",
       "   0.06108741345146006,\n",
       "   0.05955669406457232,\n",
       "   0.058520333169778685,\n",
       "   0.06090453432575346,\n",
       "   0.06073382762738213,\n",
       "   0.0630527898408561,\n",
       "   0.06264413916069356,\n",
       "   0.0613480611002943,\n",
       "   0.06201911090369558,\n",
       "   0.0629110434808522,\n",
       "   0.062440392068660824,\n",
       "   0.06351319518242336,\n",
       "   0.06256483259522035,\n",
       "   0.06216751088082387,\n",
       "   0.06503216901734982,\n",
       "   0.063530729283033,\n",
       "   0.06337751299008496,\n",
       "   0.06272361094956662,\n",
       "   0.06447941994903779,\n",
       "   0.06387816796777328,\n",
       "   0.06152727250719634,\n",
       "   0.06408735869920205,\n",
       "   0.06256571892982157,\n",
       "   0.06175642809294948,\n",
       "   0.06404262086135958,\n",
       "   0.06290223682003454,\n",
       "   0.0639579955819006,\n",
       "   0.06262474690427144,\n",
       "   0.06504126291026241,\n",
       "   0.06563552119824073,\n",
       "   0.06647131003645682,\n",
       "   0.06358946703232453,\n",
       "   0.06259830473072665,\n",
       "   0.06436952090747526,\n",
       "   0.06365764528204357,\n",
       "   0.06382953621904496,\n",
       "   0.06245690339964687,\n",
       "   0.06149903542250107,\n",
       "   0.06350382894868392,\n",
       "   0.064204242829744,\n",
       "   0.06414104917256028,\n",
       "   0.06333795125922861,\n",
       "   0.06269611338101945,\n",
       "   0.06368271627047645,\n",
       "   0.06318921597259596,\n",
       "   0.0649081370904879,\n",
       "   0.0637927615753104,\n",
       "   0.06103617205479102,\n",
       "   0.06279065355775032,\n",
       "   0.06318924604310566,\n",
       "   0.06395484523229095,\n",
       "   0.06394186781302506,\n",
       "   0.06258764194025813,\n",
       "   0.06308949158158543,\n",
       "   0.06251712203552993,\n",
       "   0.06230509942086057,\n",
       "   0.06541423204172483,\n",
       "   0.061592445558287426,\n",
       "   0.06380363086714479,\n",
       "   0.06302408409497127,\n",
       "   0.06400549391932762,\n",
       "   0.06282414847505599,\n",
       "   0.0629419452437504,\n",
       "   0.06382191338091253,\n",
       "   0.061903194602919105,\n",
       "   0.061412204716973555,\n",
       "   0.06305833634416222,\n",
       "   0.06295163999330895,\n",
       "   0.06317485019100758,\n",
       "   0.06443362498009035,\n",
       "   0.06333829041430022,\n",
       "   0.06522325974022641,\n",
       "   0.06261463607075052,\n",
       "   0.060682747250622435,\n",
       "   0.06417772774643522,\n",
       "   0.06386289312281059,\n",
       "   0.06522998233880463,\n",
       "   0.06278136106601999,\n",
       "   0.06413203697901643,\n",
       "   0.06247979930146581,\n",
       "   0.06308154220281362,\n",
       "   0.06139410959916963,\n",
       "   0.06245279278670863,\n",
       "   0.06198397255791851,\n",
       "   0.06265919627346102],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6818444436788559,\n",
       " 'best_val_iter': 48000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 96,\n",
       " 'train_loss_mean': 0.4526471059322357,\n",
       " 'train_loss_std': 0.12993412983496969,\n",
       " 'train_accuracy_mean': 0.8343333342075347,\n",
       " 'train_accuracy_std': 0.0548158550248203,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.8508474173148474,\n",
       " 'val_loss_std': 0.14179470314582376,\n",
       " 'val_accuracy_mean': 0.6763111112515131,\n",
       " 'val_accuracy_std': 0.05848475377685013,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0115, -0.0778,  0.0524],\n",
       "                         [-0.0305, -0.0349, -0.0020],\n",
       "                         [-0.0158,  0.1034, -0.0275]],\n",
       "               \n",
       "                        [[ 0.0519, -0.0845,  0.0649],\n",
       "                         [-0.0253,  0.0089,  0.0542],\n",
       "                         [-0.0484,  0.0403, -0.0331]],\n",
       "               \n",
       "                        [[ 0.0545, -0.0248, -0.0204],\n",
       "                         [ 0.0301,  0.0602, -0.0543],\n",
       "                         [-0.0532,  0.0362, -0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0481,  0.1008,  0.0398],\n",
       "                         [ 0.0482, -0.0425,  0.0113],\n",
       "                         [-0.0911, -0.0673, -0.0469]],\n",
       "               \n",
       "                        [[-0.0092,  0.0265, -0.0018],\n",
       "                         [ 0.0446, -0.0628, -0.0604],\n",
       "                         [ 0.0496, -0.0558,  0.0711]],\n",
       "               \n",
       "                        [[-0.0320,  0.0804, -0.0688],\n",
       "                         [-0.0160,  0.0558,  0.0703],\n",
       "                         [-0.0326, -0.0326, -0.0280]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0071, -0.0199,  0.0549],\n",
       "                         [-0.0223, -0.0425, -0.0309],\n",
       "                         [ 0.0645,  0.0541, -0.0505]],\n",
       "               \n",
       "                        [[-0.0577, -0.0244,  0.0483],\n",
       "                         [ 0.0787, -0.0048,  0.0167],\n",
       "                         [-0.0691, -0.0398,  0.0441]],\n",
       "               \n",
       "                        [[ 0.0377, -0.0742,  0.0330],\n",
       "                         [ 0.0672,  0.0122, -0.0616],\n",
       "                         [-0.0667,  0.0326,  0.0182]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0424, -0.0622, -0.0432],\n",
       "                         [-0.0184, -0.0636,  0.0860],\n",
       "                         [-0.0358,  0.0483, -0.0402]],\n",
       "               \n",
       "                        [[ 0.0383,  0.0228,  0.0691],\n",
       "                         [ 0.0292, -0.0177, -0.0221],\n",
       "                         [ 0.0055,  0.0494, -0.0077]],\n",
       "               \n",
       "                        [[-0.0062, -0.0760, -0.0401],\n",
       "                         [-0.0594,  0.0223,  0.0496],\n",
       "                         [-0.0420,  0.0299,  0.0492]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0190, -0.0077, -0.0359],\n",
       "                         [ 0.0073,  0.0689, -0.0534],\n",
       "                         [ 0.0444,  0.0745, -0.0492]],\n",
       "               \n",
       "                        [[ 0.0429,  0.0223,  0.0724],\n",
       "                         [ 0.0197, -0.0301, -0.0356],\n",
       "                         [-0.0315, -0.0832,  0.0298]],\n",
       "               \n",
       "                        [[-0.0250, -0.0121, -0.0183],\n",
       "                         [-0.0136, -0.0266,  0.0769],\n",
       "                         [-0.0608, -0.0201,  0.0196]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0678,  0.0252, -0.0223],\n",
       "                         [-0.0489,  0.0072,  0.0349],\n",
       "                         [ 0.0568,  0.0435,  0.0784]],\n",
       "               \n",
       "                        [[-0.0682, -0.0912,  0.0311],\n",
       "                         [ 0.0605, -0.0843, -0.0335],\n",
       "                         [ 0.0032,  0.0237,  0.0296]],\n",
       "               \n",
       "                        [[ 0.0041, -0.0576,  0.0435],\n",
       "                         [ 0.0096, -0.0407, -0.0082],\n",
       "                         [-0.0129, -0.0269,  0.0120]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.0765e-03, -8.0369e-04, -1.8382e-03, -1.1685e-04, -1.5081e-04,\n",
       "                        9.4584e-04,  1.9420e-04,  1.0867e-04,  8.7550e-04, -1.9117e-03,\n",
       "                       -3.8360e-04,  1.2847e-04,  2.6250e-03,  1.7375e-03, -2.0646e-03,\n",
       "                        7.6395e-04, -6.3794e-04, -6.1944e-04, -7.8609e-04, -2.0221e-04,\n",
       "                        1.6218e-03, -2.4587e-04,  2.3333e-03,  2.2809e-03,  3.7578e-04,\n",
       "                        2.2750e-05,  6.9353e-04,  7.8915e-04,  1.0253e-03,  9.2878e-04,\n",
       "                        1.5839e-03, -6.1639e-05,  3.5597e-04, -3.5442e-03, -4.6407e-04,\n",
       "                        6.9834e-04,  1.1986e-03,  6.3443e-04, -1.2857e-05, -8.4554e-04,\n",
       "                        2.7600e-04, -3.0869e-03, -1.1622e-03,  2.0713e-04, -3.6686e-04,\n",
       "                       -7.5862e-05,  2.7620e-04, -1.3584e-03, -2.4717e-03, -1.9970e-05,\n",
       "                        1.1759e-03, -8.4054e-04,  1.2545e-03, -2.8829e-03,  1.1732e-03,\n",
       "                        1.6122e-04,  5.0176e-04,  3.3543e-04, -6.3797e-05, -1.7757e-03,\n",
       "                        2.5158e-04,  1.5920e-03, -5.9205e-04,  7.7316e-04,  3.4142e-04,\n",
       "                        7.5710e-04, -6.7782e-04,  1.7540e-03, -8.1071e-04, -5.7699e-04,\n",
       "                        1.4908e-04, -1.1079e-03, -1.0642e-05, -2.4850e-04,  1.3602e-03,\n",
       "                       -5.1398e-04,  8.3879e-04,  8.4650e-04, -1.5811e-04,  3.1155e-04,\n",
       "                        1.4772e-03, -2.1250e-03,  1.2379e-03,  9.4745e-05, -2.6874e-03,\n",
       "                       -8.9463e-04,  1.0693e-03, -1.5491e-03,  1.8957e-04,  2.8946e-03,\n",
       "                        1.1849e-03,  2.1482e-04,  2.0873e-04, -7.8655e-04,  1.2980e-03,\n",
       "                       -2.6352e-03,  1.9492e-03, -1.8718e-03, -1.6439e-03, -3.3162e-04,\n",
       "                        3.8143e-04, -7.6442e-04,  1.1236e-03, -1.1091e-04, -2.0625e-03,\n",
       "                        1.1619e-03, -4.9918e-04,  1.9502e-06, -1.0099e-03, -8.2773e-04,\n",
       "                        1.4017e-03,  1.5289e-03, -1.8522e-03, -2.8503e-04,  3.0935e-04,\n",
       "                        1.6916e-04, -2.2714e-04, -1.6422e-03,  4.0964e-04, -2.4318e-04,\n",
       "                       -6.1455e-05,  9.1467e-05,  9.6760e-04,  4.0069e-04,  1.5191e-03,\n",
       "                        1.3727e-04,  8.4586e-04, -1.2893e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1601, -0.0269,  0.2146, -0.1223, -0.1164,  0.0143, -0.1938, -0.2474,\n",
       "                       -0.1682, -0.1138, -0.1828, -0.1827,  0.0486, -0.1432, -0.0289, -0.1198,\n",
       "                        0.0172, -0.1315, -0.0307, -0.0695, -0.1784, -0.0808, -0.1548,  0.0150,\n",
       "                       -0.0257, -0.1688, -0.0726, -0.0996, -0.0490,  0.0397, -0.1129, -0.1187,\n",
       "                       -0.1021, -0.1569, -0.1652, -0.2654, -0.1424, -0.0779, -0.1773, -0.1431,\n",
       "                       -0.0401,  0.1688, -0.1125, -0.0631, -0.0056, -0.1462, -0.1993, -0.2671,\n",
       "                        0.1400, -0.1451,  0.0623, -0.2133,  0.2319,  0.0191, -0.0483, -0.0743,\n",
       "                       -0.2520, -0.0044, -0.1727,  0.1165, -0.1224,  0.2687, -0.0037, -0.2654,\n",
       "                       -0.1504, -0.2399, -0.1681, -0.1425, -0.0929, -0.0455,  0.0674, -0.0151,\n",
       "                       -0.1439, -0.2008, -0.0738, -0.0622, -0.1589, -0.0268, -0.0954, -0.1174,\n",
       "                        0.0008, -0.1366, -0.2038, -0.2098, -0.0300,  0.1649, -0.0133, -0.1394,\n",
       "                       -0.1591,  0.0642, -0.0768, -0.1414,  0.0458, -0.0098, -0.0927,  0.0210,\n",
       "                        0.1638,  0.0666, -0.1137, -0.1096, -0.2115, -0.0579,  0.0081,  0.0286,\n",
       "                        0.0164, -0.1907, -0.0867, -0.1441, -0.0205, -0.0744, -0.0806, -0.1129,\n",
       "                       -0.0395, -0.1456, -0.0760, -0.0752, -0.0798,  0.1828, -0.0423,  0.0638,\n",
       "                       -0.1928, -0.2187, -0.1924, -0.1746,  0.1367, -0.0291, -0.2259, -0.1710],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.0777, 1.0709, 1.0801, 0.8988, 0.9437, 1.0149, 0.9128, 0.8826, 0.9809,\n",
       "                       0.9149, 0.9107, 0.9115, 1.0526, 0.8928, 1.0546, 1.0420, 1.0268, 0.9431,\n",
       "                       0.9681, 0.9646, 0.9394, 0.8664, 0.9175, 1.0001, 0.9011, 0.8854, 0.9461,\n",
       "                       0.9155, 0.9748, 0.9375, 1.0159, 0.9061, 1.0190, 1.2193, 0.9277, 0.9384,\n",
       "                       0.9600, 0.9206, 0.9119, 0.9612, 0.9499, 1.0274, 0.9329, 0.9598, 1.0849,\n",
       "                       1.0767, 0.9834, 0.9287, 1.1041, 0.9253, 1.0169, 0.9134, 1.0635, 1.0854,\n",
       "                       1.0065, 0.9070, 1.1188, 1.1279, 0.9739, 1.0818, 0.9082, 1.0035, 0.9521,\n",
       "                       0.9313, 0.9319, 0.8501, 0.8868, 0.9298, 1.0671, 0.9991, 0.9329, 0.9704,\n",
       "                       0.8722, 0.8916, 0.9964, 0.8846, 0.9151, 1.0916, 1.0985, 1.0259, 1.0244,\n",
       "                       0.9868, 0.8907, 0.9007, 1.1403, 1.1089, 1.0677, 1.0212, 0.9175, 0.9868,\n",
       "                       1.0047, 0.9299, 1.0141, 1.0329, 0.9991, 1.0203, 1.1726, 0.9629, 0.9522,\n",
       "                       0.9195, 0.8685, 1.0409, 1.1071, 0.9493, 0.9616, 0.9556, 0.9301, 0.8482,\n",
       "                       1.0131, 0.9587, 1.0176, 1.0275, 0.9890, 1.1160, 0.9377, 1.0863, 0.9138,\n",
       "                       1.0243, 0.9302, 0.9634, 0.8600, 0.8900, 1.2006, 1.0526, 1.0483, 0.9347,\n",
       "                       0.9339, 0.8952], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-3.1068e-02,  1.4934e-02, -2.4708e-02],\n",
       "                         [-4.9840e-02, -2.8395e-02, -7.1771e-02],\n",
       "                         [-2.5563e-02,  2.5884e-02, -1.4416e-02]],\n",
       "               \n",
       "                        [[ 7.1535e-02,  3.6242e-02, -8.3086e-02],\n",
       "                         [-5.5571e-02, -6.0254e-03, -3.9204e-02],\n",
       "                         [-2.6580e-02,  7.6422e-02,  2.6242e-02]],\n",
       "               \n",
       "                        [[ 7.9697e-02,  1.2020e-02, -8.9087e-04],\n",
       "                         [-2.4565e-02,  6.1961e-02,  8.5690e-02],\n",
       "                         [ 1.0954e-02,  5.8338e-02,  8.1456e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3707e-04, -3.0362e-02, -3.2934e-02],\n",
       "                         [-5.1523e-02, -6.6658e-02, -3.6795e-02],\n",
       "                         [ 7.4546e-03,  1.6017e-03, -1.5633e-02]],\n",
       "               \n",
       "                        [[ 1.1219e-02,  3.6779e-02, -7.3502e-03],\n",
       "                         [-9.8202e-03,  2.9635e-02, -1.4038e-02],\n",
       "                         [ 3.2854e-02, -5.6267e-02, -1.8176e-02]],\n",
       "               \n",
       "                        [[-1.0652e-02,  4.8453e-02, -3.1925e-02],\n",
       "                         [-2.8970e-02,  1.5542e-02, -4.6588e-03],\n",
       "                         [-4.3812e-02, -2.1815e-02,  4.8127e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4941e-02, -3.0805e-02,  3.8058e-02],\n",
       "                         [ 3.1757e-02,  3.3032e-02, -4.8846e-02],\n",
       "                         [ 3.0079e-02, -1.2053e-03, -3.8244e-02]],\n",
       "               \n",
       "                        [[ 3.1452e-02,  6.4167e-02, -2.9219e-03],\n",
       "                         [-4.8711e-02,  4.5315e-04,  2.8953e-02],\n",
       "                         [-1.8084e-02, -3.7616e-02,  6.2568e-03]],\n",
       "               \n",
       "                        [[ 6.1893e-02,  2.1873e-02, -4.9432e-02],\n",
       "                         [-1.2321e-02, -1.1952e-03, -2.0872e-02],\n",
       "                         [-2.9053e-02, -1.8110e-02, -4.7141e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1403e-02,  5.6295e-03,  4.0007e-02],\n",
       "                         [-4.6823e-02, -2.6105e-02, -2.8252e-02],\n",
       "                         [-1.2773e-02,  1.2540e-02, -1.2665e-02]],\n",
       "               \n",
       "                        [[-2.8528e-02, -7.3125e-02, -6.5685e-02],\n",
       "                         [ 1.5529e-02,  4.3157e-03,  4.4955e-03],\n",
       "                         [ 1.7821e-02, -4.4237e-02,  1.1965e-02]],\n",
       "               \n",
       "                        [[-1.4344e-02, -6.4940e-02,  1.5923e-02],\n",
       "                         [ 2.7153e-02, -2.8147e-02, -7.4200e-04],\n",
       "                         [-1.2954e-02, -3.2091e-02,  2.8604e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.7807e-02, -2.8579e-02, -4.7183e-02],\n",
       "                         [-2.5427e-03, -3.0682e-03, -4.7967e-02],\n",
       "                         [ 3.1416e-02, -1.7803e-02, -1.8947e-03]],\n",
       "               \n",
       "                        [[-3.5335e-03,  2.9050e-02, -5.2048e-02],\n",
       "                         [ 2.8188e-02,  1.9930e-02, -2.0456e-02],\n",
       "                         [ 3.0739e-02,  4.1309e-02, -4.7110e-02]],\n",
       "               \n",
       "                        [[-3.3564e-02, -4.0410e-02, -8.6391e-02],\n",
       "                         [-2.9952e-02, -1.4456e-02,  2.4784e-02],\n",
       "                         [-4.7339e-02, -4.5708e-02,  7.1623e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.2363e-02,  1.3501e-02, -1.5803e-04],\n",
       "                         [-1.9702e-02, -4.1223e-02, -1.4040e-02],\n",
       "                         [-2.1804e-02, -3.4117e-02, -4.1499e-02]],\n",
       "               \n",
       "                        [[ 5.0175e-02,  4.8530e-02,  6.1841e-02],\n",
       "                         [-5.2386e-02, -3.6055e-02, -2.1579e-02],\n",
       "                         [-6.1513e-02, -3.6027e-03, -4.1555e-02]],\n",
       "               \n",
       "                        [[-3.5838e-02, -6.2272e-02, -8.0042e-03],\n",
       "                         [-3.8722e-02,  7.4427e-03,  1.1058e-02],\n",
       "                         [-1.4307e-02, -3.2467e-02, -3.4531e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3628e-02,  2.7285e-03,  1.5521e-02],\n",
       "                         [ 6.4739e-02, -1.6291e-02,  2.1665e-02],\n",
       "                         [ 6.1771e-02, -5.1059e-02, -4.8901e-02]],\n",
       "               \n",
       "                        [[-2.6963e-02,  1.9296e-02,  5.3938e-02],\n",
       "                         [-5.3194e-02, -1.1404e-02,  1.8829e-02],\n",
       "                         [-7.9276e-02, -2.1864e-02, -3.8196e-02]],\n",
       "               \n",
       "                        [[ 8.4808e-02,  6.7533e-02,  6.2033e-02],\n",
       "                         [ 6.5936e-02, -2.6663e-02,  7.5278e-03],\n",
       "                         [ 2.4147e-02, -4.6637e-02, -4.0440e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.3273e-03,  1.0368e-02,  2.9113e-02],\n",
       "                         [ 4.7940e-02, -2.6338e-02,  3.7946e-02],\n",
       "                         [ 3.2950e-02,  4.6247e-02,  2.5710e-02]],\n",
       "               \n",
       "                        [[-9.7459e-03,  2.0474e-02,  4.2825e-02],\n",
       "                         [ 1.6884e-02,  4.6780e-03, -1.5560e-02],\n",
       "                         [-6.3945e-02,  6.8826e-03,  3.1620e-02]],\n",
       "               \n",
       "                        [[ 1.1923e-02, -2.2587e-02, -1.0285e-03],\n",
       "                         [-6.0087e-03,  9.3974e-05,  8.8612e-03],\n",
       "                         [ 8.7517e-03,  4.7585e-03, -3.5720e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.3675e-02, -4.8650e-02,  1.3389e-02],\n",
       "                         [-4.8180e-02, -1.0838e-01, -1.8075e-02],\n",
       "                         [-6.6564e-02, -4.1220e-02, -3.8812e-02]],\n",
       "               \n",
       "                        [[ 4.3953e-03,  5.0149e-02,  6.3318e-02],\n",
       "                         [ 4.8209e-02,  2.4266e-02,  2.1014e-02],\n",
       "                         [ 1.6416e-02, -3.3700e-02, -4.2817e-02]],\n",
       "               \n",
       "                        [[-2.8837e-02,  2.7409e-03,  2.8331e-02],\n",
       "                         [-5.9724e-03, -5.8118e-02, -7.0318e-02],\n",
       "                         [-6.0833e-02, -5.2656e-02, -5.2772e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.0133e-02,  3.9366e-03,  4.7938e-02],\n",
       "                         [-1.2157e-02,  1.9478e-02, -2.6585e-02],\n",
       "                         [-2.0719e-02, -2.5532e-02, -4.1482e-02]],\n",
       "               \n",
       "                        [[ 2.7662e-02,  5.0188e-02,  5.1921e-02],\n",
       "                         [-7.6517e-03, -5.1657e-02, -2.7490e-02],\n",
       "                         [-2.9292e-02, -3.8551e-02, -2.9929e-03]],\n",
       "               \n",
       "                        [[-2.6770e-02, -3.3468e-02, -3.3695e-02],\n",
       "                         [-5.5176e-02, -2.4818e-02, -5.8880e-02],\n",
       "                         [-1.5056e-02, -6.2207e-02, -4.8363e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.1598e-03, -4.3907e-02,  1.5628e-02],\n",
       "                         [ 1.6393e-02,  3.9933e-02, -4.7980e-03],\n",
       "                         [-9.2348e-03,  8.1859e-02,  3.9698e-02]],\n",
       "               \n",
       "                        [[-3.7246e-02,  1.7502e-02,  3.7163e-02],\n",
       "                         [ 5.7703e-02, -2.4650e-02, -5.0248e-03],\n",
       "                         [-2.8848e-02, -1.8945e-02,  4.7855e-02]],\n",
       "               \n",
       "                        [[ 1.3576e-03, -4.7517e-02,  3.5737e-02],\n",
       "                         [ 1.5863e-02,  3.9620e-02,  9.4381e-03],\n",
       "                         [ 5.0647e-04, -2.3891e-02,  3.4428e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9621e-02, -5.0091e-03, -5.7347e-02],\n",
       "                         [ 6.7124e-02,  8.4320e-02, -2.3866e-02],\n",
       "                         [ 1.6802e-02,  4.7377e-02,  3.4077e-03]],\n",
       "               \n",
       "                        [[-5.5406e-02, -5.9962e-02, -3.6674e-02],\n",
       "                         [ 3.1510e-02,  4.7255e-02,  1.2530e-03],\n",
       "                         [-8.1551e-03,  1.8949e-02, -5.1895e-02]],\n",
       "               \n",
       "                        [[-1.0334e-02,  2.3689e-02, -2.3868e-02],\n",
       "                         [-4.3318e-03,  2.3800e-02,  3.7886e-02],\n",
       "                         [ 2.9135e-02, -6.1549e-03, -1.9589e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 5.9933e-08, -6.1479e-05,  1.1136e-06,  1.1191e-06,  2.5666e-07,\n",
       "                        1.5369e-03, -1.1775e-06, -6.2422e-07, -1.1607e-06, -8.1503e-04,\n",
       "                       -1.8291e-06, -2.3342e-06, -2.2181e-04,  6.1000e-07, -2.3925e-06,\n",
       "                       -3.0629e-07, -3.1087e-06,  8.1563e-07,  9.3452e-08, -5.7189e-04,\n",
       "                       -3.2095e-07, -9.1869e-09,  4.8346e-06, -1.7036e-05,  1.3274e-02,\n",
       "                       -1.3903e-05,  1.6652e-05, -2.7412e-05, -3.4676e-03,  5.0787e-06,\n",
       "                        2.6765e-06,  4.6576e-07,  4.0706e-06,  1.7675e-05, -8.4356e-07,\n",
       "                        2.6364e-06,  4.4227e-09, -1.2458e-05,  2.1915e-05,  4.9682e-07,\n",
       "                        7.0539e-07, -1.6904e-06,  2.0231e-05, -5.5559e-06,  7.0082e-07,\n",
       "                       -3.0102e-06, -1.5954e-04, -3.8790e-06, -1.2362e-06, -3.6231e-06,\n",
       "                        7.5977e-07, -4.2942e-07,  1.9629e-06, -1.0215e-02, -4.5511e-07,\n",
       "                        9.4783e-03, -3.0247e-03, -1.9373e-05,  5.5702e-05, -1.3319e-02,\n",
       "                       -9.0035e-07, -1.1355e-06,  3.0399e-06,  2.3563e-03,  3.7166e-07,\n",
       "                        1.0655e-02,  7.9245e-06,  3.9385e-07, -2.3954e-06,  6.9563e-07,\n",
       "                       -9.1358e-07,  1.7789e-06, -1.0133e-06, -9.4920e-05, -3.5892e-05,\n",
       "                        2.6434e-06, -3.6071e-06,  2.4756e-06,  2.6890e-06,  7.2722e-05,\n",
       "                        2.2625e-07, -9.3507e-07,  2.5611e-06,  1.9175e-06, -6.8006e-06,\n",
       "                        3.2866e-07,  3.1739e-06, -7.8642e-03, -4.1726e-06,  4.7427e-07,\n",
       "                        3.0530e-06,  1.0925e-02,  2.6354e-07,  1.7425e-06, -1.6997e-06,\n",
       "                       -2.4809e-06, -7.8697e-07,  1.2117e-02, -1.6563e-06,  1.2712e-06,\n",
       "                        4.4987e-07,  5.1062e-06,  1.0757e-02, -1.8630e-06,  7.0586e-08,\n",
       "                        5.5715e-07, -1.2244e-04,  1.5775e-05,  1.1858e-02,  9.6294e-03,\n",
       "                       -1.0860e-06,  2.7695e-05, -3.0826e-05, -1.0798e-02,  3.5214e-08,\n",
       "                       -4.0514e-07,  1.3645e-02,  2.0043e-05,  1.9055e-04, -8.7982e-03,\n",
       "                       -1.0212e-06, -1.3086e-05, -2.2918e-08, -2.2165e-05, -1.8928e-06,\n",
       "                       -1.2180e-05, -1.3773e-02, -7.5184e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0872, -0.1572, -0.1664, -0.2343, -0.1549, -0.1571, -0.1078, -0.1666,\n",
       "                       -0.1076, -0.1716, -0.3466, -0.2546, -0.1173, -0.2540, -0.1237, -0.1846,\n",
       "                       -0.2605, -0.0961, -0.0841, -0.2577, -0.1466, -0.1738, -0.2186, -0.2139,\n",
       "                       -0.2324, -0.1326, -0.1120, -0.1670, -0.1783, -0.2671, -0.0948, -0.3858,\n",
       "                       -0.2320, -0.0944, -0.2653, -0.2115, -0.2098, -0.0639, -0.3093, -0.2235,\n",
       "                       -0.1407, -0.1924, -0.2923, -0.1656, -0.2135, -0.2605, -0.1603, -0.1156,\n",
       "                       -0.1902, -0.2313, -0.0166, -0.1515, -0.3205, -0.2022, -0.0888, -0.1692,\n",
       "                       -0.2692, -0.1670, -0.2828, -0.2521, -0.0130, -0.2669, -0.1723, -0.2151,\n",
       "                       -0.1083, -0.0798, -0.1164, -0.2391, -0.1477, -0.1890, -0.2052, -0.0476,\n",
       "                       -0.1590, -0.1589, -0.2383, -0.0808, -0.2007, -0.1882, -0.1486, -0.1086,\n",
       "                       -0.1875, -0.2214, -0.0991,  0.0091, -0.2162, -0.2209, -0.0630, -0.2359,\n",
       "                       -0.1235, -0.1990, -0.2295, -0.2036, -0.2071, -0.2148, -0.1414, -0.0935,\n",
       "                       -0.1874, -0.1651, -0.1903, -0.1393, -0.1805, -0.1963, -0.1423, -0.2635,\n",
       "                       -0.2242, -0.2068, -0.1134, -0.2680, -0.1862, -0.1696, -0.2529, -0.2078,\n",
       "                       -0.1584, -0.2204, -0.2384, -0.1047, -0.1335, -0.2205, -0.1528, -0.2487,\n",
       "                       -0.1355, -0.1146, -0.1673, -0.0136, -0.1558, -0.2546, -0.1811, -0.1617],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.0451, 0.9745, 1.0224, 0.9569, 0.9861, 1.0045, 1.0278, 1.0634, 1.0389,\n",
       "                       1.0216, 0.9842, 1.1099, 1.0157, 0.9824, 1.0965, 1.0307, 0.8674, 0.9835,\n",
       "                       1.0076, 0.9665, 0.9861, 0.9705, 1.0266, 1.0917, 0.9736, 1.0308, 0.9726,\n",
       "                       0.9838, 0.9616, 1.0102, 0.9881, 1.0250, 0.9429, 0.9924, 0.9120, 1.0300,\n",
       "                       0.9982, 1.0233, 0.8428, 0.9264, 0.9836, 0.9378, 1.0010, 0.9307, 1.0222,\n",
       "                       0.9822, 1.0209, 0.9528, 1.0514, 0.9630, 1.0344, 1.0062, 0.9767, 0.9748,\n",
       "                       0.9827, 0.9348, 0.9054, 1.0206, 1.0095, 1.0173, 1.0312, 1.0670, 0.9866,\n",
       "                       0.9704, 0.9777, 0.9320, 0.9356, 0.9721, 0.9230, 1.0510, 0.9052, 1.0517,\n",
       "                       0.9739, 0.9824, 1.0422, 1.0663, 0.9386, 0.9604, 1.0494, 0.9874, 1.0360,\n",
       "                       0.9525, 1.0250, 0.9620, 0.9574, 1.0049, 1.0482, 0.9686, 0.9571, 0.9879,\n",
       "                       0.9952, 0.9969, 0.9531, 0.9737, 0.9241, 1.0071, 0.9846, 0.9989, 1.0562,\n",
       "                       0.9546, 0.9243, 1.0043, 0.9716, 1.0233, 1.0340, 1.0055, 0.9923, 0.9698,\n",
       "                       1.0110, 0.9383, 0.9961, 1.0366, 0.9825, 1.0127, 0.8616, 0.9989, 1.0212,\n",
       "                       0.9751, 0.9882, 1.0225, 0.9810, 1.0087, 0.9795, 1.0189, 0.9698, 1.0015,\n",
       "                       1.0015, 1.0593], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-3.5635e-02,  6.6883e-02,  1.6516e-02],\n",
       "                         [-2.0717e-02,  4.5892e-03, -7.0361e-03],\n",
       "                         [-4.0502e-03, -5.3581e-02, -6.3822e-03]],\n",
       "               \n",
       "                        [[ 2.4649e-03, -2.7850e-02,  9.0671e-03],\n",
       "                         [-4.3892e-02,  2.0201e-02, -2.6540e-02],\n",
       "                         [-6.6467e-04,  1.1619e-02, -5.7028e-02]],\n",
       "               \n",
       "                        [[ 4.4089e-03, -2.2262e-02,  1.7012e-02],\n",
       "                         [ 6.2686e-02,  2.4853e-02, -3.6876e-02],\n",
       "                         [-1.6284e-02,  2.1929e-02, -1.3400e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5534e-02, -3.8601e-02, -6.5782e-02],\n",
       "                         [ 2.6658e-02,  4.9235e-03,  4.0978e-02],\n",
       "                         [ 1.1825e-03, -2.5821e-02, -1.8333e-02]],\n",
       "               \n",
       "                        [[-2.6654e-02, -5.5350e-02,  6.5079e-03],\n",
       "                         [-6.1993e-04, -2.8721e-02, -2.6483e-02],\n",
       "                         [ 2.8866e-02,  1.8139e-02,  4.3261e-03]],\n",
       "               \n",
       "                        [[-3.5204e-03,  5.2470e-02,  3.2148e-02],\n",
       "                         [ 1.7759e-02,  1.0482e-02, -2.4699e-02],\n",
       "                         [ 1.1681e-02,  3.0295e-02,  2.6309e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.6161e-02,  1.5252e-02, -2.2819e-02],\n",
       "                         [ 1.0151e-02, -4.9394e-03, -3.1928e-02],\n",
       "                         [-7.3092e-03, -1.6657e-02,  1.1107e-02]],\n",
       "               \n",
       "                        [[-3.2528e-02,  3.4879e-02,  5.5768e-02],\n",
       "                         [ 2.8912e-02,  8.1838e-02,  7.0347e-02],\n",
       "                         [-5.9509e-02, -3.2777e-02,  1.6557e-03]],\n",
       "               \n",
       "                        [[ 1.0216e-02,  4.4529e-02, -8.3797e-02],\n",
       "                         [ 2.7144e-02, -1.2044e-02, -7.0979e-02],\n",
       "                         [ 2.4772e-02, -3.1583e-02, -4.7915e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.8065e-03,  5.4751e-02, -9.6700e-03],\n",
       "                         [-3.1591e-03, -3.3694e-02, -7.3251e-02],\n",
       "                         [ 7.2923e-05,  4.5793e-03, -5.0973e-02]],\n",
       "               \n",
       "                        [[ 5.8034e-03, -1.8566e-02,  9.1351e-03],\n",
       "                         [-5.1339e-02, -2.9027e-02, -1.2111e-02],\n",
       "                         [-2.9039e-02,  1.3079e-02,  3.4591e-02]],\n",
       "               \n",
       "                        [[ 6.6866e-02, -2.1931e-02, -4.6208e-02],\n",
       "                         [ 1.5051e-02,  3.2506e-02, -1.3522e-02],\n",
       "                         [-1.3544e-02,  3.2223e-02, -5.6732e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.1565e-03, -1.2166e-02, -6.4260e-02],\n",
       "                         [-4.6180e-02, -8.3588e-02, -3.3062e-02],\n",
       "                         [-3.1706e-02, -5.0750e-02, -5.2977e-02]],\n",
       "               \n",
       "                        [[ 1.6955e-03,  1.4892e-03, -2.2856e-02],\n",
       "                         [ 3.9290e-03,  6.5597e-02, -3.6062e-02],\n",
       "                         [-3.7413e-02,  3.7076e-02, -3.6557e-02]],\n",
       "               \n",
       "                        [[ 9.4688e-02, -1.3011e-02,  7.1782e-02],\n",
       "                         [ 1.4776e-02,  7.5849e-02, -1.0182e-02],\n",
       "                         [-5.7237e-03,  2.7586e-02,  4.5924e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.3358e-03,  4.7901e-02, -1.3634e-02],\n",
       "                         [ 2.1187e-02, -3.5819e-02, -3.5869e-02],\n",
       "                         [ 1.4592e-02,  1.6706e-02,  4.2851e-02]],\n",
       "               \n",
       "                        [[-2.2170e-02, -5.4084e-02, -2.6914e-04],\n",
       "                         [ 3.5542e-02, -5.5808e-02, -6.2875e-02],\n",
       "                         [-2.2901e-03, -3.7785e-02,  2.0417e-02]],\n",
       "               \n",
       "                        [[ 1.4934e-02,  2.7684e-02,  8.1969e-02],\n",
       "                         [ 6.0429e-02,  6.1587e-02,  3.9178e-02],\n",
       "                         [-1.8586e-03, -2.7665e-03, -1.9067e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1825e-02,  4.8099e-03, -5.5348e-02],\n",
       "                         [ 5.5376e-02, -2.2714e-02, -1.8907e-02],\n",
       "                         [ 9.4701e-03,  1.6956e-02,  2.8504e-02]],\n",
       "               \n",
       "                        [[-8.7255e-03, -1.3725e-02,  3.0419e-02],\n",
       "                         [ 2.3330e-02,  3.1619e-02, -3.6503e-02],\n",
       "                         [ 1.7630e-02,  1.8091e-02, -5.2045e-03]],\n",
       "               \n",
       "                        [[ 2.6194e-02,  2.7558e-02, -5.5006e-02],\n",
       "                         [-1.1049e-02, -1.7657e-02, -2.4263e-03],\n",
       "                         [-7.5835e-03, -1.4259e-02, -4.7830e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.5579e-02,  5.8151e-02, -2.5747e-02],\n",
       "                         [ 9.2701e-02,  6.5077e-02,  3.7771e-02],\n",
       "                         [ 4.0023e-02,  1.2615e-02, -1.5215e-02]],\n",
       "               \n",
       "                        [[ 3.5414e-02, -3.3109e-02,  3.7701e-02],\n",
       "                         [-2.7612e-02, -2.7967e-02, -5.2714e-02],\n",
       "                         [-3.7227e-02,  3.3004e-02, -8.0880e-02]],\n",
       "               \n",
       "                        [[-1.6259e-02,  4.5828e-03, -7.5952e-03],\n",
       "                         [-4.2413e-02, -9.9031e-03, -3.3439e-02],\n",
       "                         [-4.7989e-02,  6.6052e-02, -1.1792e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.2100e-02,  6.1930e-02, -8.5012e-03],\n",
       "                         [-4.9399e-02, -2.1669e-02, -2.1487e-02],\n",
       "                         [-9.4657e-03,  3.6917e-02, -3.9345e-02]],\n",
       "               \n",
       "                        [[ 1.1505e-02, -3.4558e-02,  3.1325e-02],\n",
       "                         [-1.0512e-04, -2.2429e-02,  3.3865e-02],\n",
       "                         [-1.2944e-02,  3.7145e-02,  1.9752e-02]],\n",
       "               \n",
       "                        [[ 2.9329e-03,  1.7710e-02,  6.5504e-03],\n",
       "                         [ 2.7545e-02,  2.8484e-02,  2.6443e-02],\n",
       "                         [-4.8555e-03, -4.9553e-02,  1.7128e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.9563e-03, -1.2934e-02,  1.1113e-02],\n",
       "                         [-1.8983e-02,  4.7031e-02, -4.0295e-02],\n",
       "                         [-1.3619e-02, -1.9917e-02, -3.2294e-02]],\n",
       "               \n",
       "                        [[-3.2172e-02, -2.8210e-02, -4.2929e-02],\n",
       "                         [ 4.4360e-03, -5.5791e-02,  1.2082e-02],\n",
       "                         [ 4.9856e-02, -1.0001e-02, -4.5965e-02]],\n",
       "               \n",
       "                        [[ 1.4553e-02,  6.3308e-02, -2.4518e-02],\n",
       "                         [-1.3177e-03,  5.6378e-02, -6.8796e-03],\n",
       "                         [-7.8446e-03,  2.0849e-02,  1.5414e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.5729e-02, -3.0278e-02,  3.3063e-04],\n",
       "                         [-3.3246e-02, -2.2090e-02,  2.8049e-02],\n",
       "                         [-9.2174e-02, -6.1077e-02, -5.3846e-03]],\n",
       "               \n",
       "                        [[ 1.3817e-02,  1.4932e-02, -3.2316e-02],\n",
       "                         [ 2.9696e-02,  1.5589e-02, -4.2233e-02],\n",
       "                         [ 3.3876e-02,  2.2564e-02,  1.0178e-02]],\n",
       "               \n",
       "                        [[-1.2799e-02, -1.3293e-02, -3.9625e-02],\n",
       "                         [-4.9859e-02,  3.1562e-02, -1.0173e-02],\n",
       "                         [-4.0080e-02, -3.2473e-02, -3.6179e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 5.4078e-02,  7.9191e-02,  1.7750e-03],\n",
       "                         [ 4.3058e-02,  4.6749e-02, -3.7460e-02],\n",
       "                         [ 1.1480e-03, -1.3536e-02, -6.2119e-02]],\n",
       "               \n",
       "                        [[ 3.5932e-02,  4.2279e-02,  7.7663e-03],\n",
       "                         [-3.6863e-02, -1.1133e-02, -4.4179e-02],\n",
       "                         [-5.6386e-03, -1.8559e-02, -5.7912e-02]],\n",
       "               \n",
       "                        [[-7.8392e-02,  3.8048e-02,  3.1213e-02],\n",
       "                         [-5.2759e-02,  8.1980e-03, -3.1568e-02],\n",
       "                         [-7.3119e-03, -2.0015e-02, -3.6982e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-2.1351e-03,  3.4871e-03,  1.7402e-03,  3.3579e-03,  1.0868e-03,\n",
       "                        7.4822e-04,  3.1768e-03,  6.2910e-04,  6.6134e-04, -1.6108e-03,\n",
       "                       -2.7385e-03,  6.7116e-04, -9.6336e-04,  3.2656e-03,  1.8089e-03,\n",
       "                        3.7092e-04,  2.3694e-03, -1.4622e-03,  1.2253e-03,  1.7384e-03,\n",
       "                        8.2374e-04,  1.4852e-03, -1.9792e-04, -6.7408e-04,  2.5066e-03,\n",
       "                       -1.6606e-03, -7.2716e-04,  2.5755e-03, -1.5390e-03,  1.5680e-03,\n",
       "                       -1.4368e-03,  6.3476e-05, -1.6477e-03,  8.5359e-04, -1.1303e-03,\n",
       "                        1.8436e-03,  2.7225e-03, -9.7316e-04,  3.3027e-03, -4.5836e-03,\n",
       "                        1.0710e-03,  2.4210e-03,  9.4007e-04,  2.7616e-04, -2.6665e-03,\n",
       "                        1.0428e-03, -4.5181e-03, -1.0973e-03,  2.9153e-03,  1.1452e-03,\n",
       "                       -2.6050e-04,  1.7071e-04,  2.0001e-03,  1.2810e-04, -1.9158e-04,\n",
       "                       -4.1856e-03, -3.1744e-03, -7.9482e-04, -3.0404e-04,  3.5577e-03,\n",
       "                        8.4278e-04,  1.1236e-03, -5.2809e-03,  1.9397e-03, -1.0979e-03,\n",
       "                        1.9149e-03, -3.6256e-03,  1.5284e-03, -4.2810e-04, -4.3189e-03,\n",
       "                        7.6343e-04, -1.4412e-03, -1.3658e-03,  2.3325e-03,  2.2334e-03,\n",
       "                        1.4130e-03,  3.4702e-03,  8.2384e-04, -5.8385e-04, -2.3952e-03,\n",
       "                        3.0020e-03, -2.1871e-03, -2.9691e-03, -7.2833e-04, -3.2114e-06,\n",
       "                       -4.5352e-04,  1.9773e-03, -1.5028e-03,  4.9686e-04, -5.7484e-04,\n",
       "                        4.2452e-03,  1.2303e-03, -1.5338e-03, -7.8657e-04,  2.1939e-03,\n",
       "                        3.3835e-03,  3.8203e-03,  3.1000e-03, -3.5266e-03,  2.6436e-03,\n",
       "                        9.1690e-04, -3.8383e-03,  5.2958e-04, -6.9434e-05,  1.7816e-03,\n",
       "                        4.8468e-03,  6.3447e-04,  2.1172e-03,  1.7666e-03, -2.1690e-03,\n",
       "                       -2.0607e-03, -3.0691e-03,  4.9682e-04, -5.3011e-04, -3.4542e-03,\n",
       "                       -1.0074e-03,  2.9607e-04, -2.5081e-03, -1.1018e-03, -3.7656e-04,\n",
       "                       -3.8288e-04,  8.6251e-04, -4.0439e-04,  7.3346e-04, -1.4695e-03,\n",
       "                       -5.3210e-05,  2.3575e-03, -2.5863e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.2745, -0.3432, -0.3225, -0.3161, -0.3458, -0.2965, -0.2656, -0.4619,\n",
       "                       -0.3528, -0.3486, -0.3127, -0.4352, -0.2927, -0.3775, -0.3179, -0.2020,\n",
       "                       -0.2585, -0.2389, -0.4363, -0.3397, -0.2641, -0.3822, -0.2232, -0.3648,\n",
       "                       -0.2792, -0.3051, -0.2956, -0.2948, -0.2251, -0.2499, -0.3535, -0.2657,\n",
       "                       -0.3528, -0.2497, -0.2430, -0.2566, -0.4218, -0.2970, -0.2469, -0.2932,\n",
       "                       -0.2682, -0.2900, -0.2373, -0.2746, -0.2579, -0.3706, -0.2705, -0.2992,\n",
       "                       -0.2342, -0.4011, -0.3201, -0.3366, -0.2704, -0.3693, -0.3214, -0.2929,\n",
       "                       -0.3103, -0.2991, -0.3957, -0.3168, -0.2977, -0.3472, -0.2490, -0.3007,\n",
       "                       -0.2593, -0.2996, -0.3261, -0.2612, -0.2965, -0.3719, -0.3670, -0.3033,\n",
       "                       -0.3026, -0.3622, -0.3271, -0.3308, -0.3027, -0.3465, -0.2958, -0.3535,\n",
       "                       -0.2374, -0.3401, -0.2038, -0.3748, -0.2901, -0.2589, -0.3164, -0.2999,\n",
       "                       -0.2608, -0.2464, -0.2377, -0.2692, -0.3142, -0.3819, -0.3478, -0.3193,\n",
       "                       -0.3010, -0.2188, -0.2880, -0.3890, -0.2799, -0.2614, -0.2786, -0.3609,\n",
       "                       -0.3335, -0.3494, -0.2471, -0.3684, -0.2899, -0.3292, -0.3916, -0.2884,\n",
       "                       -0.2767, -0.3124, -0.2516, -0.3787, -0.2954, -0.3044, -0.3517, -0.3989,\n",
       "                       -0.2850, -0.3227, -0.3362, -0.2426, -0.3302, -0.2835, -0.2408, -0.2675],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.8677, 0.9911, 0.9240, 0.8862, 0.9875, 0.9502, 0.9117, 1.0771, 0.9400,\n",
       "                       0.9616, 0.9689, 1.0767, 0.8404, 1.1268, 0.9214, 0.9644, 1.1083, 0.8781,\n",
       "                       1.0999, 0.9300, 0.8585, 0.9549, 0.8933, 0.9367, 0.9463, 0.9033, 0.9782,\n",
       "                       0.9583, 0.9087, 0.9267, 1.1003, 0.9585, 0.9880, 0.9270, 1.0583, 1.0271,\n",
       "                       1.0955, 0.9693, 0.9092, 0.9284, 0.8522, 0.9795, 0.9794, 0.8672, 0.9084,\n",
       "                       0.9751, 0.8588, 0.8368, 0.9086, 0.9760, 0.9658, 1.0371, 0.9347, 0.9943,\n",
       "                       0.9089, 0.9417, 0.8640, 0.9460, 0.9710, 0.8725, 0.9926, 1.0481, 1.1232,\n",
       "                       0.9393, 0.9935, 0.9556, 0.8583, 0.8728, 0.8861, 1.0107, 1.2039, 0.9133,\n",
       "                       0.8829, 1.0437, 0.9347, 0.9958, 1.0021, 0.8523, 0.9745, 0.9859, 0.9213,\n",
       "                       0.9626, 1.0658, 1.0581, 0.8953, 0.9729, 0.9728, 0.8852, 1.1389, 0.9146,\n",
       "                       0.9648, 0.8962, 1.0027, 1.0481, 0.9987, 0.9533, 0.9388, 0.9414, 0.9178,\n",
       "                       1.0637, 0.9441, 0.9439, 0.8276, 0.9136, 0.8780, 0.8991, 0.8927, 0.9721,\n",
       "                       0.9828, 0.9578, 0.8569, 1.0319, 0.9744, 1.0446, 0.9831, 0.9474, 0.9019,\n",
       "                       1.0095, 0.9606, 1.0142, 0.9484, 1.1066, 1.0178, 0.9520, 0.8968, 0.9898,\n",
       "                       0.9332, 0.8357], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-4.3702e-02, -1.3627e-02, -3.3736e-02],\n",
       "                         [ 5.9619e-02,  1.6159e-02, -2.8567e-02],\n",
       "                         [ 2.7072e-02,  1.9254e-02,  3.9468e-02]],\n",
       "               \n",
       "                        [[ 1.8669e-02, -3.7957e-02,  9.8068e-03],\n",
       "                         [ 4.1648e-03, -3.6611e-02, -4.8710e-02],\n",
       "                         [ 3.7416e-04,  2.0676e-03, -2.5078e-02]],\n",
       "               \n",
       "                        [[ 5.8284e-02,  5.1901e-03,  5.3009e-02],\n",
       "                         [ 7.2872e-04, -1.6713e-02,  3.5374e-02],\n",
       "                         [-1.0131e-02,  5.0062e-02, -1.9479e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.3767e-03, -1.2034e-03, -4.2708e-02],\n",
       "                         [ 3.1162e-02, -5.4115e-02, -1.8587e-02],\n",
       "                         [-3.1021e-04,  2.3716e-02, -1.8927e-02]],\n",
       "               \n",
       "                        [[-2.0851e-02, -4.9040e-02, -1.3251e-02],\n",
       "                         [ 1.1836e-02,  8.5989e-03,  5.9320e-02],\n",
       "                         [-1.5005e-02, -3.3563e-02, -3.0291e-02]],\n",
       "               \n",
       "                        [[ 3.2447e-02, -4.5090e-02,  3.4413e-02],\n",
       "                         [ 1.4186e-02, -8.0078e-03, -5.5526e-02],\n",
       "                         [ 2.3710e-02, -1.7888e-02, -2.6468e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3535e-03, -4.6939e-02, -3.8000e-02],\n",
       "                         [ 4.5195e-02, -6.4326e-02,  7.6383e-03],\n",
       "                         [ 7.2541e-03, -7.9630e-02, -4.7468e-02]],\n",
       "               \n",
       "                        [[ 2.0901e-02,  1.5754e-02,  2.3625e-02],\n",
       "                         [-5.4572e-02, -4.4321e-02, -4.3351e-02],\n",
       "                         [-5.6763e-02,  5.0064e-02, -1.2850e-02]],\n",
       "               \n",
       "                        [[-1.3336e-02,  3.8217e-02,  2.6076e-02],\n",
       "                         [ 3.3369e-02,  2.9154e-02,  2.0140e-02],\n",
       "                         [-2.7863e-03,  4.9059e-02,  2.7505e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.4442e-04, -6.9905e-03,  5.1513e-02],\n",
       "                         [-2.9548e-02, -4.9088e-02,  3.1979e-02],\n",
       "                         [ 3.9791e-02,  1.4876e-02, -1.2177e-02]],\n",
       "               \n",
       "                        [[-8.2850e-04,  5.1148e-02, -1.5545e-02],\n",
       "                         [-1.2785e-02,  7.3915e-02,  5.4139e-02],\n",
       "                         [ 9.3461e-02,  4.4485e-02,  3.1602e-02]],\n",
       "               \n",
       "                        [[-2.1151e-03, -4.5330e-04, -4.4565e-02],\n",
       "                         [-1.0282e-01, -4.2757e-02, -7.6484e-02],\n",
       "                         [-1.8677e-02,  1.2074e-02, -3.7740e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.0233e-03,  3.4157e-02,  4.8908e-02],\n",
       "                         [-1.3790e-02,  3.5846e-02,  5.0121e-02],\n",
       "                         [ 1.1362e-02, -1.8063e-02, -1.4542e-02]],\n",
       "               \n",
       "                        [[ 4.2676e-03, -1.4356e-02,  1.2788e-02],\n",
       "                         [-1.6214e-02,  8.6472e-03,  1.4520e-03],\n",
       "                         [ 1.4571e-02, -2.7827e-02, -3.9042e-02]],\n",
       "               \n",
       "                        [[ 4.9498e-02,  5.5938e-03, -5.5657e-03],\n",
       "                         [-5.8689e-03,  5.9269e-02,  1.8440e-02],\n",
       "                         [ 1.4232e-02,  5.8087e-02,  1.3712e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.9255e-02,  3.7683e-02, -3.5907e-02],\n",
       "                         [ 2.3164e-02, -2.1429e-02, -1.3765e-02],\n",
       "                         [ 4.2311e-03,  5.1843e-02,  4.9206e-02]],\n",
       "               \n",
       "                        [[ 7.8560e-02,  6.9009e-02,  5.1072e-02],\n",
       "                         [ 5.7993e-02,  9.7662e-02,  7.6212e-02],\n",
       "                         [-1.2178e-02,  5.2104e-02,  3.1011e-02]],\n",
       "               \n",
       "                        [[-2.1720e-02,  6.2952e-04,  3.7310e-02],\n",
       "                         [-3.9092e-03, -3.1375e-02, -3.3830e-02],\n",
       "                         [-2.1882e-02,  6.3913e-03,  2.2926e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.4090e-02, -4.3765e-02, -3.4845e-03],\n",
       "                         [ 4.8406e-02,  8.7374e-03, -6.2583e-03],\n",
       "                         [-5.6868e-02, -6.9677e-02, -8.0262e-02]],\n",
       "               \n",
       "                        [[-4.3687e-02,  2.0018e-02, -3.3008e-02],\n",
       "                         [ 1.4243e-02, -4.5496e-02, -2.0843e-02],\n",
       "                         [-3.3093e-02, -2.0732e-02,  1.6793e-02]],\n",
       "               \n",
       "                        [[-1.1730e-02,  2.7783e-02,  5.2310e-02],\n",
       "                         [ 2.8892e-02,  4.5672e-02, -3.0685e-02],\n",
       "                         [-5.3753e-02,  1.4569e-03, -7.2215e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2346e-03,  3.3736e-03, -5.3751e-02],\n",
       "                         [ 8.0512e-03, -4.7253e-03, -2.7187e-02],\n",
       "                         [ 4.6004e-02,  6.3030e-03, -1.2130e-02]],\n",
       "               \n",
       "                        [[-2.0353e-02,  9.5935e-02,  1.7028e-02],\n",
       "                         [ 6.2968e-04,  7.9693e-02, -1.6896e-03],\n",
       "                         [-4.4462e-02,  8.0178e-03, -1.1550e-04]],\n",
       "               \n",
       "                        [[-5.9322e-02,  1.6931e-02, -3.5418e-02],\n",
       "                         [-2.4703e-02,  1.8146e-03,  2.5648e-02],\n",
       "                         [-2.5140e-02, -2.5331e-02, -5.2890e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.6826e-02,  8.7188e-03, -1.4631e-02],\n",
       "                         [-4.2427e-03,  3.1592e-03,  3.4011e-02],\n",
       "                         [ 2.4498e-02, -4.3636e-03,  1.3631e-02]],\n",
       "               \n",
       "                        [[-4.8765e-03, -2.5093e-02, -4.6500e-02],\n",
       "                         [ 1.4969e-02,  4.3267e-02,  4.7689e-02],\n",
       "                         [ 3.1678e-02, -2.9005e-03,  1.8742e-02]],\n",
       "               \n",
       "                        [[-1.6282e-04,  3.7926e-02, -3.3435e-02],\n",
       "                         [-3.7151e-02, -5.9865e-03,  1.1217e-02],\n",
       "                         [-2.7224e-02,  4.4045e-02,  2.3198e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.8922e-02, -5.0440e-02, -2.4127e-02],\n",
       "                         [ 1.3196e-03, -3.4582e-02, -6.3706e-03],\n",
       "                         [-2.4766e-02,  2.4784e-02, -5.1412e-02]],\n",
       "               \n",
       "                        [[-8.5140e-02, -1.1747e-01, -1.0574e-02],\n",
       "                         [-7.6822e-02, -1.8023e-02, -1.2739e-02],\n",
       "                         [-2.4550e-02, -9.7639e-02,  5.0995e-02]],\n",
       "               \n",
       "                        [[ 1.0049e-02,  7.0355e-03, -2.8812e-02],\n",
       "                         [-6.1743e-02,  2.9907e-02, -4.9510e-02],\n",
       "                         [-1.2106e-02, -1.6349e-02, -4.7471e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.3281e-02, -5.9659e-02, -7.4845e-03],\n",
       "                         [ 3.9666e-03, -2.4579e-02, -1.6912e-02],\n",
       "                         [-3.0060e-02, -5.2557e-02, -3.4569e-02]],\n",
       "               \n",
       "                        [[ 1.1601e-02,  2.0093e-02,  1.9330e-02],\n",
       "                         [ 1.1518e-02, -3.4297e-03, -3.9855e-03],\n",
       "                         [ 1.1395e-02,  3.4817e-02,  6.0226e-03]],\n",
       "               \n",
       "                        [[ 3.0721e-02, -1.4320e-02, -2.4393e-02],\n",
       "                         [-7.5937e-02, -3.5532e-02, -2.3386e-02],\n",
       "                         [-4.9772e-02, -6.8097e-02, -2.4774e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7394e-05, -1.8980e-02,  4.6820e-02],\n",
       "                         [ 3.1611e-02, -5.9833e-02,  3.1288e-02],\n",
       "                         [-4.7890e-02, -1.8718e-02, -2.2998e-02]],\n",
       "               \n",
       "                        [[ 1.0739e-02,  4.0940e-02,  4.8134e-02],\n",
       "                         [ 4.7797e-02,  4.2867e-02, -1.0555e-02],\n",
       "                         [ 4.3696e-02,  1.1575e-01,  2.4784e-02]],\n",
       "               \n",
       "                        [[ 2.0929e-02,  5.0720e-02, -3.8524e-03],\n",
       "                         [ 6.9852e-03,  5.0116e-02, -2.8043e-02],\n",
       "                         [-1.1385e-02,  6.5274e-03,  1.4342e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.5599, -0.5552, -0.5566,  0.5535,  0.5584,  0.5569, -0.5565,  0.5590,\n",
       "                       -0.5581, -0.5547,  0.5560, -0.5532, -0.5587,  0.5597, -0.5554, -0.5580,\n",
       "                        0.5588,  0.5554, -0.5597, -0.5538,  0.5582, -0.5558, -0.5580,  0.5537,\n",
       "                        0.5585, -0.5426,  0.5531,  0.5594, -0.5537, -0.5554, -0.5546,  0.5546,\n",
       "                       -0.5588,  0.5417,  0.5531, -0.5571, -0.5582, -0.5558, -0.5579,  0.5563,\n",
       "                       -0.5596, -0.5546,  0.5582, -0.5543, -0.5566,  0.5547, -0.5583, -0.5600,\n",
       "                        0.5525,  0.5590,  0.5591, -0.5529,  0.5544, -0.5542, -0.5560, -0.5583,\n",
       "                        0.5550, -0.5568, -0.5573, -0.5581,  0.5572,  0.5543,  0.5578, -0.5564,\n",
       "                        0.5584,  0.5558,  0.5579,  0.5603,  0.5548,  0.5582,  0.5539,  0.5539,\n",
       "                       -0.5558,  0.5583, -0.5625, -0.5550,  0.5596, -0.5543,  0.5566,  0.5571,\n",
       "                       -0.5600, -0.5554, -0.5594,  0.5592,  0.5555,  0.5557,  0.5542, -0.5608,\n",
       "                       -0.5536,  0.5579, -0.5575,  0.5527, -0.5549,  0.5600,  0.5552,  0.5595,\n",
       "                        0.5599,  0.5586,  0.5556, -0.5573, -0.5529,  0.5576,  0.5554, -0.5570,\n",
       "                       -0.5573, -0.5582, -0.5583, -0.5566, -0.5555, -0.5548, -0.5574, -0.5599,\n",
       "                        0.5589,  0.5589,  0.5558,  0.5593, -0.5566,  0.5566,  0.5548,  0.5579,\n",
       "                       -0.5469, -0.5545, -0.5583,  0.5559, -0.5517,  0.5581, -0.5545,  0.5590],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.0752, -0.2222, -0.2209, -0.0301, -0.1385, -0.2287, -0.1965, -0.1863,\n",
       "                       -0.1171, -0.1848, -0.1981, -0.2096, -0.0811, -0.1206, -0.1761, -0.1511,\n",
       "                       -0.0310, -0.1785, -0.1354, -0.2051, -0.1303, -0.1971, -0.1930, -0.1128,\n",
       "                       -0.2056, -0.1850, -0.2187, -0.0820, -0.1456, -0.1416, -0.1590, -0.0678,\n",
       "                       -0.1887, -0.1743, -0.1259, -0.2086, -0.1691, -0.2446, -0.0669, -0.1646,\n",
       "                       -0.1385, -0.1979, -0.1650, -0.2105, -0.2164, -0.1499, -0.2251, -0.1195,\n",
       "                       -0.0877, -0.0550, -0.1372, -0.1770, -0.2247, -0.2134, -0.1610, -0.1654,\n",
       "                       -0.2468, -0.1107, -0.1712, -0.1072, -0.2446, -0.1986, -0.1468, -0.1368,\n",
       "                       -0.1576, -0.1597, -0.2342, -0.2002, -0.1029, -0.1339, -0.0791, -0.1650,\n",
       "                       -0.1137, -0.1226, -0.2262, -0.1769, -0.1857, -0.1967, -0.1823, -0.1826,\n",
       "                       -0.0848, -0.2136, -0.0045, -0.2291, -0.1357, -0.2389, -0.1607, -0.1593,\n",
       "                       -0.0241, -0.1573, -0.0325, -0.1927, -0.1623, -0.1636, -0.2368, -0.2161,\n",
       "                       -0.1971, -0.1735, -0.1392, -0.1891, -0.2072, -0.1061, -0.2262, -0.1545,\n",
       "                       -0.2028, -0.0613, -0.1243, -0.0735, -0.1948, -0.1561, -0.1174, -0.0502,\n",
       "                       -0.1899, -0.1496, -0.1963, -0.1685, -0.2141, -0.0210, -0.2279, -0.1681,\n",
       "                       -0.2047, -0.1384, -0.1491, -0.2190, -0.0969, -0.2133, -0.0907, -0.1880],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.8093, 0.7495, 0.7997, 0.8392, 0.8219, 0.7918, 0.8508, 0.7993, 0.9625,\n",
       "                       0.7940, 0.7750, 0.7383, 0.8417, 0.7778, 0.7483, 0.9617, 0.8718, 0.8679,\n",
       "                       0.8584, 0.6964, 0.7606, 0.7884, 0.8327, 0.7895, 0.7564, 0.7520, 0.8513,\n",
       "                       0.7525, 0.7667, 0.8095, 1.0495, 0.8087, 0.6924, 0.7637, 0.8062, 0.9847,\n",
       "                       0.8062, 0.8280, 0.9108, 1.0359, 0.7953, 0.7623, 0.7348, 0.7962, 0.7166,\n",
       "                       0.9111, 0.7272, 0.7804, 0.8488, 0.8290, 0.9651, 0.9863, 0.7369, 0.7757,\n",
       "                       0.9778, 0.7277, 0.8836, 0.7600, 0.7946, 0.8953, 0.8346, 0.7383, 0.9778,\n",
       "                       0.8786, 0.8494, 0.8047, 0.8066, 0.8885, 0.8185, 0.7463, 0.7989, 0.7729,\n",
       "                       0.8339, 0.9020, 0.7465, 0.7171, 0.7810, 0.8230, 0.9260, 0.7882, 0.8264,\n",
       "                       0.9635, 0.8743, 0.7630, 0.7732, 0.7391, 0.8376, 0.7785, 0.8861, 1.0193,\n",
       "                       0.8233, 0.9787, 0.8576, 0.8626, 0.8288, 0.8182, 0.7184, 0.7724, 0.8488,\n",
       "                       0.7623, 0.7973, 0.8505, 0.7446, 1.0277, 0.8610, 0.8332, 0.8319, 0.8343,\n",
       "                       0.9999, 0.9363, 0.8064, 0.8397, 0.7709, 0.7834, 0.7463, 0.8550, 0.7467,\n",
       "                       0.8445, 0.8815, 0.7924, 0.7761, 0.9897, 0.7584, 0.7370, 0.8763, 0.6919,\n",
       "                       0.7355, 0.7686], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0053,  0.0019,  0.0046,  ..., -0.0081,  0.0156, -0.0086],\n",
       "                       [-0.0151, -0.0111,  0.0108,  ..., -0.0063,  0.0078, -0.0164],\n",
       "                       [ 0.0050, -0.0080,  0.0010,  ..., -0.0224,  0.0091, -0.0180],\n",
       "                       [ 0.0016, -0.0087,  0.0101,  ...,  0.0078, -0.0055,  0.0038],\n",
       "                       [-0.0126,  0.0035,  0.0124,  ..., -0.0014,  0.0023, -0.0030]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0076,  0.0032,  0.0015, -0.0034, -0.0013], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[ 1.9450e-01, -9.9795e-02,  5.8899e-02,  2.0421e-01, -6.8798e-02,\n",
       "                        -2.0007e-01, -1.7882e-01,  1.9901e-01,  7.3716e-02, -7.7455e-02,\n",
       "                        -1.7553e-01,  1.2926e-01, -2.1960e-01, -5.1855e-02,  1.6968e-01,\n",
       "                        -7.0568e-02, -1.7667e-01,  8.5367e-02, -1.8126e-01, -4.0493e-02],\n",
       "                       [ 3.1346e-01, -2.4672e-01,  3.6882e-01, -2.9854e-01,  4.4060e-01,\n",
       "                        -3.9375e-02,  3.2006e-01,  2.4884e-01,  6.6769e-01, -1.2758e-01,\n",
       "                        -5.8387e-01, -8.7315e-02, -4.9148e-01, -3.9044e-02, -2.8058e-01,\n",
       "                        -1.2313e-01, -6.4514e-01, -3.9344e-01, -7.0918e-01, -4.2825e-01],\n",
       "                       [-1.6792e-01, -9.5964e-03, -4.3481e-02,  6.7062e-03,  1.1711e-01,\n",
       "                        -1.6487e-01, -6.0568e-02, -1.0674e-02,  1.3948e-01,  2.4193e-02,\n",
       "                         3.2932e-02,  1.9114e-01,  1.7074e-01,  1.4639e-01,  1.9790e-01,\n",
       "                        -1.4125e-01,  1.1538e-01,  1.8761e-01,  8.7872e-03, -1.8317e-01],\n",
       "                       [-8.6637e-02, -8.4453e-02, -1.6488e-02, -1.0229e-01, -1.2699e-01,\n",
       "                        -5.9725e-02,  7.7862e-02,  8.2837e-02,  1.6639e-01,  1.4864e-01,\n",
       "                        -6.2536e-02, -6.9908e-02, -7.0712e-02, -5.1099e-02, -1.4010e-01,\n",
       "                        -1.6697e-01,  1.6291e-01,  1.0379e-01, -7.5108e-02,  2.9448e-04],\n",
       "                       [-5.8330e-01, -5.2382e-01,  2.3097e-01, -2.0290e-01,  1.2620e-01,\n",
       "                        -5.2184e-01,  2.3124e-01,  1.6222e-01, -4.6191e-01, -4.8259e-01,\n",
       "                         2.0806e-01, -2.4699e-01, -2.1455e-01, -2.8649e-01, -5.9960e-03,\n",
       "                        -3.3979e-01, -7.1683e-02, -2.2812e-01,  9.2595e-01, -4.0021e-01],\n",
       "                       [-2.2718e-01, -3.8532e-01,  3.6588e-01, -4.9189e-01,  6.1618e-01,\n",
       "                        -4.3076e-01,  6.6215e-01,  1.3517e-01,  2.0993e-01, -6.5571e-01,\n",
       "                         2.4551e-01, -3.9373e-01, -5.2038e-01, -5.6636e-01, -3.1780e-01,\n",
       "                        -4.5481e-01, -3.0907e-01, -5.8442e-01, -1.5079e-01, -4.3089e-01],\n",
       "                       [-4.3577e-01, -2.5230e-01,  3.8415e-01, -3.5953e-01,  5.0516e-01,\n",
       "                        -4.9321e-01,  4.9733e-01, -2.0980e-01, -1.5045e-01, -5.2452e-01,\n",
       "                         1.5987e-01, -1.4204e-01, -7.4178e-02, -3.6433e-01, -4.2967e-01,\n",
       "                        -4.3072e-01, -4.1603e-03, -4.0346e-01,  5.7543e-01, -3.4250e-01],\n",
       "                       [-2.1789e-01,  1.4847e-01, -1.7759e-01, -1.5096e-02,  1.6466e-01,\n",
       "                         1.9650e-01, -2.1852e-02, -1.2478e-02, -1.0809e-02, -1.5789e-01,\n",
       "                        -1.5054e-01, -1.8219e-01,  1.2927e-01, -4.6389e-02,  2.1401e-01,\n",
       "                         1.6270e-01, -2.1683e-01,  3.1173e-02, -1.7408e-01,  5.3669e-02],\n",
       "                       [-2.0245e-01, -2.5325e-01,  2.2297e-01, -3.6453e-01,  2.6300e-01,\n",
       "                        -3.9136e-01,  2.1834e-01, -1.5191e-01, -3.9730e-01, -3.6379e-01,\n",
       "                         2.3120e-01, -4.2832e-02, -2.1437e-01, -1.7172e-01,  1.6827e-01,\n",
       "                        -3.6454e-01,  1.3989e-01, -1.1689e-01,  8.3335e-01, -4.2437e-02],\n",
       "                       [-4.6539e-01, -4.4326e-01, -1.8556e-02, -3.0206e-01,  1.8009e-01,\n",
       "                        -2.9671e-01,  9.9821e-03,  1.4084e-01, -5.4199e-01, -2.9132e-02,\n",
       "                         5.4752e-01, -1.5445e-01, -2.9636e-01, -2.3680e-01, -1.8175e-01,\n",
       "                        -2.4527e-01, -2.0243e-02, -3.0596e-01,  4.4147e-01, -3.4644e-02],\n",
       "                       [-5.1960e-01, -3.0793e-01,  3.9824e-01, -2.8498e-02,  2.7860e-01,\n",
       "                        -2.3621e-01,  1.3768e-01, -1.1015e-03, -1.6295e-01, -2.3110e-01,\n",
       "                         2.8005e-01, -2.1967e-01,  6.6372e-02, -1.7816e-01,  2.1774e-01,\n",
       "                        -2.7426e-01,  3.1498e-01, -1.8950e-01,  4.5967e-01, -2.2480e-01],\n",
       "                       [ 8.6240e-02, -1.0932e-01, -6.6633e-02,  3.1452e-01,  9.8137e-02,\n",
       "                         1.0588e-01, -2.6359e-01,  1.9449e-01, -3.9574e-02, -9.9108e-04,\n",
       "                         1.2918e-01,  1.1637e-01,  2.6810e-01, -7.8607e-02,  2.6053e-01,\n",
       "                         1.5103e-02,  2.4161e-01,  3.2538e-02,  9.5170e-02,  7.7577e-02],\n",
       "                       [ 2.2077e-01, -1.1893e-01,  3.3556e-03,  1.4105e-01,  1.9006e-01,\n",
       "                         5.2740e-02, -1.9452e-01,  2.2726e-02,  1.2223e-01,  4.7893e-02,\n",
       "                         1.0646e-01,  2.3871e-01,  9.1381e-02,  1.1641e-01,  1.0854e-01,\n",
       "                         1.8654e-01, -1.0111e-01,  1.9331e-01,  1.4477e-01, -3.2333e-03],\n",
       "                       [-4.7303e-02, -7.3556e-02, -1.5947e-01,  2.0988e-01, -1.0523e-01,\n",
       "                         1.4511e-01,  3.9058e-02, -2.0894e-01,  1.4523e-01,  1.4160e-01,\n",
       "                         2.1420e-01,  5.6961e-02,  1.9555e-01,  1.8229e-01, -1.8977e-01,\n",
       "                         2.0819e-01,  7.4866e-02, -1.2519e-01, -4.2634e-02, -2.1221e-01],\n",
       "                       [-4.0376e-01, -3.3445e-01,  2.3031e-01, -8.1199e-02, -9.7037e-02,\n",
       "                        -3.8110e-01,  3.1125e-01, -3.0648e-02, -6.1843e-01, -2.2994e-01,\n",
       "                         1.8667e-01, -2.6246e-02,  1.0519e-01, -2.5016e-01, -2.1890e-02,\n",
       "                        -1.8432e-01,  2.6919e-01, -3.7928e-01,  5.7429e-01, -3.0198e-01],\n",
       "                       [ 6.2884e-02,  1.5268e-01, -9.8051e-02, -7.9451e-02,  5.4087e-02,\n",
       "                        -8.8575e-02, -2.0254e-01,  7.8244e-03, -2.1483e-01,  1.6323e-01,\n",
       "                         5.5022e-02,  1.6895e-01,  1.5126e-01, -4.8578e-04, -1.3752e-01,\n",
       "                         1.3657e-02,  8.7678e-02,  7.2777e-02, -2.0556e-01,  1.6001e-01],\n",
       "                       [-1.7184e-01,  1.5199e-01, -1.4167e-01,  4.3341e-02, -1.1432e-01,\n",
       "                         1.8257e-02,  1.5661e-01,  6.9146e-03,  1.4682e-01,  1.5712e-01,\n",
       "                        -1.8079e-01, -2.4925e-02,  9.8675e-02,  1.8632e-01,  7.6713e-02,\n",
       "                        -7.8911e-02, -9.5569e-02, -1.8848e-01,  2.3741e-02, -1.2041e-01],\n",
       "                       [-3.4018e-01, -5.4116e-01,  7.2154e-01, -5.9619e-01,  7.5661e-01,\n",
       "                        -4.8814e-01,  6.5774e-01,  3.5671e-01,  4.0474e-01, -4.2754e-01,\n",
       "                         1.1906e-01, -4.7903e-01, -7.5758e-01, -4.7793e-01, -9.1343e-01,\n",
       "                        -7.0370e-01, -7.4674e-01, -4.1812e-01, -1.0789e-01, -7.1230e-01],\n",
       "                       [-3.7166e-01, -3.5114e-01,  9.7455e-02, -5.1995e-01,  1.1000e-01,\n",
       "                        -4.2867e-01,  2.9503e-01,  8.6841e-02, -1.5219e-01, -2.4102e-01,\n",
       "                         1.7106e-01, -4.9874e-01,  3.9382e-03, -4.8100e-01, -2.7536e-01,\n",
       "                        -5.0917e-01, -2.2812e-01, -5.0284e-01,  4.0104e-01, -5.0062e-01],\n",
       "                       [ 5.7188e-02, -1.6221e-01,  8.6147e-03, -1.6054e-01,  2.3078e-01,\n",
       "                        -2.4389e-01,  3.5177e-01,  4.9625e-01,  8.3814e-02, -1.8604e-01,\n",
       "                        -8.6181e-02,  7.3271e-02, -4.9693e-01, -2.3796e-01, -3.8974e-01,\n",
       "                        -3.0115e-01, -3.0443e-01,  1.8303e-02, -6.3854e-01, -2.5684e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([-0.1579,  0.3830, -0.2280, -0.1649,  0.1597,  0.5916,  0.4159,  0.0164,\n",
       "                        0.1514,  0.3201,  0.2737,  0.1193,  0.1049, -0.1574,  0.2005,  0.1238,\n",
       "                        0.1505,  0.7526,  0.3516,  0.0970], device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 0.1263, -0.0772,  0.1627, -0.0859, -0.2019, -0.2602, -0.4369,  0.1521,\n",
       "                        -0.2212, -0.2299, -0.1524,  0.1932,  0.2122,  0.1275, -0.0518,  0.0328,\n",
       "                         0.0779, -0.4234, -0.0063, -0.1765],\n",
       "                       [ 0.0947, -0.0592, -0.1505,  0.1282,  0.2700,  0.3092,  0.1813, -0.1357,\n",
       "                         0.3183,  0.2896,  0.0864,  0.0237,  0.1133,  0.0974,  0.3301, -0.0959,\n",
       "                        -0.1545,  0.3448,  0.2064, -0.0190],\n",
       "                       [-0.1231, -0.2893, -0.1265,  0.1026, -0.4462, -0.1169, -0.4914, -0.1678,\n",
       "                        -0.3340, -0.3942, -0.2506,  0.0273,  0.0290,  0.0147, -0.4609,  0.0933,\n",
       "                         0.0714, -0.5103, -0.3784, -0.4865],\n",
       "                       [ 0.2137,  0.0238,  0.1494,  0.1903, -0.3170, -0.1248, -0.1031,  0.0262,\n",
       "                        -0.2517,  0.1737, -0.1939, -0.1401, -0.0070,  0.0077, -0.2880,  0.0615,\n",
       "                        -0.1830, -0.2516, -0.0809,  0.1210],\n",
       "                       [-0.1682,  0.3614, -0.1215,  0.1635, -0.1256,  0.2145,  0.3066,  0.2003,\n",
       "                        -0.0317, -0.3850,  0.1504,  0.1634, -0.1465, -0.0818,  0.0308,  0.0231,\n",
       "                         0.0909,  0.0798,  0.2517, -0.0961],\n",
       "                       [ 0.1320,  0.0388,  0.1741,  0.0834,  0.2778,  0.1680, -0.0524,  0.0265,\n",
       "                         0.0478, -0.0274, -0.0982,  0.1646,  0.0666,  0.0608,  0.1382, -0.0616,\n",
       "                         0.2075,  0.3890, -0.0512,  0.0220],\n",
       "                       [ 0.0186,  0.8560, -0.0559,  0.0717,  0.8958,  0.9087,  0.9736, -0.1300,\n",
       "                         0.5675,  0.3810,  0.6347,  0.0049,  0.0567, -0.1425,  0.6163,  0.0342,\n",
       "                         0.1900,  0.8056,  0.9387,  0.3749],\n",
       "                       [-0.0348, -0.0985, -0.2037,  0.0392,  0.0375, -0.1490, -0.1671,  0.2235,\n",
       "                         0.1203,  0.1824,  0.1228, -0.1389,  0.1232,  0.1064, -0.1620, -0.2108,\n",
       "                         0.0877, -0.3298, -0.2546, -0.0387],\n",
       "                       [-0.0676, -0.2835, -0.1881,  0.2059,  0.3445, -0.0828,  0.2131, -0.1769,\n",
       "                         0.2083,  0.2633,  0.2230, -0.0921, -0.1128, -0.0414,  0.2611, -0.1009,\n",
       "                         0.0885, -0.1704,  0.0455, -0.4391],\n",
       "                       [ 0.0187, -0.5333,  0.0174,  0.0651, -0.3656, -0.2181, -0.3764,  0.0620,\n",
       "                        -0.4032,  0.0820, -0.3893,  0.0414, -0.0107,  0.0959, -0.1434, -0.0094,\n",
       "                         0.2114, -0.6247, -0.1515, -0.3881]], device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.2632,  0.2419, -0.3432, -0.0988,  0.0510,  0.0767,  0.6427, -0.1952,\n",
       "                        0.1331, -0.0769], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.483476383447647,\n",
       "   1.3545481526851655,\n",
       "   1.3098033878803252,\n",
       "   1.2686262041330338,\n",
       "   1.218484871506691,\n",
       "   1.1743631842136384,\n",
       "   1.1345976806879043,\n",
       "   1.086325322985649,\n",
       "   1.0543912591934204,\n",
       "   1.0350105865001678,\n",
       "   0.9866085749864578,\n",
       "   0.9846874786615372,\n",
       "   0.9660252624750137,\n",
       "   0.9358155431747437,\n",
       "   0.937087476849556,\n",
       "   0.915649830698967,\n",
       "   0.8967641706466675,\n",
       "   0.8855581641197204,\n",
       "   0.8825522440671921,\n",
       "   0.8663041898012162,\n",
       "   0.8615279206037522,\n",
       "   0.8308090027570725,\n",
       "   0.8249965822696685,\n",
       "   0.8191860826015472,\n",
       "   0.8189492141604423,\n",
       "   0.8036693303585053,\n",
       "   0.7832032409310341,\n",
       "   0.775857702255249,\n",
       "   0.7779861376881599,\n",
       "   0.7800523315668106,\n",
       "   0.7531858184337616,\n",
       "   0.7541596565246582,\n",
       "   0.7361088641285897,\n",
       "   0.7286121693253517,\n",
       "   0.7292022127509117,\n",
       "   0.7095676583051681,\n",
       "   0.7120725163817405,\n",
       "   0.7002220395207405,\n",
       "   0.7061740514039994,\n",
       "   0.6800183079242706,\n",
       "   0.6920564432740212,\n",
       "   0.6727037338614463,\n",
       "   0.6708960717916489,\n",
       "   0.6698056691288948,\n",
       "   0.6596473541855812,\n",
       "   0.6498868864178657,\n",
       "   0.655269834458828,\n",
       "   0.6354625440835953,\n",
       "   0.6332014302015304,\n",
       "   0.6399552510380745,\n",
       "   0.636753729403019,\n",
       "   0.6139424102902412,\n",
       "   0.6182934874892235,\n",
       "   0.6212592880129815,\n",
       "   0.6169081155657768,\n",
       "   0.6051740693747998,\n",
       "   0.6106658101081848,\n",
       "   0.5890947519540787,\n",
       "   0.5899993230104447,\n",
       "   0.5940517761707306,\n",
       "   0.5941763424873352,\n",
       "   0.5757712825536728,\n",
       "   0.5736726365685463,\n",
       "   0.6003453350663185,\n",
       "   0.5694817636013031,\n",
       "   0.5650899789333343,\n",
       "   0.562277569591999,\n",
       "   0.5423234815001488,\n",
       "   0.5434539637863636,\n",
       "   0.5505697911977768,\n",
       "   0.5450181565880775,\n",
       "   0.5424525443017483,\n",
       "   0.5502477190196514,\n",
       "   0.5398116151988507,\n",
       "   0.5268495962321759,\n",
       "   0.5161014630198478,\n",
       "   0.5290606241226197,\n",
       "   0.5290622363686561,\n",
       "   0.5131781410574913,\n",
       "   0.513524123877287,\n",
       "   0.4992736192941666,\n",
       "   0.5125679462850093,\n",
       "   0.5042422689497471,\n",
       "   0.49598163238167764,\n",
       "   0.48699826806783675,\n",
       "   0.4952307816147804,\n",
       "   0.48522328546643256,\n",
       "   0.48399221505224704,\n",
       "   0.4750914240181446,\n",
       "   0.46963882228732107,\n",
       "   0.4760855414271355,\n",
       "   0.4710879309475422,\n",
       "   0.4571245724260807,\n",
       "   0.4625591897964478,\n",
       "   0.4592583292722702,\n",
       "   0.4662577235996723,\n",
       "   0.44587932297587396,\n",
       "   0.4526464014351368,\n",
       "   0.4377901410162449],\n",
       "  'train_loss_std': [0.14395432451489668,\n",
       "   0.11333838373880073,\n",
       "   0.12711974579300753,\n",
       "   0.12105659789091615,\n",
       "   0.13434592199589895,\n",
       "   0.1313440359206019,\n",
       "   0.13860421011488458,\n",
       "   0.13037191019217056,\n",
       "   0.1272481605095346,\n",
       "   0.12974667647832655,\n",
       "   0.13741041014183927,\n",
       "   0.15123350618087675,\n",
       "   0.1418155604692662,\n",
       "   0.1374293410478573,\n",
       "   0.13186008332118565,\n",
       "   0.140305291953603,\n",
       "   0.13605381381361312,\n",
       "   0.13910498653681638,\n",
       "   0.13889903648615332,\n",
       "   0.1434268096153552,\n",
       "   0.13734807948094938,\n",
       "   0.13614374847329086,\n",
       "   0.14652969026418683,\n",
       "   0.14201605577733833,\n",
       "   0.14244231613074756,\n",
       "   0.15172665964938425,\n",
       "   0.1386274556381588,\n",
       "   0.1448365387598719,\n",
       "   0.1387978411864321,\n",
       "   0.1392369955539013,\n",
       "   0.13945563215547616,\n",
       "   0.13514215764376322,\n",
       "   0.14039674919546777,\n",
       "   0.14985599857866935,\n",
       "   0.13834088182754622,\n",
       "   0.14977615913270442,\n",
       "   0.13755955071196108,\n",
       "   0.1426919142423052,\n",
       "   0.14972353920124865,\n",
       "   0.14629833840481885,\n",
       "   0.13974395437761303,\n",
       "   0.13765699316149732,\n",
       "   0.13424311337994166,\n",
       "   0.13918151588458477,\n",
       "   0.1359302521562211,\n",
       "   0.13696987917735345,\n",
       "   0.1381445758943189,\n",
       "   0.13321522538615158,\n",
       "   0.13803707231606446,\n",
       "   0.13306890934295446,\n",
       "   0.1344519116346583,\n",
       "   0.1382546355147701,\n",
       "   0.13382131089253216,\n",
       "   0.1375218830216236,\n",
       "   0.14794151804726205,\n",
       "   0.1303917236588258,\n",
       "   0.13799221257661803,\n",
       "   0.13942923150945669,\n",
       "   0.13231009475445332,\n",
       "   0.13433800276299657,\n",
       "   0.13837884123711383,\n",
       "   0.13936341481167122,\n",
       "   0.14135101436191946,\n",
       "   0.14932812710107446,\n",
       "   0.1299503415927997,\n",
       "   0.1355631097071234,\n",
       "   0.13544314982727282,\n",
       "   0.1397812187860245,\n",
       "   0.1285458448637036,\n",
       "   0.1353943106281065,\n",
       "   0.13053567723681753,\n",
       "   0.13188190368150735,\n",
       "   0.14005317621076105,\n",
       "   0.13371949564274818,\n",
       "   0.13816014847329,\n",
       "   0.1314713657649213,\n",
       "   0.13112349419041336,\n",
       "   0.13471776218016887,\n",
       "   0.13027689970342018,\n",
       "   0.13290112830775985,\n",
       "   0.12413065756293956,\n",
       "   0.1321072014420857,\n",
       "   0.1399392415305865,\n",
       "   0.13140487575785323,\n",
       "   0.13105213940689583,\n",
       "   0.1271608603606777,\n",
       "   0.12232877988984227,\n",
       "   0.1290914488083553,\n",
       "   0.12357111193000987,\n",
       "   0.1265193672330218,\n",
       "   0.12458573172082225,\n",
       "   0.13162288135038194,\n",
       "   0.11534974463449765,\n",
       "   0.12952297854521969,\n",
       "   0.12435834453104028,\n",
       "   0.1389928684385691,\n",
       "   0.12120323266319391,\n",
       "   0.1277518306115412,\n",
       "   0.1216219440238964],\n",
       "  'train_accuracy_mean': [0.37069333359599116,\n",
       "   0.4429600010514259,\n",
       "   0.4647066667675972,\n",
       "   0.4883599992990494,\n",
       "   0.5131999998092651,\n",
       "   0.5351200005412102,\n",
       "   0.5534133326411247,\n",
       "   0.5779733316302299,\n",
       "   0.5907733336687088,\n",
       "   0.5979866657853127,\n",
       "   0.6220266659855842,\n",
       "   0.6233199979066849,\n",
       "   0.6316133319735527,\n",
       "   0.6442533332705498,\n",
       "   0.6442933332920074,\n",
       "   0.6498933331370353,\n",
       "   0.659346665263176,\n",
       "   0.6675466662049293,\n",
       "   0.6657066665887833,\n",
       "   0.6716933337450027,\n",
       "   0.6720133323669434,\n",
       "   0.6882933328151702,\n",
       "   0.6891733328104019,\n",
       "   0.6909999992251397,\n",
       "   0.6918533331155777,\n",
       "   0.6961466667056083,\n",
       "   0.705066666841507,\n",
       "   0.708159999191761,\n",
       "   0.7087200011014938,\n",
       "   0.7072133328318596,\n",
       "   0.717360000371933,\n",
       "   0.7175333334207534,\n",
       "   0.7243199995756149,\n",
       "   0.7285599997043609,\n",
       "   0.7260400002002716,\n",
       "   0.7356933336853981,\n",
       "   0.7333333344459534,\n",
       "   0.7377333332896232,\n",
       "   0.7363199999332428,\n",
       "   0.7475466676354409,\n",
       "   0.7430000010728836,\n",
       "   0.7492400013208389,\n",
       "   0.7498533320426941,\n",
       "   0.7495199999809266,\n",
       "   0.7541600006818772,\n",
       "   0.7580000001192093,\n",
       "   0.7559466677904129,\n",
       "   0.7636399995088577,\n",
       "   0.7658799993991852,\n",
       "   0.7625333334207535,\n",
       "   0.7626399983167649,\n",
       "   0.7732266647815704,\n",
       "   0.7706933327913285,\n",
       "   0.7705733337402344,\n",
       "   0.7721866673231125,\n",
       "   0.7775466668605805,\n",
       "   0.7746533321142197,\n",
       "   0.7821866668462754,\n",
       "   0.7803599991798401,\n",
       "   0.7778133322000503,\n",
       "   0.7796400007009506,\n",
       "   0.7875466661453248,\n",
       "   0.7886400009393693,\n",
       "   0.7777466655969619,\n",
       "   0.7903733334541321,\n",
       "   0.7915733312368393,\n",
       "   0.7917733327150345,\n",
       "   0.7990533335208893,\n",
       "   0.7999999995231628,\n",
       "   0.7966133326292038,\n",
       "   0.7994533331394196,\n",
       "   0.7988000001907348,\n",
       "   0.7957999991178513,\n",
       "   0.8017200000286102,\n",
       "   0.8063333348035813,\n",
       "   0.8085466663837433,\n",
       "   0.804853335261345,\n",
       "   0.8050666663646698,\n",
       "   0.8098933339118958,\n",
       "   0.8110666654109955,\n",
       "   0.8160133337974549,\n",
       "   0.8106666659116745,\n",
       "   0.8153466678857804,\n",
       "   0.817799998998642,\n",
       "   0.821519998908043,\n",
       "   0.8179866662025451,\n",
       "   0.8210000007152557,\n",
       "   0.8225199991464615,\n",
       "   0.8258133324384689,\n",
       "   0.8262800006866455,\n",
       "   0.8266933342218399,\n",
       "   0.8276533333063125,\n",
       "   0.833626668214798,\n",
       "   0.8289600015878678,\n",
       "   0.8305066677331925,\n",
       "   0.8271066660881042,\n",
       "   0.8359733339548111,\n",
       "   0.8337599998712539,\n",
       "   0.8393333332538605],\n",
       "  'train_accuracy_std': [0.0766484566360368,\n",
       "   0.06375887423490642,\n",
       "   0.06983140344978796,\n",
       "   0.06645081097227663,\n",
       "   0.06909561705264296,\n",
       "   0.06746593476190425,\n",
       "   0.07170428686099195,\n",
       "   0.06530189113446383,\n",
       "   0.06611657844767474,\n",
       "   0.0655488958249352,\n",
       "   0.06948943519117827,\n",
       "   0.07238477309039977,\n",
       "   0.06905824857850075,\n",
       "   0.06624096003845513,\n",
       "   0.06401016017503583,\n",
       "   0.06567317884517705,\n",
       "   0.06622533740727694,\n",
       "   0.06528929708091179,\n",
       "   0.06665708840864606,\n",
       "   0.0709408149481394,\n",
       "   0.06510941014946067,\n",
       "   0.061867409215896334,\n",
       "   0.06982092257944582,\n",
       "   0.063668411695221,\n",
       "   0.06475688423526006,\n",
       "   0.06988114262025241,\n",
       "   0.06372105914721576,\n",
       "   0.0669732372169136,\n",
       "   0.06379311623137775,\n",
       "   0.0649701734573289,\n",
       "   0.06280523295637327,\n",
       "   0.06085797585225891,\n",
       "   0.06363789743475273,\n",
       "   0.0669033610935633,\n",
       "   0.06302492042454541,\n",
       "   0.06611662585150434,\n",
       "   0.06259002349539497,\n",
       "   0.062139770798644786,\n",
       "   0.06652628540686055,\n",
       "   0.06532060176314308,\n",
       "   0.06242346371620217,\n",
       "   0.060841872811850016,\n",
       "   0.05806873802044365,\n",
       "   0.06142920633719081,\n",
       "   0.06116521208393779,\n",
       "   0.060884588818522274,\n",
       "   0.060173946170244186,\n",
       "   0.058278978144496695,\n",
       "   0.06011399276989966,\n",
       "   0.05876642937223986,\n",
       "   0.05848729440810904,\n",
       "   0.06153436154620754,\n",
       "   0.05852983513984785,\n",
       "   0.06068593123072759,\n",
       "   0.06457877700587399,\n",
       "   0.0558046494432728,\n",
       "   0.05948026564097738,\n",
       "   0.05974181981651452,\n",
       "   0.05926178628905173,\n",
       "   0.059203759905654935,\n",
       "   0.060292281796880745,\n",
       "   0.05929908234629067,\n",
       "   0.05990376867607915,\n",
       "   0.06180246448648162,\n",
       "   0.056648179731089446,\n",
       "   0.059240492050440206,\n",
       "   0.059980457648278536,\n",
       "   0.05917726179649516,\n",
       "   0.05534859438199707,\n",
       "   0.05959788838774204,\n",
       "   0.056826939943452844,\n",
       "   0.05730836646921678,\n",
       "   0.06117555891421147,\n",
       "   0.05631160789883762,\n",
       "   0.060449979888514506,\n",
       "   0.05730172560835069,\n",
       "   0.05648127201393746,\n",
       "   0.056781217680472944,\n",
       "   0.056481164532692286,\n",
       "   0.056896162071701076,\n",
       "   0.053301822981599106,\n",
       "   0.05656854241906197,\n",
       "   0.057846846322377785,\n",
       "   0.056290161973020725,\n",
       "   0.054676427920292967,\n",
       "   0.05465804694123846,\n",
       "   0.052474545056419096,\n",
       "   0.0540415962790607,\n",
       "   0.05391994874793915,\n",
       "   0.05351578745123087,\n",
       "   0.05204954302900933,\n",
       "   0.05573353128008745,\n",
       "   0.049162797646936346,\n",
       "   0.054824839077500014,\n",
       "   0.05387401950640552,\n",
       "   0.05937064315086252,\n",
       "   0.052577640343184684,\n",
       "   0.05322651995237746,\n",
       "   0.05125362104696331],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.4665240701039632,\n",
       "   1.4171582623322805,\n",
       "   1.3929247081279754,\n",
       "   1.3521079965432485,\n",
       "   1.3207840005556741,\n",
       "   1.2823985087871552,\n",
       "   1.2357146400213241,\n",
       "   1.2074770802259445,\n",
       "   1.16887910703818,\n",
       "   1.1530817288160324,\n",
       "   1.1405859416723252,\n",
       "   1.1206016393502554,\n",
       "   1.1152420143286388,\n",
       "   1.1028757802645366,\n",
       "   1.0881827984253565,\n",
       "   1.0752600787083308,\n",
       "   1.0674547817309696,\n",
       "   1.0507617690165838,\n",
       "   1.0455724114179612,\n",
       "   1.029528054992358,\n",
       "   1.0347131176789601,\n",
       "   1.0308612650632858,\n",
       "   1.0205002764860789,\n",
       "   1.0117877606550854,\n",
       "   0.9991415997346242,\n",
       "   0.9970713939269383,\n",
       "   0.9776921087503433,\n",
       "   0.9844942214091619,\n",
       "   0.971526497801145,\n",
       "   0.9629404856761297,\n",
       "   0.948158764441808,\n",
       "   0.9565264155467351,\n",
       "   0.9395616263151169,\n",
       "   0.9322134967645009,\n",
       "   0.9252809810638428,\n",
       "   0.9278887983163198,\n",
       "   0.9269906002283096,\n",
       "   0.9079518123467764,\n",
       "   0.9162568575143815,\n",
       "   0.9227782398462295,\n",
       "   0.9155866428216298,\n",
       "   0.9006667524576187,\n",
       "   0.9016995388269424,\n",
       "   0.9051644812027614,\n",
       "   0.9001533788442612,\n",
       "   0.9028488105535507,\n",
       "   0.9130249049266179,\n",
       "   0.9138602681954702,\n",
       "   0.8877618835369746,\n",
       "   0.8777974281708399,\n",
       "   0.9076733261346817,\n",
       "   0.8865848875045776,\n",
       "   0.881484682559967,\n",
       "   0.885455636382103,\n",
       "   0.8732743002971013,\n",
       "   0.8691663382450739,\n",
       "   0.8906439789136251,\n",
       "   0.8844351375102997,\n",
       "   0.8689276750882466,\n",
       "   0.8560870597759883,\n",
       "   0.8502876708904902,\n",
       "   0.8479099889596303,\n",
       "   0.8629307077328364,\n",
       "   0.8676115584373474,\n",
       "   0.850867133140564,\n",
       "   0.8470532782872517,\n",
       "   0.8576970545450846,\n",
       "   0.8460821300745011,\n",
       "   0.8614348691701889,\n",
       "   0.8581943772236507,\n",
       "   0.8761467144886652,\n",
       "   0.8548586076498031,\n",
       "   0.8423090946674346,\n",
       "   0.8468090269962947,\n",
       "   0.8592915604511897,\n",
       "   0.846453515291214,\n",
       "   0.8574012688795726,\n",
       "   0.8604395176966985,\n",
       "   0.8488959797223409,\n",
       "   0.8528792436917623,\n",
       "   0.8373437591393789,\n",
       "   0.8498614275455475,\n",
       "   0.8510965873797735,\n",
       "   0.8445712782939275,\n",
       "   0.8561002175013225,\n",
       "   0.8459126494328181,\n",
       "   0.847146147886912,\n",
       "   0.8611347327629725,\n",
       "   0.8484471168120702,\n",
       "   0.8389656205972036,\n",
       "   0.8527462327480316,\n",
       "   0.8448436069488525,\n",
       "   0.8654573482275009,\n",
       "   0.8528101329008738,\n",
       "   0.847724948724111,\n",
       "   0.8371095420916875,\n",
       "   0.853082467118899,\n",
       "   0.8638810996214549,\n",
       "   0.8563886060317357],\n",
       "  'val_loss_std': [0.08925934824660577,\n",
       "   0.0962115106736041,\n",
       "   0.09785377350198164,\n",
       "   0.10308777117396366,\n",
       "   0.10379286137959226,\n",
       "   0.1100652211658379,\n",
       "   0.10810219226848666,\n",
       "   0.115063940231856,\n",
       "   0.11814714321202346,\n",
       "   0.12110769536321783,\n",
       "   0.12240733687056339,\n",
       "   0.12659264634392622,\n",
       "   0.12382647519402466,\n",
       "   0.12968275305346055,\n",
       "   0.12844136788866767,\n",
       "   0.12429290625371377,\n",
       "   0.12871664938497682,\n",
       "   0.1266904178306537,\n",
       "   0.12727888312397284,\n",
       "   0.12889609389107845,\n",
       "   0.1313175871417351,\n",
       "   0.13201137032978313,\n",
       "   0.1259526668741782,\n",
       "   0.1296961244546495,\n",
       "   0.13350331956566044,\n",
       "   0.13289292460184882,\n",
       "   0.13145396794711273,\n",
       "   0.13060179318986692,\n",
       "   0.13357975411905498,\n",
       "   0.1334472919842112,\n",
       "   0.13183806199079628,\n",
       "   0.13489732834192797,\n",
       "   0.13689987353636482,\n",
       "   0.1342893066494463,\n",
       "   0.13439243955069743,\n",
       "   0.135543301869013,\n",
       "   0.13674488682062708,\n",
       "   0.1358373287169887,\n",
       "   0.13662543119044604,\n",
       "   0.1332164788220565,\n",
       "   0.14243757029091045,\n",
       "   0.13396603707063587,\n",
       "   0.1320232131485934,\n",
       "   0.1328392030775204,\n",
       "   0.13551865885521033,\n",
       "   0.13844065492774665,\n",
       "   0.1375059595211697,\n",
       "   0.1386343597346623,\n",
       "   0.13876031742746722,\n",
       "   0.13199150121379633,\n",
       "   0.141161827694547,\n",
       "   0.1368554390194922,\n",
       "   0.14136913080192484,\n",
       "   0.12493857512129981,\n",
       "   0.13582857057996295,\n",
       "   0.13535902917639603,\n",
       "   0.14323822959728608,\n",
       "   0.13862600489213434,\n",
       "   0.13144245184657638,\n",
       "   0.13920814706771145,\n",
       "   0.13557899393781472,\n",
       "   0.13331776433182943,\n",
       "   0.13177768400348586,\n",
       "   0.140342397160721,\n",
       "   0.14008246389525553,\n",
       "   0.13495934427253403,\n",
       "   0.13348690431098875,\n",
       "   0.13508018210927444,\n",
       "   0.1364814989770138,\n",
       "   0.12875634567051716,\n",
       "   0.13803099216237788,\n",
       "   0.13945810556226512,\n",
       "   0.1347022238355996,\n",
       "   0.13466460868966482,\n",
       "   0.1309065270280309,\n",
       "   0.1371298473825938,\n",
       "   0.13512415833957742,\n",
       "   0.13527794369829224,\n",
       "   0.1410121492868729,\n",
       "   0.13858548135255375,\n",
       "   0.13391902094933214,\n",
       "   0.13720964319277953,\n",
       "   0.14410937150890238,\n",
       "   0.1348865303337384,\n",
       "   0.1388979985155829,\n",
       "   0.14354996131970943,\n",
       "   0.13804031083444798,\n",
       "   0.14501041672857284,\n",
       "   0.13926559769320654,\n",
       "   0.133890478818652,\n",
       "   0.14732048299364692,\n",
       "   0.14037577380632232,\n",
       "   0.14494928544447935,\n",
       "   0.1362168295471893,\n",
       "   0.13970073765015756,\n",
       "   0.14332653753978378,\n",
       "   0.146693152060341,\n",
       "   0.14264648011756165,\n",
       "   0.14517821504828038],\n",
       "  'val_accuracy_mean': [0.38444444532195726,\n",
       "   0.4115555561085542,\n",
       "   0.42495555559794107,\n",
       "   0.44615555594364803,\n",
       "   0.4644888890782992,\n",
       "   0.4791555555661519,\n",
       "   0.5022000006834666,\n",
       "   0.5149777763088544,\n",
       "   0.5347777769962947,\n",
       "   0.540666664938132,\n",
       "   0.5482444435358047,\n",
       "   0.5573999987045923,\n",
       "   0.5586222209533056,\n",
       "   0.5657111112276713,\n",
       "   0.5733999998370807,\n",
       "   0.5777333334088326,\n",
       "   0.5819333322842916,\n",
       "   0.5869555561741193,\n",
       "   0.591244444946448,\n",
       "   0.6012444436550141,\n",
       "   0.5970666656891505,\n",
       "   0.6003111113111178,\n",
       "   0.6002888866265614,\n",
       "   0.6054222213228544,\n",
       "   0.6103999996185303,\n",
       "   0.6132666664322217,\n",
       "   0.6210222199559212,\n",
       "   0.6195777772863706,\n",
       "   0.6230222220222156,\n",
       "   0.6269333319862683,\n",
       "   0.6322222207983335,\n",
       "   0.6310666671395302,\n",
       "   0.6359333338340124,\n",
       "   0.6402666673064232,\n",
       "   0.6431333323319753,\n",
       "   0.6444222223758698,\n",
       "   0.6442222221692403,\n",
       "   0.6508888866504033,\n",
       "   0.6474444450934728,\n",
       "   0.6436666671435038,\n",
       "   0.6492888904611269,\n",
       "   0.6522444445888201,\n",
       "   0.6544888892769813,\n",
       "   0.6525555551052094,\n",
       "   0.6522444439927737,\n",
       "   0.6543333328763644,\n",
       "   0.6491777767737706,\n",
       "   0.6498666656017303,\n",
       "   0.6586222206552823,\n",
       "   0.6625555568933487,\n",
       "   0.6531777779261271,\n",
       "   0.6570888869961102,\n",
       "   0.6623777764042219,\n",
       "   0.6622444446881612,\n",
       "   0.6637999998529752,\n",
       "   0.6656444448232651,\n",
       "   0.6576666649182638,\n",
       "   0.6585555551449458,\n",
       "   0.6678666667143504,\n",
       "   0.6745333335796992,\n",
       "   0.6744000005722046,\n",
       "   0.6759333332379659,\n",
       "   0.6685333351294199,\n",
       "   0.6675333335002264,\n",
       "   0.6761333312590917,\n",
       "   0.6757333340247472,\n",
       "   0.670711112121741,\n",
       "   0.6745999991893769,\n",
       "   0.6738444425662359,\n",
       "   0.670933333337307,\n",
       "   0.6647111114859581,\n",
       "   0.6749999994039535,\n",
       "   0.678466666340828,\n",
       "   0.6754666682084401,\n",
       "   0.6697111110885938,\n",
       "   0.6778888899087906,\n",
       "   0.6735333324472109,\n",
       "   0.6703333316246668,\n",
       "   0.674511108994484,\n",
       "   0.6746444447835287,\n",
       "   0.6797777791817983,\n",
       "   0.6743111101786295,\n",
       "   0.6767111106713612,\n",
       "   0.6751333336035411,\n",
       "   0.6725333332022031,\n",
       "   0.6786888877550761,\n",
       "   0.6757999990383784,\n",
       "   0.673799999554952,\n",
       "   0.6739555535713831,\n",
       "   0.6764444426695506,\n",
       "   0.6765333332618078,\n",
       "   0.6791555551687877,\n",
       "   0.6720222214857737,\n",
       "   0.6716888894637426,\n",
       "   0.6760888901352883,\n",
       "   0.6818444436788559,\n",
       "   0.6746666649977366,\n",
       "   0.6732000005245209,\n",
       "   0.6749777751167615],\n",
       "  'val_accuracy_std': [0.05025957484042379,\n",
       "   0.05491092657106505,\n",
       "   0.056014860953133024,\n",
       "   0.057303915876824554,\n",
       "   0.05489921667889171,\n",
       "   0.0579440170032123,\n",
       "   0.05834803579157738,\n",
       "   0.061105550215058924,\n",
       "   0.06031634521410712,\n",
       "   0.061762986879083576,\n",
       "   0.061663598869231936,\n",
       "   0.06523958087297703,\n",
       "   0.0606033678292508,\n",
       "   0.06229301844736194,\n",
       "   0.06340577131165628,\n",
       "   0.06056685322641931,\n",
       "   0.06327104455941784,\n",
       "   0.06075626592125194,\n",
       "   0.06262711152273724,\n",
       "   0.06248383168348306,\n",
       "   0.06393969275864483,\n",
       "   0.06281701437949051,\n",
       "   0.059931363273559926,\n",
       "   0.06202454358468791,\n",
       "   0.06329229072301316,\n",
       "   0.061935416068123285,\n",
       "   0.05996165125627951,\n",
       "   0.060748173305084176,\n",
       "   0.06150501102389181,\n",
       "   0.060213841196828814,\n",
       "   0.06051833851282325,\n",
       "   0.0621864815082362,\n",
       "   0.06237683816852382,\n",
       "   0.06194085544109535,\n",
       "   0.06175546846432876,\n",
       "   0.059390728577605835,\n",
       "   0.0591716491127654,\n",
       "   0.06141741551392941,\n",
       "   0.06180365114969122,\n",
       "   0.05990888315942995,\n",
       "   0.06136905750596059,\n",
       "   0.060035789344127136,\n",
       "   0.05939820142683592,\n",
       "   0.058374165846856924,\n",
       "   0.05840217808412295,\n",
       "   0.06079260435417133,\n",
       "   0.06010908336176229,\n",
       "   0.05962212130719612,\n",
       "   0.059612559863349374,\n",
       "   0.05809432565964007,\n",
       "   0.05699231617511584,\n",
       "   0.05953623024313608,\n",
       "   0.06010957450016752,\n",
       "   0.058450356632963406,\n",
       "   0.060971797022165326,\n",
       "   0.0598503654293905,\n",
       "   0.06133846862200954,\n",
       "   0.0601441699666946,\n",
       "   0.05833781553150552,\n",
       "   0.05941228396370613,\n",
       "   0.05977528211285417,\n",
       "   0.06057425206481595,\n",
       "   0.060714853963637336,\n",
       "   0.06090664298512532,\n",
       "   0.05838198749400176,\n",
       "   0.0596547109791299,\n",
       "   0.05985858400613019,\n",
       "   0.0599631613697989,\n",
       "   0.05993976845198127,\n",
       "   0.059609990782477584,\n",
       "   0.0596237427018188,\n",
       "   0.05967846078212273,\n",
       "   0.0571043630562775,\n",
       "   0.059368629623608796,\n",
       "   0.05840654132492468,\n",
       "   0.05799989487210741,\n",
       "   0.057538047456175696,\n",
       "   0.05779946214836733,\n",
       "   0.0607674827004244,\n",
       "   0.05679514491519087,\n",
       "   0.05904633261034272,\n",
       "   0.0600537363777726,\n",
       "   0.06022302652991291,\n",
       "   0.05911085845608882,\n",
       "   0.05932669264004693,\n",
       "   0.056531075242787124,\n",
       "   0.06114212978223932,\n",
       "   0.0599154102371408,\n",
       "   0.05677687006701778,\n",
       "   0.059882189065888866,\n",
       "   0.05776694063981755,\n",
       "   0.05748454031729291,\n",
       "   0.05736355312258769,\n",
       "   0.0584817127366227,\n",
       "   0.060124250617929295,\n",
       "   0.05574018951959973,\n",
       "   0.0604844655835651,\n",
       "   0.05885560704646691,\n",
       "   0.0588831216038151],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb0c68",
   "metadata": {},
   "source": [
    "### 1.1 MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     #print(key)\n",
    "#     if value.requires_grad:\n",
    "#         print(key)\n",
    "#         print(value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a599c8",
   "metadata": {},
   "source": [
    "### 1.2 Arbiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9ebc67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = arbiter_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = arbiter_system.state['best_epoch']\n",
    "\n",
    "state = arbiter_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "arbiter_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "569eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0531d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a86b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "\n",
    "x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "\n",
    "\n",
    "x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task = next(zip(x_support_set,y_support_set,x_target_set, y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "cdeb442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]], device='cuda:0')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "647183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbiter_x_support_set, arbiter_x_target_set, arbiter_y_support_set, arbiter_y_target_set, seed = train_sample\n",
    "\n",
    "arbiter_x_support_set = torch.Tensor(arbiter_x_support_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_x_target_set = torch.Tensor(arbiter_x_target_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_y_support_set = torch.Tensor(arbiter_y_support_set).long().to(device=arbiter_system.model.device)\n",
    "arbiter_y_target_set = torch.Tensor(arbiter_y_target_set).long().to(device=arbiter_system.model.device)\n",
    "\n",
    "\n",
    "arbiter_x_support_set_task, arbiter_y_support_set_task, arbiter_x_target_set_task, arbiter_y_target_set_task = next(zip(arbiter_x_support_set,arbiter_y_support_set,arbiter_x_target_set, arbiter_y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ce1c0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]], device='cuda:0')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fd4d6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = arbiter_system.model.get_inner_loop_parameter_dict(arbiter_system.model.classifier.named_parameters())\n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = arbiter_x_target_set_task.shape\n",
    "\n",
    "arbiter_x_support_set_task = arbiter_x_support_set_task.view(-1, c, h, w)\n",
    "arbiter_y_support_set_task = arbiter_y_support_set_task.view(-1)\n",
    "arbiter_x_target_set_task = arbiter_x_target_set_task.view(-1, c, h, w)\n",
    "arbiter_y_target_set_task = arbiter_y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = arbiter_system.model.net_forward(\n",
    "            x=arbiter_x_support_set_task,\n",
    "            y=arbiter_y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "    \n",
    "    if arbiter_system.model.args.arbiter:\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                retain_graph=True)\n",
    "\n",
    "        names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "\n",
    "        for key, weight in names_weights_copy.items():\n",
    "            weight_norm = torch.norm(weight, p=2)\n",
    "            per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "        for key, grad in names_grads_copy.items():\n",
    "            gradient_l2norm = torch.norm(grad, p=2)\n",
    "            per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "        per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                    per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        generated_gradient_rate = arbiter_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "            g += 1\n",
    "\n",
    "    names_weights_copy,names_grads_copy = arbiter_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                      support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_arbiter.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=arbiter_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in arbiter_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d16650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "y_support_set_task = y_support_set_task.view(-1)\n",
    "x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "y_target_set_task = y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "            x=x_support_set_task,\n",
    "            y=y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "\n",
    "\n",
    "    names_weights_copy,names_grads_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                   support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_maml.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=maml_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575454f0",
   "metadata": {},
   "source": [
    "## landscape 함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "aec9618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape==  torch.Size([5, 3, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "tensor([[[[ 7.2663e-04,  4.2728e-04,  4.5608e-04],\n",
      "          [ 7.9664e-04,  6.2476e-04,  5.7781e-04],\n",
      "          [ 1.2541e-03,  9.2775e-04,  8.9281e-04]],\n",
      "\n",
      "         [[ 7.0192e-04,  3.6079e-04,  3.9703e-04],\n",
      "          [ 7.7191e-04,  5.5503e-04,  5.2302e-04],\n",
      "          [ 1.2415e-03,  8.6858e-04,  8.4888e-04]],\n",
      "\n",
      "         [[ 7.3466e-04,  3.7905e-04,  4.8354e-04],\n",
      "          [ 8.4406e-04,  6.5638e-04,  7.1711e-04],\n",
      "          [ 1.3712e-03,  1.0470e-03,  1.1152e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.0408e-04, -4.9018e-04, -6.8128e-04],\n",
      "          [-3.5310e-04, -5.5947e-04, -7.1738e-04],\n",
      "          [-3.3640e-04, -3.0521e-04, -5.1460e-04]],\n",
      "\n",
      "         [[-3.0302e-04, -4.7939e-04, -6.7903e-04],\n",
      "          [-2.9602e-04, -5.0438e-04, -6.7108e-04],\n",
      "          [-2.3251e-04, -2.2702e-04, -4.4284e-04]],\n",
      "\n",
      "         [[ 1.5364e-04, -1.2521e-04, -3.4575e-04],\n",
      "          [ 1.3126e-04, -1.2870e-04, -3.0403e-04],\n",
      "          [ 1.7356e-04,  1.3268e-04, -1.1192e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.4179e-04,  8.8114e-04,  3.8465e-04],\n",
      "          [ 7.6582e-04,  8.6468e-04,  5.2987e-04],\n",
      "          [ 1.0738e-03,  1.2573e-03,  9.3824e-04]],\n",
      "\n",
      "         [[ 7.5347e-04,  1.0210e-03,  5.8088e-04],\n",
      "          [ 8.9439e-04,  1.0305e-03,  7.2646e-04],\n",
      "          [ 1.2331e-03,  1.4320e-03,  1.1171e-03]],\n",
      "\n",
      "         [[ 1.4244e-03,  1.6722e-03,  1.2078e-03],\n",
      "          [ 1.5171e-03,  1.6476e-03,  1.3490e-03],\n",
      "          [ 1.9238e-03,  2.0464e-03,  1.7082e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.5903e-04, -4.2623e-04, -2.6846e-04],\n",
      "          [-2.0200e-04, -1.8765e-04, -1.6850e-04],\n",
      "          [-3.7754e-04, -6.9312e-05,  7.5768e-06]],\n",
      "\n",
      "         [[-5.2821e-04, -3.2960e-04, -1.3827e-04],\n",
      "          [-3.5327e-05, -6.3867e-05, -2.3310e-05],\n",
      "          [-1.3890e-04,  1.4537e-04,  2.2274e-04]],\n",
      "\n",
      "         [[ 4.8596e-04,  5.6630e-04,  7.3922e-04],\n",
      "          [ 7.4381e-04,  6.7406e-04,  7.0981e-04],\n",
      "          [ 5.2338e-04,  8.3841e-04,  8.4310e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1008e-04,  4.3198e-04,  3.9261e-04],\n",
      "          [ 3.0008e-04,  4.3119e-04,  5.6998e-04],\n",
      "          [ 1.8652e-04,  4.1778e-04,  6.6856e-04]],\n",
      "\n",
      "         [[ 4.6794e-04,  4.9159e-04,  4.7940e-04],\n",
      "          [ 3.7067e-04,  5.0855e-04,  6.6217e-04],\n",
      "          [ 2.7389e-04,  5.2027e-04,  7.7590e-04]],\n",
      "\n",
      "         [[ 8.9595e-04,  9.4691e-04,  9.0630e-04],\n",
      "          [ 7.8609e-04,  9.3962e-04,  1.0654e-03],\n",
      "          [ 6.7119e-04,  9.2428e-04,  1.1507e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0753e-03,  8.8576e-04,  5.6486e-04],\n",
      "          [ 6.7501e-04,  1.5993e-04,  4.0462e-05],\n",
      "          [ 2.6206e-04,  2.0239e-04,  3.1842e-04]],\n",
      "\n",
      "         [[ 9.5923e-04,  7.5780e-04,  4.2991e-04],\n",
      "          [ 5.6457e-04,  7.8566e-05, -3.7849e-05],\n",
      "          [ 1.4565e-04,  7.3640e-05,  2.9087e-04]],\n",
      "\n",
      "         [[ 1.2293e-03,  1.1337e-03,  8.1356e-04],\n",
      "          [ 8.6339e-04,  4.3196e-04,  3.2190e-04],\n",
      "          [ 4.8344e-04,  4.5447e-04,  6.1392e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.3652e-04, -6.7159e-06,  1.5653e-04,  1.6693e-04, -1.5269e-04,\n",
      "         6.4676e-05, -2.8740e-05, -2.6938e-05, -1.0947e-04, -8.1897e-05,\n",
      "         1.0627e-04, -4.7908e-06, -1.1976e-04,  5.6580e-05, -2.1798e-04,\n",
      "         2.4985e-04, -1.1086e-04, -3.3580e-05,  4.3365e-06, -2.1118e-04,\n",
      "         7.6307e-06,  1.7761e-04, -2.8353e-05, -8.9946e-05,  1.4991e-04,\n",
      "         3.9939e-05, -9.4771e-05,  9.7171e-05, -9.6975e-06, -1.2926e-04,\n",
      "         1.5227e-04, -2.8335e-04,  1.1744e-04, -9.2717e-05,  4.9779e-05,\n",
      "         2.5146e-04, -2.7397e-05, -2.3639e-04,  2.4755e-04, -1.3624e-05,\n",
      "         2.9319e-05, -9.9795e-06, -1.7169e-04,  3.0040e-04,  1.5188e-04,\n",
      "         1.5106e-05,  9.4060e-05,  1.4499e-07,  8.7849e-05, -1.8934e-04,\n",
      "         5.4535e-05, -9.0502e-05, -2.6341e-05, -1.7889e-05,  6.9484e-05,\n",
      "         3.5235e-04, -3.3596e-04,  7.5983e-05,  2.0773e-05,  1.1583e-04,\n",
      "        -1.3048e-05, -1.3927e-04, -4.8000e-05,  1.0815e-04, -1.1365e-04,\n",
      "         2.5512e-05, -5.6124e-05, -2.8669e-04,  1.1695e-04, -1.0551e-05,\n",
      "         9.8473e-05,  9.8117e-05,  9.6490e-05, -4.7746e-05, -3.2595e-05,\n",
      "        -2.1464e-04, -3.7268e-05,  1.2508e-04, -1.2852e-04,  1.6939e-04,\n",
      "        -1.7460e-05,  1.9742e-05, -7.3279e-05, -1.3503e-04, -2.5948e-04,\n",
      "        -1.9931e-05, -7.5822e-05,  3.9495e-05, -1.1925e-04,  1.0811e-04,\n",
      "         7.2685e-05,  7.1205e-05, -3.2603e-06,  9.7987e-05,  2.4911e-05,\n",
      "         1.2056e-04, -4.4926e-05,  3.4448e-05,  1.2890e-04, -9.2086e-05,\n",
      "        -5.5361e-05,  4.9385e-06, -9.4339e-05, -2.4603e-04, -5.1509e-06,\n",
      "        -7.1151e-05,  2.4470e-05,  2.2702e-04, -6.2130e-05,  7.5235e-05,\n",
      "        -1.2610e-04,  7.4134e-05, -1.4965e-04, -2.2465e-06,  8.3988e-06,\n",
      "         5.6915e-05,  8.4972e-05,  2.3092e-05,  5.8268e-05,  1.2606e-04,\n",
      "        -6.6947e-05, -1.6232e-04,  1.2867e-04,  9.7657e-05, -3.7390e-05,\n",
      "        -2.1146e-05,  6.9697e-05, -7.1240e-05], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 2.5385e-04,  1.6660e-04,  1.3888e-04],\n",
      "          [ 2.2637e-04,  2.1631e-04,  1.7947e-04],\n",
      "          [ 1.0840e-04,  1.6296e-04,  1.5353e-04]],\n",
      "\n",
      "         [[-9.1793e-05, -6.2741e-05, -5.8411e-06],\n",
      "          [-8.6428e-06, -2.2961e-05, -2.7009e-05],\n",
      "          [-1.6383e-04, -1.3884e-04, -1.3438e-04]],\n",
      "\n",
      "         [[ 1.4046e-04,  1.5919e-04,  1.4898e-04],\n",
      "          [ 5.8261e-05,  8.5887e-05,  1.5609e-04],\n",
      "          [ 4.3493e-05,  6.6899e-05,  1.2285e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2951e-04, -1.0542e-04, -8.9061e-05],\n",
      "          [-1.9687e-04, -1.1894e-04, -2.4200e-05],\n",
      "          [-1.2673e-04, -7.1087e-05, -6.2802e-05]],\n",
      "\n",
      "         [[ 2.0218e-04,  1.8091e-04,  1.6632e-04],\n",
      "          [ 1.7478e-04,  1.8309e-04,  1.5504e-04],\n",
      "          [ 9.9122e-05,  1.4081e-04,  1.3878e-04]],\n",
      "\n",
      "         [[ 2.2847e-04,  2.3409e-04,  1.8072e-04],\n",
      "          [ 2.4094e-04,  2.4680e-04,  1.8674e-04],\n",
      "          [ 2.0365e-04,  2.0760e-04,  1.6565e-04]]],\n",
      "\n",
      "\n",
      "        [[[-4.5374e-05, -5.1792e-05, -4.8404e-05],\n",
      "          [-5.3173e-05, -5.8498e-05, -7.6364e-05],\n",
      "          [-4.0432e-05, -3.0735e-05, -7.4470e-05]],\n",
      "\n",
      "         [[ 1.1264e-04,  1.5800e-04,  1.5948e-04],\n",
      "          [ 1.3535e-04,  1.3040e-04,  1.2584e-04],\n",
      "          [ 1.2705e-04,  8.3856e-05,  4.9213e-05]],\n",
      "\n",
      "         [[-7.3436e-05, -6.0442e-05, -5.9716e-05],\n",
      "          [-7.1980e-05, -5.7631e-05, -6.4275e-05],\n",
      "          [-4.6255e-05, -4.6497e-05, -4.9289e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4504e-04,  1.4385e-04,  1.3620e-04],\n",
      "          [ 1.5771e-04,  1.6732e-04,  1.4398e-04],\n",
      "          [ 1.6544e-04,  1.8903e-04,  1.8680e-04]],\n",
      "\n",
      "         [[-2.2366e-05, -1.4939e-05, -1.2793e-05],\n",
      "          [-1.2084e-05, -1.8764e-05, -2.8715e-05],\n",
      "          [-2.2842e-05, -6.7625e-06, -3.8208e-05]],\n",
      "\n",
      "         [[ 7.2494e-05,  7.6167e-05,  7.0722e-05],\n",
      "          [ 5.9026e-05,  6.1237e-05,  5.9060e-05],\n",
      "          [ 5.2114e-05,  4.8010e-05,  5.8922e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.1311e-04, -7.9359e-05, -5.3366e-05],\n",
      "          [-1.1782e-04, -8.1776e-05, -1.1531e-04],\n",
      "          [-1.4340e-04, -1.1222e-04, -1.4454e-04]],\n",
      "\n",
      "         [[ 1.5277e-04,  1.5633e-04,  1.6441e-04],\n",
      "          [ 2.3942e-04,  1.9039e-04,  1.5771e-04],\n",
      "          [ 2.5455e-04,  2.5259e-04,  2.6552e-04]],\n",
      "\n",
      "         [[-9.5438e-05, -1.1531e-04, -1.1221e-04],\n",
      "          [-1.0132e-04, -1.0177e-04, -1.3401e-04],\n",
      "          [-1.1761e-04, -8.8060e-05, -1.2563e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2668e-04,  9.2479e-05,  8.7041e-05],\n",
      "          [ 1.1211e-04,  1.3787e-04,  9.0167e-05],\n",
      "          [ 1.2280e-04,  1.3942e-04,  1.1980e-04]],\n",
      "\n",
      "         [[-1.3556e-04, -1.1903e-04, -8.8704e-05],\n",
      "          [-1.2733e-04, -1.2322e-04, -1.2607e-04],\n",
      "          [-1.6631e-04, -1.3542e-04, -1.4083e-04]],\n",
      "\n",
      "         [[ 1.9871e-04,  1.9677e-04,  2.2150e-04],\n",
      "          [ 2.0390e-04,  1.9886e-04,  2.0570e-04],\n",
      "          [ 2.0093e-04,  2.0337e-04,  1.9635e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 8.0198e-05,  8.6778e-05,  7.9213e-05],\n",
      "          [ 1.4962e-04,  1.4501e-04,  1.4431e-04],\n",
      "          [ 1.3051e-04,  1.2730e-04,  1.4352e-04]],\n",
      "\n",
      "         [[-2.9848e-04, -3.1907e-04, -2.7802e-04],\n",
      "          [-1.9429e-04, -1.7046e-04, -1.8047e-04],\n",
      "          [-1.7022e-04, -1.6493e-04, -1.6823e-04]],\n",
      "\n",
      "         [[ 7.0430e-05,  6.6578e-05,  6.5689e-05],\n",
      "          [ 1.2408e-04,  9.8582e-05,  7.6987e-05],\n",
      "          [ 4.7573e-05,  5.7909e-05,  1.0357e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.1535e-06, -7.9936e-05, -1.0574e-04],\n",
      "          [-1.0769e-04, -1.3531e-04, -1.1924e-04],\n",
      "          [-1.1356e-04, -1.1660e-04, -6.8385e-05]],\n",
      "\n",
      "         [[ 1.5323e-05,  2.5961e-05,  3.2119e-05],\n",
      "          [ 8.7572e-05,  9.4819e-05,  1.0294e-04],\n",
      "          [ 8.2052e-05,  8.7280e-05,  9.0287e-05]],\n",
      "\n",
      "         [[ 2.6026e-05,  3.7659e-05,  3.3630e-05],\n",
      "          [ 3.8456e-05,  4.9297e-05,  4.9539e-05],\n",
      "          [ 2.9441e-05,  3.8599e-05,  3.9253e-05]]],\n",
      "\n",
      "\n",
      "        [[[-5.0623e-05, -6.9789e-05, -2.7806e-05],\n",
      "          [-3.6763e-05, -7.5556e-05, -3.3534e-05],\n",
      "          [-3.6795e-05, -5.0411e-05, -3.6025e-05]],\n",
      "\n",
      "         [[ 5.4334e-05,  8.1686e-05,  6.8792e-05],\n",
      "          [ 5.9767e-05,  7.6513e-05,  4.2539e-05],\n",
      "          [ 3.2378e-05,  3.7467e-05,  4.0067e-05]],\n",
      "\n",
      "         [[-6.7015e-05, -5.1572e-05, -6.2296e-05],\n",
      "          [-5.3279e-05, -6.0346e-05, -6.0961e-05],\n",
      "          [-4.1974e-05, -2.4092e-05, -4.7650e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.3363e-05,  1.0498e-04,  1.1920e-04],\n",
      "          [ 1.0755e-04,  1.2729e-04,  1.2825e-04],\n",
      "          [ 1.2671e-04,  1.4029e-04,  9.3059e-05]],\n",
      "\n",
      "         [[-3.5456e-05, -5.5331e-05, -3.2060e-05],\n",
      "          [-2.9739e-05, -4.8756e-05, -3.2926e-05],\n",
      "          [-1.9604e-05, -2.9548e-05, -2.4969e-05]],\n",
      "\n",
      "         [[-3.6587e-05, -3.8518e-05, -5.3389e-05],\n",
      "          [-3.5559e-05, -3.2093e-05, -4.4861e-05],\n",
      "          [-2.9187e-05, -3.1602e-05, -4.6782e-05]]],\n",
      "\n",
      "\n",
      "        [[[-4.4737e-05, -4.4222e-05, -4.1715e-05],\n",
      "          [-4.1513e-05, -5.7095e-05, -4.5282e-05],\n",
      "          [-4.1562e-05, -5.4060e-05, -4.1759e-05]],\n",
      "\n",
      "         [[ 1.6499e-05,  1.5109e-05,  1.3015e-05],\n",
      "          [ 1.0832e-05,  5.0028e-06,  1.7310e-05],\n",
      "          [ 2.6547e-05,  1.9993e-05,  4.3496e-05]],\n",
      "\n",
      "         [[-4.6219e-05, -5.6296e-05, -3.8539e-05],\n",
      "          [-5.3552e-05, -5.4168e-05, -2.8469e-05],\n",
      "          [-3.8669e-05, -2.9475e-05, -1.9453e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.6800e-05, -7.5357e-05, -6.7112e-05],\n",
      "          [-2.5008e-05, -2.7825e-05, -1.3169e-05],\n",
      "          [-4.2824e-06,  8.2846e-06,  3.7716e-06]],\n",
      "\n",
      "         [[-5.4847e-05, -5.9422e-05, -5.8581e-05],\n",
      "          [-5.6255e-05, -7.1540e-05, -6.9962e-05],\n",
      "          [-3.5824e-05, -6.0655e-05, -5.8674e-05]],\n",
      "\n",
      "         [[-2.8166e-05, -2.8089e-05, -1.5036e-05],\n",
      "          [-3.3885e-05, -3.2323e-05, -1.6269e-05],\n",
      "          [-4.7259e-05, -4.5392e-05, -3.0855e-05]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 1.2246e-04, -3.6449e-06,  7.2876e-05],\n",
      "          [ 1.0965e-04,  4.4175e-05,  9.2855e-05],\n",
      "          [ 9.0930e-05,  1.3394e-04,  1.3526e-04]],\n",
      "\n",
      "         [[-7.1502e-05, -8.8469e-05, -1.6589e-04],\n",
      "          [-7.8597e-05, -9.8720e-05, -1.2725e-04],\n",
      "          [-1.7307e-05, -1.6068e-04, -8.2483e-05]],\n",
      "\n",
      "         [[ 9.7585e-05,  1.0532e-04, -1.6863e-04],\n",
      "          [ 3.9608e-05,  5.9329e-05,  4.7696e-06],\n",
      "          [ 1.8008e-04,  1.8674e-05,  1.6497e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2838e-05, -3.5301e-05,  6.8710e-05],\n",
      "          [ 2.1039e-06,  8.5503e-05,  1.3881e-05],\n",
      "          [ 4.4945e-06,  4.5393e-06, -1.7881e-05]],\n",
      "\n",
      "         [[ 2.1344e-04,  8.5109e-05, -7.3134e-05],\n",
      "          [ 1.2446e-04,  1.7439e-04,  8.2572e-05],\n",
      "          [ 1.7289e-04,  8.7872e-05,  1.7018e-04]],\n",
      "\n",
      "         [[ 8.5553e-05,  4.9352e-05,  6.3952e-05],\n",
      "          [ 9.9195e-05,  1.8204e-04,  9.8082e-05],\n",
      "          [ 1.3440e-04,  1.7354e-04,  1.3532e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4683e-04,  1.1819e-04,  9.0544e-05],\n",
      "          [ 1.9984e-04,  9.5175e-05,  3.0105e-05],\n",
      "          [ 1.4356e-04,  1.0083e-04,  1.3595e-04]],\n",
      "\n",
      "         [[ 3.1374e-05,  7.4156e-05,  2.0888e-05],\n",
      "          [ 1.6932e-05,  1.0584e-04,  1.0989e-04],\n",
      "          [ 1.0419e-05,  3.4500e-05,  9.0725e-05]],\n",
      "\n",
      "         [[ 5.2375e-05, -1.1145e-05,  1.5156e-04],\n",
      "          [-2.8811e-05, -4.3288e-05,  1.1286e-04],\n",
      "          [ 4.3572e-05, -7.3976e-05,  3.2157e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.1045e-05, -1.8854e-05,  1.1093e-05],\n",
      "          [ 1.3410e-05,  3.2185e-05, -1.1509e-05],\n",
      "          [-5.2008e-05,  1.4951e-05, -2.9136e-05]],\n",
      "\n",
      "         [[-1.6174e-05, -1.2856e-05, -5.9456e-05],\n",
      "          [-8.6570e-05, -1.1871e-04, -4.6219e-05],\n",
      "          [-6.8589e-05, -1.2488e-04, -5.8702e-05]],\n",
      "\n",
      "         [[ 8.9798e-05,  5.8682e-05,  4.2926e-05],\n",
      "          [-7.7515e-06, -6.6560e-05, -3.9631e-05],\n",
      "          [-8.1734e-05, -1.6061e-04, -9.3584e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0245e-05,  1.3099e-04,  6.6948e-05],\n",
      "          [ 1.0684e-04,  1.7900e-04,  8.3539e-05],\n",
      "          [ 1.2973e-04,  2.0837e-04,  7.6269e-05]],\n",
      "\n",
      "         [[-2.1298e-05,  2.0174e-05, -5.9835e-05],\n",
      "          [-4.1477e-05, -1.2549e-04, -1.6794e-04],\n",
      "          [-2.1468e-04, -2.2172e-04, -2.0388e-04]],\n",
      "\n",
      "         [[-1.6101e-04,  7.3023e-05,  5.4368e-05],\n",
      "          [ 4.5202e-05, -1.4359e-04, -2.5168e-05],\n",
      "          [ 4.7084e-05, -5.8308e-05, -1.1037e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.8242e-05, -7.9850e-07,  2.9843e-05],\n",
      "          [-1.6740e-05,  4.1829e-05,  1.2699e-04],\n",
      "          [ 7.9454e-05,  3.5272e-05,  8.0601e-05]],\n",
      "\n",
      "         [[ 2.9160e-05,  1.2216e-04,  1.7000e-05],\n",
      "          [ 8.0452e-05, -5.6339e-06, -5.4487e-05],\n",
      "          [ 1.5548e-04,  4.2385e-05, -3.3190e-05]],\n",
      "\n",
      "         [[ 4.2326e-05,  1.0100e-04,  1.4254e-04],\n",
      "          [ 3.8842e-05,  9.9420e-05,  2.3581e-04],\n",
      "          [ 1.9445e-04,  1.6574e-04,  1.4681e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.4357e-05,  2.5424e-05,  5.1921e-05],\n",
      "          [ 1.0862e-04,  8.8027e-05,  1.8012e-04],\n",
      "          [ 1.4925e-04,  1.2956e-04,  9.7917e-05]],\n",
      "\n",
      "         [[-1.7389e-04, -2.1304e-04, -1.3245e-04],\n",
      "          [-4.8675e-05, -2.2374e-04, -2.2439e-04],\n",
      "          [-5.3668e-05, -1.4650e-04, -1.1677e-04]],\n",
      "\n",
      "         [[ 1.8188e-05,  4.2611e-05,  4.3100e-05],\n",
      "          [ 2.9234e-05,  8.5791e-05, -1.7776e-05],\n",
      "          [ 1.3081e-04, -6.6338e-06,  5.7706e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7724e-05, -4.0584e-05,  2.3447e-05],\n",
      "          [-6.0006e-05,  2.2672e-05,  2.1922e-05],\n",
      "          [ 1.7256e-06,  8.6755e-07, -1.9870e-05]],\n",
      "\n",
      "         [[ 2.7990e-04,  2.0715e-04,  2.1030e-04],\n",
      "          [ 2.3590e-04,  2.7283e-04,  2.0147e-04],\n",
      "          [ 9.2214e-05,  1.7298e-04,  1.8794e-04]],\n",
      "\n",
      "         [[ 2.9374e-04,  2.6428e-04,  3.0756e-04],\n",
      "          [ 3.1178e-04,  4.0294e-04,  3.9619e-04],\n",
      "          [ 3.1444e-04,  3.6423e-04,  2.8645e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3840e-05,  6.9574e-05,  4.9733e-05],\n",
      "          [ 6.4886e-06,  1.1265e-05, -1.1799e-05],\n",
      "          [ 5.4371e-05,  2.5231e-05,  2.7118e-05]],\n",
      "\n",
      "         [[ 2.7865e-05,  1.1716e-04,  8.5272e-05],\n",
      "          [ 1.3160e-04,  8.4335e-05,  5.7867e-05],\n",
      "          [ 5.6430e-05, -1.4868e-05, -4.2770e-05]],\n",
      "\n",
      "         [[-5.0981e-05, -3.1850e-05,  9.9302e-05],\n",
      "          [-9.6920e-05, -3.9726e-05,  1.6459e-05],\n",
      "          [-2.7260e-05, -8.3950e-05, -4.2204e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.7627e-05,  5.5908e-05,  1.8511e-06],\n",
      "          [-1.9163e-05,  5.4377e-05,  2.4603e-05],\n",
      "          [ 3.8308e-07,  1.2426e-05,  4.2644e-05]],\n",
      "\n",
      "         [[-4.6464e-05,  4.3380e-05,  9.5081e-05],\n",
      "          [-1.5779e-04, -3.3783e-06,  9.8231e-06],\n",
      "          [-1.3784e-04, -1.3476e-04, -7.0429e-05]],\n",
      "\n",
      "         [[-4.9145e-05, -3.8044e-05, -2.3626e-05],\n",
      "          [-1.0541e-04, -7.3893e-05, -4.0982e-06],\n",
      "          [-8.8101e-05, -4.9777e-05,  4.3212e-06]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3016e-04,  1.8920e-04,  3.1619e-05],\n",
      "          [ 3.0549e-04,  1.6502e-04,  6.3680e-06],\n",
      "          [ 3.2383e-04,  1.4830e-04,  1.0753e-04]],\n",
      "\n",
      "         [[-7.4944e-05,  9.5838e-05,  3.5045e-05],\n",
      "          [-7.8674e-05,  8.8490e-05,  6.5010e-06],\n",
      "          [ 2.9534e-05,  4.0597e-05,  1.5627e-06]],\n",
      "\n",
      "         [[ 1.1189e-04, -7.9731e-05, -8.6854e-05],\n",
      "          [ 9.9809e-05, -2.6528e-05, -7.1552e-05],\n",
      "          [ 1.0781e-04,  1.3936e-04, -2.0260e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.8961e-05,  5.1722e-05,  2.6632e-05],\n",
      "          [ 3.8814e-05, -4.6650e-05, -4.1199e-05],\n",
      "          [-2.7919e-05, -1.4593e-04, -8.5807e-05]],\n",
      "\n",
      "         [[-5.6665e-05, -1.3830e-04, -2.3950e-04],\n",
      "          [-1.3843e-04, -1.0434e-04, -1.7711e-04],\n",
      "          [-1.5464e-05, -4.2530e-05, -1.7816e-04]],\n",
      "\n",
      "         [[-4.0771e-05, -2.2344e-04, -3.2122e-04],\n",
      "          [-3.3410e-05, -2.3545e-04, -2.4264e-04],\n",
      "          [-1.0384e-04, -2.2223e-04, -1.1281e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 7.2760e-12,  9.0949e-12, -7.2760e-12, -7.2760e-12,  0.0000e+00,\n",
      "        -7.2760e-12,  0.0000e+00,  2.9104e-11, -2.1828e-11,  0.0000e+00,\n",
      "         2.3647e-11, -2.9104e-11,  2.9104e-11,  7.2760e-12,  1.4552e-11,\n",
      "         1.4552e-11,  1.4552e-11, -6.1846e-11,  1.8190e-11,  0.0000e+00,\n",
      "         3.6380e-12, -1.1823e-11,  2.3647e-11,  2.0009e-11,  0.0000e+00,\n",
      "        -3.2742e-11,  2.1828e-11, -2.1828e-11,  0.0000e+00, -5.4570e-12,\n",
      "         5.0932e-11,  7.2760e-12, -7.2760e-12,  7.2760e-12,  2.1828e-11,\n",
      "        -2.9104e-11,  0.0000e+00, -2.7285e-11,  1.2733e-11, -2.5466e-11,\n",
      "        -1.0004e-11,  1.0914e-11,  0.0000e+00, -1.2733e-11, -1.0004e-11,\n",
      "         5.8208e-11,  1.4552e-11, -7.2760e-12, -3.6380e-12, -7.2760e-12,\n",
      "         2.1828e-11,  7.2760e-12, -1.4552e-11, -2.1828e-11, -1.8190e-12,\n",
      "         3.6380e-12, -3.3651e-11, -2.1828e-11,  3.6380e-11, -5.4570e-12,\n",
      "         2.9104e-11, -9.0949e-12, -4.7294e-11,  3.5470e-11,  1.5461e-11,\n",
      "        -7.2760e-12, -1.4552e-11, -1.8190e-11,  1.4552e-11, -9.0949e-12,\n",
      "        -3.0923e-11,  0.0000e+00, -2.9104e-11,  1.6371e-11,  4.7294e-11,\n",
      "        -7.2760e-12, -2.5466e-11, -1.8190e-11, -9.0949e-12,  9.0949e-12,\n",
      "         2.9104e-11,  1.0914e-11,  1.4552e-11, -6.5484e-11,  7.2760e-12,\n",
      "         2.0009e-11,  1.4552e-11,  1.4552e-11, -2.9104e-11,  4.0018e-11,\n",
      "         7.2760e-12,  1.0914e-11,  7.2760e-12, -3.6380e-12,  7.2760e-12,\n",
      "         5.4570e-12,  5.4570e-12, -7.2760e-12,  3.6380e-12,  5.4570e-11,\n",
      "        -1.4552e-11, -3.6380e-12, -7.2760e-12, -3.6380e-12, -1.4552e-11,\n",
      "        -2.9104e-11,  7.2760e-12, -3.6380e-11, -1.4552e-11, -1.4552e-11,\n",
      "         1.4552e-11,  1.4552e-11,  4.0018e-11, -1.0914e-11,  1.8190e-12,\n",
      "        -7.6398e-11,  3.6380e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         3.6380e-12,  1.4552e-11, -2.5466e-11,  1.8190e-12,  2.1828e-11,\n",
      "         2.5466e-11,  4.5475e-12, -1.4552e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 9.1832e-05,  1.3474e-04, -3.4262e-04, -3.1075e-04,  4.4728e-05,\n",
      "        -4.3807e-05,  9.7449e-05,  3.1199e-04,  1.1669e-04, -4.1387e-04,\n",
      "         2.4307e-04,  4.7205e-04,  8.4047e-05, -5.1597e-05, -1.9443e-04,\n",
      "        -1.9261e-04, -7.8012e-05,  3.7061e-04, -2.0921e-04, -2.3163e-05,\n",
      "         4.0199e-05,  2.3799e-04, -3.4509e-04, -2.8878e-04,  2.2359e-04,\n",
      "         1.6512e-04, -2.2524e-05, -1.8100e-04,  1.9385e-04,  3.9377e-05,\n",
      "        -1.7556e-04, -9.9355e-05, -2.2574e-04, -1.2237e-04, -2.3197e-04,\n",
      "         3.4489e-04, -1.1045e-04,  3.7443e-05,  5.4144e-05,  1.2105e-04,\n",
      "        -2.3950e-04, -2.7499e-04,  3.6250e-04,  1.4920e-05, -2.9583e-04,\n",
      "         8.7565e-04, -7.7103e-05, -1.4486e-04, -3.4344e-05,  9.8583e-05,\n",
      "         1.9317e-04,  4.9854e-06, -1.6141e-04,  6.7063e-05,  4.1038e-04,\n",
      "         1.0786e-04, -4.4239e-04,  3.5326e-04, -9.9333e-05,  2.7572e-05,\n",
      "        -1.2447e-05,  9.7256e-05, -1.8832e-04,  1.9873e-04,  2.6206e-05,\n",
      "        -4.0038e-05, -2.6086e-04,  2.0403e-04, -2.8738e-05,  2.4573e-04,\n",
      "         8.4753e-05, -2.4702e-04,  1.3016e-04, -8.7390e-05, -2.6044e-04,\n",
      "         2.3923e-04, -4.8197e-04, -1.6809e-04, -1.3429e-04, -1.6674e-04,\n",
      "        -5.9360e-05,  7.8088e-05,  5.4450e-04, -1.0497e-04, -2.5159e-04,\n",
      "        -7.3088e-05, -1.5301e-05, -3.6336e-04, -4.2573e-04,  1.8640e-04,\n",
      "        -1.8311e-04,  5.7012e-05, -4.7773e-04, -4.4598e-04,  2.4385e-05,\n",
      "        -2.1553e-04,  1.8025e-04,  1.7575e-04, -1.0420e-04,  3.2489e-04,\n",
      "         6.1587e-05,  1.9810e-04,  7.2947e-06, -1.4032e-04, -2.4428e-04,\n",
      "         4.3960e-04, -7.0825e-05,  6.3830e-05,  1.8398e-04, -9.7965e-05,\n",
      "         7.3533e-05, -1.4729e-04,  5.7036e-04, -7.3541e-04,  3.6728e-05,\n",
      "         4.1448e-04,  3.4054e-04,  4.6224e-04,  1.0084e-05, -2.2234e-05,\n",
      "        -3.5415e-04,  1.9630e-04,  3.6495e-04,  9.2937e-05,  6.0906e-05,\n",
      "         3.8265e-04, -1.4654e-04, -3.6044e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.4552e-11,  0.0000e+00, -1.8190e-12,  5.8208e-11,  0.0000e+00,\n",
      "         1.2733e-11, -7.2760e-12,  2.5466e-11, -1.8190e-11,  3.6380e-12,\n",
      "         3.6380e-12,  0.0000e+00,  7.2760e-12,  0.0000e+00,  1.4552e-11,\n",
      "         4.0018e-11, -2.5466e-11,  7.2760e-12,  0.0000e+00,  8.0036e-11,\n",
      "         1.5461e-11,  2.9104e-11, -7.2760e-12,  1.8190e-11,  2.9104e-11,\n",
      "        -3.6380e-12,  3.6380e-11, -3.6380e-12,  2.3647e-11,  5.4570e-12,\n",
      "         4.0745e-10,  3.6380e-12, -2.1828e-11,  7.2760e-12,  7.2760e-12,\n",
      "         2.1828e-11,  3.6380e-11,  2.9104e-11, -1.2733e-11, -3.6380e-12,\n",
      "        -3.6380e-12,  3.2742e-11,  3.4561e-11,  9.0949e-12,  4.0018e-11,\n",
      "         0.0000e+00,  1.4552e-11, -7.2760e-12, -2.9104e-11,  9.0949e-12,\n",
      "        -1.8190e-11, -7.2760e-12, -4.3656e-11, -1.0914e-11, -8.7311e-11,\n",
      "        -2.1828e-11,  1.1642e-10, -2.1828e-11, -8.7311e-11, -1.0914e-11,\n",
      "         0.0000e+00,  2.3647e-11,  5.8208e-11, -1.4552e-11,  2.1828e-11,\n",
      "         7.2760e-12,  2.9104e-11, -1.8190e-11,  3.6380e-11, -1.8190e-11,\n",
      "         0.0000e+00,  4.3656e-11,  2.9104e-11, -2.1828e-11,  1.4552e-11,\n",
      "        -4.0018e-11,  7.2760e-12, -3.3651e-11, -1.0914e-11, -3.6380e-12,\n",
      "        -1.4552e-11,  1.0914e-11, -1.8190e-11,  3.2742e-11, -1.9099e-11,\n",
      "         2.5466e-11, -1.4552e-11, -7.2760e-12, -2.1828e-11,  2.9104e-11,\n",
      "        -4.7294e-11, -3.6380e-12,  5.0932e-11,  7.2760e-12, -1.1642e-10,\n",
      "         0.0000e+00,  2.7285e-12, -7.2760e-12,  7.2760e-11, -6.1846e-11,\n",
      "        -2.7285e-12, -7.2760e-12,  7.2760e-12,  3.6380e-12, -7.2760e-12,\n",
      "         2.9104e-11,  3.6380e-11,  0.0000e+00,  0.0000e+00,  5.0932e-11,\n",
      "        -7.2760e-12,  3.6380e-11,  6.5938e-12,  7.2760e-12,  7.2760e-12,\n",
      "         5.4570e-12,  4.0018e-11,  2.9104e-11,  1.0914e-11,  0.0000e+00,\n",
      "         5.4570e-11, -9.0949e-13,  7.2760e-12,  8.0036e-11, -3.6380e-12,\n",
      "         3.2742e-11, -1.4552e-11, -4.3656e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[ 3.4278e-03,  4.1418e-03,  3.5998e-03,  ...,  1.6937e-03,\n",
      "          1.6584e-03,  1.7031e-03],\n",
      "        [-5.7051e-04, -7.4359e-04, -8.0040e-04,  ..., -1.7062e-03,\n",
      "         -1.4029e-03, -9.6170e-04],\n",
      "        [ 8.4098e-05, -3.3949e-06,  7.3757e-05,  ..., -1.6088e-05,\n",
      "         -3.2249e-04, -8.6392e-04],\n",
      "        [-3.0476e-03, -3.5191e-03, -2.9781e-03,  ...,  1.5768e-05,\n",
      "          5.3209e-05,  1.0833e-04],\n",
      "        [ 1.0633e-04,  1.2431e-04,  1.0496e-04,  ...,  1.2794e-05,\n",
      "          1.3750e-05,  1.4203e-05]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([ 3.0412e-03, -4.3805e-04, -1.3500e-03, -1.3150e-03,  6.1898e-05],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.2178e-09, -6.0237e-10,  5.2761e-09,  2.1424e-09, -5.1491e-10,\n",
      "        -3.2585e-09,  7.9926e-10, -1.1111e-09,  2.9589e-09,  6.8545e-10,\n",
      "         7.5397e-10, -7.3596e-10,  2.1832e-09, -3.0872e-09,  2.5638e-09,\n",
      "         1.1623e-08,  1.7553e-09, -1.5524e-09,  1.4703e-09, -1.0399e-09,\n",
      "        -1.2504e-10,  2.7969e-09, -3.5700e-09, -3.0788e-10,  1.4968e-09,\n",
      "         5.3179e-09, -5.9107e-10, -3.3483e-09, -5.1681e-09,  2.2909e-09,\n",
      "        -2.5274e-09,  7.5000e-10,  1.7266e-09, -1.6820e-09, -2.8361e-09,\n",
      "         3.5300e-09, -4.5217e-09,  6.3170e-09,  6.1955e-10, -6.6135e-09,\n",
      "        -1.6889e-10,  1.2877e-09, -3.8342e-10,  8.6838e-11,  7.3149e-10,\n",
      "         2.6829e-09,  2.3632e-09, -3.0357e-09,  8.3309e-09, -2.9912e-09,\n",
      "         2.9140e-09, -2.3060e-09,  7.7212e-10, -1.3879e-09, -2.3983e-10,\n",
      "         6.5152e-09, -1.4200e-09, -1.9426e-09,  1.6446e-09,  1.2383e-11,\n",
      "         1.8576e-09,  7.0151e-09,  8.4197e-10, -7.2719e-10,  2.4716e-09,\n",
      "         3.0074e-10, -2.1785e-09, -2.2906e-09,  3.8612e-09,  1.9914e-09,\n",
      "         3.1820e-10, -2.1017e-09,  9.0941e-10,  5.4121e-09, -1.7378e-09,\n",
      "         3.0932e-09,  2.1575e-09,  3.8788e-09,  1.8525e-10,  2.6173e-09,\n",
      "         6.2488e-10, -2.0028e-09,  1.8782e-09, -1.0257e-10, -1.7063e-09,\n",
      "        -1.9852e-09,  1.7140e-09, -4.1403e-09,  3.9502e-09, -1.6101e-09,\n",
      "         1.6242e-09, -2.8813e-09, -1.5840e-09, -1.7106e-09, -3.6542e-09,\n",
      "        -1.0721e-09,  2.8673e-09, -1.3329e-10,  3.4396e-10, -1.0678e-09,\n",
      "        -2.3290e-09, -2.4602e-09,  5.1064e-09,  2.0240e-10, -3.8161e-09,\n",
      "        -3.4134e-11, -8.4675e-10,  4.4935e-09,  1.6425e-09, -3.5616e-09,\n",
      "         4.6815e-09,  3.5667e-09,  3.0761e-09, -2.3468e-09, -1.8084e-09,\n",
      "        -4.3111e-10, -5.7499e-11, -1.8304e-09,  1.7014e-09,  3.8895e-09,\n",
      "        -2.5568e-09, -4.2614e-10, -2.9320e-09,  3.1430e-10,  2.5060e-09,\n",
      "         2.4132e-09, -5.4595e-10,  2.2906e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 5.4580e-09, -5.4439e-09,  2.4384e-09, -1.0393e-09,  2.6996e-09,\n",
      "         1.0106e-09,  6.2975e-09,  4.2848e-10,  3.5556e-09,  1.5572e-09,\n",
      "         2.6591e-09,  4.1243e-09,  3.5852e-09,  4.1866e-09,  3.1679e-09,\n",
      "        -1.4136e-08,  8.2228e-09, -1.3093e-09, -2.9910e-09, -1.9266e-09,\n",
      "        -1.9812e-09,  5.5614e-10,  4.1766e-09,  4.9437e-09, -1.4964e-08,\n",
      "         2.0041e-09, -7.0644e-09,  3.0395e-09,  4.1163e-09,  1.1099e-08,\n",
      "         2.2664e-09, -9.0209e-09,  1.2639e-10,  7.9547e-10, -5.4578e-09,\n",
      "         1.5782e-09, -6.6660e-10,  3.8392e-09, -1.0343e-10,  4.9360e-09,\n",
      "         3.6638e-09,  4.0914e-10, -2.0787e-09,  7.2483e-10,  1.9735e-09,\n",
      "        -6.7770e-09, -1.0057e-09,  2.4558e-09, -4.0745e-10, -8.3425e-09,\n",
      "         2.4133e-09,  1.5174e-10,  1.1077e-08, -2.0435e-09,  1.6921e-09,\n",
      "        -4.5799e-09,  2.8923e-09, -4.9248e-11, -1.2262e-09,  5.4504e-10,\n",
      "        -1.4171e-09,  5.8341e-10,  2.1930e-09, -3.9198e-09, -8.8969e-09,\n",
      "        -1.0839e-09, -9.3047e-10,  9.0676e-10, -2.0696e-09,  7.8676e-09,\n",
      "        -3.5950e-09,  3.5177e-09, -6.1430e-09,  1.3751e-09, -1.6779e-08,\n",
      "        -3.0351e-09,  8.1700e-09,  1.1434e-08,  1.5887e-09, -6.4596e-09,\n",
      "         1.2539e-09,  5.2600e-09,  4.2954e-09, -1.3386e-09, -5.5868e-09,\n",
      "        -1.6761e-09,  3.5467e-09,  9.9696e-10,  7.3438e-10,  6.3237e-09,\n",
      "         4.0876e-09,  2.2861e-09,  2.2185e-09, -5.8171e-09, -6.6000e-09,\n",
      "        -2.3897e-09,  7.7056e-10,  6.3018e-09,  6.2329e-10, -4.5277e-09,\n",
      "        -1.4829e-09,  6.7779e-09, -2.7043e-09, -2.6919e-09, -1.6156e-09,\n",
      "        -5.0765e-09,  1.1098e-10,  7.4229e-09,  3.3981e-10, -1.4655e-09,\n",
      "        -3.4670e-09, -9.8256e-10, -3.0781e-09, -4.8819e-09, -3.7576e-09,\n",
      "        -2.2732e-09, -3.8783e-10, -1.0796e-08,  4.2840e-09, -3.0332e-09,\n",
      "         3.1168e-09,  9.9057e-09,  6.2952e-09, -2.4770e-09,  4.1266e-09,\n",
      "        -4.1826e-09, -2.7900e-09, -5.8781e-09], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-5.5531e-09, -8.2554e-10, -4.5510e-09, -1.0225e-09, -3.6111e-09,\n",
      "         1.3344e-09, -1.7179e-09, -5.6164e-09, -9.2596e-10, -1.4798e-09,\n",
      "        -3.9692e-10,  2.8156e-09, -9.3638e-09,  5.5349e-09, -9.6911e-09,\n",
      "         2.0692e-09,  8.5529e-09,  5.5379e-09, -3.6841e-09, -2.0388e-09,\n",
      "        -6.0216e-09,  1.9665e-09,  4.5663e-10,  9.7454e-10,  1.8863e-09,\n",
      "        -2.9937e-09,  5.0907e-09,  1.8364e-10,  5.0883e-10,  2.6827e-09,\n",
      "         1.1559e-08,  3.1498e-09,  3.1847e-10,  1.9317e-09,  1.0203e-08,\n",
      "        -7.2367e-09, -1.4265e-09,  1.5819e-09, -6.4887e-10, -3.3840e-09,\n",
      "         2.6574e-09, -2.4787e-09, -2.2244e-09, -6.0926e-09,  6.8413e-09,\n",
      "         1.5270e-10,  1.9901e-09, -1.3661e-09, -1.4362e-09,  9.4791e-10,\n",
      "        -1.6188e-08, -3.1280e-09,  1.4008e-09, -1.1009e-09, -6.2801e-09,\n",
      "         6.0954e-09,  4.3585e-09,  1.3765e-09,  1.6808e-09, -4.7969e-09,\n",
      "         2.0743e-09,  4.1268e-09,  3.3388e-08, -4.6277e-10,  1.2959e-09,\n",
      "        -4.5968e-10, -1.0817e-09,  5.1385e-10,  7.2539e-10,  4.5258e-10,\n",
      "        -2.9902e-09, -2.3742e-09,  1.3270e-09, -1.7076e-09,  5.2664e-09,\n",
      "        -3.0588e-09, -5.8743e-09,  3.5411e-09, -1.9113e-09,  2.0642e-09,\n",
      "        -1.3142e-09, -4.8886e-09, -5.8474e-09, -2.7774e-09,  1.6594e-09,\n",
      "         5.0113e-09,  1.6268e-09, -3.5857e-09,  2.2560e-08,  6.7687e-10,\n",
      "        -2.6755e-09,  2.4039e-09,  4.0627e-09, -5.2235e-09, -8.3728e-10,\n",
      "         8.0578e-10, -3.3620e-09,  5.3663e-10, -7.1559e-09, -3.0877e-09,\n",
      "        -2.4988e-09,  7.5158e-10,  3.5076e-10, -7.3222e-09,  3.4690e-10,\n",
      "        -3.0334e-09,  1.0293e-08, -7.6366e-10,  9.2862e-10, -3.5190e-09,\n",
      "        -1.1268e-09, -4.1539e-09,  1.5084e-09, -2.1289e-09,  1.8116e-08,\n",
      "        -1.9376e-09,  3.9613e-09, -6.0838e-09, -2.4997e-09, -2.5218e-09,\n",
      "        -2.0824e-09, -7.1228e-09,  1.1605e-09,  1.4786e-09,  1.1097e-08,\n",
      "         4.8684e-09,  2.0601e-09,  6.0251e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-4.2241e-11, -2.9573e-10, -7.6653e-11],\n",
      "          [-2.8665e-10, -2.6736e-10,  1.5033e-11],\n",
      "          [ 1.5606e-10, -7.0232e-11,  2.2907e-11]],\n",
      "\n",
      "         [[-1.3133e-10,  1.8398e-10,  6.6362e-10],\n",
      "          [-9.4730e-11, -2.6086e-10,  5.7008e-10],\n",
      "          [ 1.8632e-12,  8.5348e-10, -5.0593e-11]],\n",
      "\n",
      "         [[-5.6674e-10, -1.7073e-10,  4.0530e-11],\n",
      "          [-3.0101e-10, -2.3818e-10, -3.5726e-10],\n",
      "          [ 1.3448e-10, -3.2486e-11,  1.6793e-10]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.4002e-10,  4.5476e-10,  4.4746e-10],\n",
      "          [-2.0777e-10, -8.4746e-10,  5.5057e-10],\n",
      "          [ 5.5206e-11, -2.6517e-10,  1.9938e-10]],\n",
      "\n",
      "         [[-1.1764e-10,  7.8878e-11,  3.5310e-10],\n",
      "          [-2.0287e-10, -2.3699e-10,  9.7250e-11],\n",
      "          [-2.8790e-10,  4.9073e-11, -1.1920e-12]],\n",
      "\n",
      "         [[-2.5419e-10,  9.9399e-11, -2.1305e-10],\n",
      "          [ 5.3783e-10,  1.9081e-10,  4.1651e-10],\n",
      "          [ 4.7087e-11, -1.9607e-10, -1.5244e-10]]],\n",
      "\n",
      "\n",
      "        [[[-5.8600e-11, -1.0968e-10, -2.9576e-10],\n",
      "          [ 5.9200e-12,  1.2548e-10,  1.0199e-10],\n",
      "          [-3.8706e-11, -4.6987e-11,  5.5475e-11]],\n",
      "\n",
      "         [[-3.4003e-10, -2.0665e-10, -2.8046e-10],\n",
      "          [-1.9719e-10,  6.1259e-11,  2.1932e-10],\n",
      "          [-1.2046e-11,  6.3958e-11, -2.8128e-10]],\n",
      "\n",
      "         [[ 1.7816e-11,  3.3095e-10,  2.9373e-10],\n",
      "          [ 3.9636e-11, -7.9035e-11, -3.2367e-10],\n",
      "          [-1.0084e-10, -1.1476e-10, -5.0475e-11]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5391e-10,  2.9035e-10, -2.2776e-10],\n",
      "          [-4.6987e-11,  3.5650e-10,  7.1571e-11],\n",
      "          [ 2.5370e-11,  3.9247e-10,  5.8113e-11]],\n",
      "\n",
      "         [[ 3.5142e-11,  8.9486e-11, -2.3916e-11],\n",
      "          [-1.4400e-11,  4.8243e-11,  5.4458e-11],\n",
      "          [-3.7338e-11,  7.3412e-11, -3.3940e-11]],\n",
      "\n",
      "         [[-3.8681e-10,  3.1402e-10, -1.2620e-10],\n",
      "          [-5.8584e-11, -2.1536e-11, -8.7758e-11],\n",
      "          [-1.5223e-10, -1.0775e-10, -6.6785e-11]]],\n",
      "\n",
      "\n",
      "        [[[-8.4735e-11,  1.8535e-10,  1.9536e-10],\n",
      "          [-1.9617e-10, -7.6758e-11, -6.0672e-10],\n",
      "          [ 1.4531e-10, -1.4082e-10,  3.7555e-10]],\n",
      "\n",
      "         [[-2.5351e-10,  6.6313e-10,  3.1336e-11],\n",
      "          [-4.4347e-10, -1.9793e-10,  3.0767e-10],\n",
      "          [-1.5828e-10, -7.7020e-11,  3.8393e-11]],\n",
      "\n",
      "         [[-3.5407e-10, -5.8715e-10, -2.1111e-10],\n",
      "          [-1.3591e-10, -9.6396e-11,  5.1486e-10],\n",
      "          [-2.0872e-10,  8.2833e-11,  2.1862e-10]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9931e-10,  3.8213e-10,  9.1512e-10],\n",
      "          [ 7.6901e-10,  1.8854e-09,  2.4716e-09],\n",
      "          [ 3.6083e-10, -3.2130e-10,  6.8673e-10]],\n",
      "\n",
      "         [[-1.4713e-10, -2.3691e-10,  3.0562e-11],\n",
      "          [ 1.2729e-10,  1.7339e-11, -2.7742e-11],\n",
      "          [ 1.1921e-10, -5.6526e-11,  1.1090e-10]],\n",
      "\n",
      "         [[-2.5466e-10, -2.0480e-10, -3.5365e-10],\n",
      "          [-3.1045e-10, -2.7093e-10, -1.5301e-10],\n",
      "          [-1.7862e-10, -3.5179e-10, -3.6377e-10]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.0073e-11,  1.5499e-11, -5.9227e-11],\n",
      "          [-1.0931e-11, -5.8790e-10, -1.2304e-10],\n",
      "          [ 2.1877e-10, -1.8011e-10,  2.3626e-12]],\n",
      "\n",
      "         [[-3.3170e-10,  6.5077e-11, -2.1397e-10],\n",
      "          [-4.4718e-10,  1.5042e-10,  1.0452e-10],\n",
      "          [ 9.3167e-11, -4.0282e-11, -8.6891e-11]],\n",
      "\n",
      "         [[-1.3496e-10, -8.2116e-11, -7.4451e-11],\n",
      "          [ 2.8176e-11, -3.6075e-10,  4.5779e-11],\n",
      "          [-1.0852e-10,  3.7740e-12,  1.6597e-10]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.0483e-11,  1.7689e-09,  2.3038e-11],\n",
      "          [ 5.2587e-10,  1.3896e-09,  8.4859e-11],\n",
      "          [ 1.0968e-09,  1.3040e-09, -7.0154e-11]],\n",
      "\n",
      "         [[ 7.3933e-11, -9.9245e-12, -5.1337e-11],\n",
      "          [ 4.0647e-10,  2.7861e-11,  9.8591e-12],\n",
      "          [ 3.9410e-10,  4.9675e-11,  8.3156e-11]],\n",
      "\n",
      "         [[-1.8399e-10, -5.2050e-10, -3.6803e-11],\n",
      "          [ 1.1705e-11, -3.5055e-10,  5.6831e-11],\n",
      "          [-2.0114e-12, -1.5771e-10, -2.4628e-11]]],\n",
      "\n",
      "\n",
      "        [[[-8.2466e-11, -1.8034e-11, -9.0334e-11],\n",
      "          [ 2.5918e-10, -3.6916e-10, -1.7511e-10],\n",
      "          [-1.2217e-11, -1.7037e-11,  1.4766e-10]],\n",
      "\n",
      "         [[ 5.0166e-10,  1.4469e-10,  4.4080e-10],\n",
      "          [-1.7760e-11, -4.0739e-10, -2.7656e-11],\n",
      "          [ 4.1773e-10,  4.0410e-10,  1.7189e-10]],\n",
      "\n",
      "         [[-6.6351e-11, -1.5022e-11,  4.9193e-11],\n",
      "          [-7.2610e-11,  3.9966e-11,  6.0006e-11],\n",
      "          [-1.4739e-12,  1.0077e-10, -1.0651e-10]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0837e-10, -2.8289e-10, -3.0946e-12],\n",
      "          [ 1.1093e-10, -5.0200e-10, -7.2781e-11],\n",
      "          [ 1.2283e-10, -6.1530e-10, -5.4710e-10]],\n",
      "\n",
      "         [[ 1.1594e-10, -6.4264e-12,  7.2808e-11],\n",
      "          [-9.5600e-12, -2.4065e-10,  3.0859e-11],\n",
      "          [ 5.8525e-12, -8.7343e-11, -2.7887e-11]],\n",
      "\n",
      "         [[-1.3960e-10, -1.0339e-11, -1.1225e-10],\n",
      "          [ 3.6589e-11,  2.1554e-10,  1.0968e-10],\n",
      "          [ 8.9120e-11,  3.2128e-10, -2.0433e-10]]],\n",
      "\n",
      "\n",
      "        [[[-1.6797e-11, -3.1255e-10, -3.2286e-11],\n",
      "          [-2.7693e-10,  2.3127e-10,  1.8999e-10],\n",
      "          [-1.4484e-10,  2.9187e-10, -4.2877e-11]],\n",
      "\n",
      "         [[-1.4690e-09, -4.4120e-10, -6.0540e-11],\n",
      "          [-2.1441e-10, -1.7053e-10, -5.9004e-12],\n",
      "          [-2.2777e-10, -1.5574e-10, -6.2550e-10]],\n",
      "\n",
      "         [[ 3.1039e-10,  4.7079e-10,  9.6533e-11],\n",
      "          [-9.5523e-11,  8.4001e-11, -1.6307e-10],\n",
      "          [ 4.1739e-11, -3.0496e-10,  3.0118e-10]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3936e-11,  1.4215e-10,  3.9776e-10],\n",
      "          [-2.7510e-11, -4.3880e-11,  7.0690e-10],\n",
      "          [ 2.0686e-10,  4.3436e-10,  6.7601e-10]],\n",
      "\n",
      "         [[-2.6904e-10,  3.0034e-10,  1.5884e-10],\n",
      "          [-2.8754e-10, -1.1055e-10,  5.4811e-11],\n",
      "          [-1.9085e-10, -2.0413e-10,  6.6135e-11]],\n",
      "\n",
      "         [[-1.0072e-09, -6.6006e-10,  1.3725e-11],\n",
      "          [ 6.5892e-11, -2.3936e-10, -3.7138e-10],\n",
      "          [-1.9588e-10, -3.2274e-10, -6.5373e-11]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[ 7.1376e-09,  1.2118e-08,  1.1138e-08,  ...,  1.6429e-08,\n",
      "          5.2584e-09,  5.2234e-09],\n",
      "        [-7.1809e-09, -1.6404e-08, -1.5070e-08,  ..., -2.5115e-08,\n",
      "         -6.0448e-09, -7.0180e-09],\n",
      "        [ 9.6502e-10,  1.4302e-09,  1.5612e-09,  ...,  2.1273e-09,\n",
      "          1.1091e-09,  8.4544e-10],\n",
      "        [ 2.6980e-09,  3.3285e-09,  3.4957e-09,  ...,  2.3584e-09,\n",
      "          1.1075e-09,  1.0567e-09],\n",
      "        [ 2.9863e-09,  4.9921e-09,  5.5045e-09,  ...,  6.3838e-09,\n",
      "          2.8068e-09,  2.3922e-09]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.5682e-08, -1.9283e-08,  2.3878e-09,  3.3584e-09,  6.4435e-09],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = loss_landscape_join.landscape(maml_system.model.classifier, arbiter_system.model.classifier, args_arbiter)\n",
    "ls.show_2djoin(x_support_set_task, y_support_set_task, title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
