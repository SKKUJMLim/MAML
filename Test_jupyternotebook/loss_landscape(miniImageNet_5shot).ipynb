{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16249129",
   "metadata": {},
   "source": [
    "## [참고]\n",
    "### https://cocoa-t.tistory.com/entry/PyHessian-Loss-Landscape-%EC%8B%9C%EA%B0%81%ED%99%94-PyHessian-Neural-Networks-Through-the-Lens-of-the-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af476",
   "metadata": {},
   "source": [
    "# 0. Dataset 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7235fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=\"mini_imagenet_full_size\"\n",
    "# dataset=\"tiered_imagenet\"\n",
    "# dataset=\"CIFAR_FS\"\n",
    "dataset=\"CUB\"\n",
    "\n",
    "# title = 'miniImageNet'\n",
    "# title = 'tieredImageNet'\n",
    "# title = 'CIFAR-FS'\n",
    "title = 'CUB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005193c",
   "metadata": {},
   "source": [
    "# 1. MAML 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f0d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_maml = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_filter128\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_maml.im_shape = (2, 3, args_maml.image_height, args_maml.image_width)\n",
    "\n",
    "args_maml.use_cuda = torch.cuda.is_available()\n",
    "args_maml.seed = 104\n",
    "args_maml.reverse_channels=False\n",
    "args_maml.labels_as_int=False\n",
    "args_maml.reset_stored_filepaths=False\n",
    "args_maml.num_of_gpus=1\n",
    "\n",
    "args_maml.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9052a",
   "metadata": {},
   "source": [
    "## 2. Arbiter 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_arbiter = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 150,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_arbiter.im_shape = (2, 3, args_arbiter.image_height, args_arbiter.image_width)\n",
    "\n",
    "args_arbiter.use_cuda = torch.cuda.is_available()\n",
    "args_arbiter.seed = 104\n",
    "args_arbiter.reverse_channels=False\n",
    "args_arbiter.labels_as_int=False\n",
    "args_arbiter.reset_stored_filepaths=False\n",
    "args_arbiter.num_of_gpus=1\n",
    "\n",
    "args_arbiter.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f7d8",
   "metadata": {},
   "source": [
    "## 3. Model 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803156ee",
   "metadata": {},
   "source": [
    "### 3.1. MAML Model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f85286c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_filter128\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 2953, 'train': 5885, 'val': 2950}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_maml = MAMLFewShotClassifier(args=args_maml, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_maml.image_height, args_maml.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model_maml, data=data, args=args_maml, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3acf",
   "metadata": {},
   "source": [
    "### 3.2.  Arbiter 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25651dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 2953, 'train': 5885, 'val': 2950}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_arbiter = MAMLFewShotClassifier(args=args_arbiter, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_arbiter.image_height, args_arbiter.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "arbiter_system = ExperimentBuilder(model=model_arbiter, data=data, args=args_arbiter, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a2ff6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6359555553396543,\n",
       " 'best_val_iter': 49000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 98,\n",
       " 'train_loss_mean': 0.6403910377025605,\n",
       " 'train_loss_std': 0.1255260544055785,\n",
       " 'train_accuracy_mean': 0.7639866684675216,\n",
       " 'train_accuracy_std': 0.06118401968549816,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.9505268172423045,\n",
       " 'val_loss_std': 0.15325273037358733,\n",
       " 'val_accuracy_mean': 0.6287999994556109,\n",
       " 'val_accuracy_std': 0.06361374163962585,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-0.0255, -0.0844,  0.0421],\n",
       "                         [-0.0809, -0.0591, -0.0046],\n",
       "                         [-0.0325,  0.1015, -0.0134]],\n",
       "               \n",
       "                        [[ 0.0662, -0.0385,  0.0898],\n",
       "                         [-0.0224,  0.0329,  0.0889],\n",
       "                         [-0.0375,  0.0514, -0.0026]],\n",
       "               \n",
       "                        [[ 0.0667, -0.0065, -0.0467],\n",
       "                         [ 0.0322,  0.0595, -0.0776],\n",
       "                         [-0.0642,  0.0074, -0.0751]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0714,  0.1215,  0.0690],\n",
       "                         [ 0.0657, -0.0497,  0.0189],\n",
       "                         [-0.1058, -0.0985, -0.0763]],\n",
       "               \n",
       "                        [[-0.0225,  0.0162, -0.0007],\n",
       "                         [ 0.0545, -0.0631, -0.0476],\n",
       "                         [ 0.0542, -0.0559,  0.0761]],\n",
       "               \n",
       "                        [[-0.0650,  0.0606, -0.0772],\n",
       "                         [-0.0224,  0.0663,  0.0857],\n",
       "                         [-0.0305, -0.0117, -0.0181]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0324, -0.0386,  0.0566],\n",
       "                         [-0.0143, -0.0344, -0.0204],\n",
       "                         [ 0.0755,  0.0579, -0.0644]],\n",
       "               \n",
       "                        [[-0.0797, -0.0395,  0.0388],\n",
       "                         [ 0.0813, -0.0030,  0.0204],\n",
       "                         [-0.0698, -0.0439,  0.0303]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0722,  0.0254],\n",
       "                         [ 0.0777,  0.0115, -0.0638],\n",
       "                         [-0.0668,  0.0245,  0.0016]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0359, -0.0860, -0.0662],\n",
       "                         [-0.0050, -0.0928,  0.0520],\n",
       "                         [-0.0283,  0.0588, -0.0609]],\n",
       "               \n",
       "                        [[ 0.0648,  0.0180,  0.0776],\n",
       "                         [ 0.0499, -0.0222, -0.0206],\n",
       "                         [ 0.0198,  0.0787,  0.0061]],\n",
       "               \n",
       "                        [[ 0.0096, -0.0855, -0.0410],\n",
       "                         [-0.0569,  0.0049,  0.0344],\n",
       "                         [-0.0491,  0.0319,  0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0077,  0.0151, -0.0113],\n",
       "                         [ 0.0198,  0.0805, -0.0301],\n",
       "                         [ 0.0457,  0.0801, -0.0407]],\n",
       "               \n",
       "                        [[ 0.0811,  0.0580,  0.1031],\n",
       "                         [ 0.0596,  0.0067,  0.0002],\n",
       "                         [-0.0017, -0.0517,  0.0510]],\n",
       "               \n",
       "                        [[-0.0385, -0.0292, -0.0301],\n",
       "                         [-0.0170, -0.0443,  0.0586],\n",
       "                         [-0.0917, -0.0518,  0.0024]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0323,  0.0842, -0.0393],\n",
       "                         [-0.0391,  0.0747,  0.0626],\n",
       "                         [ 0.0786,  0.0831,  0.0922]],\n",
       "               \n",
       "                        [[-0.0269, -0.0427,  0.0134],\n",
       "                         [ 0.0379, -0.0726, -0.0621],\n",
       "                         [-0.0407, -0.0233, -0.0411]],\n",
       "               \n",
       "                        [[ 0.0461, -0.0238,  0.0385],\n",
       "                         [-0.0098, -0.0332, -0.0305],\n",
       "                         [-0.0480, -0.0654, -0.0453]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.0175,  0.0289,  0.0017, -0.0053,  0.0039,  0.0073,  0.0086,  0.0064,\n",
       "                        0.0009, -0.0196,  0.0216,  0.0025, -0.0158,  0.0111,  0.0017,  0.0114,\n",
       "                        0.0064,  0.0143,  0.0169, -0.0116, -0.0180, -0.0081, -0.0033, -0.0074,\n",
       "                       -0.0037,  0.0029,  0.0011,  0.0125,  0.0148,  0.0091,  0.0051, -0.0029,\n",
       "                       -0.0089,  0.0250,  0.0072, -0.0091,  0.0011,  0.0039,  0.0159,  0.0169,\n",
       "                       -0.0124, -0.0267,  0.0020, -0.0068, -0.0071,  0.0006, -0.0248, -0.0077,\n",
       "                        0.0044,  0.0075, -0.0075,  0.0021,  0.0150, -0.0009, -0.0197, -0.0072,\n",
       "                        0.0030,  0.0047, -0.0043, -0.0002, -0.0109, -0.0002,  0.0212,  0.0022,\n",
       "                       -0.0252, -0.0014,  0.0105, -0.0034, -0.0002, -0.0239, -0.0024, -0.0128,\n",
       "                        0.0037,  0.0041,  0.0119, -0.0083,  0.0055, -0.0148,  0.0025, -0.0067,\n",
       "                        0.0161,  0.0038, -0.0007,  0.0080,  0.0148, -0.0060, -0.0045,  0.0282,\n",
       "                       -0.0009, -0.0187, -0.0101, -0.0353,  0.0054,  0.0013, -0.0063, -0.0174,\n",
       "                       -0.0096, -0.0242,  0.0046, -0.0104, -0.0169,  0.0135,  0.0113,  0.0116,\n",
       "                        0.0058, -0.0031, -0.0009, -0.0004,  0.0068, -0.0032, -0.0076,  0.0107,\n",
       "                        0.0072, -0.0025, -0.0163,  0.0089,  0.0125, -0.0108,  0.0024,  0.0226,\n",
       "                       -0.0144,  0.0032, -0.0118, -0.0031,  0.0021, -0.0323,  0.0027, -0.0002],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1631,  0.1944, -0.1138,  0.1166,  0.1172, -0.0240, -0.2111, -0.1388,\n",
       "                       -0.1821, -0.0969,  0.2105, -0.1737,  0.3352, -0.0586,  0.3851,  0.2701,\n",
       "                        0.0005,  0.0484,  0.2549, -0.0579,  0.0446, -0.1371, -0.0903,  0.0135,\n",
       "                        0.0157, -0.0954, -0.0991,  0.3129,  0.2739,  0.1327,  0.0106, -0.0495,\n",
       "                        0.1543, -0.1553, -0.0760, -0.0628,  0.0274,  0.1642, -0.1468, -0.0709,\n",
       "                        0.0016,  0.3324, -0.1130,  0.1172,  0.2649, -0.1730, -0.1429, -0.1181,\n",
       "                       -0.0757,  0.0019,  0.2784, -0.0801,  0.3400,  0.2097, -0.0662, -0.0745,\n",
       "                       -0.0332,  0.2216,  0.0059,  0.3655, -0.1121,  0.2333,  0.0753, -0.1723,\n",
       "                        0.1420, -0.0555,  0.2159, -0.0682,  0.3034, -0.0017,  0.1455,  0.3801,\n",
       "                       -0.0174, -0.1359,  0.2083, -0.1117,  0.0331,  0.3195,  0.1055, -0.1900,\n",
       "                        0.4710, -0.0528, -0.1267, -0.0556,  0.3039,  0.3539,  0.3447,  0.2085,\n",
       "                       -0.1442,  0.1741, -0.1334,  0.0495,  0.0896,  0.1874, -0.0507,  0.2346,\n",
       "                        0.0595,  0.0425, -0.0443, -0.1186,  0.0008,  0.0851, -0.1202, -0.0469,\n",
       "                       -0.0340,  0.0512,  0.1297, -0.1034,  0.0106,  0.2907,  0.2411,  0.4341,\n",
       "                       -0.0890,  0.0216, -0.0916,  0.1807,  0.0130,  0.4608, -0.0359,  0.4015,\n",
       "                       -0.0879, -0.0734, -0.1045, -0.1029,  0.3831, -0.0157, -0.0910, -0.0575],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.9696, 1.1085, 0.8447, 1.1021, 0.9693, 0.9709, 0.9787, 0.9343, 0.8471,\n",
       "                       0.9381, 1.1029, 0.9399, 1.1080, 0.9657, 1.1290, 1.1905, 0.8940, 1.0025,\n",
       "                       0.9887, 0.9973, 0.9491, 0.9175, 0.9180, 0.9997, 0.9615, 0.9512, 0.9187,\n",
       "                       0.9743, 1.0115, 1.0331, 1.0684, 0.9661, 1.1221, 0.9391, 0.9968, 0.9734,\n",
       "                       0.9412, 1.0812, 0.9791, 1.0023, 1.0361, 1.1265, 0.9472, 1.1115, 0.9799,\n",
       "                       0.9660, 0.9552, 0.9922, 0.9178, 1.0445, 0.9584, 0.9479, 1.0931, 0.9649,\n",
       "                       0.9693, 0.9627, 0.9913, 1.0533, 1.0207, 1.0808, 1.0113, 1.0104, 0.9997,\n",
       "                       0.9408, 1.0169, 0.9237, 0.9685, 1.0111, 1.1905, 0.9716, 1.0283, 1.1558,\n",
       "                       0.9849, 0.9728, 0.9640, 0.9509, 1.0301, 1.1699, 1.1360, 0.8957, 1.0791,\n",
       "                       1.0465, 0.9238, 1.0184, 1.0511, 1.1042, 0.9738, 1.0620, 0.8964, 0.9912,\n",
       "                       0.8152, 0.9297, 0.9573, 1.0192, 0.9643, 0.9865, 0.9691, 0.9859, 0.9162,\n",
       "                       1.0091, 0.9118, 1.0460, 0.9582, 1.0327, 0.9797, 1.0288, 1.0011, 0.9599,\n",
       "                       1.0142, 1.0149, 0.9774, 1.1019, 1.0000, 0.9757, 0.9746, 1.1659, 0.9524,\n",
       "                       1.0279, 1.0224, 1.1533, 0.9746, 0.9960, 0.9669, 0.9357, 1.0148, 0.9732,\n",
       "                       0.9627, 0.9500], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 0.0073,  0.0615,  0.0120],\n",
       "                         [-0.0109,  0.0211, -0.0202],\n",
       "                         [ 0.0240,  0.0670, -0.0142]],\n",
       "               \n",
       "                        [[ 0.0408,  0.0463, -0.0002],\n",
       "                         [-0.0267, -0.0258, -0.0401],\n",
       "                         [-0.0322,  0.0412, -0.0157]],\n",
       "               \n",
       "                        [[ 0.0051, -0.0471, -0.0257],\n",
       "                         [-0.0522, -0.0002,  0.0558],\n",
       "                         [ 0.0197,  0.0114, -0.0256]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0189, -0.0542, -0.0090],\n",
       "                         [-0.0584, -0.0753, -0.0285],\n",
       "                         [-0.0501, -0.0588, -0.0454]],\n",
       "               \n",
       "                        [[ 0.0147,  0.0626, -0.0057],\n",
       "                         [-0.0027,  0.0439, -0.0190],\n",
       "                         [ 0.0332, -0.0397,  0.0007]],\n",
       "               \n",
       "                        [[-0.0371,  0.0505, -0.0220],\n",
       "                         [-0.0072,  0.0231,  0.0021],\n",
       "                         [-0.0321, -0.0171,  0.0332]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0149, -0.0397,  0.0341],\n",
       "                         [ 0.0416,  0.0181, -0.0336],\n",
       "                         [ 0.0185,  0.0093,  0.0077]],\n",
       "               \n",
       "                        [[ 0.0276,  0.0237, -0.0054],\n",
       "                         [-0.0764,  0.0186,  0.0145],\n",
       "                         [-0.0378, -0.0316,  0.0317]],\n",
       "               \n",
       "                        [[ 0.0133, -0.0043, -0.0575],\n",
       "                         [-0.0120, -0.0424, -0.0046],\n",
       "                         [-0.0632, -0.0027, -0.0540]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0045, -0.0387, -0.0022],\n",
       "                         [-0.0536, -0.0996, -0.0563],\n",
       "                         [-0.0248,  0.0001, -0.0399]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0114, -0.0219],\n",
       "                         [ 0.0516,  0.0429,  0.0258],\n",
       "                         [ 0.0443, -0.0104,  0.0394]],\n",
       "               \n",
       "                        [[-0.0065, -0.0368,  0.0156],\n",
       "                         [ 0.0126, -0.0725, -0.0404],\n",
       "                         [-0.0613, -0.0766, -0.0209]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0573, -0.0550, -0.0113],\n",
       "                         [-0.0121,  0.0007, -0.0628],\n",
       "                         [ 0.0059,  0.0141, -0.0043]],\n",
       "               \n",
       "                        [[ 0.0305, -0.0125, -0.0485],\n",
       "                         [ 0.0321,  0.0384, -0.0741],\n",
       "                         [ 0.0067,  0.0431, -0.0392]],\n",
       "               \n",
       "                        [[-0.0431,  0.0168, -0.0467],\n",
       "                         [-0.0372,  0.0102,  0.0053],\n",
       "                         [ 0.0024,  0.0143,  0.0398]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0455,  0.0046, -0.0200],\n",
       "                         [-0.0405, -0.0600, -0.0352],\n",
       "                         [-0.0408, -0.0624, -0.0764]],\n",
       "               \n",
       "                        [[ 0.0437,  0.0360,  0.0398],\n",
       "                         [-0.0529, -0.0126, -0.0394],\n",
       "                         [-0.0420,  0.0150, -0.0273]],\n",
       "               \n",
       "                        [[-0.0355, -0.0435,  0.0117],\n",
       "                         [-0.0331,  0.0213,  0.0256],\n",
       "                         [ 0.0059, -0.0061, -0.0219]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0504,  0.0111,  0.0091],\n",
       "                         [ 0.0700,  0.0102, -0.0127],\n",
       "                         [ 0.0472, -0.0750, -0.0749]],\n",
       "               \n",
       "                        [[ 0.0126,  0.0457,  0.0538],\n",
       "                         [-0.0469, -0.0615, -0.0298],\n",
       "                         [-0.0257,  0.0307,  0.0064]],\n",
       "               \n",
       "                        [[ 0.0381,  0.0234,  0.0319],\n",
       "                         [ 0.0427, -0.0436, -0.0046],\n",
       "                         [-0.0133, -0.0470, -0.0157]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0282, -0.0291, -0.0278],\n",
       "                         [ 0.0064, -0.0879, -0.0409],\n",
       "                         [-0.0045, -0.0201, -0.0508]],\n",
       "               \n",
       "                        [[ 0.0012,  0.0317,  0.0562],\n",
       "                         [ 0.0444,  0.0208,  0.0051],\n",
       "                         [-0.0373,  0.0474,  0.0533]],\n",
       "               \n",
       "                        [[-0.0329, -0.0453, -0.0214],\n",
       "                         [-0.0277,  0.0045,  0.0224],\n",
       "                         [-0.0115,  0.0191, -0.0240]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0053, -0.0619,  0.0081],\n",
       "                         [-0.0290, -0.0561,  0.0356],\n",
       "                         [-0.0297,  0.0281,  0.0076]],\n",
       "               \n",
       "                        [[ 0.0019,  0.0412, -0.0531],\n",
       "                         [-0.0054,  0.0489, -0.0376],\n",
       "                         [-0.0378, -0.0377,  0.0578]],\n",
       "               \n",
       "                        [[-0.0227,  0.0314,  0.0516],\n",
       "                         [ 0.0181, -0.0008, -0.0313],\n",
       "                         [-0.0155,  0.0141,  0.0279]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0321, -0.0038,  0.0526],\n",
       "                         [-0.0037,  0.0310, -0.0090],\n",
       "                         [-0.0042, -0.0025, -0.0319]],\n",
       "               \n",
       "                        [[ 0.0066,  0.0332,  0.0456],\n",
       "                         [-0.0186, -0.0539, -0.0037],\n",
       "                         [-0.0198, -0.0442,  0.0125]],\n",
       "               \n",
       "                        [[-0.0379, -0.0151,  0.0192],\n",
       "                         [-0.0437,  0.0384,  0.0344],\n",
       "                         [ 0.0192,  0.0085,  0.0302]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0173, -0.0070, -0.0065],\n",
       "                         [ 0.0314,  0.0438, -0.0579],\n",
       "                         [-0.0199,  0.0290, -0.0254]],\n",
       "               \n",
       "                        [[-0.0348,  0.0351,  0.0423],\n",
       "                         [ 0.0056, -0.0599, -0.0338],\n",
       "                         [-0.0596, -0.0250,  0.0199]],\n",
       "               \n",
       "                        [[-0.0150, -0.0501,  0.0067],\n",
       "                         [-0.0007,  0.0117,  0.0053],\n",
       "                         [ 0.0055, -0.0287,  0.0358]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0159, -0.0148, -0.0628],\n",
       "                         [ 0.0441,  0.0680, -0.0351],\n",
       "                         [ 0.0223,  0.0213, -0.0230]],\n",
       "               \n",
       "                        [[-0.0644, -0.0820, -0.0552],\n",
       "                         [ 0.0076,  0.0235, -0.0232],\n",
       "                         [-0.0340, -0.0002, -0.0556]],\n",
       "               \n",
       "                        [[-0.0215,  0.0316,  0.0109],\n",
       "                         [-0.0233,  0.0090,  0.0530],\n",
       "                         [-0.0053, -0.0364, -0.0194]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-1.6949e-02, -8.4122e-03, -6.4646e-03, -4.2481e-03, -2.1994e-03,\n",
       "                        3.4437e-03, -1.9195e-03,  2.9394e-03, -8.6836e-04,  2.9146e-03,\n",
       "                       -4.8363e-03, -2.9786e-03,  8.2595e-04,  2.0065e-03,  1.7538e-03,\n",
       "                        5.7412e-03, -3.8066e-05,  3.0353e-03, -2.8385e-03,  2.7388e-03,\n",
       "                       -5.7158e-03,  1.9158e-03, -2.4051e-03, -2.9564e-03,  2.5613e-03,\n",
       "                        2.3439e-04, -2.2504e-03,  7.3651e-03, -1.2441e-03, -8.3805e-05,\n",
       "                        8.6782e-03, -3.5325e-03,  3.8182e-03,  1.5983e-03,  1.2204e-03,\n",
       "                       -6.0784e-03, -4.5049e-04,  4.5410e-03,  3.2182e-04, -2.8716e-03,\n",
       "                       -2.2109e-03,  3.7276e-03,  7.1509e-04, -2.3736e-03,  3.0177e-04,\n",
       "                        3.1425e-03,  4.0651e-03, -3.7909e-03, -1.2114e-03, -1.6332e-03,\n",
       "                       -8.7488e-03, -9.9171e-04,  8.2588e-04, -7.7100e-04, -8.6759e-03,\n",
       "                        3.9878e-03, -2.7320e-03, -8.2349e-03,  2.0059e-03,  6.6739e-04,\n",
       "                        2.1313e-03, -2.2211e-03,  1.5898e-03,  6.0315e-04, -7.0056e-03,\n",
       "                        3.8265e-03,  4.4242e-03, -4.5509e-03,  2.2188e-04, -2.7838e-03,\n",
       "                        1.4363e-03,  4.0279e-04, -1.7379e-03, -4.3296e-03,  2.3067e-03,\n",
       "                       -5.1976e-03, -2.9156e-04, -1.3852e-03,  7.8145e-03,  1.6117e-03,\n",
       "                        6.3610e-03, -2.0517e-03, -5.3275e-03,  4.3522e-03, -2.8026e-03,\n",
       "                        3.9732e-03,  5.5070e-03, -7.5503e-05,  1.0788e-03,  2.9781e-03,\n",
       "                       -9.4866e-03,  5.1489e-04, -3.9218e-04, -5.8387e-05, -1.9815e-03,\n",
       "                        6.0945e-04,  1.0402e-03,  9.8662e-04, -5.3993e-04, -2.6299e-04,\n",
       "                        1.4962e-03, -5.3848e-04,  1.7470e-03, -3.8175e-03, -1.7810e-03,\n",
       "                       -1.8436e-03, -2.5974e-03,  2.9317e-03,  2.3702e-03,  2.2829e-04,\n",
       "                        2.0404e-03,  5.7490e-03,  1.5104e-03,  7.5078e-04, -1.6547e-03,\n",
       "                        4.6222e-03,  1.2253e-04,  3.9792e-03,  4.9803e-03,  4.8457e-04,\n",
       "                       -3.8067e-03, -2.3458e-03,  1.2213e-03, -6.6374e-03, -5.1143e-03,\n",
       "                       -1.9425e-03, -1.7261e-03,  2.3845e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0239, -0.1029, -0.1356, -0.1313, -0.1094, -0.1200, -0.0928, -0.1694,\n",
       "                       -0.0216, -0.0378, -0.0961, -0.0686, -0.2043, -0.1061, -0.1571, -0.1857,\n",
       "                       -0.0801,  0.1526, -0.0903, -0.1624, -0.2084, -0.0794, -0.1526, -0.0904,\n",
       "                       -0.1907, -0.0218, -0.1831,  0.0054, -0.0388, -0.1296,  0.0191, -0.1421,\n",
       "                       -0.0195, -0.1232, -0.0756, -0.1430, -0.2357, -0.0668, -0.1038, -0.1295,\n",
       "                       -0.1906, -0.0274, -0.1887, -0.1435, -0.1363, -0.1023, -0.1372, -0.0549,\n",
       "                       -0.0128, -0.1253, -0.1443, -0.0114, -0.0838, -0.1771, -0.0118, -0.0546,\n",
       "                       -0.0105, -0.1224, -0.1915, -0.1396, -0.1079, -0.0798, -0.1026, -0.0963,\n",
       "                       -0.0723, -0.0284, -0.0720, -0.0425, -0.1336, -0.0733, -0.1823, -0.0852,\n",
       "                       -0.0766, -0.0089, -0.0376, -0.1518, -0.0774, -0.1717,  0.0542, -0.1189,\n",
       "                       -0.0779, -0.1253, -0.0342, -0.0763, -0.1555, -0.0973, -0.0166, -0.0855,\n",
       "                       -0.0579, -0.1178, -0.1892, -0.0419, -0.0477, -0.0605, -0.0799, -0.0898,\n",
       "                       -0.0811, -0.0729, -0.1795, -0.1760, -0.1528, -0.1409, -0.1406, -0.1007,\n",
       "                       -0.1355, -0.1034, -0.0196, -0.1602, -0.0788, -0.0995, -0.1055, -0.0866,\n",
       "                       -0.0884, -0.0945, -0.1266, -0.0264, -0.0842, -0.1382, -0.0980, -0.1124,\n",
       "                       -0.1105, -0.0172, -0.1221, -0.0662, -0.0260, -0.0930, -0.0811, -0.1032],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9428, 0.9862, 1.0353, 1.0197, 0.9484, 1.0319, 1.0427, 0.9799, 1.0889,\n",
       "                       0.9699, 1.0409, 0.9796, 0.9760, 1.0362, 1.0216, 1.0216, 1.0361, 0.9781,\n",
       "                       1.0763, 1.0203, 1.0362, 1.0268, 1.0446, 0.9615, 1.0413, 1.0226, 1.0333,\n",
       "                       1.0005, 1.0836, 1.0583, 0.9296, 0.9788, 1.0094, 0.9958, 1.0039, 0.9190,\n",
       "                       1.0555, 0.9978, 0.9221, 0.9902, 1.0340, 0.9762, 1.0726, 1.0702, 0.9993,\n",
       "                       0.9641, 1.0156, 1.0302, 1.0744, 0.9556, 0.9469, 0.9848, 0.9398, 0.9957,\n",
       "                       0.9438, 1.0218, 1.0183, 0.9649, 0.9851, 0.9523, 0.9629, 0.9934, 0.9863,\n",
       "                       0.9597, 0.9350, 1.0053, 1.0129, 1.0250, 0.9279, 1.0033, 0.9282, 0.9721,\n",
       "                       1.0010, 0.9276, 1.0228, 0.9233, 1.0320, 1.0337, 0.9309, 0.9750, 1.0079,\n",
       "                       0.9799, 1.0670, 0.9271, 1.0485, 0.9817, 0.9526, 0.9983, 1.1004, 0.9980,\n",
       "                       1.0839, 1.0081, 0.9850, 1.0833, 0.9840, 0.9629, 0.9918, 0.9759, 1.0162,\n",
       "                       1.0476, 0.9885, 1.0907, 0.9968, 1.0194, 0.9749, 0.9604, 0.9823, 0.9796,\n",
       "                       1.0114, 1.0355, 0.9358, 1.0666, 0.9849, 1.0066, 0.9954, 1.0633, 1.0803,\n",
       "                       0.9390, 0.9113, 1.0046, 0.9884, 1.0092, 1.0470, 0.9643, 0.9522, 1.0208,\n",
       "                       1.0418, 0.9650], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-5.6612e-04,  5.5613e-02, -1.4964e-02],\n",
       "                         [-3.1003e-02, -5.2417e-03, -3.5244e-02],\n",
       "                         [ 1.5153e-02, -2.5045e-02,  8.3373e-03]],\n",
       "               \n",
       "                        [[ 2.6745e-02, -1.6569e-02,  9.6491e-03],\n",
       "                         [-2.3582e-02, -1.7072e-02, -3.0692e-02],\n",
       "                         [ 8.8099e-03,  1.9431e-02, -1.0071e-02]],\n",
       "               \n",
       "                        [[ 2.2886e-02,  1.2657e-02,  2.7444e-02],\n",
       "                         [ 6.7986e-02,  2.3340e-02, -6.3387e-02],\n",
       "                         [-8.0550e-03,  4.3950e-03, -2.6644e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.2784e-02, -4.7080e-02, -5.8843e-02],\n",
       "                         [ 5.8877e-02,  3.3930e-02,  5.3777e-02],\n",
       "                         [-3.0208e-06, -3.6364e-02, -3.0298e-02]],\n",
       "               \n",
       "                        [[-1.8547e-02, -3.3489e-02,  1.4912e-02],\n",
       "                         [ 4.4668e-02,  1.5992e-02, -3.8668e-02],\n",
       "                         [ 1.9676e-02,  3.2789e-02,  3.8013e-02]],\n",
       "               \n",
       "                        [[ 9.6036e-03,  1.6508e-02,  6.3899e-02],\n",
       "                         [-2.2815e-02, -2.2302e-02, -3.1161e-02],\n",
       "                         [ 1.2610e-03,  2.5403e-03,  4.1579e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3809e-03,  3.2775e-03,  3.4006e-02],\n",
       "                         [ 1.2260e-02,  2.4693e-03, -3.2722e-02],\n",
       "                         [-2.4378e-02, -6.5934e-02, -2.4807e-02]],\n",
       "               \n",
       "                        [[-4.2487e-02, -2.5946e-02, -2.8232e-02],\n",
       "                         [-1.6780e-02,  6.6708e-03, -4.2812e-03],\n",
       "                         [-1.7184e-02, -2.3446e-03,  1.5289e-02]],\n",
       "               \n",
       "                        [[ 8.0496e-03,  5.3279e-02,  9.2603e-03],\n",
       "                         [ 3.7587e-02,  3.2692e-02,  6.3311e-03],\n",
       "                         [ 4.8233e-02,  1.8544e-02, -1.1102e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.0004e-02,  3.0527e-02, -4.1664e-02],\n",
       "                         [-5.6780e-02, -4.1942e-02, -5.5847e-02],\n",
       "                         [-4.5592e-02, -2.2262e-02, -7.3276e-02]],\n",
       "               \n",
       "                        [[-2.0432e-02, -1.4052e-02,  1.8676e-03],\n",
       "                         [-9.4970e-02, -1.9388e-02, -1.8318e-02],\n",
       "                         [-6.2985e-02, -3.7837e-02, -3.0712e-02]],\n",
       "               \n",
       "                        [[-9.6093e-03, -8.8685e-02, -7.3937e-02],\n",
       "                         [-2.9326e-02,  7.3677e-04,  8.5064e-03],\n",
       "                         [-4.6906e-02, -1.3926e-02, -2.8744e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.4945e-02, -1.6051e-03, -6.4338e-02],\n",
       "                         [-9.1217e-04, -1.0536e-02,  1.0642e-02],\n",
       "                         [ 5.3609e-02,  2.5719e-02,  1.7490e-02]],\n",
       "               \n",
       "                        [[ 3.2974e-02,  7.0537e-02,  6.3091e-02],\n",
       "                         [-2.3824e-02,  3.8106e-02,  3.0877e-03],\n",
       "                         [-8.1763e-02, -1.9468e-02, -8.9834e-02]],\n",
       "               \n",
       "                        [[ 5.2738e-02,  1.4020e-02,  6.0283e-02],\n",
       "                         [-8.0464e-03,  9.1787e-02,  2.4450e-02],\n",
       "                         [-7.7595e-04,  4.2054e-02, -5.3030e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.1894e-02,  7.4259e-03, -4.8836e-02],\n",
       "                         [-3.9580e-03, -3.1565e-02, -1.5890e-02],\n",
       "                         [-2.4345e-02, -1.7730e-02,  5.6163e-02]],\n",
       "               \n",
       "                        [[-1.3640e-02, -2.9441e-02, -1.1606e-02],\n",
       "                         [ 6.9270e-05, -5.4901e-02, -6.5660e-02],\n",
       "                         [-7.3340e-02, -7.6530e-02, -5.1404e-02]],\n",
       "               \n",
       "                        [[-1.6647e-02,  2.0942e-02,  5.6214e-02],\n",
       "                         [ 1.0635e-02,  3.8857e-02,  2.6654e-02],\n",
       "                         [-1.2568e-02, -2.9424e-04,  1.8442e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.2160e-02, -4.8316e-02, -5.4221e-02],\n",
       "                         [ 3.5441e-02, -3.3133e-02, -1.6483e-02],\n",
       "                         [ 1.4458e-02,  2.3074e-02,  1.1153e-02]],\n",
       "               \n",
       "                        [[-4.4017e-02,  5.9920e-03,  1.8519e-02],\n",
       "                         [-1.0042e-02,  1.2894e-02, -3.1734e-02],\n",
       "                         [-4.1772e-03,  5.4501e-02,  8.8462e-03]],\n",
       "               \n",
       "                        [[-2.9016e-02,  7.5986e-03, -2.8667e-02],\n",
       "                         [ 3.5275e-02,  5.9086e-03,  3.4249e-02],\n",
       "                         [ 6.0786e-02,  2.2129e-02,  1.7514e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.5898e-02,  3.3125e-02,  1.3987e-02],\n",
       "                         [ 1.1372e-02,  4.8750e-03, -5.3042e-03],\n",
       "                         [-1.7770e-02, -3.6008e-02, -4.0320e-02]],\n",
       "               \n",
       "                        [[-8.7624e-03, -1.3168e-02,  6.0018e-02],\n",
       "                         [-6.0902e-03, -3.3698e-03, -2.7294e-02],\n",
       "                         [-3.2872e-02,  3.5987e-02, -7.3005e-02]],\n",
       "               \n",
       "                        [[ 1.0989e-02, -4.6545e-02, -6.0721e-02],\n",
       "                         [-7.2877e-02, -8.1973e-02, -7.7765e-02],\n",
       "                         [-3.0052e-02,  2.0828e-02, -2.5616e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.8925e-02,  4.8917e-02,  2.8418e-02],\n",
       "                         [-2.1285e-02, -2.5500e-02, -7.5975e-03],\n",
       "                         [-2.8078e-03,  2.9815e-02, -1.3520e-02]],\n",
       "               \n",
       "                        [[ 2.1450e-04, -2.8705e-02,  2.4119e-02],\n",
       "                         [-3.6257e-03, -3.2389e-02,  4.0854e-02],\n",
       "                         [-3.2382e-02,  3.5661e-03,  7.8317e-03]],\n",
       "               \n",
       "                        [[-3.0559e-02,  2.0490e-02,  5.0351e-03],\n",
       "                         [ 5.0072e-02,  4.6661e-02, -1.5058e-02],\n",
       "                         [ 1.7608e-02, -1.3727e-02,  2.3573e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.6052e-02, -3.8395e-03,  3.6573e-02],\n",
       "                         [-5.7949e-04,  7.1536e-02, -1.0412e-02],\n",
       "                         [-1.5482e-03,  2.2744e-02, -8.5390e-03]],\n",
       "               \n",
       "                        [[-6.3845e-02, -4.6169e-02, -4.0007e-02],\n",
       "                         [-1.1293e-02, -3.5827e-02,  1.3014e-02],\n",
       "                         [ 5.1841e-02, -1.6890e-02, -7.2922e-02]],\n",
       "               \n",
       "                        [[ 4.1673e-02,  1.7039e-02, -8.0290e-02],\n",
       "                         [-4.1658e-04, -2.3544e-02, -1.0676e-01],\n",
       "                         [ 4.5347e-03, -2.7998e-02, -2.3365e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.9852e-02,  5.9324e-03,  9.6843e-04],\n",
       "                         [-1.5981e-02, -3.1834e-02,  5.2805e-02],\n",
       "                         [-3.6868e-02, -1.7340e-02,  3.9554e-02]],\n",
       "               \n",
       "                        [[ 6.7321e-02,  2.9623e-02,  5.4673e-03],\n",
       "                         [ 2.0529e-02,  6.5971e-03, -2.3391e-02],\n",
       "                         [ 1.7442e-02,  3.5414e-02,  3.6108e-02]],\n",
       "               \n",
       "                        [[-3.3463e-02, -2.1611e-02, -1.9678e-02],\n",
       "                         [-4.8424e-02,  3.5010e-02,  3.9424e-02],\n",
       "                         [-5.9420e-03,  4.3907e-02,  3.3360e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9797e-02,  4.3752e-02,  1.3179e-02],\n",
       "                         [ 4.3918e-02,  3.1366e-02, -2.0022e-02],\n",
       "                         [-3.3028e-02, -2.8256e-02, -3.0205e-02]],\n",
       "               \n",
       "                        [[-1.2938e-02,  1.2830e-02,  2.9501e-02],\n",
       "                         [-1.7726e-02, -7.1185e-03, -3.7811e-02],\n",
       "                         [ 2.8611e-02, -2.0118e-02, -6.1311e-02]],\n",
       "               \n",
       "                        [[-1.0113e-02,  5.4758e-02,  2.1487e-02],\n",
       "                         [ 3.2794e-03,  1.9351e-02, -2.8451e-02],\n",
       "                         [ 8.8019e-02, -1.0072e-02, -3.6928e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 4.6061e-04, -2.6240e-03, -4.8668e-03,  1.3499e-03, -6.7688e-04,\n",
       "                       -1.3051e-03, -3.9467e-04,  1.6521e-03, -7.8187e-03,  4.3409e-03,\n",
       "                       -2.4901e-03, -1.1735e-03, -1.9624e-04, -7.4011e-03,  1.8107e-05,\n",
       "                       -4.4562e-03,  2.1683e-03, -2.7531e-03,  9.8409e-05,  1.1074e-03,\n",
       "                       -5.6326e-03,  6.3480e-03,  5.0846e-03,  1.4609e-03, -3.3035e-03,\n",
       "                       -3.3362e-03,  9.9636e-04, -5.7492e-04, -3.1613e-03, -1.0247e-02,\n",
       "                        4.1812e-03, -1.9299e-03,  4.8126e-03,  6.2209e-03,  9.1613e-03,\n",
       "                       -3.4160e-03,  1.0048e-02, -1.8047e-02,  1.5066e-04, -9.0010e-04,\n",
       "                       -3.3024e-03,  1.1539e-02,  1.5924e-03, -7.8076e-04,  2.7959e-03,\n",
       "                       -3.8437e-03,  5.5420e-04,  5.6630e-03,  1.7700e-03, -1.1816e-03,\n",
       "                       -3.2343e-03, -1.1565e-03,  5.6338e-03, -2.6592e-03, -2.2296e-03,\n",
       "                       -7.1357e-04,  1.0998e-03, -8.9221e-04, -3.3544e-03, -5.9220e-03,\n",
       "                        3.0695e-03, -2.3067e-03,  7.3235e-03,  6.6048e-03,  1.7583e-03,\n",
       "                        1.2300e-03, -8.0600e-04, -1.3774e-02,  2.6487e-03, -2.2824e-03,\n",
       "                       -4.4781e-03, -4.0840e-03, -4.4604e-04, -9.0035e-04,  1.4574e-04,\n",
       "                        2.9544e-03,  1.5378e-03,  3.9039e-03, -1.7558e-03,  3.4783e-03,\n",
       "                       -3.1708e-03,  6.7397e-03,  2.6774e-03, -4.4915e-03,  1.7781e-03,\n",
       "                        1.4992e-03,  2.2443e-03, -9.6971e-04, -1.8576e-03,  2.4945e-03,\n",
       "                        9.1154e-03,  4.9740e-03,  3.8111e-03,  4.0308e-04,  3.2366e-03,\n",
       "                       -1.2821e-03, -6.6860e-04, -6.5602e-03,  2.9548e-03,  3.9707e-03,\n",
       "                        3.1116e-03, -3.0379e-03,  1.4848e-03, -2.9219e-03, -2.7217e-03,\n",
       "                       -3.1725e-03, -2.3843e-03, -2.0663e-04, -3.5932e-03, -2.9701e-03,\n",
       "                        2.3363e-03, -4.0607e-03,  1.1661e-03,  3.8860e-03, -7.5729e-03,\n",
       "                       -2.9014e-03,  7.2572e-04, -6.2508e-03,  1.2011e-04,  2.4776e-03,\n",
       "                        1.8391e-03, -9.3481e-04,  2.6883e-03, -1.7414e-03,  3.9286e-03,\n",
       "                        3.7851e-03,  5.6336e-03,  5.9318e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.1391, -0.2048, -0.1393, -0.1732, -0.1353, -0.1210, -0.1603, -0.2054,\n",
       "                       -0.0353, -0.1253, -0.1125, -0.0369, -0.2840, -0.1378, -0.1706, -0.2121,\n",
       "                       -0.0803, -0.1494, -0.1289, -0.1470, -0.0888, -0.0605, -0.1538, -0.0884,\n",
       "                       -0.0962, -0.0889, -0.1651, -0.1394, -0.2496, -0.1341, -0.1365, -0.1336,\n",
       "                       -0.2978, -0.0906, -0.1493, -0.2138, -0.0082, -0.2035, -0.2011, -0.2006,\n",
       "                       -0.1547, -0.2458, -0.1979, -0.0222, -0.1107, -0.1439, -0.0984, -0.0936,\n",
       "                       -0.1489, -0.1794, -0.1324, -0.1430, -0.2192, -0.1099, -0.1431, -0.1055,\n",
       "                       -0.1645, -0.1377, -0.1245, -0.0560, -0.2493, -0.0998, -0.1132, -0.2081,\n",
       "                       -0.0350, -0.1173, -0.0356, -0.0940, -0.1193, -0.1307, -0.0620, -0.2028,\n",
       "                       -0.1571, -0.1631, -0.0327, -0.1306, -0.0988, -0.1368, -0.0456, -0.0868,\n",
       "                       -0.1458, -0.1827, -0.1800, -0.1469, -0.0483, -0.1266, -0.1537, -0.1395,\n",
       "                       -0.1806, -0.0839, -0.2118, -0.0111, -0.1126, -0.1722, -0.1317, -0.1179,\n",
       "                       -0.1163, -0.1933, -0.1038, -0.1531, -0.1672, -0.1171, -0.0923, -0.0193,\n",
       "                       -0.0547, -0.2908, -0.1291, -0.1831, -0.1212, -0.1551, -0.0190, -0.0806,\n",
       "                       -0.2205, -0.0160,  0.0108, -0.1951, -0.1521, -0.3187, -0.1491, -0.1353,\n",
       "                       -0.0793, -0.1182,  0.0170, -0.1368, -0.1985, -0.1302, -0.1994, -0.1754],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([1.0277, 1.0128, 0.9804, 0.9433, 0.9412, 0.9652, 0.9658, 0.9749, 1.0517,\n",
       "                       1.0016, 0.9643, 1.0066, 1.0405, 0.9912, 0.9841, 0.9218, 1.1151, 1.0733,\n",
       "                       0.9509, 0.9652, 1.0227, 0.9606, 1.0235, 0.9114, 1.0268, 1.0234, 1.0819,\n",
       "                       0.9045, 1.0696, 1.0600, 0.9874, 1.0240, 1.0417, 1.0809, 0.9790, 1.0562,\n",
       "                       0.9363, 1.1194, 0.9735, 0.9849, 0.9253, 1.0497, 1.0019, 0.9294, 0.9515,\n",
       "                       1.0383, 1.0002, 0.9651, 0.9030, 0.9744, 1.0132, 0.9474, 1.0187, 0.9983,\n",
       "                       0.9703, 0.9965, 0.9904, 0.9778, 0.9497, 1.0112, 1.0529, 0.9374, 0.9905,\n",
       "                       1.0483, 1.0328, 1.0214, 0.9460, 0.9351, 0.9422, 0.9163, 0.9332, 1.0385,\n",
       "                       0.9540, 0.9346, 0.9807, 0.9988, 1.0332, 0.9307, 0.9741, 0.9708, 1.0372,\n",
       "                       0.9618, 0.9719, 1.0110, 0.9977, 0.9824, 0.9189, 0.9367, 1.1518, 0.9375,\n",
       "                       1.0127, 0.9777, 0.9833, 1.0089, 0.9567, 0.9944, 0.9539, 0.9537, 0.9933,\n",
       "                       1.0257, 0.9628, 0.9432, 0.9521, 0.9561, 1.0270, 1.0515, 1.0031, 0.9811,\n",
       "                       1.0540, 0.9654, 1.0987, 0.9105, 0.8965, 0.9892, 0.9852, 0.9682, 0.8834,\n",
       "                       1.0933, 1.0380, 0.9390, 0.9505, 1.0529, 1.0772, 1.0426, 0.9539, 1.0445,\n",
       "                       0.9834, 1.0106], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-5.4627e-02,  7.3330e-04,  1.0867e-02],\n",
       "                         [-1.2974e-02,  1.8572e-02, -8.2295e-03],\n",
       "                         [-6.2027e-03,  3.7114e-03,  6.4246e-02]],\n",
       "               \n",
       "                        [[-4.5182e-02, -5.4583e-02, -4.1419e-02],\n",
       "                         [-4.6200e-02, -4.0708e-02, -3.4592e-02],\n",
       "                         [-3.8282e-02,  5.8337e-03, -4.8956e-02]],\n",
       "               \n",
       "                        [[ 6.5746e-02,  1.5244e-02,  8.7147e-02],\n",
       "                         [-3.4472e-02, -2.5536e-02,  2.4144e-02],\n",
       "                         [-3.4479e-02,  4.1504e-02,  2.4551e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.6482e-02, -7.8470e-03, -3.9585e-02],\n",
       "                         [ 2.1164e-02, -1.7489e-02,  1.2096e-02],\n",
       "                         [ 2.4554e-02,  6.7232e-02,  4.6818e-02]],\n",
       "               \n",
       "                        [[-1.3402e-02, -3.8527e-03, -2.4207e-02],\n",
       "                         [ 1.4606e-02, -7.9428e-03, -1.4672e-02],\n",
       "                         [ 2.4145e-02, -2.7772e-02, -3.5707e-02]],\n",
       "               \n",
       "                        [[ 5.1828e-02, -9.1739e-03,  8.9307e-03],\n",
       "                         [ 3.5144e-03, -6.4678e-03, -4.3382e-02],\n",
       "                         [-9.9676e-03, -3.8737e-02, -2.5570e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4494e-03, -8.8308e-03, -2.3047e-02],\n",
       "                         [ 5.3864e-03, -3.2619e-02, -1.8617e-02],\n",
       "                         [ 3.7532e-03, -3.0453e-03, -3.3701e-02]],\n",
       "               \n",
       "                        [[-1.7086e-02, -1.4788e-02, -3.8940e-03],\n",
       "                         [-6.3392e-02, -3.3190e-02, -4.4878e-02],\n",
       "                         [-3.9955e-02,  1.3318e-02,  1.3630e-02]],\n",
       "               \n",
       "                        [[-2.2971e-02,  1.3972e-02,  4.7967e-02],\n",
       "                         [ 2.2433e-02,  3.2503e-03, -3.8789e-04],\n",
       "                         [ 1.0756e-02,  1.0665e-02,  4.0786e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.7207e-02, -3.5612e-02,  3.2990e-02],\n",
       "                         [-1.0289e-02, -5.2001e-02,  2.2325e-02],\n",
       "                         [ 4.3186e-02, -5.0887e-03, -3.1521e-02]],\n",
       "               \n",
       "                        [[-7.2463e-03,  2.2586e-02, -4.4914e-02],\n",
       "                         [-2.0083e-02, -1.9633e-04, -3.0193e-02],\n",
       "                         [ 3.4482e-02, -5.3447e-03,  2.6395e-02]],\n",
       "               \n",
       "                        [[ 2.2625e-02, -2.5691e-03, -5.2997e-02],\n",
       "                         [-3.9718e-02, -5.9920e-02, -7.0250e-02],\n",
       "                         [-3.1720e-02, -5.6156e-03, -3.6102e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.1116e-02,  4.1400e-02,  7.7896e-02],\n",
       "                         [-5.2314e-03,  5.7153e-02,  6.5850e-02],\n",
       "                         [ 6.1248e-02,  3.1912e-02,  8.1214e-03]],\n",
       "               \n",
       "                        [[-1.7594e-02, -2.7667e-02,  6.5020e-03],\n",
       "                         [-2.5013e-02,  3.1521e-02,  1.0809e-02],\n",
       "                         [-4.3111e-03, -8.4143e-05, -1.1235e-02]],\n",
       "               \n",
       "                        [[ 2.0829e-02,  8.6329e-03, -4.2440e-02],\n",
       "                         [-2.5366e-02,  2.7901e-02, -9.4518e-03],\n",
       "                         [ 8.4475e-03,  5.2818e-02, -3.1964e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2820e-02, -1.8504e-02, -6.6513e-02],\n",
       "                         [-1.3898e-03, -2.1544e-02, -1.7567e-02],\n",
       "                         [-5.2538e-02,  1.3536e-02,  1.3664e-03]],\n",
       "               \n",
       "                        [[ 5.7266e-02,  3.3359e-02,  2.5032e-03],\n",
       "                         [ 6.3723e-02,  6.3739e-02,  5.4753e-02],\n",
       "                         [-1.3828e-02,  1.9169e-02,  1.1911e-02]],\n",
       "               \n",
       "                        [[-2.6761e-02,  1.5723e-02, -1.2717e-02],\n",
       "                         [-2.4329e-02,  3.9293e-03, -3.9389e-02],\n",
       "                         [ 1.6449e-02,  1.2233e-02, -1.2991e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0369e-02,  2.0749e-02,  7.4456e-02],\n",
       "                         [ 7.0774e-02,  4.0400e-02,  2.0487e-02],\n",
       "                         [ 3.9115e-02,  3.1699e-02,  2.0401e-02]],\n",
       "               \n",
       "                        [[-2.9395e-02,  2.5912e-02, -1.5670e-02],\n",
       "                         [ 3.6106e-02, -5.6428e-03, -1.2668e-02],\n",
       "                         [ 1.1267e-02, -1.9298e-02,  1.0444e-02]],\n",
       "               \n",
       "                        [[ 6.6017e-02,  9.4842e-02,  1.0661e-01],\n",
       "                         [ 5.6030e-02,  4.9520e-02, -1.6742e-02],\n",
       "                         [ 3.1501e-02,  5.3863e-02,  6.5480e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.5279e-03,  2.6637e-02, -1.5511e-02],\n",
       "                         [ 1.0471e-02, -2.2111e-02, -1.6427e-02],\n",
       "                         [ 6.6035e-02,  2.6296e-02,  9.1037e-04]],\n",
       "               \n",
       "                        [[-1.9806e-02, -5.3623e-03, -2.7325e-02],\n",
       "                         [-2.5083e-02, -1.0296e-02, -1.6746e-02],\n",
       "                         [ 2.9134e-02, -5.8477e-03, -2.6059e-03]],\n",
       "               \n",
       "                        [[-3.5956e-02,  9.9449e-03,  7.3160e-03],\n",
       "                         [-3.3580e-02, -1.2026e-02,  1.8119e-02],\n",
       "                         [-5.9664e-02, -2.7382e-02, -9.3631e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0678e-02,  2.4934e-02,  2.0218e-02],\n",
       "                         [ 4.2124e-02,  6.6200e-03,  6.8485e-02],\n",
       "                         [ 4.8555e-02,  3.4361e-03, -4.3516e-03]],\n",
       "               \n",
       "                        [[ 2.7361e-02, -1.3040e-02,  1.2497e-02],\n",
       "                         [ 2.7245e-02,  7.9802e-03,  6.1445e-02],\n",
       "                         [ 2.1408e-02, -2.5271e-02, -1.4236e-02]],\n",
       "               \n",
       "                        [[ 2.6388e-02,  1.0168e-02,  5.2068e-03],\n",
       "                         [-1.7016e-02, -1.5717e-02, -2.3489e-02],\n",
       "                         [ 2.1516e-03,  2.9778e-02, -4.5113e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0460e-02, -1.1264e-02,  1.8085e-02],\n",
       "                         [ 6.7404e-02, -4.3560e-03,  7.5595e-02],\n",
       "                         [ 3.1166e-02,  8.9611e-02,  2.6672e-02]],\n",
       "               \n",
       "                        [[ 3.6279e-03, -4.4067e-02,  2.6130e-02],\n",
       "                         [-2.2956e-02,  3.1149e-02,  1.0416e-02],\n",
       "                         [ 2.2577e-02, -3.9788e-02,  4.6509e-02]],\n",
       "               \n",
       "                        [[ 1.1394e-02,  2.5146e-03, -3.5653e-02],\n",
       "                         [-4.5091e-02,  1.8573e-02, -5.8179e-02],\n",
       "                         [-2.0710e-02, -1.1970e-02, -3.7716e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.0215e-02, -2.5592e-02, -7.2966e-03],\n",
       "                         [-6.4089e-03,  8.9794e-03,  4.4473e-03],\n",
       "                         [-5.4829e-02, -2.1084e-02,  1.4473e-03]],\n",
       "               \n",
       "                        [[ 5.4302e-02,  6.5507e-02,  8.2479e-02],\n",
       "                         [ 5.4617e-02,  6.6363e-02,  1.0264e-01],\n",
       "                         [ 6.9296e-02,  1.1446e-01,  1.0007e-01]],\n",
       "               \n",
       "                        [[-2.8678e-02, -2.0243e-02, -2.5346e-02],\n",
       "                         [-5.8157e-02, -1.5913e-02, -2.3890e-02],\n",
       "                         [ 6.0083e-03, -2.9671e-02, -3.1314e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5507e-02, -1.0065e-02,  3.0881e-02],\n",
       "                         [ 3.1270e-03, -3.6857e-02,  1.6174e-02],\n",
       "                         [-4.4302e-02, -1.9763e-02, -2.2433e-02]],\n",
       "               \n",
       "                        [[ 4.3493e-02,  8.6395e-02,  3.7174e-02],\n",
       "                         [ 2.9114e-02,  2.4833e-02, -1.4103e-02],\n",
       "                         [ 3.0108e-02,  8.4913e-02,  2.5513e-02]],\n",
       "               \n",
       "                        [[ 5.9403e-03,  9.7578e-03, -2.9943e-02],\n",
       "                         [ 1.4497e-03,  2.5015e-02, -1.4867e-02],\n",
       "                         [ 3.0806e-02,  3.3726e-02,  1.3615e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.0357,  0.0166,  0.0038, -0.0068, -0.0011, -0.0061, -0.0103, -0.0034,\n",
       "                       -0.0166,  0.0006,  0.0040, -0.0227, -0.0109, -0.0218, -0.0041,  0.0159,\n",
       "                        0.0249,  0.0085,  0.0003, -0.0096, -0.0051,  0.0175, -0.0161,  0.0572,\n",
       "                       -0.0335,  0.0107,  0.0247,  0.0042,  0.0377, -0.0171, -0.1015, -0.0228,\n",
       "                       -0.0087, -0.0797, -0.0014,  0.0367, -0.0072, -0.0147,  0.0137, -0.0163,\n",
       "                       -0.0089,  0.0034, -0.0174, -0.0188, -0.0105,  0.0368, -0.0123, -0.0034,\n",
       "                        0.0002,  0.0054, -0.0139,  0.0104, -0.0051, -0.0017,  0.0151, -0.0027,\n",
       "                        0.0474,  0.0014,  0.0009,  0.0025, -0.0011, -0.0130,  0.0087, -0.0031,\n",
       "                       -0.0616, -0.0055, -0.0170, -0.0149, -0.0131, -0.0219, -0.0087,  0.0045,\n",
       "                        0.0055,  0.0209, -0.0233, -0.0038, -0.0206, -0.0121,  0.0124, -0.0020,\n",
       "                       -0.0793,  0.0078,  0.0091, -0.0271,  0.0183, -0.0040, -0.0276, -0.0200,\n",
       "                       -0.0018, -0.0120,  0.0062,  0.0035, -0.0193, -0.0043,  0.0060, -0.0033,\n",
       "                        0.0979,  0.0046,  0.0230, -0.0159,  0.0214,  0.0229, -0.0133,  0.0055,\n",
       "                        0.0117, -0.0413,  0.0519, -0.0069, -0.0121,  0.0104,  0.0603,  0.0050,\n",
       "                       -0.0129, -0.0009,  0.0235,  0.0443, -0.0130, -0.0091, -0.0486,  0.0067,\n",
       "                       -0.0302,  0.0049, -0.0119, -0.0003,  0.0028, -0.0006, -0.0140, -0.0030],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([ 0.0080, -0.1051, -0.0583, -0.2114, -0.0156, -0.3025, -0.1431, -0.1690,\n",
       "                       -0.1414, -0.1600, -0.0668, -0.2989, -0.3124, -0.2248, -0.1399, -0.2922,\n",
       "                       -0.1752, -0.2401, -0.0152, -0.2591, -0.2447, -0.1701, -0.1470, -0.2224,\n",
       "                       -0.0362, -0.2455, -0.1750, -0.0396, -0.2098, -0.1359, -0.3048, -0.0343,\n",
       "                        0.0020, -0.1888, -0.2021, -0.0549, -0.1270, -0.1581, -0.1407, -0.0158,\n",
       "                       -0.0772, -0.0932, -0.1724, -0.2669, -0.1154, -0.2852, -0.1887, -0.0445,\n",
       "                       -0.1951, -0.1330, -0.0634, -0.0719, -0.0925, -0.0487, -0.1158, -0.0116,\n",
       "                       -0.3001, -0.2889, -0.1159, -0.0192, -0.1558, -0.1853, -0.1915, -0.1553,\n",
       "                       -0.1455, -0.2951, -0.2013,  0.0113, -0.0357, -0.2242, -0.0636, -0.3184,\n",
       "                       -0.2581, -0.1903, -0.0680, -0.1121, -0.2960, -0.1810, -0.0241, -0.2858,\n",
       "                       -0.0688, -0.1451, -0.0308, -0.1205, -0.3220, -0.3186, -0.0910, -0.1209,\n",
       "                       -0.0116, -0.1675, -0.0689, -0.1094, -0.1885, -0.1492,  0.0643, -0.2327,\n",
       "                       -0.1868, -0.1117, -0.1298, -0.1076, -0.2204, -0.2048, -0.1689, -0.1174,\n",
       "                       -0.1029, -0.1313, -0.2718, -0.1771, -0.0555, -0.1106, -0.2870, -0.0997,\n",
       "                       -0.1625, -0.1487, -0.3023, -0.2512, -0.2352, -0.0299, -0.0183, -0.0160,\n",
       "                       -0.2046, -0.2446, -0.2953, -0.1546, -0.1913, -0.0767, -0.0154, -0.0886],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([1.3516, 1.1140, 1.1335, 1.0717, 1.3636, 0.7596, 1.1063, 1.2343, 1.0519,\n",
       "                       1.5841, 1.0131, 1.0762, 1.1994, 1.4031, 1.1075, 0.9533, 1.0498, 1.0185,\n",
       "                       1.1805, 1.0876, 0.7843, 1.0940, 1.0523, 0.8384, 1.0683, 0.7602, 0.9357,\n",
       "                       1.2673, 0.9396, 1.0553, 0.9735, 1.3504, 1.2847, 1.0209, 1.1235, 1.0936,\n",
       "                       1.1686, 0.9953, 1.0376, 1.3279, 1.3441, 1.0842, 1.2170, 1.0637, 1.1351,\n",
       "                       1.0148, 1.0655, 1.2049, 1.2501, 1.0279, 1.1874, 1.2345, 1.1056, 1.6950,\n",
       "                       1.0704, 1.3799, 0.9915, 1.0065, 1.1655, 1.3675, 1.2508, 0.9559, 0.8257,\n",
       "                       1.2192, 1.0724, 1.2390, 0.9456, 1.2447, 1.2510, 0.8256, 1.2509, 0.9981,\n",
       "                       1.2127, 1.0307, 1.1309, 1.1541, 1.0804, 1.2754, 1.1647, 0.7533, 1.2607,\n",
       "                       1.0583, 1.2901, 0.9747, 0.7526, 1.0185, 1.1555, 1.1468, 1.3335, 1.1562,\n",
       "                       1.3142, 1.2976, 1.2818, 1.0301, 1.4043, 0.9505, 0.9716, 1.0872, 1.1282,\n",
       "                       1.0112, 0.9712, 0.9378, 1.1418, 1.2569, 1.0719, 1.2371, 0.8701, 1.1466,\n",
       "                       1.2526, 1.2469, 0.7931, 1.2396, 1.0986, 1.1581, 0.7296, 0.8709, 1.0038,\n",
       "                       1.3437, 1.2107, 1.2521, 0.9667, 1.2116, 0.9963, 1.1073, 1.0488, 1.2081,\n",
       "                       1.1719, 1.4677], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-4.9095e-03, -1.0411e-02,  5.6891e-03,  ..., -4.5805e-03,\n",
       "                         3.6054e-03,  7.6953e-04],\n",
       "                       [-1.1041e-02, -4.2250e-03,  1.1841e-02,  ..., -7.0961e-03,\n",
       "                         1.0293e-02, -1.3303e-02],\n",
       "                       [ 1.2086e-02,  3.3581e-03,  1.2667e-05,  ..., -2.6210e-03,\n",
       "                         6.9780e-03, -2.0183e-02],\n",
       "                       [-1.2930e-02, -5.7542e-03,  8.0124e-03,  ..., -9.6367e-03,\n",
       "                         2.3009e-04, -4.6410e-03],\n",
       "                       [-7.4690e-03, -9.8778e-03,  1.0700e-02,  ..., -4.4659e-03,\n",
       "                         9.0888e-03, -5.3043e-03]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0318,  0.0158,  0.0506, -0.1405,  0.1046], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.481154550075531,\n",
       "   1.3577819538116456,\n",
       "   1.3073870079517365,\n",
       "   1.2560416444540023,\n",
       "   1.2139344412088393,\n",
       "   1.1738374639749527,\n",
       "   1.1513489400148391,\n",
       "   1.1156441736221314,\n",
       "   1.1000408185720443,\n",
       "   1.1022559398412703,\n",
       "   1.0600120342969894,\n",
       "   1.0610931652784348,\n",
       "   1.0509943833351136,\n",
       "   1.0297512702941896,\n",
       "   1.0259157729148864,\n",
       "   1.0170646781921386,\n",
       "   1.0025434201955796,\n",
       "   0.9979545515775681,\n",
       "   0.9981109420061112,\n",
       "   0.983209489941597,\n",
       "   0.9851244906187058,\n",
       "   0.9585923129320144,\n",
       "   0.9533718949556351,\n",
       "   0.9537534577846527,\n",
       "   0.9565720618963242,\n",
       "   0.9413936160802842,\n",
       "   0.9310720965862275,\n",
       "   0.9261977025270463,\n",
       "   0.927066831946373,\n",
       "   0.9329067448377609,\n",
       "   0.912413406252861,\n",
       "   0.9153839558362961,\n",
       "   0.8964474000930787,\n",
       "   0.8877046661376953,\n",
       "   0.8930694156885147,\n",
       "   0.8728554663658142,\n",
       "   0.8791505703926087,\n",
       "   0.8727824296951294,\n",
       "   0.876187061548233,\n",
       "   0.8546114403605461,\n",
       "   0.8619972977638245,\n",
       "   0.8517441082596778,\n",
       "   0.8415768438577652,\n",
       "   0.8492840473651886,\n",
       "   0.843542504310608,\n",
       "   0.8249881924390793,\n",
       "   0.8268159502148629,\n",
       "   0.8218312242031097,\n",
       "   0.8166390725374222,\n",
       "   0.8181124683618546,\n",
       "   0.8208748137354851,\n",
       "   0.8045872502923012,\n",
       "   0.8046765422224998,\n",
       "   0.7958034241199493,\n",
       "   0.8077864516973495,\n",
       "   0.7975693554878235,\n",
       "   0.790548490345478,\n",
       "   0.7801121383309364,\n",
       "   0.7803864649534226,\n",
       "   0.7814394291639328,\n",
       "   0.7828030265569687,\n",
       "   0.7666941681504249,\n",
       "   0.7606288375258445,\n",
       "   0.7601050273180008,\n",
       "   0.7594932028055191,\n",
       "   0.7595013257265091,\n",
       "   0.7552100366353989,\n",
       "   0.7360448144674301,\n",
       "   0.7414670512080193,\n",
       "   0.7494295625090599,\n",
       "   0.7371460669636727,\n",
       "   0.7371414548754692,\n",
       "   0.7387767916321755,\n",
       "   0.7346621882319451,\n",
       "   0.729343874335289,\n",
       "   0.7154981326460839,\n",
       "   0.7241853189468384,\n",
       "   0.7200521076917649,\n",
       "   0.7168885003328324,\n",
       "   0.7137106335759162,\n",
       "   0.704644744694233,\n",
       "   0.712933917939663,\n",
       "   0.7101412235498429,\n",
       "   0.6976962233185768,\n",
       "   0.6835261573195457,\n",
       "   0.6959152084589004,\n",
       "   0.6946925252676011,\n",
       "   0.6936626036167145,\n",
       "   0.6855437688231468,\n",
       "   0.6757366685271263,\n",
       "   0.6853866739869118,\n",
       "   0.6831811745166778,\n",
       "   0.669134651184082,\n",
       "   0.674112578690052,\n",
       "   0.6681912115812302,\n",
       "   0.6822096589207649,\n",
       "   0.6632646207809448,\n",
       "   0.6614208935499192,\n",
       "   0.6562968887090683],\n",
       "  'train_loss_std': [0.1826323852296664,\n",
       "   0.1304803816692784,\n",
       "   0.13473082044900705,\n",
       "   0.1264148518281296,\n",
       "   0.1309927541645151,\n",
       "   0.13041765483289183,\n",
       "   0.13595038986421248,\n",
       "   0.1282007766301599,\n",
       "   0.125523515884704,\n",
       "   0.12920474467392193,\n",
       "   0.13149506488701052,\n",
       "   0.14458368235504027,\n",
       "   0.13563396875998307,\n",
       "   0.1340685289431707,\n",
       "   0.1269818281368983,\n",
       "   0.13682861508570374,\n",
       "   0.13270618698630288,\n",
       "   0.13596521509053586,\n",
       "   0.13223582941469023,\n",
       "   0.14225810632161273,\n",
       "   0.13308502067756578,\n",
       "   0.12679021754557895,\n",
       "   0.1413390589588845,\n",
       "   0.1359445235222156,\n",
       "   0.13791269515262905,\n",
       "   0.14443757268196517,\n",
       "   0.13392421180560674,\n",
       "   0.13951165945190877,\n",
       "   0.13877931301643015,\n",
       "   0.1358298267473773,\n",
       "   0.13501797264604687,\n",
       "   0.1349164184967577,\n",
       "   0.13695523673551566,\n",
       "   0.1451483073167463,\n",
       "   0.13982310347214305,\n",
       "   0.14314590149939144,\n",
       "   0.1363050076063552,\n",
       "   0.14002422585038526,\n",
       "   0.14759149039856237,\n",
       "   0.14450472539140388,\n",
       "   0.13188158767524813,\n",
       "   0.1350245778423253,\n",
       "   0.12905263484849128,\n",
       "   0.13764389183831624,\n",
       "   0.13881268250807685,\n",
       "   0.1354287122386498,\n",
       "   0.13985902688178561,\n",
       "   0.13178799312143952,\n",
       "   0.1342801378552925,\n",
       "   0.13451919669575796,\n",
       "   0.12829729237562648,\n",
       "   0.14118260486342377,\n",
       "   0.1308724887873948,\n",
       "   0.13919309404204383,\n",
       "   0.14872840812009974,\n",
       "   0.12976774577852346,\n",
       "   0.12731262007723013,\n",
       "   0.13881869922137072,\n",
       "   0.13462943655004367,\n",
       "   0.13770271167993803,\n",
       "   0.13847298443940023,\n",
       "   0.140252588668378,\n",
       "   0.1382651704000421,\n",
       "   0.13250643508855578,\n",
       "   0.13567018807668704,\n",
       "   0.14114834201387394,\n",
       "   0.13345703945506426,\n",
       "   0.13655771799823885,\n",
       "   0.12798300171630722,\n",
       "   0.1373892975461676,\n",
       "   0.12939987675503944,\n",
       "   0.1371566591935981,\n",
       "   0.1380250530490947,\n",
       "   0.13158649704415568,\n",
       "   0.13477428405938588,\n",
       "   0.13420257553948292,\n",
       "   0.1407853177902386,\n",
       "   0.14061222669207435,\n",
       "   0.13677513237132138,\n",
       "   0.13573551411253723,\n",
       "   0.13190207288199612,\n",
       "   0.132529215541455,\n",
       "   0.13839148092956385,\n",
       "   0.130502342266507,\n",
       "   0.1302888656064939,\n",
       "   0.1315539211756657,\n",
       "   0.12965038840868331,\n",
       "   0.13434221155013185,\n",
       "   0.13612430655629618,\n",
       "   0.13094215138925933,\n",
       "   0.13315066921480567,\n",
       "   0.13470950451381783,\n",
       "   0.1279483889866256,\n",
       "   0.13657829005808028,\n",
       "   0.12884267597012028,\n",
       "   0.14187728639938374,\n",
       "   0.12587805437370733,\n",
       "   0.13249774895751423,\n",
       "   0.13068641180660404],\n",
       "  'train_accuracy_mean': [0.4261333337724209,\n",
       "   0.4493066667318344,\n",
       "   0.4682266671061516,\n",
       "   0.4951066664457321,\n",
       "   0.5127066660523415,\n",
       "   0.5336266663074494,\n",
       "   0.5442933322191238,\n",
       "   0.5627199993133545,\n",
       "   0.5695866670012474,\n",
       "   0.565373331964016,\n",
       "   0.588639999628067,\n",
       "   0.5888799980282784,\n",
       "   0.5933466667532921,\n",
       "   0.601453332722187,\n",
       "   0.6016533324122428,\n",
       "   0.603493331849575,\n",
       "   0.6135866670608521,\n",
       "   0.6145866674780845,\n",
       "   0.6122266656756401,\n",
       "   0.6214533323645591,\n",
       "   0.6191599994301796,\n",
       "   0.6309333310723305,\n",
       "   0.6338266662359238,\n",
       "   0.6329333322644234,\n",
       "   0.6336533327102661,\n",
       "   0.6381600015163421,\n",
       "   0.6439333313703537,\n",
       "   0.6469600001573562,\n",
       "   0.6427999994754792,\n",
       "   0.6396266660690307,\n",
       "   0.6505066667199135,\n",
       "   0.6491199991106987,\n",
       "   0.6559199989438057,\n",
       "   0.6606533325314522,\n",
       "   0.6594800000190735,\n",
       "   0.6673466663360595,\n",
       "   0.6655200003981591,\n",
       "   0.6673333329558373,\n",
       "   0.6650399996638298,\n",
       "   0.6748266662359238,\n",
       "   0.6705600000619888,\n",
       "   0.6772666668891907,\n",
       "   0.6799200012087822,\n",
       "   0.6772000007033349,\n",
       "   0.6801466667056084,\n",
       "   0.6869466667175292,\n",
       "   0.686026665687561,\n",
       "   0.6902533336877823,\n",
       "   0.6920133324265481,\n",
       "   0.6886266642808914,\n",
       "   0.68794666659832,\n",
       "   0.6986266648769379,\n",
       "   0.6940266666412354,\n",
       "   0.699613334774971,\n",
       "   0.6942933332920075,\n",
       "   0.7006933341026306,\n",
       "   0.7033600001335144,\n",
       "   0.7074799988865852,\n",
       "   0.7057733334302903,\n",
       "   0.7049866656064987,\n",
       "   0.7036266678571701,\n",
       "   0.7119333344697952,\n",
       "   0.7138933347463607,\n",
       "   0.715853333234787,\n",
       "   0.7141999994516373,\n",
       "   0.7140400002002716,\n",
       "   0.7168533338308334,\n",
       "   0.725626667380333,\n",
       "   0.7227066665887832,\n",
       "   0.7193200001716614,\n",
       "   0.725413333773613,\n",
       "   0.7234800004959107,\n",
       "   0.723253332734108,\n",
       "   0.7253733327388764,\n",
       "   0.7260933326482772,\n",
       "   0.7316666649580001,\n",
       "   0.7288933338522912,\n",
       "   0.7306133338212967,\n",
       "   0.7303333345651627,\n",
       "   0.7314266653060914,\n",
       "   0.735506667137146,\n",
       "   0.7332399996519089,\n",
       "   0.7349199990034103,\n",
       "   0.7404933340549469,\n",
       "   0.7425599994659424,\n",
       "   0.7415066667795182,\n",
       "   0.7392666659355164,\n",
       "   0.7413466668128967,\n",
       "   0.7460399987697601,\n",
       "   0.746693333029747,\n",
       "   0.7457199994325637,\n",
       "   0.7445866672992706,\n",
       "   0.7506400004625321,\n",
       "   0.7486000003814697,\n",
       "   0.7513066667318344,\n",
       "   0.7477466658353805,\n",
       "   0.7541600004434585,\n",
       "   0.7561466666460037,\n",
       "   0.7561733330488205],\n",
       "  'train_accuracy_std': [0.06591361322123487,\n",
       "   0.06227793756495563,\n",
       "   0.06835308367811276,\n",
       "   0.06538203616534438,\n",
       "   0.06831468476400906,\n",
       "   0.06832798748275302,\n",
       "   0.07193369262052633,\n",
       "   0.06729224365959229,\n",
       "   0.06510100054305314,\n",
       "   0.06479930119070763,\n",
       "   0.06795959040083484,\n",
       "   0.07165900098586855,\n",
       "   0.06741513048744034,\n",
       "   0.0680922502859073,\n",
       "   0.06673413008510871,\n",
       "   0.06553351161599054,\n",
       "   0.06747923416971312,\n",
       "   0.06650268395691467,\n",
       "   0.0683710270020473,\n",
       "   0.07152263884476014,\n",
       "   0.06612652072070295,\n",
       "   0.06206032981381363,\n",
       "   0.0703668251657764,\n",
       "   0.06648789418471994,\n",
       "   0.06728701246625754,\n",
       "   0.07155660559517912,\n",
       "   0.066334473167138,\n",
       "   0.06834229755388486,\n",
       "   0.06930627723079129,\n",
       "   0.06537307174004116,\n",
       "   0.06784433944616584,\n",
       "   0.06691306983620103,\n",
       "   0.0668841311562931,\n",
       "   0.069106165892124,\n",
       "   0.06901591305095696,\n",
       "   0.06915219765764169,\n",
       "   0.0647862887261049,\n",
       "   0.06623258767813986,\n",
       "   0.0718419459488728,\n",
       "   0.07169497800973891,\n",
       "   0.06508761424464551,\n",
       "   0.0657706116016513,\n",
       "   0.06296801911217323,\n",
       "   0.06590080222099384,\n",
       "   0.06840842025300041,\n",
       "   0.06627492772510031,\n",
       "   0.06543352288977157,\n",
       "   0.06403907092916886,\n",
       "   0.06840104002154876,\n",
       "   0.06458743119987016,\n",
       "   0.06211642687526532,\n",
       "   0.06738927239924632,\n",
       "   0.06253929844881151,\n",
       "   0.06659951138714797,\n",
       "   0.07149553758333171,\n",
       "   0.06295719321740238,\n",
       "   0.06128475736175209,\n",
       "   0.06637372883572769,\n",
       "   0.06555880956323262,\n",
       "   0.0679099067738081,\n",
       "   0.06434354386270719,\n",
       "   0.06817295001884771,\n",
       "   0.06506917563911233,\n",
       "   0.061723078532587426,\n",
       "   0.0640697890322431,\n",
       "   0.0658114347887569,\n",
       "   0.06567976711477352,\n",
       "   0.06569548135653552,\n",
       "   0.061942325345967345,\n",
       "   0.06522289835833038,\n",
       "   0.06401411539999399,\n",
       "   0.06662449371062536,\n",
       "   0.06412274824441129,\n",
       "   0.06401123695914206,\n",
       "   0.06404671884218813,\n",
       "   0.06506594910096727,\n",
       "   0.06656206531730184,\n",
       "   0.06710573129074256,\n",
       "   0.06630150024425562,\n",
       "   0.06420529746841241,\n",
       "   0.06424250044563738,\n",
       "   0.06427503455762233,\n",
       "   0.06539770887594931,\n",
       "   0.06324433107517077,\n",
       "   0.06368971393939064,\n",
       "   0.06288010494075127,\n",
       "   0.06242716640396464,\n",
       "   0.06472409529633813,\n",
       "   0.06565402924516457,\n",
       "   0.0637313745094133,\n",
       "   0.06218461308944313,\n",
       "   0.06251565245503438,\n",
       "   0.06131912101622321,\n",
       "   0.06498748440065961,\n",
       "   0.062471889729398905,\n",
       "   0.06593726125482527,\n",
       "   0.059265363121513114,\n",
       "   0.06267656471033763,\n",
       "   0.061985131336572095],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.471176369190216,\n",
       "   1.4203386040528616,\n",
       "   1.3806963209311167,\n",
       "   1.3533262141545614,\n",
       "   1.3228069829940796,\n",
       "   1.2923235789934795,\n",
       "   1.2658109680811565,\n",
       "   1.2479879687229791,\n",
       "   1.22375756641229,\n",
       "   1.2312382558981578,\n",
       "   1.2062380532423655,\n",
       "   1.201440934141477,\n",
       "   1.1956076474984487,\n",
       "   1.193373558918635,\n",
       "   1.1765884820620218,\n",
       "   1.17311465660731,\n",
       "   1.187868253191312,\n",
       "   1.1736191300551098,\n",
       "   1.1559246160586676,\n",
       "   1.1491659792264302,\n",
       "   1.1453562692801158,\n",
       "   1.1545843807856242,\n",
       "   1.1417370017369588,\n",
       "   1.1338486299912134,\n",
       "   1.1240334182977676,\n",
       "   1.1281356239318847,\n",
       "   1.1178909635543823,\n",
       "   1.1150680551926295,\n",
       "   1.103643309076627,\n",
       "   1.1039624404907227,\n",
       "   1.0983565264940263,\n",
       "   1.0944552268584569,\n",
       "   1.0878954009215036,\n",
       "   1.0826534553368887,\n",
       "   1.084435091416041,\n",
       "   1.0796767693758011,\n",
       "   1.0691722273826598,\n",
       "   1.075462252298991,\n",
       "   1.0627307752768198,\n",
       "   1.069868769844373,\n",
       "   1.058972169359525,\n",
       "   1.0591596617301304,\n",
       "   1.056293647289276,\n",
       "   1.0516797292232514,\n",
       "   1.0464338860909144,\n",
       "   1.04096601943175,\n",
       "   1.0390473582347235,\n",
       "   1.0437529691060383,\n",
       "   1.0289837962388992,\n",
       "   1.0326292578379312,\n",
       "   1.0349435208241144,\n",
       "   1.03252683142821,\n",
       "   1.0310525498787562,\n",
       "   1.0104590525229773,\n",
       "   1.0139344509442647,\n",
       "   1.0106956521670023,\n",
       "   1.032102631131808,\n",
       "   1.0090903135140736,\n",
       "   1.0105778209368388,\n",
       "   1.0125532406568527,\n",
       "   1.0054045470555624,\n",
       "   0.9964819796880087,\n",
       "   0.9941920403639476,\n",
       "   0.9932256497939428,\n",
       "   0.9877218504746755,\n",
       "   0.9872963384787241,\n",
       "   0.9900266283750534,\n",
       "   0.9877198950449626,\n",
       "   0.9829322874546051,\n",
       "   0.9856233690182368,\n",
       "   0.9900106473763783,\n",
       "   0.9901331992944081,\n",
       "   0.9795691275596619,\n",
       "   0.9728402330478032,\n",
       "   0.9765372564395268,\n",
       "   0.9615845106045405,\n",
       "   0.9775967270135879,\n",
       "   0.9674115592241287,\n",
       "   0.9670706542332967,\n",
       "   0.9565932681163152,\n",
       "   0.9645783323049545,\n",
       "   0.954222569068273,\n",
       "   0.9548276960849762,\n",
       "   0.9615878440936406,\n",
       "   0.9601044146219889,\n",
       "   0.9499993185202281,\n",
       "   0.951361569960912,\n",
       "   0.9399580576022466,\n",
       "   0.9446669640143712,\n",
       "   0.9545165171225866,\n",
       "   0.9450459994872411,\n",
       "   0.9451093810796738,\n",
       "   0.9568006992340088,\n",
       "   0.9583671281735102,\n",
       "   0.9470258406798044,\n",
       "   0.9410540425777435,\n",
       "   0.9342471953233084,\n",
       "   0.9384942634900411,\n",
       "   0.9439093967278799],\n",
       "  'val_loss_std': [0.11203740130864129,\n",
       "   0.10715929112252705,\n",
       "   0.09827341826141245,\n",
       "   0.10125932640351137,\n",
       "   0.10043295801446125,\n",
       "   0.10740659524183253,\n",
       "   0.11049734687425121,\n",
       "   0.11983441107245077,\n",
       "   0.11930227996812569,\n",
       "   0.11529669617151428,\n",
       "   0.11472055706061447,\n",
       "   0.11461747819445803,\n",
       "   0.1188223925285219,\n",
       "   0.12176546787710572,\n",
       "   0.11885147128588246,\n",
       "   0.11581106846402765,\n",
       "   0.11909995483656824,\n",
       "   0.12196800945397059,\n",
       "   0.12398465967775586,\n",
       "   0.12429102030429559,\n",
       "   0.12626138369322082,\n",
       "   0.12399722233293942,\n",
       "   0.12768290815715796,\n",
       "   0.12657912390755227,\n",
       "   0.1270241844349698,\n",
       "   0.12868818947660365,\n",
       "   0.1277192766153749,\n",
       "   0.1314605584614048,\n",
       "   0.13090726697788443,\n",
       "   0.1326811614541339,\n",
       "   0.12793874582377707,\n",
       "   0.1295033536529318,\n",
       "   0.13508594559952292,\n",
       "   0.1256389745632251,\n",
       "   0.1276599462085666,\n",
       "   0.13297767868085042,\n",
       "   0.13163224099730259,\n",
       "   0.1336244294053933,\n",
       "   0.13202423489457363,\n",
       "   0.1308216289214086,\n",
       "   0.1334729536791329,\n",
       "   0.13540187290704866,\n",
       "   0.1331894646012306,\n",
       "   0.13424498999450782,\n",
       "   0.13375660790981328,\n",
       "   0.13291695676870965,\n",
       "   0.13276573331422253,\n",
       "   0.13617518701132503,\n",
       "   0.13315697343223637,\n",
       "   0.131602270458581,\n",
       "   0.13268724730948828,\n",
       "   0.133248200070377,\n",
       "   0.1334498839268254,\n",
       "   0.13341375022411908,\n",
       "   0.1332544375230059,\n",
       "   0.13522877668616448,\n",
       "   0.13646125097249184,\n",
       "   0.13827714189175325,\n",
       "   0.13632873247995536,\n",
       "   0.1397483090422933,\n",
       "   0.14107534466995766,\n",
       "   0.13191791023486107,\n",
       "   0.13609270302166718,\n",
       "   0.1373436265325103,\n",
       "   0.13782754732955282,\n",
       "   0.13881340803437905,\n",
       "   0.13607888968501025,\n",
       "   0.13811387686912519,\n",
       "   0.13513214094033638,\n",
       "   0.13763618903021407,\n",
       "   0.14120659189166673,\n",
       "   0.13886795139057656,\n",
       "   0.13668628195830615,\n",
       "   0.13594511421080996,\n",
       "   0.1339807457042434,\n",
       "   0.13636625945424435,\n",
       "   0.14274674896351155,\n",
       "   0.13894214011758702,\n",
       "   0.13304951145628094,\n",
       "   0.13509201511513172,\n",
       "   0.14027035249475067,\n",
       "   0.13705290740066123,\n",
       "   0.13767863220396992,\n",
       "   0.1387881382663321,\n",
       "   0.13586516452145064,\n",
       "   0.14269873294848856,\n",
       "   0.13649695558837954,\n",
       "   0.13155420116093455,\n",
       "   0.13704813390960915,\n",
       "   0.14400514639874926,\n",
       "   0.14107233424250729,\n",
       "   0.14060849573919304,\n",
       "   0.14711724698209286,\n",
       "   0.1410907367007103,\n",
       "   0.13930975593546616,\n",
       "   0.14175003598064614,\n",
       "   0.14033359981714238,\n",
       "   0.13476484911151734,\n",
       "   0.13750004130782806],\n",
       "  'val_accuracy_mean': [0.4045111114283403,\n",
       "   0.41722222248713176,\n",
       "   0.4324888893961906,\n",
       "   0.4456444451212883,\n",
       "   0.45973333438237507,\n",
       "   0.47355555643637975,\n",
       "   0.485844445625941,\n",
       "   0.49588888804117837,\n",
       "   0.5084666676322619,\n",
       "   0.5044666656851768,\n",
       "   0.5158888885378837,\n",
       "   0.5172444433967273,\n",
       "   0.5201333321134249,\n",
       "   0.5220222216844559,\n",
       "   0.5298222202062607,\n",
       "   0.53033333192269,\n",
       "   0.5256666652361552,\n",
       "   0.5339777773618698,\n",
       "   0.5385111107428868,\n",
       "   0.5424444432059924,\n",
       "   0.543244443833828,\n",
       "   0.5373111102978388,\n",
       "   0.5459999985496203,\n",
       "   0.550755555431048,\n",
       "   0.553444446225961,\n",
       "   0.5526666683952014,\n",
       "   0.5570666654904683,\n",
       "   0.5590888892610868,\n",
       "   0.5632888871431351,\n",
       "   0.563711110452811,\n",
       "   0.5653777765234311,\n",
       "   0.568422221938769,\n",
       "   0.5713999994595845,\n",
       "   0.5731777773300807,\n",
       "   0.5676222208142281,\n",
       "   0.5737999984622002,\n",
       "   0.5795555543899537,\n",
       "   0.5761999988555908,\n",
       "   0.5801555547118187,\n",
       "   0.5775555542111397,\n",
       "   0.584933332502842,\n",
       "   0.584577779173851,\n",
       "   0.5832666645447413,\n",
       "   0.5870222210884094,\n",
       "   0.5891777774691582,\n",
       "   0.5897111116846403,\n",
       "   0.5926888887087504,\n",
       "   0.5913777764638265,\n",
       "   0.5966444445649782,\n",
       "   0.5947777767976125,\n",
       "   0.593177777826786,\n",
       "   0.5961777770519257,\n",
       "   0.5967333329717318,\n",
       "   0.6039111100633939,\n",
       "   0.5999999997019768,\n",
       "   0.6032444436351458,\n",
       "   0.5947777771949768,\n",
       "   0.6045777769883474,\n",
       "   0.6029333333174388,\n",
       "   0.6017555550734202,\n",
       "   0.6071777763962746,\n",
       "   0.6112444439530372,\n",
       "   0.6100222223997116,\n",
       "   0.6110222214460372,\n",
       "   0.6153333310286204,\n",
       "   0.6140222209692001,\n",
       "   0.6119555546840032,\n",
       "   0.6158888883392016,\n",
       "   0.6154888861378034,\n",
       "   0.6144222230712573,\n",
       "   0.612022221883138,\n",
       "   0.6133555539449056,\n",
       "   0.6153111106157303,\n",
       "   0.6194888902703921,\n",
       "   0.616088885863622,\n",
       "   0.6242444427808126,\n",
       "   0.6175777745246888,\n",
       "   0.6214222213625908,\n",
       "   0.6218444431821505,\n",
       "   0.6247555573781332,\n",
       "   0.622711109717687,\n",
       "   0.6277111102143923,\n",
       "   0.6264444435636203,\n",
       "   0.6237333337465922,\n",
       "   0.6238444447517395,\n",
       "   0.6310888887445132,\n",
       "   0.6279999990264574,\n",
       "   0.6320666670799255,\n",
       "   0.6319333316882452,\n",
       "   0.630444445113341,\n",
       "   0.6305555550257365,\n",
       "   0.6290666659673055,\n",
       "   0.6284222219387691,\n",
       "   0.6274222214023272,\n",
       "   0.6303555554151535,\n",
       "   0.6347555540998777,\n",
       "   0.6359333327412605,\n",
       "   0.6359555553396543,\n",
       "   0.6325333312153816],\n",
       "  'val_accuracy_std': [0.05353799162002322,\n",
       "   0.0524720981914053,\n",
       "   0.05174179064930801,\n",
       "   0.0545594439699906,\n",
       "   0.05402814078556591,\n",
       "   0.05838780569726321,\n",
       "   0.058177808417806676,\n",
       "   0.059355695041157855,\n",
       "   0.061534849233953345,\n",
       "   0.05942979616153935,\n",
       "   0.06035072230962436,\n",
       "   0.057592111503469996,\n",
       "   0.05651335922892677,\n",
       "   0.06100806981501532,\n",
       "   0.06108741345146006,\n",
       "   0.05955669406457232,\n",
       "   0.058520333169778685,\n",
       "   0.06090453432575346,\n",
       "   0.06073382762738213,\n",
       "   0.0630527898408561,\n",
       "   0.06264413916069356,\n",
       "   0.0613480611002943,\n",
       "   0.06201911090369558,\n",
       "   0.0629110434808522,\n",
       "   0.062440392068660824,\n",
       "   0.06351319518242336,\n",
       "   0.06256483259522035,\n",
       "   0.06216751088082387,\n",
       "   0.06503216901734982,\n",
       "   0.063530729283033,\n",
       "   0.06337751299008496,\n",
       "   0.06272361094956662,\n",
       "   0.06447941994903779,\n",
       "   0.06387816796777328,\n",
       "   0.06152727250719634,\n",
       "   0.06408735869920205,\n",
       "   0.06256571892982157,\n",
       "   0.06175642809294948,\n",
       "   0.06404262086135958,\n",
       "   0.06290223682003454,\n",
       "   0.0639579955819006,\n",
       "   0.06262474690427144,\n",
       "   0.06504126291026241,\n",
       "   0.06563552119824073,\n",
       "   0.06647131003645682,\n",
       "   0.06358946703232453,\n",
       "   0.06259830473072665,\n",
       "   0.06436952090747526,\n",
       "   0.06365764528204357,\n",
       "   0.06382953621904496,\n",
       "   0.06245690339964687,\n",
       "   0.06149903542250107,\n",
       "   0.06350382894868392,\n",
       "   0.064204242829744,\n",
       "   0.06414104917256028,\n",
       "   0.06333795125922861,\n",
       "   0.06269611338101945,\n",
       "   0.06368271627047645,\n",
       "   0.06318921597259596,\n",
       "   0.0649081370904879,\n",
       "   0.0637927615753104,\n",
       "   0.06103617205479102,\n",
       "   0.06279065355775032,\n",
       "   0.06318924604310566,\n",
       "   0.06395484523229095,\n",
       "   0.06394186781302506,\n",
       "   0.06258764194025813,\n",
       "   0.06308949158158543,\n",
       "   0.06251712203552993,\n",
       "   0.06230509942086057,\n",
       "   0.06541423204172483,\n",
       "   0.061592445558287426,\n",
       "   0.06380363086714479,\n",
       "   0.06302408409497127,\n",
       "   0.06400549391932762,\n",
       "   0.06282414847505599,\n",
       "   0.0629419452437504,\n",
       "   0.06382191338091253,\n",
       "   0.061903194602919105,\n",
       "   0.061412204716973555,\n",
       "   0.06305833634416222,\n",
       "   0.06295163999330895,\n",
       "   0.06317485019100758,\n",
       "   0.06443362498009035,\n",
       "   0.06333829041430022,\n",
       "   0.06522325974022641,\n",
       "   0.06261463607075052,\n",
       "   0.060682747250622435,\n",
       "   0.06417772774643522,\n",
       "   0.06386289312281059,\n",
       "   0.06522998233880463,\n",
       "   0.06278136106601999,\n",
       "   0.06413203697901643,\n",
       "   0.06247979930146581,\n",
       "   0.06308154220281362,\n",
       "   0.06139410959916963,\n",
       "   0.06245279278670863,\n",
       "   0.06198397255791851,\n",
       "   0.06265919627346102],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6818444436788559,\n",
       " 'best_val_iter': 48000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 96,\n",
       " 'train_loss_mean': 0.4526471059322357,\n",
       " 'train_loss_std': 0.12993412983496969,\n",
       " 'train_accuracy_mean': 0.8343333342075347,\n",
       " 'train_accuracy_std': 0.0548158550248203,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.8508474173148474,\n",
       " 'val_loss_std': 0.14179470314582376,\n",
       " 'val_accuracy_mean': 0.6763111112515131,\n",
       " 'val_accuracy_std': 0.05848475377685013,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0115, -0.0778,  0.0524],\n",
       "                         [-0.0305, -0.0349, -0.0020],\n",
       "                         [-0.0158,  0.1034, -0.0275]],\n",
       "               \n",
       "                        [[ 0.0519, -0.0845,  0.0649],\n",
       "                         [-0.0253,  0.0089,  0.0542],\n",
       "                         [-0.0484,  0.0403, -0.0331]],\n",
       "               \n",
       "                        [[ 0.0545, -0.0248, -0.0204],\n",
       "                         [ 0.0301,  0.0602, -0.0543],\n",
       "                         [-0.0532,  0.0362, -0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0481,  0.1008,  0.0398],\n",
       "                         [ 0.0482, -0.0425,  0.0113],\n",
       "                         [-0.0911, -0.0673, -0.0469]],\n",
       "               \n",
       "                        [[-0.0092,  0.0265, -0.0018],\n",
       "                         [ 0.0446, -0.0628, -0.0604],\n",
       "                         [ 0.0496, -0.0558,  0.0711]],\n",
       "               \n",
       "                        [[-0.0320,  0.0804, -0.0688],\n",
       "                         [-0.0160,  0.0558,  0.0703],\n",
       "                         [-0.0326, -0.0326, -0.0280]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0071, -0.0199,  0.0549],\n",
       "                         [-0.0223, -0.0425, -0.0309],\n",
       "                         [ 0.0645,  0.0541, -0.0505]],\n",
       "               \n",
       "                        [[-0.0577, -0.0244,  0.0483],\n",
       "                         [ 0.0787, -0.0048,  0.0167],\n",
       "                         [-0.0691, -0.0398,  0.0441]],\n",
       "               \n",
       "                        [[ 0.0377, -0.0742,  0.0330],\n",
       "                         [ 0.0672,  0.0122, -0.0616],\n",
       "                         [-0.0667,  0.0326,  0.0182]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0424, -0.0622, -0.0432],\n",
       "                         [-0.0184, -0.0636,  0.0860],\n",
       "                         [-0.0358,  0.0483, -0.0402]],\n",
       "               \n",
       "                        [[ 0.0383,  0.0228,  0.0691],\n",
       "                         [ 0.0292, -0.0177, -0.0221],\n",
       "                         [ 0.0055,  0.0494, -0.0077]],\n",
       "               \n",
       "                        [[-0.0062, -0.0760, -0.0401],\n",
       "                         [-0.0594,  0.0223,  0.0496],\n",
       "                         [-0.0420,  0.0299,  0.0492]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0190, -0.0077, -0.0359],\n",
       "                         [ 0.0073,  0.0689, -0.0534],\n",
       "                         [ 0.0444,  0.0745, -0.0492]],\n",
       "               \n",
       "                        [[ 0.0429,  0.0223,  0.0724],\n",
       "                         [ 0.0197, -0.0301, -0.0356],\n",
       "                         [-0.0315, -0.0832,  0.0298]],\n",
       "               \n",
       "                        [[-0.0250, -0.0121, -0.0183],\n",
       "                         [-0.0136, -0.0266,  0.0769],\n",
       "                         [-0.0608, -0.0201,  0.0196]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0678,  0.0252, -0.0223],\n",
       "                         [-0.0489,  0.0072,  0.0349],\n",
       "                         [ 0.0568,  0.0435,  0.0784]],\n",
       "               \n",
       "                        [[-0.0682, -0.0912,  0.0311],\n",
       "                         [ 0.0605, -0.0843, -0.0335],\n",
       "                         [ 0.0032,  0.0237,  0.0296]],\n",
       "               \n",
       "                        [[ 0.0041, -0.0576,  0.0435],\n",
       "                         [ 0.0096, -0.0407, -0.0082],\n",
       "                         [-0.0129, -0.0269,  0.0120]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.0765e-03, -8.0369e-04, -1.8382e-03, -1.1685e-04, -1.5081e-04,\n",
       "                        9.4584e-04,  1.9420e-04,  1.0867e-04,  8.7550e-04, -1.9117e-03,\n",
       "                       -3.8360e-04,  1.2847e-04,  2.6250e-03,  1.7375e-03, -2.0646e-03,\n",
       "                        7.6395e-04, -6.3794e-04, -6.1944e-04, -7.8609e-04, -2.0221e-04,\n",
       "                        1.6218e-03, -2.4587e-04,  2.3333e-03,  2.2809e-03,  3.7578e-04,\n",
       "                        2.2750e-05,  6.9353e-04,  7.8915e-04,  1.0253e-03,  9.2878e-04,\n",
       "                        1.5839e-03, -6.1639e-05,  3.5597e-04, -3.5442e-03, -4.6407e-04,\n",
       "                        6.9834e-04,  1.1986e-03,  6.3443e-04, -1.2857e-05, -8.4554e-04,\n",
       "                        2.7600e-04, -3.0869e-03, -1.1622e-03,  2.0713e-04, -3.6686e-04,\n",
       "                       -7.5862e-05,  2.7620e-04, -1.3584e-03, -2.4717e-03, -1.9970e-05,\n",
       "                        1.1759e-03, -8.4054e-04,  1.2545e-03, -2.8829e-03,  1.1732e-03,\n",
       "                        1.6122e-04,  5.0176e-04,  3.3543e-04, -6.3797e-05, -1.7757e-03,\n",
       "                        2.5158e-04,  1.5920e-03, -5.9205e-04,  7.7316e-04,  3.4142e-04,\n",
       "                        7.5710e-04, -6.7782e-04,  1.7540e-03, -8.1071e-04, -5.7699e-04,\n",
       "                        1.4908e-04, -1.1079e-03, -1.0642e-05, -2.4850e-04,  1.3602e-03,\n",
       "                       -5.1398e-04,  8.3879e-04,  8.4650e-04, -1.5811e-04,  3.1155e-04,\n",
       "                        1.4772e-03, -2.1250e-03,  1.2379e-03,  9.4745e-05, -2.6874e-03,\n",
       "                       -8.9463e-04,  1.0693e-03, -1.5491e-03,  1.8957e-04,  2.8946e-03,\n",
       "                        1.1849e-03,  2.1482e-04,  2.0873e-04, -7.8655e-04,  1.2980e-03,\n",
       "                       -2.6352e-03,  1.9492e-03, -1.8718e-03, -1.6439e-03, -3.3162e-04,\n",
       "                        3.8143e-04, -7.6442e-04,  1.1236e-03, -1.1091e-04, -2.0625e-03,\n",
       "                        1.1619e-03, -4.9918e-04,  1.9502e-06, -1.0099e-03, -8.2773e-04,\n",
       "                        1.4017e-03,  1.5289e-03, -1.8522e-03, -2.8503e-04,  3.0935e-04,\n",
       "                        1.6916e-04, -2.2714e-04, -1.6422e-03,  4.0964e-04, -2.4318e-04,\n",
       "                       -6.1455e-05,  9.1467e-05,  9.6760e-04,  4.0069e-04,  1.5191e-03,\n",
       "                        1.3727e-04,  8.4586e-04, -1.2893e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1601, -0.0269,  0.2146, -0.1223, -0.1164,  0.0143, -0.1938, -0.2474,\n",
       "                       -0.1682, -0.1138, -0.1828, -0.1827,  0.0486, -0.1432, -0.0289, -0.1198,\n",
       "                        0.0172, -0.1315, -0.0307, -0.0695, -0.1784, -0.0808, -0.1548,  0.0150,\n",
       "                       -0.0257, -0.1688, -0.0726, -0.0996, -0.0490,  0.0397, -0.1129, -0.1187,\n",
       "                       -0.1021, -0.1569, -0.1652, -0.2654, -0.1424, -0.0779, -0.1773, -0.1431,\n",
       "                       -0.0401,  0.1688, -0.1125, -0.0631, -0.0056, -0.1462, -0.1993, -0.2671,\n",
       "                        0.1400, -0.1451,  0.0623, -0.2133,  0.2319,  0.0191, -0.0483, -0.0743,\n",
       "                       -0.2520, -0.0044, -0.1727,  0.1165, -0.1224,  0.2687, -0.0037, -0.2654,\n",
       "                       -0.1504, -0.2399, -0.1681, -0.1425, -0.0929, -0.0455,  0.0674, -0.0151,\n",
       "                       -0.1439, -0.2008, -0.0738, -0.0622, -0.1589, -0.0268, -0.0954, -0.1174,\n",
       "                        0.0008, -0.1366, -0.2038, -0.2098, -0.0300,  0.1649, -0.0133, -0.1394,\n",
       "                       -0.1591,  0.0642, -0.0768, -0.1414,  0.0458, -0.0098, -0.0927,  0.0210,\n",
       "                        0.1638,  0.0666, -0.1137, -0.1096, -0.2115, -0.0579,  0.0081,  0.0286,\n",
       "                        0.0164, -0.1907, -0.0867, -0.1441, -0.0205, -0.0744, -0.0806, -0.1129,\n",
       "                       -0.0395, -0.1456, -0.0760, -0.0752, -0.0798,  0.1828, -0.0423,  0.0638,\n",
       "                       -0.1928, -0.2187, -0.1924, -0.1746,  0.1367, -0.0291, -0.2259, -0.1710],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.0777, 1.0709, 1.0801, 0.8988, 0.9437, 1.0149, 0.9128, 0.8826, 0.9809,\n",
       "                       0.9149, 0.9107, 0.9115, 1.0526, 0.8928, 1.0546, 1.0420, 1.0268, 0.9431,\n",
       "                       0.9681, 0.9646, 0.9394, 0.8664, 0.9175, 1.0001, 0.9011, 0.8854, 0.9461,\n",
       "                       0.9155, 0.9748, 0.9375, 1.0159, 0.9061, 1.0190, 1.2193, 0.9277, 0.9384,\n",
       "                       0.9600, 0.9206, 0.9119, 0.9612, 0.9499, 1.0274, 0.9329, 0.9598, 1.0849,\n",
       "                       1.0767, 0.9834, 0.9287, 1.1041, 0.9253, 1.0169, 0.9134, 1.0635, 1.0854,\n",
       "                       1.0065, 0.9070, 1.1188, 1.1279, 0.9739, 1.0818, 0.9082, 1.0035, 0.9521,\n",
       "                       0.9313, 0.9319, 0.8501, 0.8868, 0.9298, 1.0671, 0.9991, 0.9329, 0.9704,\n",
       "                       0.8722, 0.8916, 0.9964, 0.8846, 0.9151, 1.0916, 1.0985, 1.0259, 1.0244,\n",
       "                       0.9868, 0.8907, 0.9007, 1.1403, 1.1089, 1.0677, 1.0212, 0.9175, 0.9868,\n",
       "                       1.0047, 0.9299, 1.0141, 1.0329, 0.9991, 1.0203, 1.1726, 0.9629, 0.9522,\n",
       "                       0.9195, 0.8685, 1.0409, 1.1071, 0.9493, 0.9616, 0.9556, 0.9301, 0.8482,\n",
       "                       1.0131, 0.9587, 1.0176, 1.0275, 0.9890, 1.1160, 0.9377, 1.0863, 0.9138,\n",
       "                       1.0243, 0.9302, 0.9634, 0.8600, 0.8900, 1.2006, 1.0526, 1.0483, 0.9347,\n",
       "                       0.9339, 0.8952], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-3.1068e-02,  1.4934e-02, -2.4708e-02],\n",
       "                         [-4.9840e-02, -2.8395e-02, -7.1771e-02],\n",
       "                         [-2.5563e-02,  2.5884e-02, -1.4416e-02]],\n",
       "               \n",
       "                        [[ 7.1535e-02,  3.6242e-02, -8.3086e-02],\n",
       "                         [-5.5571e-02, -6.0254e-03, -3.9204e-02],\n",
       "                         [-2.6580e-02,  7.6422e-02,  2.6242e-02]],\n",
       "               \n",
       "                        [[ 7.9697e-02,  1.2020e-02, -8.9087e-04],\n",
       "                         [-2.4565e-02,  6.1961e-02,  8.5690e-02],\n",
       "                         [ 1.0954e-02,  5.8338e-02,  8.1456e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3707e-04, -3.0362e-02, -3.2934e-02],\n",
       "                         [-5.1523e-02, -6.6658e-02, -3.6795e-02],\n",
       "                         [ 7.4546e-03,  1.6017e-03, -1.5633e-02]],\n",
       "               \n",
       "                        [[ 1.1219e-02,  3.6779e-02, -7.3502e-03],\n",
       "                         [-9.8202e-03,  2.9635e-02, -1.4038e-02],\n",
       "                         [ 3.2854e-02, -5.6267e-02, -1.8176e-02]],\n",
       "               \n",
       "                        [[-1.0652e-02,  4.8453e-02, -3.1925e-02],\n",
       "                         [-2.8970e-02,  1.5542e-02, -4.6588e-03],\n",
       "                         [-4.3812e-02, -2.1815e-02,  4.8127e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4941e-02, -3.0805e-02,  3.8058e-02],\n",
       "                         [ 3.1757e-02,  3.3032e-02, -4.8846e-02],\n",
       "                         [ 3.0079e-02, -1.2053e-03, -3.8244e-02]],\n",
       "               \n",
       "                        [[ 3.1452e-02,  6.4167e-02, -2.9219e-03],\n",
       "                         [-4.8711e-02,  4.5315e-04,  2.8953e-02],\n",
       "                         [-1.8084e-02, -3.7616e-02,  6.2568e-03]],\n",
       "               \n",
       "                        [[ 6.1893e-02,  2.1873e-02, -4.9432e-02],\n",
       "                         [-1.2321e-02, -1.1952e-03, -2.0872e-02],\n",
       "                         [-2.9053e-02, -1.8110e-02, -4.7141e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1403e-02,  5.6295e-03,  4.0007e-02],\n",
       "                         [-4.6823e-02, -2.6105e-02, -2.8252e-02],\n",
       "                         [-1.2773e-02,  1.2540e-02, -1.2665e-02]],\n",
       "               \n",
       "                        [[-2.8528e-02, -7.3125e-02, -6.5685e-02],\n",
       "                         [ 1.5529e-02,  4.3157e-03,  4.4955e-03],\n",
       "                         [ 1.7821e-02, -4.4237e-02,  1.1965e-02]],\n",
       "               \n",
       "                        [[-1.4344e-02, -6.4940e-02,  1.5923e-02],\n",
       "                         [ 2.7153e-02, -2.8147e-02, -7.4200e-04],\n",
       "                         [-1.2954e-02, -3.2091e-02,  2.8604e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.7807e-02, -2.8579e-02, -4.7183e-02],\n",
       "                         [-2.5427e-03, -3.0682e-03, -4.7967e-02],\n",
       "                         [ 3.1416e-02, -1.7803e-02, -1.8947e-03]],\n",
       "               \n",
       "                        [[-3.5335e-03,  2.9050e-02, -5.2048e-02],\n",
       "                         [ 2.8188e-02,  1.9930e-02, -2.0456e-02],\n",
       "                         [ 3.0739e-02,  4.1309e-02, -4.7110e-02]],\n",
       "               \n",
       "                        [[-3.3564e-02, -4.0410e-02, -8.6391e-02],\n",
       "                         [-2.9952e-02, -1.4456e-02,  2.4784e-02],\n",
       "                         [-4.7339e-02, -4.5708e-02,  7.1623e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.2363e-02,  1.3501e-02, -1.5803e-04],\n",
       "                         [-1.9702e-02, -4.1223e-02, -1.4040e-02],\n",
       "                         [-2.1804e-02, -3.4117e-02, -4.1499e-02]],\n",
       "               \n",
       "                        [[ 5.0175e-02,  4.8530e-02,  6.1841e-02],\n",
       "                         [-5.2386e-02, -3.6055e-02, -2.1579e-02],\n",
       "                         [-6.1513e-02, -3.6027e-03, -4.1555e-02]],\n",
       "               \n",
       "                        [[-3.5838e-02, -6.2272e-02, -8.0042e-03],\n",
       "                         [-3.8722e-02,  7.4427e-03,  1.1058e-02],\n",
       "                         [-1.4307e-02, -3.2467e-02, -3.4531e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3628e-02,  2.7285e-03,  1.5521e-02],\n",
       "                         [ 6.4739e-02, -1.6291e-02,  2.1665e-02],\n",
       "                         [ 6.1771e-02, -5.1059e-02, -4.8901e-02]],\n",
       "               \n",
       "                        [[-2.6963e-02,  1.9296e-02,  5.3938e-02],\n",
       "                         [-5.3194e-02, -1.1404e-02,  1.8829e-02],\n",
       "                         [-7.9276e-02, -2.1864e-02, -3.8196e-02]],\n",
       "               \n",
       "                        [[ 8.4808e-02,  6.7533e-02,  6.2033e-02],\n",
       "                         [ 6.5936e-02, -2.6663e-02,  7.5278e-03],\n",
       "                         [ 2.4147e-02, -4.6637e-02, -4.0440e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.3273e-03,  1.0368e-02,  2.9113e-02],\n",
       "                         [ 4.7940e-02, -2.6338e-02,  3.7946e-02],\n",
       "                         [ 3.2950e-02,  4.6247e-02,  2.5710e-02]],\n",
       "               \n",
       "                        [[-9.7459e-03,  2.0474e-02,  4.2825e-02],\n",
       "                         [ 1.6884e-02,  4.6780e-03, -1.5560e-02],\n",
       "                         [-6.3945e-02,  6.8826e-03,  3.1620e-02]],\n",
       "               \n",
       "                        [[ 1.1923e-02, -2.2587e-02, -1.0285e-03],\n",
       "                         [-6.0087e-03,  9.3974e-05,  8.8612e-03],\n",
       "                         [ 8.7517e-03,  4.7585e-03, -3.5720e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.3675e-02, -4.8650e-02,  1.3389e-02],\n",
       "                         [-4.8180e-02, -1.0838e-01, -1.8075e-02],\n",
       "                         [-6.6564e-02, -4.1220e-02, -3.8812e-02]],\n",
       "               \n",
       "                        [[ 4.3953e-03,  5.0149e-02,  6.3318e-02],\n",
       "                         [ 4.8209e-02,  2.4266e-02,  2.1014e-02],\n",
       "                         [ 1.6416e-02, -3.3700e-02, -4.2817e-02]],\n",
       "               \n",
       "                        [[-2.8837e-02,  2.7409e-03,  2.8331e-02],\n",
       "                         [-5.9724e-03, -5.8118e-02, -7.0318e-02],\n",
       "                         [-6.0833e-02, -5.2656e-02, -5.2772e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.0133e-02,  3.9366e-03,  4.7938e-02],\n",
       "                         [-1.2157e-02,  1.9478e-02, -2.6585e-02],\n",
       "                         [-2.0719e-02, -2.5532e-02, -4.1482e-02]],\n",
       "               \n",
       "                        [[ 2.7662e-02,  5.0188e-02,  5.1921e-02],\n",
       "                         [-7.6517e-03, -5.1657e-02, -2.7490e-02],\n",
       "                         [-2.9292e-02, -3.8551e-02, -2.9929e-03]],\n",
       "               \n",
       "                        [[-2.6770e-02, -3.3468e-02, -3.3695e-02],\n",
       "                         [-5.5176e-02, -2.4818e-02, -5.8880e-02],\n",
       "                         [-1.5056e-02, -6.2207e-02, -4.8363e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.1598e-03, -4.3907e-02,  1.5628e-02],\n",
       "                         [ 1.6393e-02,  3.9933e-02, -4.7980e-03],\n",
       "                         [-9.2348e-03,  8.1859e-02,  3.9698e-02]],\n",
       "               \n",
       "                        [[-3.7246e-02,  1.7502e-02,  3.7163e-02],\n",
       "                         [ 5.7703e-02, -2.4650e-02, -5.0248e-03],\n",
       "                         [-2.8848e-02, -1.8945e-02,  4.7855e-02]],\n",
       "               \n",
       "                        [[ 1.3576e-03, -4.7517e-02,  3.5737e-02],\n",
       "                         [ 1.5863e-02,  3.9620e-02,  9.4381e-03],\n",
       "                         [ 5.0647e-04, -2.3891e-02,  3.4428e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9621e-02, -5.0091e-03, -5.7347e-02],\n",
       "                         [ 6.7124e-02,  8.4320e-02, -2.3866e-02],\n",
       "                         [ 1.6802e-02,  4.7377e-02,  3.4077e-03]],\n",
       "               \n",
       "                        [[-5.5406e-02, -5.9962e-02, -3.6674e-02],\n",
       "                         [ 3.1510e-02,  4.7255e-02,  1.2530e-03],\n",
       "                         [-8.1551e-03,  1.8949e-02, -5.1895e-02]],\n",
       "               \n",
       "                        [[-1.0334e-02,  2.3689e-02, -2.3868e-02],\n",
       "                         [-4.3318e-03,  2.3800e-02,  3.7886e-02],\n",
       "                         [ 2.9135e-02, -6.1549e-03, -1.9589e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 5.9933e-08, -6.1479e-05,  1.1136e-06,  1.1191e-06,  2.5666e-07,\n",
       "                        1.5369e-03, -1.1775e-06, -6.2422e-07, -1.1607e-06, -8.1503e-04,\n",
       "                       -1.8291e-06, -2.3342e-06, -2.2181e-04,  6.1000e-07, -2.3925e-06,\n",
       "                       -3.0629e-07, -3.1087e-06,  8.1563e-07,  9.3452e-08, -5.7189e-04,\n",
       "                       -3.2095e-07, -9.1869e-09,  4.8346e-06, -1.7036e-05,  1.3274e-02,\n",
       "                       -1.3903e-05,  1.6652e-05, -2.7412e-05, -3.4676e-03,  5.0787e-06,\n",
       "                        2.6765e-06,  4.6576e-07,  4.0706e-06,  1.7675e-05, -8.4356e-07,\n",
       "                        2.6364e-06,  4.4227e-09, -1.2458e-05,  2.1915e-05,  4.9682e-07,\n",
       "                        7.0539e-07, -1.6904e-06,  2.0231e-05, -5.5559e-06,  7.0082e-07,\n",
       "                       -3.0102e-06, -1.5954e-04, -3.8790e-06, -1.2362e-06, -3.6231e-06,\n",
       "                        7.5977e-07, -4.2942e-07,  1.9629e-06, -1.0215e-02, -4.5511e-07,\n",
       "                        9.4783e-03, -3.0247e-03, -1.9373e-05,  5.5702e-05, -1.3319e-02,\n",
       "                       -9.0035e-07, -1.1355e-06,  3.0399e-06,  2.3563e-03,  3.7166e-07,\n",
       "                        1.0655e-02,  7.9245e-06,  3.9385e-07, -2.3954e-06,  6.9563e-07,\n",
       "                       -9.1358e-07,  1.7789e-06, -1.0133e-06, -9.4920e-05, -3.5892e-05,\n",
       "                        2.6434e-06, -3.6071e-06,  2.4756e-06,  2.6890e-06,  7.2722e-05,\n",
       "                        2.2625e-07, -9.3507e-07,  2.5611e-06,  1.9175e-06, -6.8006e-06,\n",
       "                        3.2866e-07,  3.1739e-06, -7.8642e-03, -4.1726e-06,  4.7427e-07,\n",
       "                        3.0530e-06,  1.0925e-02,  2.6354e-07,  1.7425e-06, -1.6997e-06,\n",
       "                       -2.4809e-06, -7.8697e-07,  1.2117e-02, -1.6563e-06,  1.2712e-06,\n",
       "                        4.4987e-07,  5.1062e-06,  1.0757e-02, -1.8630e-06,  7.0586e-08,\n",
       "                        5.5715e-07, -1.2244e-04,  1.5775e-05,  1.1858e-02,  9.6294e-03,\n",
       "                       -1.0860e-06,  2.7695e-05, -3.0826e-05, -1.0798e-02,  3.5214e-08,\n",
       "                       -4.0514e-07,  1.3645e-02,  2.0043e-05,  1.9055e-04, -8.7982e-03,\n",
       "                       -1.0212e-06, -1.3086e-05, -2.2918e-08, -2.2165e-05, -1.8928e-06,\n",
       "                       -1.2180e-05, -1.3773e-02, -7.5184e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0872, -0.1572, -0.1664, -0.2343, -0.1549, -0.1571, -0.1078, -0.1666,\n",
       "                       -0.1076, -0.1716, -0.3466, -0.2546, -0.1173, -0.2540, -0.1237, -0.1846,\n",
       "                       -0.2605, -0.0961, -0.0841, -0.2577, -0.1466, -0.1738, -0.2186, -0.2139,\n",
       "                       -0.2324, -0.1326, -0.1120, -0.1670, -0.1783, -0.2671, -0.0948, -0.3858,\n",
       "                       -0.2320, -0.0944, -0.2653, -0.2115, -0.2098, -0.0639, -0.3093, -0.2235,\n",
       "                       -0.1407, -0.1924, -0.2923, -0.1656, -0.2135, -0.2605, -0.1603, -0.1156,\n",
       "                       -0.1902, -0.2313, -0.0166, -0.1515, -0.3205, -0.2022, -0.0888, -0.1692,\n",
       "                       -0.2692, -0.1670, -0.2828, -0.2521, -0.0130, -0.2669, -0.1723, -0.2151,\n",
       "                       -0.1083, -0.0798, -0.1164, -0.2391, -0.1477, -0.1890, -0.2052, -0.0476,\n",
       "                       -0.1590, -0.1589, -0.2383, -0.0808, -0.2007, -0.1882, -0.1486, -0.1086,\n",
       "                       -0.1875, -0.2214, -0.0991,  0.0091, -0.2162, -0.2209, -0.0630, -0.2359,\n",
       "                       -0.1235, -0.1990, -0.2295, -0.2036, -0.2071, -0.2148, -0.1414, -0.0935,\n",
       "                       -0.1874, -0.1651, -0.1903, -0.1393, -0.1805, -0.1963, -0.1423, -0.2635,\n",
       "                       -0.2242, -0.2068, -0.1134, -0.2680, -0.1862, -0.1696, -0.2529, -0.2078,\n",
       "                       -0.1584, -0.2204, -0.2384, -0.1047, -0.1335, -0.2205, -0.1528, -0.2487,\n",
       "                       -0.1355, -0.1146, -0.1673, -0.0136, -0.1558, -0.2546, -0.1811, -0.1617],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.0451, 0.9745, 1.0224, 0.9569, 0.9861, 1.0045, 1.0278, 1.0634, 1.0389,\n",
       "                       1.0216, 0.9842, 1.1099, 1.0157, 0.9824, 1.0965, 1.0307, 0.8674, 0.9835,\n",
       "                       1.0076, 0.9665, 0.9861, 0.9705, 1.0266, 1.0917, 0.9736, 1.0308, 0.9726,\n",
       "                       0.9838, 0.9616, 1.0102, 0.9881, 1.0250, 0.9429, 0.9924, 0.9120, 1.0300,\n",
       "                       0.9982, 1.0233, 0.8428, 0.9264, 0.9836, 0.9378, 1.0010, 0.9307, 1.0222,\n",
       "                       0.9822, 1.0209, 0.9528, 1.0514, 0.9630, 1.0344, 1.0062, 0.9767, 0.9748,\n",
       "                       0.9827, 0.9348, 0.9054, 1.0206, 1.0095, 1.0173, 1.0312, 1.0670, 0.9866,\n",
       "                       0.9704, 0.9777, 0.9320, 0.9356, 0.9721, 0.9230, 1.0510, 0.9052, 1.0517,\n",
       "                       0.9739, 0.9824, 1.0422, 1.0663, 0.9386, 0.9604, 1.0494, 0.9874, 1.0360,\n",
       "                       0.9525, 1.0250, 0.9620, 0.9574, 1.0049, 1.0482, 0.9686, 0.9571, 0.9879,\n",
       "                       0.9952, 0.9969, 0.9531, 0.9737, 0.9241, 1.0071, 0.9846, 0.9989, 1.0562,\n",
       "                       0.9546, 0.9243, 1.0043, 0.9716, 1.0233, 1.0340, 1.0055, 0.9923, 0.9698,\n",
       "                       1.0110, 0.9383, 0.9961, 1.0366, 0.9825, 1.0127, 0.8616, 0.9989, 1.0212,\n",
       "                       0.9751, 0.9882, 1.0225, 0.9810, 1.0087, 0.9795, 1.0189, 0.9698, 1.0015,\n",
       "                       1.0015, 1.0593], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-3.5635e-02,  6.6883e-02,  1.6516e-02],\n",
       "                         [-2.0717e-02,  4.5892e-03, -7.0361e-03],\n",
       "                         [-4.0502e-03, -5.3581e-02, -6.3822e-03]],\n",
       "               \n",
       "                        [[ 2.4649e-03, -2.7850e-02,  9.0671e-03],\n",
       "                         [-4.3892e-02,  2.0201e-02, -2.6540e-02],\n",
       "                         [-6.6467e-04,  1.1619e-02, -5.7028e-02]],\n",
       "               \n",
       "                        [[ 4.4089e-03, -2.2262e-02,  1.7012e-02],\n",
       "                         [ 6.2686e-02,  2.4853e-02, -3.6876e-02],\n",
       "                         [-1.6284e-02,  2.1929e-02, -1.3400e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5534e-02, -3.8601e-02, -6.5782e-02],\n",
       "                         [ 2.6658e-02,  4.9235e-03,  4.0978e-02],\n",
       "                         [ 1.1825e-03, -2.5821e-02, -1.8333e-02]],\n",
       "               \n",
       "                        [[-2.6654e-02, -5.5350e-02,  6.5079e-03],\n",
       "                         [-6.1993e-04, -2.8721e-02, -2.6483e-02],\n",
       "                         [ 2.8866e-02,  1.8139e-02,  4.3261e-03]],\n",
       "               \n",
       "                        [[-3.5204e-03,  5.2470e-02,  3.2148e-02],\n",
       "                         [ 1.7759e-02,  1.0482e-02, -2.4699e-02],\n",
       "                         [ 1.1681e-02,  3.0295e-02,  2.6309e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.6161e-02,  1.5252e-02, -2.2819e-02],\n",
       "                         [ 1.0151e-02, -4.9394e-03, -3.1928e-02],\n",
       "                         [-7.3092e-03, -1.6657e-02,  1.1107e-02]],\n",
       "               \n",
       "                        [[-3.2528e-02,  3.4879e-02,  5.5768e-02],\n",
       "                         [ 2.8912e-02,  8.1838e-02,  7.0347e-02],\n",
       "                         [-5.9509e-02, -3.2777e-02,  1.6557e-03]],\n",
       "               \n",
       "                        [[ 1.0216e-02,  4.4529e-02, -8.3797e-02],\n",
       "                         [ 2.7144e-02, -1.2044e-02, -7.0979e-02],\n",
       "                         [ 2.4772e-02, -3.1583e-02, -4.7915e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.8065e-03,  5.4751e-02, -9.6700e-03],\n",
       "                         [-3.1591e-03, -3.3694e-02, -7.3251e-02],\n",
       "                         [ 7.2923e-05,  4.5793e-03, -5.0973e-02]],\n",
       "               \n",
       "                        [[ 5.8034e-03, -1.8566e-02,  9.1351e-03],\n",
       "                         [-5.1339e-02, -2.9027e-02, -1.2111e-02],\n",
       "                         [-2.9039e-02,  1.3079e-02,  3.4591e-02]],\n",
       "               \n",
       "                        [[ 6.6866e-02, -2.1931e-02, -4.6208e-02],\n",
       "                         [ 1.5051e-02,  3.2506e-02, -1.3522e-02],\n",
       "                         [-1.3544e-02,  3.2223e-02, -5.6732e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.1565e-03, -1.2166e-02, -6.4260e-02],\n",
       "                         [-4.6180e-02, -8.3588e-02, -3.3062e-02],\n",
       "                         [-3.1706e-02, -5.0750e-02, -5.2977e-02]],\n",
       "               \n",
       "                        [[ 1.6955e-03,  1.4892e-03, -2.2856e-02],\n",
       "                         [ 3.9290e-03,  6.5597e-02, -3.6062e-02],\n",
       "                         [-3.7413e-02,  3.7076e-02, -3.6557e-02]],\n",
       "               \n",
       "                        [[ 9.4688e-02, -1.3011e-02,  7.1782e-02],\n",
       "                         [ 1.4776e-02,  7.5849e-02, -1.0182e-02],\n",
       "                         [-5.7237e-03,  2.7586e-02,  4.5924e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.3358e-03,  4.7901e-02, -1.3634e-02],\n",
       "                         [ 2.1187e-02, -3.5819e-02, -3.5869e-02],\n",
       "                         [ 1.4592e-02,  1.6706e-02,  4.2851e-02]],\n",
       "               \n",
       "                        [[-2.2170e-02, -5.4084e-02, -2.6914e-04],\n",
       "                         [ 3.5542e-02, -5.5808e-02, -6.2875e-02],\n",
       "                         [-2.2901e-03, -3.7785e-02,  2.0417e-02]],\n",
       "               \n",
       "                        [[ 1.4934e-02,  2.7684e-02,  8.1969e-02],\n",
       "                         [ 6.0429e-02,  6.1587e-02,  3.9178e-02],\n",
       "                         [-1.8586e-03, -2.7665e-03, -1.9067e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1825e-02,  4.8099e-03, -5.5348e-02],\n",
       "                         [ 5.5376e-02, -2.2714e-02, -1.8907e-02],\n",
       "                         [ 9.4701e-03,  1.6956e-02,  2.8504e-02]],\n",
       "               \n",
       "                        [[-8.7255e-03, -1.3725e-02,  3.0419e-02],\n",
       "                         [ 2.3330e-02,  3.1619e-02, -3.6503e-02],\n",
       "                         [ 1.7630e-02,  1.8091e-02, -5.2045e-03]],\n",
       "               \n",
       "                        [[ 2.6194e-02,  2.7558e-02, -5.5006e-02],\n",
       "                         [-1.1049e-02, -1.7657e-02, -2.4263e-03],\n",
       "                         [-7.5835e-03, -1.4259e-02, -4.7830e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.5579e-02,  5.8151e-02, -2.5747e-02],\n",
       "                         [ 9.2701e-02,  6.5077e-02,  3.7771e-02],\n",
       "                         [ 4.0023e-02,  1.2615e-02, -1.5215e-02]],\n",
       "               \n",
       "                        [[ 3.5414e-02, -3.3109e-02,  3.7701e-02],\n",
       "                         [-2.7612e-02, -2.7967e-02, -5.2714e-02],\n",
       "                         [-3.7227e-02,  3.3004e-02, -8.0880e-02]],\n",
       "               \n",
       "                        [[-1.6259e-02,  4.5828e-03, -7.5952e-03],\n",
       "                         [-4.2413e-02, -9.9031e-03, -3.3439e-02],\n",
       "                         [-4.7989e-02,  6.6052e-02, -1.1792e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.2100e-02,  6.1930e-02, -8.5012e-03],\n",
       "                         [-4.9399e-02, -2.1669e-02, -2.1487e-02],\n",
       "                         [-9.4657e-03,  3.6917e-02, -3.9345e-02]],\n",
       "               \n",
       "                        [[ 1.1505e-02, -3.4558e-02,  3.1325e-02],\n",
       "                         [-1.0512e-04, -2.2429e-02,  3.3865e-02],\n",
       "                         [-1.2944e-02,  3.7145e-02,  1.9752e-02]],\n",
       "               \n",
       "                        [[ 2.9329e-03,  1.7710e-02,  6.5504e-03],\n",
       "                         [ 2.7545e-02,  2.8484e-02,  2.6443e-02],\n",
       "                         [-4.8555e-03, -4.9553e-02,  1.7128e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.9563e-03, -1.2934e-02,  1.1113e-02],\n",
       "                         [-1.8983e-02,  4.7031e-02, -4.0295e-02],\n",
       "                         [-1.3619e-02, -1.9917e-02, -3.2294e-02]],\n",
       "               \n",
       "                        [[-3.2172e-02, -2.8210e-02, -4.2929e-02],\n",
       "                         [ 4.4360e-03, -5.5791e-02,  1.2082e-02],\n",
       "                         [ 4.9856e-02, -1.0001e-02, -4.5965e-02]],\n",
       "               \n",
       "                        [[ 1.4553e-02,  6.3308e-02, -2.4518e-02],\n",
       "                         [-1.3177e-03,  5.6378e-02, -6.8796e-03],\n",
       "                         [-7.8446e-03,  2.0849e-02,  1.5414e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.5729e-02, -3.0278e-02,  3.3063e-04],\n",
       "                         [-3.3246e-02, -2.2090e-02,  2.8049e-02],\n",
       "                         [-9.2174e-02, -6.1077e-02, -5.3846e-03]],\n",
       "               \n",
       "                        [[ 1.3817e-02,  1.4932e-02, -3.2316e-02],\n",
       "                         [ 2.9696e-02,  1.5589e-02, -4.2233e-02],\n",
       "                         [ 3.3876e-02,  2.2564e-02,  1.0178e-02]],\n",
       "               \n",
       "                        [[-1.2799e-02, -1.3293e-02, -3.9625e-02],\n",
       "                         [-4.9859e-02,  3.1562e-02, -1.0173e-02],\n",
       "                         [-4.0080e-02, -3.2473e-02, -3.6179e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 5.4078e-02,  7.9191e-02,  1.7750e-03],\n",
       "                         [ 4.3058e-02,  4.6749e-02, -3.7460e-02],\n",
       "                         [ 1.1480e-03, -1.3536e-02, -6.2119e-02]],\n",
       "               \n",
       "                        [[ 3.5932e-02,  4.2279e-02,  7.7663e-03],\n",
       "                         [-3.6863e-02, -1.1133e-02, -4.4179e-02],\n",
       "                         [-5.6386e-03, -1.8559e-02, -5.7912e-02]],\n",
       "               \n",
       "                        [[-7.8392e-02,  3.8048e-02,  3.1213e-02],\n",
       "                         [-5.2759e-02,  8.1980e-03, -3.1568e-02],\n",
       "                         [-7.3119e-03, -2.0015e-02, -3.6982e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-2.1351e-03,  3.4871e-03,  1.7402e-03,  3.3579e-03,  1.0868e-03,\n",
       "                        7.4822e-04,  3.1768e-03,  6.2910e-04,  6.6134e-04, -1.6108e-03,\n",
       "                       -2.7385e-03,  6.7116e-04, -9.6336e-04,  3.2656e-03,  1.8089e-03,\n",
       "                        3.7092e-04,  2.3694e-03, -1.4622e-03,  1.2253e-03,  1.7384e-03,\n",
       "                        8.2374e-04,  1.4852e-03, -1.9792e-04, -6.7408e-04,  2.5066e-03,\n",
       "                       -1.6606e-03, -7.2716e-04,  2.5755e-03, -1.5390e-03,  1.5680e-03,\n",
       "                       -1.4368e-03,  6.3476e-05, -1.6477e-03,  8.5359e-04, -1.1303e-03,\n",
       "                        1.8436e-03,  2.7225e-03, -9.7316e-04,  3.3027e-03, -4.5836e-03,\n",
       "                        1.0710e-03,  2.4210e-03,  9.4007e-04,  2.7616e-04, -2.6665e-03,\n",
       "                        1.0428e-03, -4.5181e-03, -1.0973e-03,  2.9153e-03,  1.1452e-03,\n",
       "                       -2.6050e-04,  1.7071e-04,  2.0001e-03,  1.2810e-04, -1.9158e-04,\n",
       "                       -4.1856e-03, -3.1744e-03, -7.9482e-04, -3.0404e-04,  3.5577e-03,\n",
       "                        8.4278e-04,  1.1236e-03, -5.2809e-03,  1.9397e-03, -1.0979e-03,\n",
       "                        1.9149e-03, -3.6256e-03,  1.5284e-03, -4.2810e-04, -4.3189e-03,\n",
       "                        7.6343e-04, -1.4412e-03, -1.3658e-03,  2.3325e-03,  2.2334e-03,\n",
       "                        1.4130e-03,  3.4702e-03,  8.2384e-04, -5.8385e-04, -2.3952e-03,\n",
       "                        3.0020e-03, -2.1871e-03, -2.9691e-03, -7.2833e-04, -3.2114e-06,\n",
       "                       -4.5352e-04,  1.9773e-03, -1.5028e-03,  4.9686e-04, -5.7484e-04,\n",
       "                        4.2452e-03,  1.2303e-03, -1.5338e-03, -7.8657e-04,  2.1939e-03,\n",
       "                        3.3835e-03,  3.8203e-03,  3.1000e-03, -3.5266e-03,  2.6436e-03,\n",
       "                        9.1690e-04, -3.8383e-03,  5.2958e-04, -6.9434e-05,  1.7816e-03,\n",
       "                        4.8468e-03,  6.3447e-04,  2.1172e-03,  1.7666e-03, -2.1690e-03,\n",
       "                       -2.0607e-03, -3.0691e-03,  4.9682e-04, -5.3011e-04, -3.4542e-03,\n",
       "                       -1.0074e-03,  2.9607e-04, -2.5081e-03, -1.1018e-03, -3.7656e-04,\n",
       "                       -3.8288e-04,  8.6251e-04, -4.0439e-04,  7.3346e-04, -1.4695e-03,\n",
       "                       -5.3210e-05,  2.3575e-03, -2.5863e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.2745, -0.3432, -0.3225, -0.3161, -0.3458, -0.2965, -0.2656, -0.4619,\n",
       "                       -0.3528, -0.3486, -0.3127, -0.4352, -0.2927, -0.3775, -0.3179, -0.2020,\n",
       "                       -0.2585, -0.2389, -0.4363, -0.3397, -0.2641, -0.3822, -0.2232, -0.3648,\n",
       "                       -0.2792, -0.3051, -0.2956, -0.2948, -0.2251, -0.2499, -0.3535, -0.2657,\n",
       "                       -0.3528, -0.2497, -0.2430, -0.2566, -0.4218, -0.2970, -0.2469, -0.2932,\n",
       "                       -0.2682, -0.2900, -0.2373, -0.2746, -0.2579, -0.3706, -0.2705, -0.2992,\n",
       "                       -0.2342, -0.4011, -0.3201, -0.3366, -0.2704, -0.3693, -0.3214, -0.2929,\n",
       "                       -0.3103, -0.2991, -0.3957, -0.3168, -0.2977, -0.3472, -0.2490, -0.3007,\n",
       "                       -0.2593, -0.2996, -0.3261, -0.2612, -0.2965, -0.3719, -0.3670, -0.3033,\n",
       "                       -0.3026, -0.3622, -0.3271, -0.3308, -0.3027, -0.3465, -0.2958, -0.3535,\n",
       "                       -0.2374, -0.3401, -0.2038, -0.3748, -0.2901, -0.2589, -0.3164, -0.2999,\n",
       "                       -0.2608, -0.2464, -0.2377, -0.2692, -0.3142, -0.3819, -0.3478, -0.3193,\n",
       "                       -0.3010, -0.2188, -0.2880, -0.3890, -0.2799, -0.2614, -0.2786, -0.3609,\n",
       "                       -0.3335, -0.3494, -0.2471, -0.3684, -0.2899, -0.3292, -0.3916, -0.2884,\n",
       "                       -0.2767, -0.3124, -0.2516, -0.3787, -0.2954, -0.3044, -0.3517, -0.3989,\n",
       "                       -0.2850, -0.3227, -0.3362, -0.2426, -0.3302, -0.2835, -0.2408, -0.2675],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.8677, 0.9911, 0.9240, 0.8862, 0.9875, 0.9502, 0.9117, 1.0771, 0.9400,\n",
       "                       0.9616, 0.9689, 1.0767, 0.8404, 1.1268, 0.9214, 0.9644, 1.1083, 0.8781,\n",
       "                       1.0999, 0.9300, 0.8585, 0.9549, 0.8933, 0.9367, 0.9463, 0.9033, 0.9782,\n",
       "                       0.9583, 0.9087, 0.9267, 1.1003, 0.9585, 0.9880, 0.9270, 1.0583, 1.0271,\n",
       "                       1.0955, 0.9693, 0.9092, 0.9284, 0.8522, 0.9795, 0.9794, 0.8672, 0.9084,\n",
       "                       0.9751, 0.8588, 0.8368, 0.9086, 0.9760, 0.9658, 1.0371, 0.9347, 0.9943,\n",
       "                       0.9089, 0.9417, 0.8640, 0.9460, 0.9710, 0.8725, 0.9926, 1.0481, 1.1232,\n",
       "                       0.9393, 0.9935, 0.9556, 0.8583, 0.8728, 0.8861, 1.0107, 1.2039, 0.9133,\n",
       "                       0.8829, 1.0437, 0.9347, 0.9958, 1.0021, 0.8523, 0.9745, 0.9859, 0.9213,\n",
       "                       0.9626, 1.0658, 1.0581, 0.8953, 0.9729, 0.9728, 0.8852, 1.1389, 0.9146,\n",
       "                       0.9648, 0.8962, 1.0027, 1.0481, 0.9987, 0.9533, 0.9388, 0.9414, 0.9178,\n",
       "                       1.0637, 0.9441, 0.9439, 0.8276, 0.9136, 0.8780, 0.8991, 0.8927, 0.9721,\n",
       "                       0.9828, 0.9578, 0.8569, 1.0319, 0.9744, 1.0446, 0.9831, 0.9474, 0.9019,\n",
       "                       1.0095, 0.9606, 1.0142, 0.9484, 1.1066, 1.0178, 0.9520, 0.8968, 0.9898,\n",
       "                       0.9332, 0.8357], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-4.3702e-02, -1.3627e-02, -3.3736e-02],\n",
       "                         [ 5.9619e-02,  1.6159e-02, -2.8567e-02],\n",
       "                         [ 2.7072e-02,  1.9254e-02,  3.9468e-02]],\n",
       "               \n",
       "                        [[ 1.8669e-02, -3.7957e-02,  9.8068e-03],\n",
       "                         [ 4.1648e-03, -3.6611e-02, -4.8710e-02],\n",
       "                         [ 3.7416e-04,  2.0676e-03, -2.5078e-02]],\n",
       "               \n",
       "                        [[ 5.8284e-02,  5.1901e-03,  5.3009e-02],\n",
       "                         [ 7.2872e-04, -1.6713e-02,  3.5374e-02],\n",
       "                         [-1.0131e-02,  5.0062e-02, -1.9479e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.3767e-03, -1.2034e-03, -4.2708e-02],\n",
       "                         [ 3.1162e-02, -5.4115e-02, -1.8587e-02],\n",
       "                         [-3.1021e-04,  2.3716e-02, -1.8927e-02]],\n",
       "               \n",
       "                        [[-2.0851e-02, -4.9040e-02, -1.3251e-02],\n",
       "                         [ 1.1836e-02,  8.5989e-03,  5.9320e-02],\n",
       "                         [-1.5005e-02, -3.3563e-02, -3.0291e-02]],\n",
       "               \n",
       "                        [[ 3.2447e-02, -4.5090e-02,  3.4413e-02],\n",
       "                         [ 1.4186e-02, -8.0078e-03, -5.5526e-02],\n",
       "                         [ 2.3710e-02, -1.7888e-02, -2.6468e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3535e-03, -4.6939e-02, -3.8000e-02],\n",
       "                         [ 4.5195e-02, -6.4326e-02,  7.6383e-03],\n",
       "                         [ 7.2541e-03, -7.9630e-02, -4.7468e-02]],\n",
       "               \n",
       "                        [[ 2.0901e-02,  1.5754e-02,  2.3625e-02],\n",
       "                         [-5.4572e-02, -4.4321e-02, -4.3351e-02],\n",
       "                         [-5.6763e-02,  5.0064e-02, -1.2850e-02]],\n",
       "               \n",
       "                        [[-1.3336e-02,  3.8217e-02,  2.6076e-02],\n",
       "                         [ 3.3369e-02,  2.9154e-02,  2.0140e-02],\n",
       "                         [-2.7863e-03,  4.9059e-02,  2.7505e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.4442e-04, -6.9905e-03,  5.1513e-02],\n",
       "                         [-2.9548e-02, -4.9088e-02,  3.1979e-02],\n",
       "                         [ 3.9791e-02,  1.4876e-02, -1.2177e-02]],\n",
       "               \n",
       "                        [[-8.2850e-04,  5.1148e-02, -1.5545e-02],\n",
       "                         [-1.2785e-02,  7.3915e-02,  5.4139e-02],\n",
       "                         [ 9.3461e-02,  4.4485e-02,  3.1602e-02]],\n",
       "               \n",
       "                        [[-2.1151e-03, -4.5330e-04, -4.4565e-02],\n",
       "                         [-1.0282e-01, -4.2757e-02, -7.6484e-02],\n",
       "                         [-1.8677e-02,  1.2074e-02, -3.7740e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.0233e-03,  3.4157e-02,  4.8908e-02],\n",
       "                         [-1.3790e-02,  3.5846e-02,  5.0121e-02],\n",
       "                         [ 1.1362e-02, -1.8063e-02, -1.4542e-02]],\n",
       "               \n",
       "                        [[ 4.2676e-03, -1.4356e-02,  1.2788e-02],\n",
       "                         [-1.6214e-02,  8.6472e-03,  1.4520e-03],\n",
       "                         [ 1.4571e-02, -2.7827e-02, -3.9042e-02]],\n",
       "               \n",
       "                        [[ 4.9498e-02,  5.5938e-03, -5.5657e-03],\n",
       "                         [-5.8689e-03,  5.9269e-02,  1.8440e-02],\n",
       "                         [ 1.4232e-02,  5.8087e-02,  1.3712e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.9255e-02,  3.7683e-02, -3.5907e-02],\n",
       "                         [ 2.3164e-02, -2.1429e-02, -1.3765e-02],\n",
       "                         [ 4.2311e-03,  5.1843e-02,  4.9206e-02]],\n",
       "               \n",
       "                        [[ 7.8560e-02,  6.9009e-02,  5.1072e-02],\n",
       "                         [ 5.7993e-02,  9.7662e-02,  7.6212e-02],\n",
       "                         [-1.2178e-02,  5.2104e-02,  3.1011e-02]],\n",
       "               \n",
       "                        [[-2.1720e-02,  6.2952e-04,  3.7310e-02],\n",
       "                         [-3.9092e-03, -3.1375e-02, -3.3830e-02],\n",
       "                         [-2.1882e-02,  6.3913e-03,  2.2926e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.4090e-02, -4.3765e-02, -3.4845e-03],\n",
       "                         [ 4.8406e-02,  8.7374e-03, -6.2583e-03],\n",
       "                         [-5.6868e-02, -6.9677e-02, -8.0262e-02]],\n",
       "               \n",
       "                        [[-4.3687e-02,  2.0018e-02, -3.3008e-02],\n",
       "                         [ 1.4243e-02, -4.5496e-02, -2.0843e-02],\n",
       "                         [-3.3093e-02, -2.0732e-02,  1.6793e-02]],\n",
       "               \n",
       "                        [[-1.1730e-02,  2.7783e-02,  5.2310e-02],\n",
       "                         [ 2.8892e-02,  4.5672e-02, -3.0685e-02],\n",
       "                         [-5.3753e-02,  1.4569e-03, -7.2215e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2346e-03,  3.3736e-03, -5.3751e-02],\n",
       "                         [ 8.0512e-03, -4.7253e-03, -2.7187e-02],\n",
       "                         [ 4.6004e-02,  6.3030e-03, -1.2130e-02]],\n",
       "               \n",
       "                        [[-2.0353e-02,  9.5935e-02,  1.7028e-02],\n",
       "                         [ 6.2968e-04,  7.9693e-02, -1.6896e-03],\n",
       "                         [-4.4462e-02,  8.0178e-03, -1.1550e-04]],\n",
       "               \n",
       "                        [[-5.9322e-02,  1.6931e-02, -3.5418e-02],\n",
       "                         [-2.4703e-02,  1.8146e-03,  2.5648e-02],\n",
       "                         [-2.5140e-02, -2.5331e-02, -5.2890e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.6826e-02,  8.7188e-03, -1.4631e-02],\n",
       "                         [-4.2427e-03,  3.1592e-03,  3.4011e-02],\n",
       "                         [ 2.4498e-02, -4.3636e-03,  1.3631e-02]],\n",
       "               \n",
       "                        [[-4.8765e-03, -2.5093e-02, -4.6500e-02],\n",
       "                         [ 1.4969e-02,  4.3267e-02,  4.7689e-02],\n",
       "                         [ 3.1678e-02, -2.9005e-03,  1.8742e-02]],\n",
       "               \n",
       "                        [[-1.6282e-04,  3.7926e-02, -3.3435e-02],\n",
       "                         [-3.7151e-02, -5.9865e-03,  1.1217e-02],\n",
       "                         [-2.7224e-02,  4.4045e-02,  2.3198e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.8922e-02, -5.0440e-02, -2.4127e-02],\n",
       "                         [ 1.3196e-03, -3.4582e-02, -6.3706e-03],\n",
       "                         [-2.4766e-02,  2.4784e-02, -5.1412e-02]],\n",
       "               \n",
       "                        [[-8.5140e-02, -1.1747e-01, -1.0574e-02],\n",
       "                         [-7.6822e-02, -1.8023e-02, -1.2739e-02],\n",
       "                         [-2.4550e-02, -9.7639e-02,  5.0995e-02]],\n",
       "               \n",
       "                        [[ 1.0049e-02,  7.0355e-03, -2.8812e-02],\n",
       "                         [-6.1743e-02,  2.9907e-02, -4.9510e-02],\n",
       "                         [-1.2106e-02, -1.6349e-02, -4.7471e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.3281e-02, -5.9659e-02, -7.4845e-03],\n",
       "                         [ 3.9666e-03, -2.4579e-02, -1.6912e-02],\n",
       "                         [-3.0060e-02, -5.2557e-02, -3.4569e-02]],\n",
       "               \n",
       "                        [[ 1.1601e-02,  2.0093e-02,  1.9330e-02],\n",
       "                         [ 1.1518e-02, -3.4297e-03, -3.9855e-03],\n",
       "                         [ 1.1395e-02,  3.4817e-02,  6.0226e-03]],\n",
       "               \n",
       "                        [[ 3.0721e-02, -1.4320e-02, -2.4393e-02],\n",
       "                         [-7.5937e-02, -3.5532e-02, -2.3386e-02],\n",
       "                         [-4.9772e-02, -6.8097e-02, -2.4774e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7394e-05, -1.8980e-02,  4.6820e-02],\n",
       "                         [ 3.1611e-02, -5.9833e-02,  3.1288e-02],\n",
       "                         [-4.7890e-02, -1.8718e-02, -2.2998e-02]],\n",
       "               \n",
       "                        [[ 1.0739e-02,  4.0940e-02,  4.8134e-02],\n",
       "                         [ 4.7797e-02,  4.2867e-02, -1.0555e-02],\n",
       "                         [ 4.3696e-02,  1.1575e-01,  2.4784e-02]],\n",
       "               \n",
       "                        [[ 2.0929e-02,  5.0720e-02, -3.8524e-03],\n",
       "                         [ 6.9852e-03,  5.0116e-02, -2.8043e-02],\n",
       "                         [-1.1385e-02,  6.5274e-03,  1.4342e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.5599, -0.5552, -0.5566,  0.5535,  0.5584,  0.5569, -0.5565,  0.5590,\n",
       "                       -0.5581, -0.5547,  0.5560, -0.5532, -0.5587,  0.5597, -0.5554, -0.5580,\n",
       "                        0.5588,  0.5554, -0.5597, -0.5538,  0.5582, -0.5558, -0.5580,  0.5537,\n",
       "                        0.5585, -0.5426,  0.5531,  0.5594, -0.5537, -0.5554, -0.5546,  0.5546,\n",
       "                       -0.5588,  0.5417,  0.5531, -0.5571, -0.5582, -0.5558, -0.5579,  0.5563,\n",
       "                       -0.5596, -0.5546,  0.5582, -0.5543, -0.5566,  0.5547, -0.5583, -0.5600,\n",
       "                        0.5525,  0.5590,  0.5591, -0.5529,  0.5544, -0.5542, -0.5560, -0.5583,\n",
       "                        0.5550, -0.5568, -0.5573, -0.5581,  0.5572,  0.5543,  0.5578, -0.5564,\n",
       "                        0.5584,  0.5558,  0.5579,  0.5603,  0.5548,  0.5582,  0.5539,  0.5539,\n",
       "                       -0.5558,  0.5583, -0.5625, -0.5550,  0.5596, -0.5543,  0.5566,  0.5571,\n",
       "                       -0.5600, -0.5554, -0.5594,  0.5592,  0.5555,  0.5557,  0.5542, -0.5608,\n",
       "                       -0.5536,  0.5579, -0.5575,  0.5527, -0.5549,  0.5600,  0.5552,  0.5595,\n",
       "                        0.5599,  0.5586,  0.5556, -0.5573, -0.5529,  0.5576,  0.5554, -0.5570,\n",
       "                       -0.5573, -0.5582, -0.5583, -0.5566, -0.5555, -0.5548, -0.5574, -0.5599,\n",
       "                        0.5589,  0.5589,  0.5558,  0.5593, -0.5566,  0.5566,  0.5548,  0.5579,\n",
       "                       -0.5469, -0.5545, -0.5583,  0.5559, -0.5517,  0.5581, -0.5545,  0.5590],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.0752, -0.2222, -0.2209, -0.0301, -0.1385, -0.2287, -0.1965, -0.1863,\n",
       "                       -0.1171, -0.1848, -0.1981, -0.2096, -0.0811, -0.1206, -0.1761, -0.1511,\n",
       "                       -0.0310, -0.1785, -0.1354, -0.2051, -0.1303, -0.1971, -0.1930, -0.1128,\n",
       "                       -0.2056, -0.1850, -0.2187, -0.0820, -0.1456, -0.1416, -0.1590, -0.0678,\n",
       "                       -0.1887, -0.1743, -0.1259, -0.2086, -0.1691, -0.2446, -0.0669, -0.1646,\n",
       "                       -0.1385, -0.1979, -0.1650, -0.2105, -0.2164, -0.1499, -0.2251, -0.1195,\n",
       "                       -0.0877, -0.0550, -0.1372, -0.1770, -0.2247, -0.2134, -0.1610, -0.1654,\n",
       "                       -0.2468, -0.1107, -0.1712, -0.1072, -0.2446, -0.1986, -0.1468, -0.1368,\n",
       "                       -0.1576, -0.1597, -0.2342, -0.2002, -0.1029, -0.1339, -0.0791, -0.1650,\n",
       "                       -0.1137, -0.1226, -0.2262, -0.1769, -0.1857, -0.1967, -0.1823, -0.1826,\n",
       "                       -0.0848, -0.2136, -0.0045, -0.2291, -0.1357, -0.2389, -0.1607, -0.1593,\n",
       "                       -0.0241, -0.1573, -0.0325, -0.1927, -0.1623, -0.1636, -0.2368, -0.2161,\n",
       "                       -0.1971, -0.1735, -0.1392, -0.1891, -0.2072, -0.1061, -0.2262, -0.1545,\n",
       "                       -0.2028, -0.0613, -0.1243, -0.0735, -0.1948, -0.1561, -0.1174, -0.0502,\n",
       "                       -0.1899, -0.1496, -0.1963, -0.1685, -0.2141, -0.0210, -0.2279, -0.1681,\n",
       "                       -0.2047, -0.1384, -0.1491, -0.2190, -0.0969, -0.2133, -0.0907, -0.1880],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.8093, 0.7495, 0.7997, 0.8392, 0.8219, 0.7918, 0.8508, 0.7993, 0.9625,\n",
       "                       0.7940, 0.7750, 0.7383, 0.8417, 0.7778, 0.7483, 0.9617, 0.8718, 0.8679,\n",
       "                       0.8584, 0.6964, 0.7606, 0.7884, 0.8327, 0.7895, 0.7564, 0.7520, 0.8513,\n",
       "                       0.7525, 0.7667, 0.8095, 1.0495, 0.8087, 0.6924, 0.7637, 0.8062, 0.9847,\n",
       "                       0.8062, 0.8280, 0.9108, 1.0359, 0.7953, 0.7623, 0.7348, 0.7962, 0.7166,\n",
       "                       0.9111, 0.7272, 0.7804, 0.8488, 0.8290, 0.9651, 0.9863, 0.7369, 0.7757,\n",
       "                       0.9778, 0.7277, 0.8836, 0.7600, 0.7946, 0.8953, 0.8346, 0.7383, 0.9778,\n",
       "                       0.8786, 0.8494, 0.8047, 0.8066, 0.8885, 0.8185, 0.7463, 0.7989, 0.7729,\n",
       "                       0.8339, 0.9020, 0.7465, 0.7171, 0.7810, 0.8230, 0.9260, 0.7882, 0.8264,\n",
       "                       0.9635, 0.8743, 0.7630, 0.7732, 0.7391, 0.8376, 0.7785, 0.8861, 1.0193,\n",
       "                       0.8233, 0.9787, 0.8576, 0.8626, 0.8288, 0.8182, 0.7184, 0.7724, 0.8488,\n",
       "                       0.7623, 0.7973, 0.8505, 0.7446, 1.0277, 0.8610, 0.8332, 0.8319, 0.8343,\n",
       "                       0.9999, 0.9363, 0.8064, 0.8397, 0.7709, 0.7834, 0.7463, 0.8550, 0.7467,\n",
       "                       0.8445, 0.8815, 0.7924, 0.7761, 0.9897, 0.7584, 0.7370, 0.8763, 0.6919,\n",
       "                       0.7355, 0.7686], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0053,  0.0019,  0.0046,  ..., -0.0081,  0.0156, -0.0086],\n",
       "                       [-0.0151, -0.0111,  0.0108,  ..., -0.0063,  0.0078, -0.0164],\n",
       "                       [ 0.0050, -0.0080,  0.0010,  ..., -0.0224,  0.0091, -0.0180],\n",
       "                       [ 0.0016, -0.0087,  0.0101,  ...,  0.0078, -0.0055,  0.0038],\n",
       "                       [-0.0126,  0.0035,  0.0124,  ..., -0.0014,  0.0023, -0.0030]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0076,  0.0032,  0.0015, -0.0034, -0.0013], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[ 1.9450e-01, -9.9795e-02,  5.8899e-02,  2.0421e-01, -6.8798e-02,\n",
       "                        -2.0007e-01, -1.7882e-01,  1.9901e-01,  7.3716e-02, -7.7455e-02,\n",
       "                        -1.7553e-01,  1.2926e-01, -2.1960e-01, -5.1855e-02,  1.6968e-01,\n",
       "                        -7.0568e-02, -1.7667e-01,  8.5367e-02, -1.8126e-01, -4.0493e-02],\n",
       "                       [ 3.1346e-01, -2.4672e-01,  3.6882e-01, -2.9854e-01,  4.4060e-01,\n",
       "                        -3.9375e-02,  3.2006e-01,  2.4884e-01,  6.6769e-01, -1.2758e-01,\n",
       "                        -5.8387e-01, -8.7315e-02, -4.9148e-01, -3.9044e-02, -2.8058e-01,\n",
       "                        -1.2313e-01, -6.4514e-01, -3.9344e-01, -7.0918e-01, -4.2825e-01],\n",
       "                       [-1.6792e-01, -9.5964e-03, -4.3481e-02,  6.7062e-03,  1.1711e-01,\n",
       "                        -1.6487e-01, -6.0568e-02, -1.0674e-02,  1.3948e-01,  2.4193e-02,\n",
       "                         3.2932e-02,  1.9114e-01,  1.7074e-01,  1.4639e-01,  1.9790e-01,\n",
       "                        -1.4125e-01,  1.1538e-01,  1.8761e-01,  8.7872e-03, -1.8317e-01],\n",
       "                       [-8.6637e-02, -8.4453e-02, -1.6488e-02, -1.0229e-01, -1.2699e-01,\n",
       "                        -5.9725e-02,  7.7862e-02,  8.2837e-02,  1.6639e-01,  1.4864e-01,\n",
       "                        -6.2536e-02, -6.9908e-02, -7.0712e-02, -5.1099e-02, -1.4010e-01,\n",
       "                        -1.6697e-01,  1.6291e-01,  1.0379e-01, -7.5108e-02,  2.9448e-04],\n",
       "                       [-5.8330e-01, -5.2382e-01,  2.3097e-01, -2.0290e-01,  1.2620e-01,\n",
       "                        -5.2184e-01,  2.3124e-01,  1.6222e-01, -4.6191e-01, -4.8259e-01,\n",
       "                         2.0806e-01, -2.4699e-01, -2.1455e-01, -2.8649e-01, -5.9960e-03,\n",
       "                        -3.3979e-01, -7.1683e-02, -2.2812e-01,  9.2595e-01, -4.0021e-01],\n",
       "                       [-2.2718e-01, -3.8532e-01,  3.6588e-01, -4.9189e-01,  6.1618e-01,\n",
       "                        -4.3076e-01,  6.6215e-01,  1.3517e-01,  2.0993e-01, -6.5571e-01,\n",
       "                         2.4551e-01, -3.9373e-01, -5.2038e-01, -5.6636e-01, -3.1780e-01,\n",
       "                        -4.5481e-01, -3.0907e-01, -5.8442e-01, -1.5079e-01, -4.3089e-01],\n",
       "                       [-4.3577e-01, -2.5230e-01,  3.8415e-01, -3.5953e-01,  5.0516e-01,\n",
       "                        -4.9321e-01,  4.9733e-01, -2.0980e-01, -1.5045e-01, -5.2452e-01,\n",
       "                         1.5987e-01, -1.4204e-01, -7.4178e-02, -3.6433e-01, -4.2967e-01,\n",
       "                        -4.3072e-01, -4.1603e-03, -4.0346e-01,  5.7543e-01, -3.4250e-01],\n",
       "                       [-2.1789e-01,  1.4847e-01, -1.7759e-01, -1.5096e-02,  1.6466e-01,\n",
       "                         1.9650e-01, -2.1852e-02, -1.2478e-02, -1.0809e-02, -1.5789e-01,\n",
       "                        -1.5054e-01, -1.8219e-01,  1.2927e-01, -4.6389e-02,  2.1401e-01,\n",
       "                         1.6270e-01, -2.1683e-01,  3.1173e-02, -1.7408e-01,  5.3669e-02],\n",
       "                       [-2.0245e-01, -2.5325e-01,  2.2297e-01, -3.6453e-01,  2.6300e-01,\n",
       "                        -3.9136e-01,  2.1834e-01, -1.5191e-01, -3.9730e-01, -3.6379e-01,\n",
       "                         2.3120e-01, -4.2832e-02, -2.1437e-01, -1.7172e-01,  1.6827e-01,\n",
       "                        -3.6454e-01,  1.3989e-01, -1.1689e-01,  8.3335e-01, -4.2437e-02],\n",
       "                       [-4.6539e-01, -4.4326e-01, -1.8556e-02, -3.0206e-01,  1.8009e-01,\n",
       "                        -2.9671e-01,  9.9821e-03,  1.4084e-01, -5.4199e-01, -2.9132e-02,\n",
       "                         5.4752e-01, -1.5445e-01, -2.9636e-01, -2.3680e-01, -1.8175e-01,\n",
       "                        -2.4527e-01, -2.0243e-02, -3.0596e-01,  4.4147e-01, -3.4644e-02],\n",
       "                       [-5.1960e-01, -3.0793e-01,  3.9824e-01, -2.8498e-02,  2.7860e-01,\n",
       "                        -2.3621e-01,  1.3768e-01, -1.1015e-03, -1.6295e-01, -2.3110e-01,\n",
       "                         2.8005e-01, -2.1967e-01,  6.6372e-02, -1.7816e-01,  2.1774e-01,\n",
       "                        -2.7426e-01,  3.1498e-01, -1.8950e-01,  4.5967e-01, -2.2480e-01],\n",
       "                       [ 8.6240e-02, -1.0932e-01, -6.6633e-02,  3.1452e-01,  9.8137e-02,\n",
       "                         1.0588e-01, -2.6359e-01,  1.9449e-01, -3.9574e-02, -9.9108e-04,\n",
       "                         1.2918e-01,  1.1637e-01,  2.6810e-01, -7.8607e-02,  2.6053e-01,\n",
       "                         1.5103e-02,  2.4161e-01,  3.2538e-02,  9.5170e-02,  7.7577e-02],\n",
       "                       [ 2.2077e-01, -1.1893e-01,  3.3556e-03,  1.4105e-01,  1.9006e-01,\n",
       "                         5.2740e-02, -1.9452e-01,  2.2726e-02,  1.2223e-01,  4.7893e-02,\n",
       "                         1.0646e-01,  2.3871e-01,  9.1381e-02,  1.1641e-01,  1.0854e-01,\n",
       "                         1.8654e-01, -1.0111e-01,  1.9331e-01,  1.4477e-01, -3.2333e-03],\n",
       "                       [-4.7303e-02, -7.3556e-02, -1.5947e-01,  2.0988e-01, -1.0523e-01,\n",
       "                         1.4511e-01,  3.9058e-02, -2.0894e-01,  1.4523e-01,  1.4160e-01,\n",
       "                         2.1420e-01,  5.6961e-02,  1.9555e-01,  1.8229e-01, -1.8977e-01,\n",
       "                         2.0819e-01,  7.4866e-02, -1.2519e-01, -4.2634e-02, -2.1221e-01],\n",
       "                       [-4.0376e-01, -3.3445e-01,  2.3031e-01, -8.1199e-02, -9.7037e-02,\n",
       "                        -3.8110e-01,  3.1125e-01, -3.0648e-02, -6.1843e-01, -2.2994e-01,\n",
       "                         1.8667e-01, -2.6246e-02,  1.0519e-01, -2.5016e-01, -2.1890e-02,\n",
       "                        -1.8432e-01,  2.6919e-01, -3.7928e-01,  5.7429e-01, -3.0198e-01],\n",
       "                       [ 6.2884e-02,  1.5268e-01, -9.8051e-02, -7.9451e-02,  5.4087e-02,\n",
       "                        -8.8575e-02, -2.0254e-01,  7.8244e-03, -2.1483e-01,  1.6323e-01,\n",
       "                         5.5022e-02,  1.6895e-01,  1.5126e-01, -4.8578e-04, -1.3752e-01,\n",
       "                         1.3657e-02,  8.7678e-02,  7.2777e-02, -2.0556e-01,  1.6001e-01],\n",
       "                       [-1.7184e-01,  1.5199e-01, -1.4167e-01,  4.3341e-02, -1.1432e-01,\n",
       "                         1.8257e-02,  1.5661e-01,  6.9146e-03,  1.4682e-01,  1.5712e-01,\n",
       "                        -1.8079e-01, -2.4925e-02,  9.8675e-02,  1.8632e-01,  7.6713e-02,\n",
       "                        -7.8911e-02, -9.5569e-02, -1.8848e-01,  2.3741e-02, -1.2041e-01],\n",
       "                       [-3.4018e-01, -5.4116e-01,  7.2154e-01, -5.9619e-01,  7.5661e-01,\n",
       "                        -4.8814e-01,  6.5774e-01,  3.5671e-01,  4.0474e-01, -4.2754e-01,\n",
       "                         1.1906e-01, -4.7903e-01, -7.5758e-01, -4.7793e-01, -9.1343e-01,\n",
       "                        -7.0370e-01, -7.4674e-01, -4.1812e-01, -1.0789e-01, -7.1230e-01],\n",
       "                       [-3.7166e-01, -3.5114e-01,  9.7455e-02, -5.1995e-01,  1.1000e-01,\n",
       "                        -4.2867e-01,  2.9503e-01,  8.6841e-02, -1.5219e-01, -2.4102e-01,\n",
       "                         1.7106e-01, -4.9874e-01,  3.9382e-03, -4.8100e-01, -2.7536e-01,\n",
       "                        -5.0917e-01, -2.2812e-01, -5.0284e-01,  4.0104e-01, -5.0062e-01],\n",
       "                       [ 5.7188e-02, -1.6221e-01,  8.6147e-03, -1.6054e-01,  2.3078e-01,\n",
       "                        -2.4389e-01,  3.5177e-01,  4.9625e-01,  8.3814e-02, -1.8604e-01,\n",
       "                        -8.6181e-02,  7.3271e-02, -4.9693e-01, -2.3796e-01, -3.8974e-01,\n",
       "                        -3.0115e-01, -3.0443e-01,  1.8303e-02, -6.3854e-01, -2.5684e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([-0.1579,  0.3830, -0.2280, -0.1649,  0.1597,  0.5916,  0.4159,  0.0164,\n",
       "                        0.1514,  0.3201,  0.2737,  0.1193,  0.1049, -0.1574,  0.2005,  0.1238,\n",
       "                        0.1505,  0.7526,  0.3516,  0.0970], device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 0.1263, -0.0772,  0.1627, -0.0859, -0.2019, -0.2602, -0.4369,  0.1521,\n",
       "                        -0.2212, -0.2299, -0.1524,  0.1932,  0.2122,  0.1275, -0.0518,  0.0328,\n",
       "                         0.0779, -0.4234, -0.0063, -0.1765],\n",
       "                       [ 0.0947, -0.0592, -0.1505,  0.1282,  0.2700,  0.3092,  0.1813, -0.1357,\n",
       "                         0.3183,  0.2896,  0.0864,  0.0237,  0.1133,  0.0974,  0.3301, -0.0959,\n",
       "                        -0.1545,  0.3448,  0.2064, -0.0190],\n",
       "                       [-0.1231, -0.2893, -0.1265,  0.1026, -0.4462, -0.1169, -0.4914, -0.1678,\n",
       "                        -0.3340, -0.3942, -0.2506,  0.0273,  0.0290,  0.0147, -0.4609,  0.0933,\n",
       "                         0.0714, -0.5103, -0.3784, -0.4865],\n",
       "                       [ 0.2137,  0.0238,  0.1494,  0.1903, -0.3170, -0.1248, -0.1031,  0.0262,\n",
       "                        -0.2517,  0.1737, -0.1939, -0.1401, -0.0070,  0.0077, -0.2880,  0.0615,\n",
       "                        -0.1830, -0.2516, -0.0809,  0.1210],\n",
       "                       [-0.1682,  0.3614, -0.1215,  0.1635, -0.1256,  0.2145,  0.3066,  0.2003,\n",
       "                        -0.0317, -0.3850,  0.1504,  0.1634, -0.1465, -0.0818,  0.0308,  0.0231,\n",
       "                         0.0909,  0.0798,  0.2517, -0.0961],\n",
       "                       [ 0.1320,  0.0388,  0.1741,  0.0834,  0.2778,  0.1680, -0.0524,  0.0265,\n",
       "                         0.0478, -0.0274, -0.0982,  0.1646,  0.0666,  0.0608,  0.1382, -0.0616,\n",
       "                         0.2075,  0.3890, -0.0512,  0.0220],\n",
       "                       [ 0.0186,  0.8560, -0.0559,  0.0717,  0.8958,  0.9087,  0.9736, -0.1300,\n",
       "                         0.5675,  0.3810,  0.6347,  0.0049,  0.0567, -0.1425,  0.6163,  0.0342,\n",
       "                         0.1900,  0.8056,  0.9387,  0.3749],\n",
       "                       [-0.0348, -0.0985, -0.2037,  0.0392,  0.0375, -0.1490, -0.1671,  0.2235,\n",
       "                         0.1203,  0.1824,  0.1228, -0.1389,  0.1232,  0.1064, -0.1620, -0.2108,\n",
       "                         0.0877, -0.3298, -0.2546, -0.0387],\n",
       "                       [-0.0676, -0.2835, -0.1881,  0.2059,  0.3445, -0.0828,  0.2131, -0.1769,\n",
       "                         0.2083,  0.2633,  0.2230, -0.0921, -0.1128, -0.0414,  0.2611, -0.1009,\n",
       "                         0.0885, -0.1704,  0.0455, -0.4391],\n",
       "                       [ 0.0187, -0.5333,  0.0174,  0.0651, -0.3656, -0.2181, -0.3764,  0.0620,\n",
       "                        -0.4032,  0.0820, -0.3893,  0.0414, -0.0107,  0.0959, -0.1434, -0.0094,\n",
       "                         0.2114, -0.6247, -0.1515, -0.3881]], device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.2632,  0.2419, -0.3432, -0.0988,  0.0510,  0.0767,  0.6427, -0.1952,\n",
       "                        0.1331, -0.0769], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.483476383447647,\n",
       "   1.3545481526851655,\n",
       "   1.3098033878803252,\n",
       "   1.2686262041330338,\n",
       "   1.218484871506691,\n",
       "   1.1743631842136384,\n",
       "   1.1345976806879043,\n",
       "   1.086325322985649,\n",
       "   1.0543912591934204,\n",
       "   1.0350105865001678,\n",
       "   0.9866085749864578,\n",
       "   0.9846874786615372,\n",
       "   0.9660252624750137,\n",
       "   0.9358155431747437,\n",
       "   0.937087476849556,\n",
       "   0.915649830698967,\n",
       "   0.8967641706466675,\n",
       "   0.8855581641197204,\n",
       "   0.8825522440671921,\n",
       "   0.8663041898012162,\n",
       "   0.8615279206037522,\n",
       "   0.8308090027570725,\n",
       "   0.8249965822696685,\n",
       "   0.8191860826015472,\n",
       "   0.8189492141604423,\n",
       "   0.8036693303585053,\n",
       "   0.7832032409310341,\n",
       "   0.775857702255249,\n",
       "   0.7779861376881599,\n",
       "   0.7800523315668106,\n",
       "   0.7531858184337616,\n",
       "   0.7541596565246582,\n",
       "   0.7361088641285897,\n",
       "   0.7286121693253517,\n",
       "   0.7292022127509117,\n",
       "   0.7095676583051681,\n",
       "   0.7120725163817405,\n",
       "   0.7002220395207405,\n",
       "   0.7061740514039994,\n",
       "   0.6800183079242706,\n",
       "   0.6920564432740212,\n",
       "   0.6727037338614463,\n",
       "   0.6708960717916489,\n",
       "   0.6698056691288948,\n",
       "   0.6596473541855812,\n",
       "   0.6498868864178657,\n",
       "   0.655269834458828,\n",
       "   0.6354625440835953,\n",
       "   0.6332014302015304,\n",
       "   0.6399552510380745,\n",
       "   0.636753729403019,\n",
       "   0.6139424102902412,\n",
       "   0.6182934874892235,\n",
       "   0.6212592880129815,\n",
       "   0.6169081155657768,\n",
       "   0.6051740693747998,\n",
       "   0.6106658101081848,\n",
       "   0.5890947519540787,\n",
       "   0.5899993230104447,\n",
       "   0.5940517761707306,\n",
       "   0.5941763424873352,\n",
       "   0.5757712825536728,\n",
       "   0.5736726365685463,\n",
       "   0.6003453350663185,\n",
       "   0.5694817636013031,\n",
       "   0.5650899789333343,\n",
       "   0.562277569591999,\n",
       "   0.5423234815001488,\n",
       "   0.5434539637863636,\n",
       "   0.5505697911977768,\n",
       "   0.5450181565880775,\n",
       "   0.5424525443017483,\n",
       "   0.5502477190196514,\n",
       "   0.5398116151988507,\n",
       "   0.5268495962321759,\n",
       "   0.5161014630198478,\n",
       "   0.5290606241226197,\n",
       "   0.5290622363686561,\n",
       "   0.5131781410574913,\n",
       "   0.513524123877287,\n",
       "   0.4992736192941666,\n",
       "   0.5125679462850093,\n",
       "   0.5042422689497471,\n",
       "   0.49598163238167764,\n",
       "   0.48699826806783675,\n",
       "   0.4952307816147804,\n",
       "   0.48522328546643256,\n",
       "   0.48399221505224704,\n",
       "   0.4750914240181446,\n",
       "   0.46963882228732107,\n",
       "   0.4760855414271355,\n",
       "   0.4710879309475422,\n",
       "   0.4571245724260807,\n",
       "   0.4625591897964478,\n",
       "   0.4592583292722702,\n",
       "   0.4662577235996723,\n",
       "   0.44587932297587396,\n",
       "   0.4526464014351368,\n",
       "   0.4377901410162449],\n",
       "  'train_loss_std': [0.14395432451489668,\n",
       "   0.11333838373880073,\n",
       "   0.12711974579300753,\n",
       "   0.12105659789091615,\n",
       "   0.13434592199589895,\n",
       "   0.1313440359206019,\n",
       "   0.13860421011488458,\n",
       "   0.13037191019217056,\n",
       "   0.1272481605095346,\n",
       "   0.12974667647832655,\n",
       "   0.13741041014183927,\n",
       "   0.15123350618087675,\n",
       "   0.1418155604692662,\n",
       "   0.1374293410478573,\n",
       "   0.13186008332118565,\n",
       "   0.140305291953603,\n",
       "   0.13605381381361312,\n",
       "   0.13910498653681638,\n",
       "   0.13889903648615332,\n",
       "   0.1434268096153552,\n",
       "   0.13734807948094938,\n",
       "   0.13614374847329086,\n",
       "   0.14652969026418683,\n",
       "   0.14201605577733833,\n",
       "   0.14244231613074756,\n",
       "   0.15172665964938425,\n",
       "   0.1386274556381588,\n",
       "   0.1448365387598719,\n",
       "   0.1387978411864321,\n",
       "   0.1392369955539013,\n",
       "   0.13945563215547616,\n",
       "   0.13514215764376322,\n",
       "   0.14039674919546777,\n",
       "   0.14985599857866935,\n",
       "   0.13834088182754622,\n",
       "   0.14977615913270442,\n",
       "   0.13755955071196108,\n",
       "   0.1426919142423052,\n",
       "   0.14972353920124865,\n",
       "   0.14629833840481885,\n",
       "   0.13974395437761303,\n",
       "   0.13765699316149732,\n",
       "   0.13424311337994166,\n",
       "   0.13918151588458477,\n",
       "   0.1359302521562211,\n",
       "   0.13696987917735345,\n",
       "   0.1381445758943189,\n",
       "   0.13321522538615158,\n",
       "   0.13803707231606446,\n",
       "   0.13306890934295446,\n",
       "   0.1344519116346583,\n",
       "   0.1382546355147701,\n",
       "   0.13382131089253216,\n",
       "   0.1375218830216236,\n",
       "   0.14794151804726205,\n",
       "   0.1303917236588258,\n",
       "   0.13799221257661803,\n",
       "   0.13942923150945669,\n",
       "   0.13231009475445332,\n",
       "   0.13433800276299657,\n",
       "   0.13837884123711383,\n",
       "   0.13936341481167122,\n",
       "   0.14135101436191946,\n",
       "   0.14932812710107446,\n",
       "   0.1299503415927997,\n",
       "   0.1355631097071234,\n",
       "   0.13544314982727282,\n",
       "   0.1397812187860245,\n",
       "   0.1285458448637036,\n",
       "   0.1353943106281065,\n",
       "   0.13053567723681753,\n",
       "   0.13188190368150735,\n",
       "   0.14005317621076105,\n",
       "   0.13371949564274818,\n",
       "   0.13816014847329,\n",
       "   0.1314713657649213,\n",
       "   0.13112349419041336,\n",
       "   0.13471776218016887,\n",
       "   0.13027689970342018,\n",
       "   0.13290112830775985,\n",
       "   0.12413065756293956,\n",
       "   0.1321072014420857,\n",
       "   0.1399392415305865,\n",
       "   0.13140487575785323,\n",
       "   0.13105213940689583,\n",
       "   0.1271608603606777,\n",
       "   0.12232877988984227,\n",
       "   0.1290914488083553,\n",
       "   0.12357111193000987,\n",
       "   0.1265193672330218,\n",
       "   0.12458573172082225,\n",
       "   0.13162288135038194,\n",
       "   0.11534974463449765,\n",
       "   0.12952297854521969,\n",
       "   0.12435834453104028,\n",
       "   0.1389928684385691,\n",
       "   0.12120323266319391,\n",
       "   0.1277518306115412,\n",
       "   0.1216219440238964],\n",
       "  'train_accuracy_mean': [0.37069333359599116,\n",
       "   0.4429600010514259,\n",
       "   0.4647066667675972,\n",
       "   0.4883599992990494,\n",
       "   0.5131999998092651,\n",
       "   0.5351200005412102,\n",
       "   0.5534133326411247,\n",
       "   0.5779733316302299,\n",
       "   0.5907733336687088,\n",
       "   0.5979866657853127,\n",
       "   0.6220266659855842,\n",
       "   0.6233199979066849,\n",
       "   0.6316133319735527,\n",
       "   0.6442533332705498,\n",
       "   0.6442933332920074,\n",
       "   0.6498933331370353,\n",
       "   0.659346665263176,\n",
       "   0.6675466662049293,\n",
       "   0.6657066665887833,\n",
       "   0.6716933337450027,\n",
       "   0.6720133323669434,\n",
       "   0.6882933328151702,\n",
       "   0.6891733328104019,\n",
       "   0.6909999992251397,\n",
       "   0.6918533331155777,\n",
       "   0.6961466667056083,\n",
       "   0.705066666841507,\n",
       "   0.708159999191761,\n",
       "   0.7087200011014938,\n",
       "   0.7072133328318596,\n",
       "   0.717360000371933,\n",
       "   0.7175333334207534,\n",
       "   0.7243199995756149,\n",
       "   0.7285599997043609,\n",
       "   0.7260400002002716,\n",
       "   0.7356933336853981,\n",
       "   0.7333333344459534,\n",
       "   0.7377333332896232,\n",
       "   0.7363199999332428,\n",
       "   0.7475466676354409,\n",
       "   0.7430000010728836,\n",
       "   0.7492400013208389,\n",
       "   0.7498533320426941,\n",
       "   0.7495199999809266,\n",
       "   0.7541600006818772,\n",
       "   0.7580000001192093,\n",
       "   0.7559466677904129,\n",
       "   0.7636399995088577,\n",
       "   0.7658799993991852,\n",
       "   0.7625333334207535,\n",
       "   0.7626399983167649,\n",
       "   0.7732266647815704,\n",
       "   0.7706933327913285,\n",
       "   0.7705733337402344,\n",
       "   0.7721866673231125,\n",
       "   0.7775466668605805,\n",
       "   0.7746533321142197,\n",
       "   0.7821866668462754,\n",
       "   0.7803599991798401,\n",
       "   0.7778133322000503,\n",
       "   0.7796400007009506,\n",
       "   0.7875466661453248,\n",
       "   0.7886400009393693,\n",
       "   0.7777466655969619,\n",
       "   0.7903733334541321,\n",
       "   0.7915733312368393,\n",
       "   0.7917733327150345,\n",
       "   0.7990533335208893,\n",
       "   0.7999999995231628,\n",
       "   0.7966133326292038,\n",
       "   0.7994533331394196,\n",
       "   0.7988000001907348,\n",
       "   0.7957999991178513,\n",
       "   0.8017200000286102,\n",
       "   0.8063333348035813,\n",
       "   0.8085466663837433,\n",
       "   0.804853335261345,\n",
       "   0.8050666663646698,\n",
       "   0.8098933339118958,\n",
       "   0.8110666654109955,\n",
       "   0.8160133337974549,\n",
       "   0.8106666659116745,\n",
       "   0.8153466678857804,\n",
       "   0.817799998998642,\n",
       "   0.821519998908043,\n",
       "   0.8179866662025451,\n",
       "   0.8210000007152557,\n",
       "   0.8225199991464615,\n",
       "   0.8258133324384689,\n",
       "   0.8262800006866455,\n",
       "   0.8266933342218399,\n",
       "   0.8276533333063125,\n",
       "   0.833626668214798,\n",
       "   0.8289600015878678,\n",
       "   0.8305066677331925,\n",
       "   0.8271066660881042,\n",
       "   0.8359733339548111,\n",
       "   0.8337599998712539,\n",
       "   0.8393333332538605],\n",
       "  'train_accuracy_std': [0.0766484566360368,\n",
       "   0.06375887423490642,\n",
       "   0.06983140344978796,\n",
       "   0.06645081097227663,\n",
       "   0.06909561705264296,\n",
       "   0.06746593476190425,\n",
       "   0.07170428686099195,\n",
       "   0.06530189113446383,\n",
       "   0.06611657844767474,\n",
       "   0.0655488958249352,\n",
       "   0.06948943519117827,\n",
       "   0.07238477309039977,\n",
       "   0.06905824857850075,\n",
       "   0.06624096003845513,\n",
       "   0.06401016017503583,\n",
       "   0.06567317884517705,\n",
       "   0.06622533740727694,\n",
       "   0.06528929708091179,\n",
       "   0.06665708840864606,\n",
       "   0.0709408149481394,\n",
       "   0.06510941014946067,\n",
       "   0.061867409215896334,\n",
       "   0.06982092257944582,\n",
       "   0.063668411695221,\n",
       "   0.06475688423526006,\n",
       "   0.06988114262025241,\n",
       "   0.06372105914721576,\n",
       "   0.0669732372169136,\n",
       "   0.06379311623137775,\n",
       "   0.0649701734573289,\n",
       "   0.06280523295637327,\n",
       "   0.06085797585225891,\n",
       "   0.06363789743475273,\n",
       "   0.0669033610935633,\n",
       "   0.06302492042454541,\n",
       "   0.06611662585150434,\n",
       "   0.06259002349539497,\n",
       "   0.062139770798644786,\n",
       "   0.06652628540686055,\n",
       "   0.06532060176314308,\n",
       "   0.06242346371620217,\n",
       "   0.060841872811850016,\n",
       "   0.05806873802044365,\n",
       "   0.06142920633719081,\n",
       "   0.06116521208393779,\n",
       "   0.060884588818522274,\n",
       "   0.060173946170244186,\n",
       "   0.058278978144496695,\n",
       "   0.06011399276989966,\n",
       "   0.05876642937223986,\n",
       "   0.05848729440810904,\n",
       "   0.06153436154620754,\n",
       "   0.05852983513984785,\n",
       "   0.06068593123072759,\n",
       "   0.06457877700587399,\n",
       "   0.0558046494432728,\n",
       "   0.05948026564097738,\n",
       "   0.05974181981651452,\n",
       "   0.05926178628905173,\n",
       "   0.059203759905654935,\n",
       "   0.060292281796880745,\n",
       "   0.05929908234629067,\n",
       "   0.05990376867607915,\n",
       "   0.06180246448648162,\n",
       "   0.056648179731089446,\n",
       "   0.059240492050440206,\n",
       "   0.059980457648278536,\n",
       "   0.05917726179649516,\n",
       "   0.05534859438199707,\n",
       "   0.05959788838774204,\n",
       "   0.056826939943452844,\n",
       "   0.05730836646921678,\n",
       "   0.06117555891421147,\n",
       "   0.05631160789883762,\n",
       "   0.060449979888514506,\n",
       "   0.05730172560835069,\n",
       "   0.05648127201393746,\n",
       "   0.056781217680472944,\n",
       "   0.056481164532692286,\n",
       "   0.056896162071701076,\n",
       "   0.053301822981599106,\n",
       "   0.05656854241906197,\n",
       "   0.057846846322377785,\n",
       "   0.056290161973020725,\n",
       "   0.054676427920292967,\n",
       "   0.05465804694123846,\n",
       "   0.052474545056419096,\n",
       "   0.0540415962790607,\n",
       "   0.05391994874793915,\n",
       "   0.05351578745123087,\n",
       "   0.05204954302900933,\n",
       "   0.05573353128008745,\n",
       "   0.049162797646936346,\n",
       "   0.054824839077500014,\n",
       "   0.05387401950640552,\n",
       "   0.05937064315086252,\n",
       "   0.052577640343184684,\n",
       "   0.05322651995237746,\n",
       "   0.05125362104696331],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.4665240701039632,\n",
       "   1.4171582623322805,\n",
       "   1.3929247081279754,\n",
       "   1.3521079965432485,\n",
       "   1.3207840005556741,\n",
       "   1.2823985087871552,\n",
       "   1.2357146400213241,\n",
       "   1.2074770802259445,\n",
       "   1.16887910703818,\n",
       "   1.1530817288160324,\n",
       "   1.1405859416723252,\n",
       "   1.1206016393502554,\n",
       "   1.1152420143286388,\n",
       "   1.1028757802645366,\n",
       "   1.0881827984253565,\n",
       "   1.0752600787083308,\n",
       "   1.0674547817309696,\n",
       "   1.0507617690165838,\n",
       "   1.0455724114179612,\n",
       "   1.029528054992358,\n",
       "   1.0347131176789601,\n",
       "   1.0308612650632858,\n",
       "   1.0205002764860789,\n",
       "   1.0117877606550854,\n",
       "   0.9991415997346242,\n",
       "   0.9970713939269383,\n",
       "   0.9776921087503433,\n",
       "   0.9844942214091619,\n",
       "   0.971526497801145,\n",
       "   0.9629404856761297,\n",
       "   0.948158764441808,\n",
       "   0.9565264155467351,\n",
       "   0.9395616263151169,\n",
       "   0.9322134967645009,\n",
       "   0.9252809810638428,\n",
       "   0.9278887983163198,\n",
       "   0.9269906002283096,\n",
       "   0.9079518123467764,\n",
       "   0.9162568575143815,\n",
       "   0.9227782398462295,\n",
       "   0.9155866428216298,\n",
       "   0.9006667524576187,\n",
       "   0.9016995388269424,\n",
       "   0.9051644812027614,\n",
       "   0.9001533788442612,\n",
       "   0.9028488105535507,\n",
       "   0.9130249049266179,\n",
       "   0.9138602681954702,\n",
       "   0.8877618835369746,\n",
       "   0.8777974281708399,\n",
       "   0.9076733261346817,\n",
       "   0.8865848875045776,\n",
       "   0.881484682559967,\n",
       "   0.885455636382103,\n",
       "   0.8732743002971013,\n",
       "   0.8691663382450739,\n",
       "   0.8906439789136251,\n",
       "   0.8844351375102997,\n",
       "   0.8689276750882466,\n",
       "   0.8560870597759883,\n",
       "   0.8502876708904902,\n",
       "   0.8479099889596303,\n",
       "   0.8629307077328364,\n",
       "   0.8676115584373474,\n",
       "   0.850867133140564,\n",
       "   0.8470532782872517,\n",
       "   0.8576970545450846,\n",
       "   0.8460821300745011,\n",
       "   0.8614348691701889,\n",
       "   0.8581943772236507,\n",
       "   0.8761467144886652,\n",
       "   0.8548586076498031,\n",
       "   0.8423090946674346,\n",
       "   0.8468090269962947,\n",
       "   0.8592915604511897,\n",
       "   0.846453515291214,\n",
       "   0.8574012688795726,\n",
       "   0.8604395176966985,\n",
       "   0.8488959797223409,\n",
       "   0.8528792436917623,\n",
       "   0.8373437591393789,\n",
       "   0.8498614275455475,\n",
       "   0.8510965873797735,\n",
       "   0.8445712782939275,\n",
       "   0.8561002175013225,\n",
       "   0.8459126494328181,\n",
       "   0.847146147886912,\n",
       "   0.8611347327629725,\n",
       "   0.8484471168120702,\n",
       "   0.8389656205972036,\n",
       "   0.8527462327480316,\n",
       "   0.8448436069488525,\n",
       "   0.8654573482275009,\n",
       "   0.8528101329008738,\n",
       "   0.847724948724111,\n",
       "   0.8371095420916875,\n",
       "   0.853082467118899,\n",
       "   0.8638810996214549,\n",
       "   0.8563886060317357],\n",
       "  'val_loss_std': [0.08925934824660577,\n",
       "   0.0962115106736041,\n",
       "   0.09785377350198164,\n",
       "   0.10308777117396366,\n",
       "   0.10379286137959226,\n",
       "   0.1100652211658379,\n",
       "   0.10810219226848666,\n",
       "   0.115063940231856,\n",
       "   0.11814714321202346,\n",
       "   0.12110769536321783,\n",
       "   0.12240733687056339,\n",
       "   0.12659264634392622,\n",
       "   0.12382647519402466,\n",
       "   0.12968275305346055,\n",
       "   0.12844136788866767,\n",
       "   0.12429290625371377,\n",
       "   0.12871664938497682,\n",
       "   0.1266904178306537,\n",
       "   0.12727888312397284,\n",
       "   0.12889609389107845,\n",
       "   0.1313175871417351,\n",
       "   0.13201137032978313,\n",
       "   0.1259526668741782,\n",
       "   0.1296961244546495,\n",
       "   0.13350331956566044,\n",
       "   0.13289292460184882,\n",
       "   0.13145396794711273,\n",
       "   0.13060179318986692,\n",
       "   0.13357975411905498,\n",
       "   0.1334472919842112,\n",
       "   0.13183806199079628,\n",
       "   0.13489732834192797,\n",
       "   0.13689987353636482,\n",
       "   0.1342893066494463,\n",
       "   0.13439243955069743,\n",
       "   0.135543301869013,\n",
       "   0.13674488682062708,\n",
       "   0.1358373287169887,\n",
       "   0.13662543119044604,\n",
       "   0.1332164788220565,\n",
       "   0.14243757029091045,\n",
       "   0.13396603707063587,\n",
       "   0.1320232131485934,\n",
       "   0.1328392030775204,\n",
       "   0.13551865885521033,\n",
       "   0.13844065492774665,\n",
       "   0.1375059595211697,\n",
       "   0.1386343597346623,\n",
       "   0.13876031742746722,\n",
       "   0.13199150121379633,\n",
       "   0.141161827694547,\n",
       "   0.1368554390194922,\n",
       "   0.14136913080192484,\n",
       "   0.12493857512129981,\n",
       "   0.13582857057996295,\n",
       "   0.13535902917639603,\n",
       "   0.14323822959728608,\n",
       "   0.13862600489213434,\n",
       "   0.13144245184657638,\n",
       "   0.13920814706771145,\n",
       "   0.13557899393781472,\n",
       "   0.13331776433182943,\n",
       "   0.13177768400348586,\n",
       "   0.140342397160721,\n",
       "   0.14008246389525553,\n",
       "   0.13495934427253403,\n",
       "   0.13348690431098875,\n",
       "   0.13508018210927444,\n",
       "   0.1364814989770138,\n",
       "   0.12875634567051716,\n",
       "   0.13803099216237788,\n",
       "   0.13945810556226512,\n",
       "   0.1347022238355996,\n",
       "   0.13466460868966482,\n",
       "   0.1309065270280309,\n",
       "   0.1371298473825938,\n",
       "   0.13512415833957742,\n",
       "   0.13527794369829224,\n",
       "   0.1410121492868729,\n",
       "   0.13858548135255375,\n",
       "   0.13391902094933214,\n",
       "   0.13720964319277953,\n",
       "   0.14410937150890238,\n",
       "   0.1348865303337384,\n",
       "   0.1388979985155829,\n",
       "   0.14354996131970943,\n",
       "   0.13804031083444798,\n",
       "   0.14501041672857284,\n",
       "   0.13926559769320654,\n",
       "   0.133890478818652,\n",
       "   0.14732048299364692,\n",
       "   0.14037577380632232,\n",
       "   0.14494928544447935,\n",
       "   0.1362168295471893,\n",
       "   0.13970073765015756,\n",
       "   0.14332653753978378,\n",
       "   0.146693152060341,\n",
       "   0.14264648011756165,\n",
       "   0.14517821504828038],\n",
       "  'val_accuracy_mean': [0.38444444532195726,\n",
       "   0.4115555561085542,\n",
       "   0.42495555559794107,\n",
       "   0.44615555594364803,\n",
       "   0.4644888890782992,\n",
       "   0.4791555555661519,\n",
       "   0.5022000006834666,\n",
       "   0.5149777763088544,\n",
       "   0.5347777769962947,\n",
       "   0.540666664938132,\n",
       "   0.5482444435358047,\n",
       "   0.5573999987045923,\n",
       "   0.5586222209533056,\n",
       "   0.5657111112276713,\n",
       "   0.5733999998370807,\n",
       "   0.5777333334088326,\n",
       "   0.5819333322842916,\n",
       "   0.5869555561741193,\n",
       "   0.591244444946448,\n",
       "   0.6012444436550141,\n",
       "   0.5970666656891505,\n",
       "   0.6003111113111178,\n",
       "   0.6002888866265614,\n",
       "   0.6054222213228544,\n",
       "   0.6103999996185303,\n",
       "   0.6132666664322217,\n",
       "   0.6210222199559212,\n",
       "   0.6195777772863706,\n",
       "   0.6230222220222156,\n",
       "   0.6269333319862683,\n",
       "   0.6322222207983335,\n",
       "   0.6310666671395302,\n",
       "   0.6359333338340124,\n",
       "   0.6402666673064232,\n",
       "   0.6431333323319753,\n",
       "   0.6444222223758698,\n",
       "   0.6442222221692403,\n",
       "   0.6508888866504033,\n",
       "   0.6474444450934728,\n",
       "   0.6436666671435038,\n",
       "   0.6492888904611269,\n",
       "   0.6522444445888201,\n",
       "   0.6544888892769813,\n",
       "   0.6525555551052094,\n",
       "   0.6522444439927737,\n",
       "   0.6543333328763644,\n",
       "   0.6491777767737706,\n",
       "   0.6498666656017303,\n",
       "   0.6586222206552823,\n",
       "   0.6625555568933487,\n",
       "   0.6531777779261271,\n",
       "   0.6570888869961102,\n",
       "   0.6623777764042219,\n",
       "   0.6622444446881612,\n",
       "   0.6637999998529752,\n",
       "   0.6656444448232651,\n",
       "   0.6576666649182638,\n",
       "   0.6585555551449458,\n",
       "   0.6678666667143504,\n",
       "   0.6745333335796992,\n",
       "   0.6744000005722046,\n",
       "   0.6759333332379659,\n",
       "   0.6685333351294199,\n",
       "   0.6675333335002264,\n",
       "   0.6761333312590917,\n",
       "   0.6757333340247472,\n",
       "   0.670711112121741,\n",
       "   0.6745999991893769,\n",
       "   0.6738444425662359,\n",
       "   0.670933333337307,\n",
       "   0.6647111114859581,\n",
       "   0.6749999994039535,\n",
       "   0.678466666340828,\n",
       "   0.6754666682084401,\n",
       "   0.6697111110885938,\n",
       "   0.6778888899087906,\n",
       "   0.6735333324472109,\n",
       "   0.6703333316246668,\n",
       "   0.674511108994484,\n",
       "   0.6746444447835287,\n",
       "   0.6797777791817983,\n",
       "   0.6743111101786295,\n",
       "   0.6767111106713612,\n",
       "   0.6751333336035411,\n",
       "   0.6725333332022031,\n",
       "   0.6786888877550761,\n",
       "   0.6757999990383784,\n",
       "   0.673799999554952,\n",
       "   0.6739555535713831,\n",
       "   0.6764444426695506,\n",
       "   0.6765333332618078,\n",
       "   0.6791555551687877,\n",
       "   0.6720222214857737,\n",
       "   0.6716888894637426,\n",
       "   0.6760888901352883,\n",
       "   0.6818444436788559,\n",
       "   0.6746666649977366,\n",
       "   0.6732000005245209,\n",
       "   0.6749777751167615],\n",
       "  'val_accuracy_std': [0.05025957484042379,\n",
       "   0.05491092657106505,\n",
       "   0.056014860953133024,\n",
       "   0.057303915876824554,\n",
       "   0.05489921667889171,\n",
       "   0.0579440170032123,\n",
       "   0.05834803579157738,\n",
       "   0.061105550215058924,\n",
       "   0.06031634521410712,\n",
       "   0.061762986879083576,\n",
       "   0.061663598869231936,\n",
       "   0.06523958087297703,\n",
       "   0.0606033678292508,\n",
       "   0.06229301844736194,\n",
       "   0.06340577131165628,\n",
       "   0.06056685322641931,\n",
       "   0.06327104455941784,\n",
       "   0.06075626592125194,\n",
       "   0.06262711152273724,\n",
       "   0.06248383168348306,\n",
       "   0.06393969275864483,\n",
       "   0.06281701437949051,\n",
       "   0.059931363273559926,\n",
       "   0.06202454358468791,\n",
       "   0.06329229072301316,\n",
       "   0.061935416068123285,\n",
       "   0.05996165125627951,\n",
       "   0.060748173305084176,\n",
       "   0.06150501102389181,\n",
       "   0.060213841196828814,\n",
       "   0.06051833851282325,\n",
       "   0.0621864815082362,\n",
       "   0.06237683816852382,\n",
       "   0.06194085544109535,\n",
       "   0.06175546846432876,\n",
       "   0.059390728577605835,\n",
       "   0.0591716491127654,\n",
       "   0.06141741551392941,\n",
       "   0.06180365114969122,\n",
       "   0.05990888315942995,\n",
       "   0.06136905750596059,\n",
       "   0.060035789344127136,\n",
       "   0.05939820142683592,\n",
       "   0.058374165846856924,\n",
       "   0.05840217808412295,\n",
       "   0.06079260435417133,\n",
       "   0.06010908336176229,\n",
       "   0.05962212130719612,\n",
       "   0.059612559863349374,\n",
       "   0.05809432565964007,\n",
       "   0.05699231617511584,\n",
       "   0.05953623024313608,\n",
       "   0.06010957450016752,\n",
       "   0.058450356632963406,\n",
       "   0.060971797022165326,\n",
       "   0.0598503654293905,\n",
       "   0.06133846862200954,\n",
       "   0.0601441699666946,\n",
       "   0.05833781553150552,\n",
       "   0.05941228396370613,\n",
       "   0.05977528211285417,\n",
       "   0.06057425206481595,\n",
       "   0.060714853963637336,\n",
       "   0.06090664298512532,\n",
       "   0.05838198749400176,\n",
       "   0.0596547109791299,\n",
       "   0.05985858400613019,\n",
       "   0.0599631613697989,\n",
       "   0.05993976845198127,\n",
       "   0.059609990782477584,\n",
       "   0.0596237427018188,\n",
       "   0.05967846078212273,\n",
       "   0.0571043630562775,\n",
       "   0.059368629623608796,\n",
       "   0.05840654132492468,\n",
       "   0.05799989487210741,\n",
       "   0.057538047456175696,\n",
       "   0.05779946214836733,\n",
       "   0.0607674827004244,\n",
       "   0.05679514491519087,\n",
       "   0.05904633261034272,\n",
       "   0.0600537363777726,\n",
       "   0.06022302652991291,\n",
       "   0.05911085845608882,\n",
       "   0.05932669264004693,\n",
       "   0.056531075242787124,\n",
       "   0.06114212978223932,\n",
       "   0.0599154102371408,\n",
       "   0.05677687006701778,\n",
       "   0.059882189065888866,\n",
       "   0.05776694063981755,\n",
       "   0.05748454031729291,\n",
       "   0.05736355312258769,\n",
       "   0.0584817127366227,\n",
       "   0.060124250617929295,\n",
       "   0.05574018951959973,\n",
       "   0.0604844655835651,\n",
       "   0.05885560704646691,\n",
       "   0.0588831216038151],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb0c68",
   "metadata": {},
   "source": [
    "### 1.1 MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     #print(key)\n",
    "#     if value.requires_grad:\n",
    "#         print(key)\n",
    "#         print(value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a599c8",
   "metadata": {},
   "source": [
    "### 1.2 Arbiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ebc67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = arbiter_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = arbiter_system.state['best_epoch']\n",
    "\n",
    "state = arbiter_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "arbiter_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "569eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0531d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a86b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "\n",
    "x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "\n",
    "\n",
    "x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task = next(zip(x_support_set,y_support_set,x_target_set, y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cdeb442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "647183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbiter_x_support_set, arbiter_x_target_set, arbiter_y_support_set, arbiter_y_target_set, seed = train_sample\n",
    "\n",
    "arbiter_x_support_set = torch.Tensor(arbiter_x_support_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_x_target_set = torch.Tensor(arbiter_x_target_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_y_support_set = torch.Tensor(arbiter_y_support_set).long().to(device=arbiter_system.model.device)\n",
    "arbiter_y_target_set = torch.Tensor(arbiter_y_target_set).long().to(device=arbiter_system.model.device)\n",
    "\n",
    "\n",
    "arbiter_x_support_set_task, arbiter_y_support_set_task, arbiter_x_target_set_task, arbiter_y_target_set_task = next(zip(arbiter_x_support_set,arbiter_y_support_set,arbiter_x_target_set, arbiter_y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ce1c0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fd4d6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = arbiter_system.model.get_inner_loop_parameter_dict(arbiter_system.model.classifier.named_parameters())\n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = arbiter_x_target_set_task.shape\n",
    "\n",
    "arbiter_x_support_set_task = arbiter_x_support_set_task.view(-1, c, h, w)\n",
    "arbiter_y_support_set_task = arbiter_y_support_set_task.view(-1)\n",
    "arbiter_x_target_set_task = arbiter_x_target_set_task.view(-1, c, h, w)\n",
    "arbiter_y_target_set_task = arbiter_y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = arbiter_system.model.net_forward(\n",
    "            x=arbiter_x_support_set_task,\n",
    "            y=arbiter_y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "    \n",
    "    if arbiter_system.model.args.arbiter:\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                retain_graph=True)\n",
    "\n",
    "        names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "\n",
    "        for key, weight in names_weights_copy.items():\n",
    "            weight_norm = torch.norm(weight, p=2)\n",
    "            per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "        for key, grad in names_grads_copy.items():\n",
    "            gradient_l2norm = torch.norm(grad, p=2)\n",
    "            per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "        per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                    per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        generated_gradient_rate = arbiter_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "            g += 1\n",
    "\n",
    "    names_weights_copy,names_grads_copy = arbiter_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                      support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_arbiter.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=arbiter_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in arbiter_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d16650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "y_support_set_task = y_support_set_task.view(-1)\n",
    "x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "y_target_set_task = y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "            x=x_support_set_task,\n",
    "            y=y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "\n",
    "\n",
    "    names_weights_copy,names_grads_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                   support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_maml.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=maml_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575454f0",
   "metadata": {},
   "source": [
    "## landscape 함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aec9618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape==  torch.Size([25, 3, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "tensor([[[[-4.8023e-03, -2.6069e-03,  6.9418e-04],\n",
      "          [-3.2916e-03, -2.5021e-03,  1.1159e-03],\n",
      "          [-4.3987e-03, -2.8663e-03,  9.7868e-04]],\n",
      "\n",
      "         [[-7.0133e-03, -4.3075e-03, -9.7049e-04],\n",
      "          [-5.8467e-03, -5.6533e-03, -1.2390e-03],\n",
      "          [-7.4481e-03, -6.5388e-03, -1.5731e-03]],\n",
      "\n",
      "         [[-6.9600e-03, -4.6365e-03, -1.3106e-03],\n",
      "          [-5.7372e-03, -5.4343e-03, -2.8195e-03],\n",
      "          [-7.6292e-03, -7.7008e-03, -3.8203e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9904e-03,  2.8863e-03,  1.9659e-03],\n",
      "          [ 3.7003e-03,  2.9856e-03,  3.4031e-03],\n",
      "          [ 3.8569e-03,  3.0102e-03,  6.1439e-03]],\n",
      "\n",
      "         [[ 5.3248e-03,  4.9894e-03,  3.8564e-03],\n",
      "          [ 6.0268e-03,  5.2545e-03,  4.5029e-03],\n",
      "          [ 5.9742e-03,  4.9500e-03,  7.1158e-03]],\n",
      "\n",
      "         [[-3.0183e-03, -3.3681e-03, -5.6702e-03],\n",
      "          [-1.2298e-03, -1.4980e-03, -2.6118e-03],\n",
      "          [ 9.8387e-04,  7.0173e-04,  1.3690e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4622e-03,  2.5133e-03,  1.9817e-03],\n",
      "          [ 1.7617e-03,  2.1735e-03,  4.8738e-03],\n",
      "          [ 5.5501e-03,  4.6626e-03,  6.4641e-03]],\n",
      "\n",
      "         [[ 4.9278e-03,  3.2707e-03,  2.2890e-03],\n",
      "          [ 2.4346e-03,  3.4869e-03,  5.9444e-03],\n",
      "          [ 6.7598e-03,  6.7922e-03,  8.3164e-03]],\n",
      "\n",
      "         [[ 9.1308e-03,  8.2228e-03,  7.4853e-03],\n",
      "          [ 6.3670e-03,  8.1669e-03,  9.7922e-03],\n",
      "          [ 9.3684e-03,  1.0161e-02,  1.1254e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.7323e-03,  3.7677e-03,  3.2022e-03],\n",
      "          [ 1.9893e-03,  1.8560e-03,  1.8746e-03],\n",
      "          [ 1.6542e-03,  8.7888e-04,  2.9626e-03]],\n",
      "\n",
      "         [[ 7.3892e-03,  8.7193e-03,  8.0263e-03],\n",
      "          [ 6.7181e-03,  7.1250e-03,  7.1106e-03],\n",
      "          [ 6.7364e-03,  6.4841e-03,  8.6449e-03]],\n",
      "\n",
      "         [[ 9.1905e-03,  9.8436e-03,  8.9264e-03],\n",
      "          [ 8.2989e-03,  7.6741e-03,  7.3841e-03],\n",
      "          [ 7.7954e-03,  6.5209e-03,  8.5553e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.1145e-03, -3.5411e-03, -3.2741e-03],\n",
      "          [-3.7798e-03, -2.7946e-03, -2.6389e-03],\n",
      "          [-2.7937e-03, -2.0445e-03, -2.3718e-03]],\n",
      "\n",
      "         [[-5.7239e-04, -1.2508e-03, -1.1597e-03],\n",
      "          [-1.3159e-03, -6.0312e-04, -6.3539e-04],\n",
      "          [ 3.8390e-04,  9.0722e-04, -4.6513e-05]],\n",
      "\n",
      "         [[-3.1919e-03, -4.0894e-03, -4.3860e-03],\n",
      "          [-4.5621e-03, -3.8099e-03, -3.5618e-03],\n",
      "          [-2.9646e-03, -1.8599e-03, -1.8840e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5220e-02,  1.3247e-02,  1.4831e-02],\n",
      "          [ 1.6575e-02,  1.4363e-02,  1.4413e-02],\n",
      "          [ 1.5547e-02,  1.2866e-02,  1.4045e-02]],\n",
      "\n",
      "         [[ 1.5787e-02,  1.4139e-02,  1.4836e-02],\n",
      "          [ 1.6488e-02,  1.4683e-02,  1.4465e-02],\n",
      "          [ 1.5999e-02,  1.3719e-02,  1.4570e-02]],\n",
      "\n",
      "         [[ 1.0865e-02,  9.0097e-03,  9.3923e-03],\n",
      "          [ 1.1075e-02,  9.3398e-03,  9.6763e-03],\n",
      "          [ 1.1067e-02,  9.8267e-03,  1.0927e-02]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 2.2486e-04, -2.1356e-05,  3.4772e-04],\n",
      "          [-1.2019e-04, -5.8220e-05, -2.9217e-04],\n",
      "          [-1.3828e-05,  6.5144e-05, -9.3233e-05]],\n",
      "\n",
      "         [[ 9.2629e-04,  1.0837e-03,  8.3832e-04],\n",
      "          [ 3.5746e-05,  3.7122e-04,  5.5310e-04],\n",
      "          [ 7.6849e-04,  5.4397e-04,  1.2657e-03]],\n",
      "\n",
      "         [[-4.6370e-04, -4.1624e-04, -3.7764e-04],\n",
      "          [-8.4402e-05,  8.6290e-06, -1.0096e-04],\n",
      "          [-3.0306e-05,  1.9661e-04,  2.3097e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3604e-05,  3.5372e-04,  4.9225e-04],\n",
      "          [ 3.1086e-04,  2.0334e-04,  1.9373e-04],\n",
      "          [ 1.7729e-04,  1.8674e-04,  3.6502e-05]],\n",
      "\n",
      "         [[ 2.3606e-04,  1.0759e-04,  4.7168e-05],\n",
      "          [ 3.8599e-04,  2.3306e-04,  1.4941e-04],\n",
      "          [ 6.8846e-04,  4.2132e-04,  4.3349e-04]],\n",
      "\n",
      "         [[ 1.7459e-03,  2.0643e-03,  2.1586e-03],\n",
      "          [ 1.8873e-03,  2.1219e-03,  2.0910e-03],\n",
      "          [ 1.4266e-03,  1.5553e-03,  1.6532e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.9513e-04,  5.8458e-04,  2.1344e-04],\n",
      "          [ 4.4644e-04,  5.7733e-04,  3.4559e-04],\n",
      "          [ 4.9471e-04,  4.9077e-04,  2.1483e-04]],\n",
      "\n",
      "         [[ 4.7053e-04,  3.0832e-04,  1.0408e-04],\n",
      "          [ 4.8018e-04,  1.8366e-04, -3.0954e-04],\n",
      "          [-2.4061e-04, -2.4382e-04, -1.0384e-04]],\n",
      "\n",
      "         [[-6.3773e-04, -4.0526e-04, -3.5317e-05],\n",
      "          [-7.4784e-04, -5.1879e-04,  2.8566e-05],\n",
      "          [-3.3995e-04, -2.2867e-04,  8.7829e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.8797e-04,  6.4374e-04,  5.8165e-04],\n",
      "          [ 5.3483e-04,  3.8620e-04,  6.4463e-04],\n",
      "          [ 6.6048e-04,  5.1376e-04,  3.6088e-04]],\n",
      "\n",
      "         [[-1.7489e-05,  2.2615e-04,  1.2977e-04],\n",
      "          [-1.0796e-04,  1.3333e-04,  2.5191e-04],\n",
      "          [-8.0554e-05,  8.1110e-05,  3.7208e-04]],\n",
      "\n",
      "         [[ 1.5264e-05, -1.0864e-04, -1.9288e-04],\n",
      "          [-1.5618e-04, -1.0222e-04, -1.3540e-04],\n",
      "          [-2.3391e-04, -1.1741e-04, -1.3586e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2528e-04,  4.8937e-05,  1.7083e-04],\n",
      "          [ 4.3989e-04,  3.4445e-04,  5.5265e-05],\n",
      "          [ 6.5204e-04,  2.0899e-04,  3.0493e-04]],\n",
      "\n",
      "         [[ 7.1469e-04,  5.9599e-04,  2.4939e-04],\n",
      "          [ 9.7215e-04,  6.3210e-04,  5.7092e-04],\n",
      "          [ 7.3429e-04,  1.2091e-03,  1.3694e-03]],\n",
      "\n",
      "         [[-2.5711e-04, -9.2279e-05,  1.0420e-04],\n",
      "          [-2.5941e-04,  6.4587e-06,  2.4253e-04],\n",
      "          [-1.4337e-04, -3.8281e-05,  6.5972e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.9595e-04,  6.9643e-04,  5.9782e-04],\n",
      "          [ 6.5084e-04,  5.8817e-04,  5.1888e-04],\n",
      "          [ 9.4676e-04,  7.6026e-04,  4.4592e-04]],\n",
      "\n",
      "         [[ 8.6796e-05,  1.2106e-04,  1.8682e-04],\n",
      "          [ 1.4690e-04,  2.9754e-04,  4.4671e-04],\n",
      "          [ 3.3067e-04,  4.7525e-04,  6.2544e-04]],\n",
      "\n",
      "         [[ 5.2311e-04,  4.0845e-04,  4.1688e-04],\n",
      "          [ 5.3741e-04,  6.7075e-04,  4.6498e-04],\n",
      "          [ 2.7852e-04,  6.2917e-04,  4.7899e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.7222e-06,  2.1276e-04, -1.2512e-07],\n",
      "          [-2.9155e-05, -1.1837e-04,  3.7057e-05],\n",
      "          [-3.6542e-05,  6.3602e-05, -5.4495e-05]],\n",
      "\n",
      "         [[-4.3025e-04, -8.9958e-05, -2.5800e-04],\n",
      "          [-5.5813e-04, -2.4766e-04, -1.3792e-04],\n",
      "          [-1.5678e-05, -3.7913e-04, -4.1999e-04]],\n",
      "\n",
      "         [[-3.8498e-05,  6.6256e-05,  7.8932e-05],\n",
      "          [ 1.6597e-04,  1.6630e-04,  9.6206e-05],\n",
      "          [-1.4182e-04, -1.2921e-04, -1.4011e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.4364e-04, -4.0972e-04, -3.8170e-04],\n",
      "          [-2.2185e-04, -3.5980e-04, -4.1994e-04],\n",
      "          [-6.0438e-04, -4.9755e-04, -4.4364e-04]],\n",
      "\n",
      "         [[ 1.5424e-05, -1.2860e-05,  5.6455e-05],\n",
      "          [-4.0334e-05, -9.8974e-05, -1.1217e-04],\n",
      "          [ 3.6276e-05, -1.3030e-04, -1.6868e-04]],\n",
      "\n",
      "         [[-2.0684e-04, -2.7550e-04, -2.6732e-04],\n",
      "          [-1.5265e-04, -2.4039e-04, -3.1389e-04],\n",
      "          [-1.4006e-05, -1.6581e-04, -1.6697e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7779e-04,  4.4412e-04,  1.8479e-04],\n",
      "          [ 4.5880e-04,  4.2971e-04,  1.8057e-04],\n",
      "          [ 3.3974e-04,  2.7773e-04,  3.4376e-05]],\n",
      "\n",
      "         [[ 7.8898e-04,  2.5996e-04,  4.9579e-04],\n",
      "          [ 5.7493e-04,  5.3259e-04,  4.6446e-04],\n",
      "          [ 3.7678e-04,  2.1480e-04,  4.2564e-04]],\n",
      "\n",
      "         [[-6.4732e-05,  5.3462e-05, -6.9437e-05],\n",
      "          [-1.2999e-04, -1.4979e-06, -1.5435e-04],\n",
      "          [-4.4858e-05, -1.9634e-04, -3.4104e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.1480e-04,  5.6123e-04,  6.1059e-04],\n",
      "          [ 4.7530e-04,  4.9553e-04,  6.3922e-04],\n",
      "          [ 7.1236e-04,  4.5413e-04,  2.9691e-04]],\n",
      "\n",
      "         [[ 3.8463e-04,  3.1825e-04,  2.9828e-04],\n",
      "          [ 3.6996e-04,  3.5380e-04,  2.6893e-04],\n",
      "          [ 2.3470e-04,  2.3611e-04,  2.8093e-04]],\n",
      "\n",
      "         [[-4.6308e-04, -4.5959e-04, -4.7569e-04],\n",
      "          [-4.0759e-04, -5.5478e-04, -6.7339e-04],\n",
      "          [-4.4582e-04, -5.3211e-04, -6.7878e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.2349e-04,  3.1644e-05, -8.4733e-05],\n",
      "          [-2.0908e-04, -5.9844e-05, -4.9360e-05],\n",
      "          [-2.5195e-04, -1.1998e-04, -5.8793e-05]],\n",
      "\n",
      "         [[ 4.9306e-05,  4.6718e-04,  4.2520e-04],\n",
      "          [-1.1472e-04, -3.3374e-05, -1.1219e-04],\n",
      "          [-3.6118e-04, -3.4541e-04, -2.7215e-04]],\n",
      "\n",
      "         [[ 5.0263e-05,  7.4479e-05,  4.3590e-05],\n",
      "          [-7.0383e-05, -9.3136e-05, -8.6555e-05],\n",
      "          [-2.7739e-04, -7.8606e-05, -1.2869e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7233e-04, -1.3454e-04, -2.1694e-05],\n",
      "          [-1.5547e-04, -7.8398e-05,  2.2834e-06],\n",
      "          [-1.1578e-04, -8.2683e-05, -9.5495e-05]],\n",
      "\n",
      "         [[ 9.4959e-05,  1.3119e-04,  1.5541e-04],\n",
      "          [ 1.3261e-04,  6.6120e-05,  4.9894e-05],\n",
      "          [ 9.2125e-05,  7.9037e-05,  8.8615e-05]],\n",
      "\n",
      "         [[-6.0877e-04, -7.0148e-04, -7.2798e-04],\n",
      "          [-6.1776e-04, -6.9454e-04, -7.1835e-04],\n",
      "          [-6.0507e-04, -6.3858e-04, -6.6587e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-1.1642e-10,  2.3283e-10, -2.1828e-11,  0.0000e+00, -4.3656e-11,\n",
      "         1.3824e-10,  1.7462e-10, -8.7311e-11, -5.8208e-11, -3.9290e-10,\n",
      "         1.6735e-10,  1.0914e-10, -4.0018e-11, -1.7099e-10,  8.0036e-11,\n",
      "         0.0000e+00,  6.5484e-11, -4.3656e-10, -3.2014e-10, -2.0373e-10,\n",
      "         5.5297e-10,  1.4552e-10, -2.9831e-10, -1.2733e-10,  4.6566e-10,\n",
      "        -8.7311e-11, -3.2014e-10, -2.1828e-10,  1.1642e-10,  5.2387e-10,\n",
      "        -8.7311e-11,  1.7462e-10,  2.3283e-10, -1.4552e-10, -2.0373e-10,\n",
      "         4.1837e-11,  6.5484e-11,  3.2014e-10, -8.0036e-11,  1.4552e-10,\n",
      "        -2.9831e-10, -5.8208e-11, -2.4011e-10, -1.1642e-10, -4.3656e-11,\n",
      "        -1.0914e-10,  1.6007e-10,  8.0036e-11, -2.6193e-10, -2.0373e-10,\n",
      "         4.3656e-10, -1.6007e-10,  5.5297e-10,  2.9104e-11,  1.4552e-10,\n",
      "         1.1642e-10, -2.6193e-10,  2.9104e-10,  1.7099e-10, -2.9104e-11,\n",
      "        -7.2760e-12, -5.8208e-11,  4.0745e-10, -2.5466e-11, -1.4552e-10,\n",
      "         3.2014e-10,  9.4587e-11, -2.5466e-10, -1.1642e-10, -1.4552e-11,\n",
      "        -2.0373e-10, -1.1642e-10,  5.8208e-11,  6.5484e-11, -1.0914e-10,\n",
      "        -1.1642e-10,  2.9104e-11, -1.3097e-10, -8.7311e-11,  7.2760e-11,\n",
      "        -2.7649e-10, -1.9645e-10,  7.2760e-11, -1.1642e-10,  9.8225e-11,\n",
      "         3.0559e-10, -5.3114e-10,  2.0009e-11, -7.2760e-11,  1.7462e-10,\n",
      "        -8.7311e-11,  5.0932e-11, -5.8208e-11, -4.0018e-10,  2.4011e-10,\n",
      "        -4.0745e-10,  1.2733e-11,  1.1642e-10,  5.8208e-10,  1.3824e-10,\n",
      "         4.3656e-11,  2.1828e-10,  5.8208e-11,  5.2387e-10,  1.4552e-11,\n",
      "        -1.1642e-10, -1.2733e-10, -3.2014e-10,  2.0373e-10,  1.1642e-10,\n",
      "         5.0932e-11,  1.0477e-09, -8.7311e-11, -1.8190e-11, -2.9104e-10,\n",
      "         2.6193e-10, -3.2014e-10, -8.7311e-11, -2.6193e-10,  0.0000e+00,\n",
      "         1.4552e-10,  8.0036e-11, -1.1642e-10, -4.0745e-10,  8.7311e-11,\n",
      "        -1.4552e-11, -5.8208e-11,  1.1278e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 4.3989e-04,  2.7004e-04,  8.5843e-05,  1.2686e-03, -1.6132e-03,\n",
      "        -1.1310e-03, -1.8297e-04, -2.8699e-04,  9.5836e-05,  1.0864e-03,\n",
      "        -3.7981e-05, -5.1332e-05,  1.2747e-03, -1.3262e-03, -6.4381e-04,\n",
      "        -2.3127e-03,  4.7828e-04,  2.8178e-04,  1.4248e-03, -1.2604e-05,\n",
      "        -1.8321e-04,  6.6214e-04,  1.5974e-04, -1.4672e-03,  1.0549e-03,\n",
      "         1.2124e-03,  9.1415e-04,  1.4930e-03, -9.0869e-04,  1.4103e-03,\n",
      "         1.8768e-03,  6.4023e-04, -2.5929e-03, -1.4340e-03, -3.8649e-04,\n",
      "         1.8385e-03, -1.9220e-04, -1.0675e-03,  7.1890e-04,  4.8342e-04,\n",
      "        -1.6891e-03,  3.9653e-04,  3.4683e-04,  6.3811e-04,  6.4694e-04,\n",
      "         4.6845e-04, -4.8264e-05, -4.6282e-05,  1.7178e-03, -4.4186e-04,\n",
      "        -6.1300e-04,  1.6057e-04, -4.2126e-04, -1.6037e-03,  1.7225e-03,\n",
      "        -7.2802e-04, -8.1336e-04, -9.7279e-04,  1.6109e-03, -9.7325e-04,\n",
      "        -2.0903e-03, -2.1354e-04,  1.2881e-03, -6.1849e-04, -2.4654e-06,\n",
      "        -6.2616e-04,  1.9672e-03,  5.5391e-04, -7.1323e-04, -2.6850e-04,\n",
      "         5.3985e-04, -7.5086e-05, -2.5039e-04,  1.0185e-03,  5.0840e-04,\n",
      "        -8.6347e-04,  1.4779e-03, -7.3247e-05, -7.7913e-04, -5.9097e-04,\n",
      "        -2.4345e-03, -9.3378e-05,  3.6590e-04, -1.0015e-03,  6.9439e-04,\n",
      "         5.1821e-04, -1.7394e-04, -7.3264e-04,  3.8142e-04, -4.0282e-04,\n",
      "         2.0398e-03,  2.0595e-03,  3.4862e-04, -3.6779e-03,  1.1536e-03,\n",
      "        -1.1298e-03, -2.7311e-04,  2.2307e-03, -1.4214e-03, -5.1638e-04,\n",
      "        -4.5249e-04,  9.7016e-04, -1.1754e-03,  3.8907e-04,  6.3647e-06,\n",
      "         2.6613e-03,  7.8800e-04, -4.7080e-04, -4.6250e-04, -1.9262e-03,\n",
      "        -2.6831e-03,  3.3294e-04, -6.0975e-04, -3.2384e-04,  9.1663e-04,\n",
      "         6.8108e-04,  2.9262e-04,  8.3890e-04,  1.2350e-03,  8.5259e-04,\n",
      "         7.3185e-04, -3.9166e-04, -5.5755e-04, -7.2151e-04, -2.4209e-04,\n",
      "         4.8429e-04,  1.4995e-04,  1.8435e-03], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-3.4925e-10,  2.9104e-10,  5.2387e-10,  1.1642e-10,  4.3656e-11,\n",
      "        -1.9645e-10, -8.7311e-11, -1.1642e-10,  1.7462e-10,  0.0000e+00,\n",
      "         5.8208e-11,  5.0932e-10,  5.8208e-11,  1.0186e-10,  5.8208e-11,\n",
      "         1.7462e-10, -7.2760e-11, -2.6193e-10, -2.6193e-10,  5.8208e-11,\n",
      "         5.8208e-11,  1.6007e-10,  0.0000e+00, -1.3097e-10, -2.3283e-10,\n",
      "         1.7462e-10,  2.6193e-10,  1.4552e-11,  5.8208e-11,  5.8208e-11,\n",
      "         1.7462e-10,  1.8917e-10,  5.8208e-11, -1.6007e-10, -3.7835e-10,\n",
      "        -3.0559e-10, -5.8208e-11, -4.6566e-10, -2.3283e-10,  5.8208e-11,\n",
      "         2.3283e-10,  5.6752e-10,  0.0000e+00, -1.4552e-11, -2.0373e-10,\n",
      "        -5.8208e-11,  1.7462e-10, -8.7311e-11, -1.4552e-10, -2.9104e-10,\n",
      "         2.0373e-10,  8.7311e-11,  1.1642e-10,  2.4738e-10,  8.7311e-11,\n",
      "        -1.1642e-10,  3.2014e-10, -3.2014e-10,  2.3283e-10,  2.8012e-10,\n",
      "        -5.5297e-10,  2.0373e-10,  1.1642e-10, -4.0745e-10, -1.7462e-10,\n",
      "        -1.4552e-11, -2.6193e-10,  8.7311e-11, -2.6193e-10,  3.2014e-10,\n",
      "         2.2555e-10, -1.7462e-10,  2.9104e-10, -1.6735e-10,  5.8208e-11,\n",
      "         5.8208e-11,  2.0373e-10, -1.4552e-11,  0.0000e+00, -5.0932e-11,\n",
      "         3.7835e-10, -2.3283e-10,  2.4738e-10, -1.4552e-11,  1.4552e-10,\n",
      "        -1.1642e-10, -4.3656e-11, -8.7311e-11, -4.0745e-10, -1.4552e-11,\n",
      "        -5.0932e-10, -5.8208e-11,  2.9104e-11,  0.0000e+00,  2.3283e-10,\n",
      "        -1.0914e-10, -4.3656e-11,  4.3656e-10,  2.9104e-10,  6.4028e-10,\n",
      "         8.7311e-11, -3.7835e-10, -2.9104e-11,  0.0000e+00, -2.3283e-10,\n",
      "         2.9104e-10,  2.3283e-10, -1.0186e-10,  0.0000e+00,  3.0559e-10,\n",
      "        -5.8208e-11, -5.8208e-11,  1.4552e-10,  5.8208e-11, -1.1642e-10,\n",
      "        -5.8208e-11,  8.7311e-11,  2.9104e-11,  2.3283e-10,  3.4925e-10,\n",
      "         8.3674e-11,  1.4552e-11,  1.1642e-10, -1.1642e-10,  1.4552e-10,\n",
      "         2.3283e-10,  0.0000e+00,  1.4552e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-6.8359e-04,  1.4002e-03,  7.0080e-04,  1.8209e-03,  1.7563e-04,\n",
      "         7.7835e-04, -1.8455e-04,  4.4450e-04,  1.9741e-04, -2.4884e-04,\n",
      "         2.8314e-04, -7.1214e-04,  3.0346e-04,  9.4830e-04, -7.2306e-04,\n",
      "         5.8511e-04,  1.6266e-03, -3.1121e-04,  1.8801e-04,  1.6861e-03,\n",
      "        -1.6805e-03,  3.2922e-04, -2.1142e-03,  7.7247e-04,  1.7792e-04,\n",
      "         6.3429e-04,  2.6715e-04, -5.4067e-04,  4.7332e-04, -1.3608e-03,\n",
      "        -5.1872e-04,  2.0903e-03,  1.8595e-03, -1.4920e-03, -6.8779e-04,\n",
      "        -2.4142e-03,  3.0523e-04, -2.0889e-04, -4.2632e-04, -3.3047e-04,\n",
      "        -7.8910e-04, -1.5869e-05,  1.8873e-03, -5.2150e-04, -1.5849e-04,\n",
      "         1.2221e-03, -9.6331e-04,  8.5828e-04, -6.6421e-04, -1.7427e-04,\n",
      "        -8.5487e-05, -3.5567e-04, -1.2575e-03, -1.6173e-03,  2.9529e-04,\n",
      "         1.0166e-03, -1.9862e-04, -1.7260e-03, -5.3712e-04, -1.8546e-04,\n",
      "         7.1785e-04,  8.9885e-04,  1.6979e-03,  1.8824e-04, -6.2716e-04,\n",
      "         9.5918e-05, -1.9081e-03, -1.9021e-03, -1.8462e-03,  6.8131e-04,\n",
      "        -4.2160e-04,  8.1805e-04,  4.1005e-03, -1.7839e-03,  1.3884e-03,\n",
      "         2.4115e-03,  7.2545e-04,  1.2914e-03, -5.1610e-04,  7.0338e-04,\n",
      "        -7.7277e-04,  2.2443e-04,  3.2015e-03, -1.2562e-03, -1.3160e-03,\n",
      "        -1.2309e-03,  9.5790e-04,  6.6399e-04, -1.9605e-03, -1.1208e-04,\n",
      "         9.2969e-04, -1.3862e-03, -3.0726e-04,  3.3972e-04,  7.4580e-04,\n",
      "         1.9458e-04,  1.0920e-03,  1.4336e-03,  9.2950e-04, -4.5658e-04,\n",
      "         8.1970e-04, -8.5585e-04, -1.0902e-03, -6.3904e-04,  1.1160e-03,\n",
      "        -2.9693e-03, -3.3808e-04,  6.5187e-04,  1.7599e-03, -5.4831e-04,\n",
      "         2.8865e-03,  5.9562e-04,  3.9760e-04, -8.5516e-04, -9.2704e-04,\n",
      "        -1.7732e-04,  3.6988e-04,  1.0696e-03, -1.5642e-04, -2.0422e-04,\n",
      "        -7.0610e-04,  1.4189e-03,  2.2104e-05, -5.7139e-04,  2.4915e-03,\n",
      "        -9.5907e-04,  9.6360e-04, -6.4168e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-2.0139e-03,  9.0437e-04, -1.4227e-03,  2.2192e-03,  1.0147e-03,\n",
      "         2.7104e-03, -1.3375e-04,  1.8993e-04, -7.0609e-04, -5.6312e-04,\n",
      "         1.9441e-03, -6.5473e-03,  5.4801e-04,  3.5207e-04, -3.8423e-04,\n",
      "         3.3340e-04,  2.6360e-03,  4.3887e-03, -3.4872e-04,  8.2029e-04,\n",
      "        -6.3744e-04,  1.6255e-04, -8.6034e-04,  1.7829e-04,  4.6514e-04,\n",
      "        -1.7207e-04,  1.3920e-03, -5.1509e-04, -1.7381e-03, -1.4987e-03,\n",
      "         4.8623e-04,  3.3748e-03,  3.3330e-04, -1.2860e-03,  1.0515e-03,\n",
      "        -2.3113e-03, -8.0817e-05, -2.3199e-03, -1.1376e-03,  1.0687e-03,\n",
      "        -2.5366e-03,  8.9828e-04,  3.4173e-03, -3.9533e-04,  1.1040e-04,\n",
      "         1.9859e-03,  5.6702e-04,  9.8978e-04, -8.7051e-04,  2.3275e-04,\n",
      "         1.9803e-03, -9.3963e-04,  6.2957e-04, -1.8579e-03, -5.9154e-04,\n",
      "         6.7149e-04, -1.3424e-03, -3.0497e-03, -6.6860e-04,  1.3321e-03,\n",
      "        -2.4885e-03,  9.9225e-04,  2.0457e-03,  1.5154e-03, -1.6615e-03,\n",
      "         8.1751e-05, -1.9727e-03, -1.6812e-03, -1.7772e-03, -5.5867e-04,\n",
      "         2.4585e-04,  9.9643e-04,  4.3213e-03, -3.0366e-03,  3.8333e-04,\n",
      "         1.0873e-03,  5.0784e-04,  1.2434e-04, -1.3561e-03,  1.3448e-03,\n",
      "        -1.0986e-03,  5.0219e-05,  4.1449e-03, -9.0141e-04,  1.2451e-03,\n",
      "        -4.8963e-04, -7.0356e-04,  1.4749e-03, -6.9360e-04,  1.2964e-03,\n",
      "         9.7344e-05, -7.6607e-04, -1.1786e-03, -2.4444e-04, -7.8087e-05,\n",
      "        -5.3683e-05,  4.5216e-04,  1.2227e-04,  1.5846e-03,  4.3072e-05,\n",
      "         7.4168e-04,  6.6882e-04, -2.0331e-03, -1.1155e-03,  7.3474e-04,\n",
      "        -1.4264e-03, -4.6836e-05,  7.7917e-05,  4.1580e-04,  2.7758e-04,\n",
      "         2.2452e-03, -4.9233e-05, -6.9199e-06, -1.4030e-03, -1.4673e-03,\n",
      "        -1.2694e-03, -3.7927e-04,  2.7952e-03, -5.1143e-04, -1.8153e-03,\n",
      "        -1.2210e-03,  1.9677e-03, -9.1216e-05, -3.6159e-04,  1.8718e-03,\n",
      "        -8.0545e-04,  6.2685e-04, -2.6198e-03], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[-0.0037, -0.0124, -0.0149,  ..., -0.0102, -0.0188, -0.0045],\n",
      "        [ 0.0022,  0.0035,  0.0056,  ...,  0.0023,  0.0006, -0.0038],\n",
      "        [-0.0036, -0.0015, -0.0001,  ...,  0.0033,  0.0046,  0.0098],\n",
      "        [-0.0028, -0.0005, -0.0017,  ..., -0.0003,  0.0075, -0.0046],\n",
      "        [ 0.0079,  0.0109,  0.0111,  ...,  0.0049,  0.0061,  0.0030]],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([-0.0175,  0.0013,  0.0088, -0.0007,  0.0082], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.6843e-13,  3.4106e-12, -2.2737e-13, -3.9790e-13,  5.6843e-13,\n",
      "         1.8190e-12,  3.1264e-13,  5.6843e-14, -5.4001e-12, -4.5475e-13,\n",
      "        -2.2737e-13, -5.3291e-14,  3.4106e-13,  2.8422e-12, -5.6843e-14,\n",
      "         0.0000e+00, -2.9559e-12,  0.0000e+00, -3.2117e-12, -6.8212e-13,\n",
      "         1.1369e-12,  0.0000e+00, -1.8190e-12, -1.0232e-12,  4.5475e-13,\n",
      "        -3.5527e-14,  1.8758e-12, -1.4779e-12,  1.0800e-12, -1.1084e-12,\n",
      "        -1.1369e-12, -1.4211e-13,  0.0000e+00,  5.4570e-12,  2.8422e-13,\n",
      "         6.8212e-13, -1.8190e-12,  2.0748e-12,  1.4211e-14,  1.4779e-12,\n",
      "        -4.5475e-13,  1.5916e-12, -1.5916e-12,  1.1937e-12, -2.8422e-12,\n",
      "         3.0695e-12, -1.4779e-12, -1.3642e-12, -4.0927e-12,  5.6133e-13,\n",
      "        -4.8885e-12,  2.0748e-12,  4.5475e-12, -3.1832e-12,  1.4211e-12,\n",
      "        -1.7053e-13,  5.2296e-12, -9.0949e-13, -1.3642e-12,  2.2737e-12,\n",
      "        -4.5475e-13,  6.3665e-12,  5.5707e-12, -3.6948e-13, -8.5265e-13,\n",
      "        -1.0800e-12, -3.9790e-13,  2.5011e-12,  0.0000e+00, -3.1832e-12,\n",
      "         1.3642e-12, -2.8422e-12, -2.2737e-13,  5.6843e-13, -1.5916e-12,\n",
      "         5.7554e-13, -1.8190e-12,  8.8107e-13,  4.5475e-13,  1.1937e-12,\n",
      "         4.5475e-12, -4.5475e-13,  4.8885e-12, -2.2737e-13,  1.2506e-12,\n",
      "         7.9581e-13,  5.4570e-12, -1.9327e-12,  3.4106e-13,  2.7285e-12,\n",
      "         0.0000e+00,  1.9327e-12,  2.2737e-12,  5.0591e-12,  1.0232e-12,\n",
      "        -5.6843e-13,  2.5011e-12,  9.0949e-13,  1.1369e-12, -1.2506e-12,\n",
      "         3.9790e-13,  5.0022e-12, -4.5475e-13, -1.0800e-12, -2.3874e-12,\n",
      "        -4.4338e-12, -9.0949e-13,  2.8422e-13, -3.8654e-12, -1.5916e-12,\n",
      "         2.9559e-12,  2.2737e-13,  2.2737e-13,  4.7748e-12, -5.1159e-13,\n",
      "        -3.6380e-12, -9.0949e-13,  1.3642e-12, -4.5475e-13, -4.5475e-13,\n",
      "         2.8422e-13, -1.9895e-13,  1.5916e-11, -3.5243e-12,  0.0000e+00,\n",
      "        -5.6843e-13, -4.3201e-12,  1.7053e-13], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.5136e-07, -3.0921e-05,  4.2124e-05,  3.7584e-06,  8.3487e-06,\n",
      "        -6.6273e-05, -2.0287e-06,  1.4038e-05, -1.4716e-05,  2.2733e-05,\n",
      "        -1.5764e-05,  1.1200e-05,  1.0747e-05, -1.6088e-06,  2.4584e-05,\n",
      "         3.9775e-05,  6.5953e-06, -2.5041e-05, -7.8969e-06,  1.2375e-05,\n",
      "         2.3675e-06, -5.9816e-06, -4.2426e-08, -1.1685e-05, -2.4924e-05,\n",
      "         1.5105e-05,  5.3780e-06, -1.9013e-06, -3.4292e-05,  3.7643e-06,\n",
      "        -2.0096e-05, -1.0820e-05, -3.4384e-06,  7.1922e-06,  1.8015e-05,\n",
      "         2.6518e-05,  2.3537e-05,  1.6054e-05,  1.0379e-05, -1.1516e-05,\n",
      "         2.5007e-06,  1.9219e-05,  9.8713e-06, -8.6024e-06, -1.7674e-05,\n",
      "        -7.1770e-06,  2.1120e-05,  9.7201e-06,  1.8761e-05, -1.0564e-05,\n",
      "        -5.0518e-06,  1.2409e-05,  3.1975e-05,  1.4717e-05,  1.5468e-05,\n",
      "         3.8394e-05,  2.8535e-05, -9.3905e-06, -8.0576e-06,  2.6606e-05,\n",
      "        -5.4260e-05,  6.0268e-06,  1.2360e-05, -2.8392e-05,  1.8272e-06,\n",
      "        -5.1081e-05,  9.6954e-06, -3.7777e-05,  3.6451e-05, -3.1975e-05,\n",
      "         2.5446e-05,  1.1210e-05, -3.4773e-06, -5.4924e-05,  1.0392e-05,\n",
      "        -2.7974e-06,  2.5920e-05,  4.3664e-05, -3.4354e-05,  1.2084e-05,\n",
      "         1.0191e-05,  7.7323e-06,  6.2823e-06, -7.0826e-06,  2.8573e-05,\n",
      "        -1.6755e-05,  2.5681e-05, -4.2596e-05,  6.0970e-06,  3.8590e-05,\n",
      "         4.3422e-05,  2.0822e-05,  1.5526e-05, -5.7049e-07, -5.8287e-07,\n",
      "        -1.5530e-05,  3.0089e-05, -7.4841e-06,  1.2450e-05, -1.3709e-05,\n",
      "        -1.1076e-05, -2.9981e-05,  1.2317e-05, -6.9199e-06, -3.2643e-06,\n",
      "        -1.4665e-05,  2.0447e-05,  1.8461e-05,  1.3104e-05, -6.7383e-06,\n",
      "        -7.5800e-05,  7.0166e-06, -4.3143e-06,  2.0757e-05,  1.1821e-05,\n",
      "         7.1884e-06,  9.8156e-06, -4.6433e-06, -4.4317e-05, -3.1473e-05,\n",
      "         5.1298e-06,  1.1466e-05,  1.2394e-05,  7.8980e-06,  1.4827e-05,\n",
      "        -1.5048e-05, -6.5079e-06,  1.0787e-05], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-5.9117e-12,  1.0004e-11,  7.2760e-12,  3.6380e-12, -5.6843e-12,\n",
      "         6.3665e-12,  2.2737e-12, -1.8190e-12, -9.0949e-12,  1.3642e-12,\n",
      "         2.9559e-12, -4.5475e-12, -1.2506e-12,  5.4570e-12, -5.4570e-12,\n",
      "         9.0949e-12, -1.8190e-12,  1.0004e-11, -3.4106e-12,  0.0000e+00,\n",
      "        -1.1823e-11,  4.3201e-12, -3.6380e-12,  1.4097e-11, -1.1369e-11,\n",
      "        -1.4552e-11,  5.4570e-12, -3.1832e-12, -3.1832e-12, -9.0949e-13,\n",
      "         1.3642e-12, -4.0927e-12, -1.5461e-11,  7.2760e-12,  3.6380e-12,\n",
      "         2.7285e-12,  8.1855e-12, -1.4779e-12, -1.3642e-12,  1.1141e-11,\n",
      "         4.5475e-12,  9.0949e-13,  4.0927e-12,  1.0914e-11,  3.1832e-12,\n",
      "        -5.4570e-12,  7.2760e-12,  1.0004e-11,  4.0927e-12,  0.0000e+00,\n",
      "        -1.1255e-11, -8.6402e-12,  9.0949e-12,  1.8190e-12, -1.3642e-12,\n",
      "        -1.3642e-12, -9.0949e-13, -7.2760e-12,  2.2737e-12,  2.2737e-12,\n",
      "         4.0927e-12,  0.0000e+00,  3.6380e-12, -2.7285e-12, -3.6380e-12,\n",
      "        -2.7285e-12,  8.1855e-12, -2.2737e-12,  2.2737e-12,  1.1369e-12,\n",
      "        -9.0949e-13,  5.4570e-12,  9.0949e-13,  7.2760e-12, -4.5475e-13,\n",
      "        -2.7285e-12, -7.7307e-12, -7.2760e-12,  3.6380e-12,  2.2737e-12,\n",
      "         1.8190e-12, -1.8190e-12, -4.5475e-13,  7.2760e-12, -9.7771e-12,\n",
      "        -2.2737e-12, -1.8190e-12, -7.9581e-12,  7.8444e-12, -3.1832e-12,\n",
      "        -8.1855e-12, -9.0949e-12, -7.2760e-12, -3.0695e-12, -2.2737e-12,\n",
      "         9.0949e-13,  1.0914e-11,  8.1855e-12,  5.9117e-12,  2.7285e-12,\n",
      "         9.0949e-13,  6.3665e-12,  3.6380e-12,  3.1832e-12, -5.4570e-12,\n",
      "         1.4779e-12,  1.4552e-11,  1.1823e-11, -4.5475e-13, -3.8654e-12,\n",
      "        -2.7285e-12,  4.5475e-12, -3.6380e-12, -4.8885e-12, -4.5475e-13,\n",
      "         1.8190e-12,  4.5475e-12, -2.7285e-12, -1.8190e-12, -4.0927e-12,\n",
      "        -1.0914e-11, -9.8908e-12,  3.6380e-12, -9.0949e-13, -1.0914e-11,\n",
      "         1.0687e-11, -2.3874e-12, -2.2737e-12], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.8673e-05, -3.0287e-05,  3.5544e-05,  1.8799e-05,  3.2144e-05,\n",
      "        -5.5698e-05, -6.1350e-06,  3.8344e-05, -5.8188e-06, -1.6657e-05,\n",
      "        -6.2943e-05, -1.0876e-05,  5.6564e-05,  2.3280e-05, -3.4495e-05,\n",
      "         9.8595e-06,  2.8426e-05,  9.4459e-05, -2.9713e-05,  1.5060e-05,\n",
      "         4.1406e-05,  1.3775e-05, -8.2191e-06,  1.4310e-05, -2.3384e-05,\n",
      "        -8.2902e-06,  4.0376e-05, -3.6462e-05, -1.8627e-07, -1.0149e-04,\n",
      "        -1.0407e-05, -4.2114e-05, -7.2605e-05, -1.1080e-05,  1.1530e-04,\n",
      "         3.8869e-05,  3.7382e-05,  2.9844e-05,  5.5249e-05, -8.5118e-06,\n",
      "         3.1086e-05,  3.7705e-05,  7.9195e-06, -2.4187e-05, -1.4643e-05,\n",
      "        -1.0270e-05,  4.1601e-05,  2.6831e-05, -4.0477e-05, -1.7733e-05,\n",
      "        -1.6321e-05,  2.8516e-05, -2.0053e-05,  1.8736e-05,  4.7972e-06,\n",
      "         2.7407e-05, -5.4540e-05, -6.9904e-06,  6.1339e-05, -4.0475e-05,\n",
      "        -4.1833e-06,  6.7324e-06,  7.4212e-06, -1.6342e-05, -1.1705e-05,\n",
      "         1.9345e-05, -1.9565e-05,  3.0518e-05, -4.1659e-05,  1.1856e-04,\n",
      "         1.1900e-05,  3.5686e-05, -2.6982e-05, -1.2288e-05, -4.3571e-05,\n",
      "         1.4914e-05,  9.2131e-06, -2.7044e-05,  2.3719e-05,  1.7748e-05,\n",
      "         1.2067e-05, -4.1638e-05,  1.8208e-05,  9.7109e-05, -6.0757e-05,\n",
      "        -2.9423e-05, -2.3809e-05, -1.0312e-04,  6.1825e-05, -2.1590e-05,\n",
      "         2.2177e-05, -6.6082e-06,  2.7247e-05, -3.3152e-05,  1.1050e-05,\n",
      "        -9.1724e-06,  9.9221e-06,  7.1018e-05, -1.7790e-06, -9.3879e-06,\n",
      "        -3.8567e-05,  2.2989e-05,  4.0003e-05,  4.2204e-06,  1.2041e-05,\n",
      "         5.6669e-05,  3.3822e-05,  2.4012e-05,  4.4269e-05, -3.8998e-05,\n",
      "         3.4586e-05,  4.2106e-05, -2.7593e-05, -1.1837e-05, -3.8799e-05,\n",
      "         3.6292e-06, -1.8614e-05,  6.6664e-06, -5.1999e-05, -2.9180e-05,\n",
      "         7.5073e-06, -1.6508e-05, -3.1284e-05, -1.2216e-05, -1.7277e-05,\n",
      "        -4.8672e-05, -1.3609e-05, -6.4874e-05], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 9.0949e-12,  8.1855e-12,  0.0000e+00,  4.0927e-12,  8.1855e-12,\n",
      "         6.3665e-12, -1.8190e-12, -2.7285e-12,  4.0927e-12, -3.6380e-12,\n",
      "        -7.2760e-12, -9.0949e-13, -1.0914e-11, -7.2760e-12,  1.8190e-12,\n",
      "         4.5475e-12,  8.8676e-12,  3.6380e-12,  7.2760e-12,  3.6380e-12,\n",
      "        -1.8190e-12,  1.0914e-11, -1.8190e-12, -1.8190e-12,  0.0000e+00,\n",
      "        -1.2278e-11,  1.1823e-11, -5.9117e-12, -1.4211e-12, -5.0022e-12,\n",
      "         9.0949e-12, -1.8190e-12, -2.7285e-12,  2.9559e-12, -1.8190e-11,\n",
      "        -4.5475e-13, -7.2760e-12,  6.3665e-12,  3.6380e-12, -7.2760e-12,\n",
      "         3.6380e-12, -1.3642e-12,  7.2760e-12,  6.8212e-12,  1.0914e-11,\n",
      "         3.1832e-12,  1.8190e-12, -6.8212e-13,  0.0000e+00, -4.5475e-12,\n",
      "        -6.3665e-12,  1.8190e-12,  1.3642e-12, -4.5475e-12, -5.4570e-12,\n",
      "        -1.0914e-11, -1.8190e-12, -1.3642e-12, -1.5461e-11, -9.0949e-13,\n",
      "        -2.0009e-11, -7.2760e-12,  1.7280e-11, -1.4552e-11, -8.6402e-12,\n",
      "        -9.0949e-12,  5.4570e-12,  8.6402e-12, -7.7307e-12, -2.7285e-12,\n",
      "         0.0000e+00,  2.7285e-12,  5.4570e-12, -2.2737e-12, -9.0949e-13,\n",
      "         3.6380e-12,  1.8190e-12, -9.0949e-13,  0.0000e+00, -5.0022e-12,\n",
      "        -9.0949e-13,  0.0000e+00,  5.4570e-12,  3.6380e-12, -2.2737e-12,\n",
      "         0.0000e+00, -1.0004e-11,  2.9559e-12,  2.2737e-11,  0.0000e+00,\n",
      "         2.7285e-12,  7.5033e-12, -1.6371e-11,  5.9117e-12,  5.4570e-12,\n",
      "        -5.4570e-12, -3.6380e-12,  1.8190e-12,  2.4556e-11,  9.0949e-13,\n",
      "        -8.6402e-12,  1.4552e-11, -9.0949e-13, -5.4570e-12,  3.6380e-12,\n",
      "         3.6380e-12, -6.3665e-12,  9.0949e-12, -1.6371e-11,  1.3642e-12,\n",
      "        -1.8190e-12, -2.7285e-12, -4.5475e-12,  3.4106e-12,  3.6380e-12,\n",
      "         4.5475e-12, -4.5475e-13, -4.5475e-12, -7.2760e-12,  4.5475e-12,\n",
      "         7.7307e-12,  0.0000e+00, -2.7285e-12,  4.5475e-13,  7.2760e-12,\n",
      "        -5.0022e-12,  9.0949e-13, -3.1832e-12], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-1.8930e-05, -1.7208e-05,  7.2406e-06, -3.3128e-05,  2.2074e-05,\n",
      "        -9.8727e-06, -1.0790e-05, -8.4756e-06, -2.1658e-05, -2.2605e-06,\n",
      "        -2.7376e-05,  7.6843e-06, -5.6416e-05, -1.1903e-05, -2.9009e-05,\n",
      "        -5.3092e-06,  1.8383e-05,  3.2509e-06,  7.1461e-06, -1.5723e-05,\n",
      "        -1.6032e-05, -4.6089e-07, -3.2353e-06, -2.0758e-05, -9.9925e-06,\n",
      "         2.2170e-05, -6.5893e-05, -6.1472e-06, -2.3707e-06, -2.2118e-05,\n",
      "         6.7913e-05,  6.3030e-05, -9.7968e-06,  6.7243e-05,  1.5338e-04,\n",
      "         3.3281e-05, -2.6687e-05, -3.7964e-05,  1.9391e-07, -5.8884e-05,\n",
      "         1.7766e-05, -7.9967e-06,  1.8235e-04,  3.2040e-05, -2.7940e-05,\n",
      "         5.3259e-05,  9.6361e-06,  1.4116e-05,  1.6212e-05, -2.3739e-05,\n",
      "         9.6889e-05,  2.3947e-06,  1.9591e-05, -5.1340e-06, -1.7911e-05,\n",
      "         1.5489e-04,  3.5641e-05,  1.8293e-05,  2.9237e-05, -4.2966e-05,\n",
      "        -1.1999e-05, -1.7845e-05,  1.7446e-04, -3.6075e-06,  1.9480e-05,\n",
      "         2.3357e-05,  2.8786e-05, -4.2229e-05, -3.5361e-05,  4.2175e-06,\n",
      "         8.3202e-06, -1.5923e-05, -2.1657e-05, -4.4153e-05,  1.2810e-05,\n",
      "        -4.5844e-05,  3.5298e-05, -1.9637e-06,  3.2388e-05, -1.2429e-05,\n",
      "        -3.8848e-05, -3.1952e-05,  4.3859e-05, -1.7068e-05,  1.5587e-06,\n",
      "         2.3543e-05, -2.8240e-06,  1.7388e-05, -3.4257e-05,  4.8059e-07,\n",
      "         3.0289e-05,  2.9370e-05, -3.1590e-05,  1.1434e-05,  1.0926e-05,\n",
      "         4.2281e-05, -2.3887e-05, -3.5136e-05, -5.3610e-05, -2.9535e-05,\n",
      "         1.2727e-05,  5.4161e-05,  6.9869e-06,  8.0464e-06,  2.1718e-06,\n",
      "        -5.5025e-05, -4.2292e-05, -3.1064e-05, -7.0173e-05, -2.4208e-05,\n",
      "        -6.2678e-06,  5.1445e-05, -3.3316e-05,  9.4382e-06,  1.0572e-04,\n",
      "        -2.6638e-05, -1.0868e-05, -1.2399e-05, -2.4497e-05,  7.5615e-06,\n",
      "        -5.6809e-05,  2.4694e-05,  8.7833e-06, -3.1825e-05,  3.7898e-05,\n",
      "         1.2731e-05,  2.8737e-05, -3.8373e-05], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[ 6.2065e-06,  2.6104e-05,  3.1448e-05,  ...,  2.3977e-05,\n",
      "         -1.9818e-06, -4.5007e-05],\n",
      "        [ 1.6468e-05,  7.6827e-06, -6.2407e-05,  ..., -3.0708e-05,\n",
      "         -1.2349e-05, -1.5137e-05],\n",
      "        [ 2.0632e-05,  2.5595e-05,  4.9404e-05,  ...,  5.4085e-05,\n",
      "          7.1823e-05,  8.3188e-05],\n",
      "        [-2.4643e-05, -5.0268e-05, -1.1680e-05,  ..., -3.0232e-05,\n",
      "         -3.3939e-06, -1.2621e-05],\n",
      "        [-1.8661e-05, -9.1113e-06, -6.7592e-06,  ..., -1.7117e-05,\n",
      "         -5.4092e-05, -1.0414e-05]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([-1.8382e-06, -6.2162e-05,  1.8171e-04, -4.2223e-05, -7.5468e-05],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = loss_landscape_join.landscape(maml_system.model.classifier, arbiter_system.model.classifier, args_arbiter)\n",
    "ls.show_2djoin(x_support_set_task, y_support_set_task, title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
