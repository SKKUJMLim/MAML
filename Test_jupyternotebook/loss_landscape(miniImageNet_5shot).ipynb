{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16249129",
   "metadata": {},
   "source": [
    "## [참고]\n",
    "### https://cocoa-t.tistory.com/entry/PyHessian-Loss-Landscape-%EC%8B%9C%EA%B0%81%ED%99%94-PyHessian-Neural-Networks-Through-the-Lens-of-the-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af476",
   "metadata": {},
   "source": [
    "# 0. Dataset 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7235fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"mini_imagenet_full_size\"\n",
    "# dataset=\"tiered_imagenet\"\n",
    "# dataset=\"CIFAR_FS\"\n",
    "# dataset=\"CUB\"\n",
    "\n",
    "title = 'miniImageNet'\n",
    "# title = 'tieredImageNet'\n",
    "# title = 'CIFAR-FS'\n",
    "# title = 'CUB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005193c",
   "metadata": {},
   "source": [
    "# 1. MAML 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f0d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_maml = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_5way_5shot_filter64\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":64,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_maml.im_shape = (2, 3, args_maml.image_height, args_maml.image_width)\n",
    "\n",
    "args_maml.use_cuda = torch.cuda.is_available()\n",
    "args_maml.seed = 104\n",
    "args_maml.reverse_channels=False\n",
    "args_maml.labels_as_int=False\n",
    "args_maml.reset_stored_filepaths=False\n",
    "args_maml.num_of_gpus=1\n",
    "\n",
    "args_maml.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9052a",
   "metadata": {},
   "source": [
    "## 2. Arbiter 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_arbiter = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 101,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_arbiter.im_shape = (2, 3, args_arbiter.image_height, args_arbiter.image_width)\n",
    "\n",
    "args_arbiter.use_cuda = torch.cuda.is_available()\n",
    "args_arbiter.seed = 104\n",
    "args_arbiter.reverse_channels=False\n",
    "args_arbiter.labels_as_int=False\n",
    "args_arbiter.reset_stored_filepaths=False\n",
    "args_arbiter.num_of_gpus=1\n",
    "\n",
    "args_arbiter.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f7d8",
   "metadata": {},
   "source": [
    "## 3. Model 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803156ee",
   "metadata": {},
   "source": [
    "### 3.1. MAML Model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f85286c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_5way_5shot_filter64\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_maml = MAMLFewShotClassifier(args=args_maml, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_maml.image_height, args_maml.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model_maml, data=data, args=args_maml, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3acf",
   "metadata": {},
   "source": [
    "### 3.2.  Arbiter 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25651dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50500\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_arbiter = MAMLFewShotClassifier(args=args_arbiter, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_arbiter.image_height, args_arbiter.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "arbiter_system = ExperimentBuilder(model=model_arbiter, data=data, args=args_arbiter, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a2ff6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6116666648785273,\n",
       " 'best_val_iter': 45500,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 91,\n",
       " 'train_loss_mean': 0.5506827815175056,\n",
       " 'train_loss_std': 0.1347330680555223,\n",
       " 'train_accuracy_mean': 0.7948933341503144,\n",
       " 'train_accuracy_std': 0.05953420825581363,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 1.0046450330813725,\n",
       " 'val_loss_std': 0.1571665716406741,\n",
       " 'val_accuracy_mean': 0.6093777750929197,\n",
       " 'val_accuracy_std': 0.0643605435900373,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.1425e-01, -4.4933e-01,  1.7018e-01],\n",
       "                         [-1.7254e-01, -2.7310e-02,  9.3902e-02],\n",
       "                         [-1.1717e-01,  4.5088e-01, -4.0048e-02]],\n",
       "               \n",
       "                        [[ 2.4871e-01, -3.8809e-01,  2.3791e-01],\n",
       "                         [-1.0773e-01,  5.8160e-02,  1.5152e-01],\n",
       "                         [-1.9190e-01,  2.8830e-01, -1.5601e-01]],\n",
       "               \n",
       "                        [[ 3.4503e-01, -2.4130e-01,  1.1107e-02],\n",
       "                         [ 5.8856e-02,  1.1183e-01, -1.6049e-01],\n",
       "                         [-1.7040e-01,  1.9904e-01, -2.8729e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.5724e-01,  3.5376e-01,  2.6145e-01],\n",
       "                         [ 2.2268e-01, -2.9287e-01, -4.0536e-02],\n",
       "                         [-2.4458e-01, -3.9195e-01, -2.7559e-01]],\n",
       "               \n",
       "                        [[-1.5052e-01, -1.0293e-01, -1.1279e-01],\n",
       "                         [ 1.3595e-01, -2.1226e-01, -8.7453e-02],\n",
       "                         [ 1.9349e-01,  3.9514e-02,  2.1235e-01]],\n",
       "               \n",
       "                        [[-2.9972e-01, -8.9993e-03, -1.9661e-01],\n",
       "                         [-1.0305e-01,  1.5704e-01,  1.7131e-01],\n",
       "                         [ 6.2591e-03,  2.5766e-01,  8.3135e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.5220e-01, -8.7272e-02,  3.6978e-01],\n",
       "                         [-1.5598e-01, -3.5780e-01, -2.0308e-01],\n",
       "                         [-5.8473e-02,  5.9604e-02, -2.4067e-01]],\n",
       "               \n",
       "                        [[-6.0130e-02, -2.1631e-01,  1.1681e-02],\n",
       "                         [ 1.8765e-01, -2.9733e-01, -2.9815e-01],\n",
       "                         [ 1.0875e-01,  1.0133e-01,  7.0407e-03]],\n",
       "               \n",
       "                        [[ 1.9262e-01, -1.5284e-01,  1.1929e-01],\n",
       "                         [ 2.3647e-01, -3.4826e-01, -3.8164e-01],\n",
       "                         [ 2.1311e-01,  8.4489e-02, -7.2797e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-8.4951e-03, -1.5571e-01, -1.9053e-01],\n",
       "                         [ 6.1550e-02,  3.8455e-01,  4.6697e-01],\n",
       "                         [-8.5795e-02, -2.0821e-01, -2.6607e-01]],\n",
       "               \n",
       "                        [[-1.0180e-01, -2.6297e-01, -2.6089e-01],\n",
       "                         [ 1.9029e-01,  3.1206e-01,  4.0120e-01],\n",
       "                         [ 5.5657e-03, -1.0900e-01, -1.8492e-01]],\n",
       "               \n",
       "                        [[ 4.2985e-02, -7.1504e-02, -5.0023e-02],\n",
       "                         [-1.5526e-02,  1.8296e-01,  1.3825e-01],\n",
       "                         [-6.2402e-02, -9.4225e-02, -6.3561e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.0798e-01,  1.0266e-01, -2.5602e-01],\n",
       "                         [ 2.3282e-01,  4.3741e-01,  3.7466e-02],\n",
       "                         [-4.5110e-01,  2.6200e-01, -6.6773e-02]],\n",
       "               \n",
       "                        [[-2.8795e-01, -5.8905e-02, -7.1594e-02],\n",
       "                         [ 4.0443e-01,  1.5080e-01,  2.2495e-02],\n",
       "                         [-4.3889e-01,  1.3316e-01, -1.2865e-01]],\n",
       "               \n",
       "                        [[-7.4229e-02, -3.3152e-02,  1.6747e-04],\n",
       "                         [ 3.1752e-01, -8.1946e-02,  1.3254e-01],\n",
       "                         [-8.4182e-02, -9.3233e-02, -8.5916e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.3777e-02,  2.0849e-01,  3.1493e-01],\n",
       "                         [-2.8116e-01,  1.9551e-01,  1.1700e-01],\n",
       "                         [-7.7545e-02,  5.5102e-02, -1.8253e-01]],\n",
       "               \n",
       "                        [[ 3.3258e-01, -1.3814e-03, -8.9905e-02],\n",
       "                         [ 1.7441e-01, -2.3459e-01, -2.7387e-01],\n",
       "                         [-7.7950e-02, -2.8888e-01, -1.0685e-01]],\n",
       "               \n",
       "                        [[-2.2439e-01, -2.8082e-01, -2.6374e-01],\n",
       "                         [ 1.8899e-01,  1.0728e-01,  2.1259e-01],\n",
       "                         [ 9.9554e-02,  1.6593e-01,  2.4591e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-2.1974e-01, -1.0048e-02,  5.4799e-02, -1.6550e-01,  4.5056e-02,\n",
       "                        4.6803e-02, -4.2921e-02, -4.6358e-02, -7.3666e-03, -1.0152e-02,\n",
       "                       -3.3494e-02,  3.7006e-02, -5.0691e-02, -4.0396e-02, -1.9255e-01,\n",
       "                       -3.0390e-02,  1.3884e-01, -7.9025e-03,  3.2168e-02, -3.7331e-03,\n",
       "                        2.3686e-02,  2.3907e-03,  1.7946e-02, -1.6191e-02, -1.1648e-01,\n",
       "                        1.5137e-02,  3.0031e-02,  5.5802e-02,  3.0414e-02,  9.4580e-02,\n",
       "                        8.5888e-03, -3.5225e-03,  2.7190e-03,  1.4011e-02, -1.0153e-01,\n",
       "                        8.7998e-03,  1.8984e-02, -7.8792e-02, -6.4151e-03,  3.4397e-02,\n",
       "                       -2.4677e-01, -2.7835e-01,  4.7902e-02, -6.6826e-03, -7.2411e-02,\n",
       "                       -9.7357e-02,  8.7957e-02,  1.0613e-03,  6.6394e-02, -3.0103e-02,\n",
       "                       -8.2059e-02,  1.7308e-01, -2.3401e-01,  1.8221e-04,  8.4949e-02,\n",
       "                        9.2546e-03, -5.6294e-02, -5.7813e-02, -7.6219e-02,  1.3246e-02,\n",
       "                        6.3603e-02,  2.2194e-02,  8.0497e-02,  1.1162e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.0768, -0.2647, -0.5560, -0.1482, -0.1001, -0.5294, -0.5443,  0.5139,\n",
       "                       -0.7352, -0.5212, -0.2146,  0.3618, -0.0911, -0.4789, -0.2472, -0.0073,\n",
       "                        0.1666, -0.6075, -0.8713, -0.0098, -0.8245, -0.5607, -0.4030, -0.8242,\n",
       "                       -0.6997, -0.4335, -0.3676, -0.5829, -0.1611, -0.0993,  0.3263, -0.2027,\n",
       "                       -0.0942, -0.5569, -0.0435, -0.6011, -0.2119,  0.3112, -0.4785, -0.6165,\n",
       "                       -0.1936,  1.5433, -0.3875,  0.3764,  0.1528, -0.2944, -0.5504, -0.3287,\n",
       "                       -0.2695, -0.6077, -0.7200,  0.0224,  0.9720, -0.3319, -0.6574, -0.7891,\n",
       "                        0.3938,  0.4673,  0.1754, -0.0224, -0.0829,  0.1660, -0.1263, -0.8972],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.1060, 1.0835, 0.5497, 1.6271, 1.0290, 0.6296, 0.5750, 1.5240, 0.6860,\n",
       "                       0.6148, 1.1041, 1.0063, 0.9291, 0.8256, 1.6631, 0.8922, 1.0038, 1.0168,\n",
       "                       0.9262, 0.6544, 0.9209, 0.5810, 1.0364, 0.6603, 0.8955, 0.6233, 0.6575,\n",
       "                       0.9348, 0.5648, 0.7168, 0.8408, 0.5489, 1.7692, 0.5362, 0.5860, 0.7129,\n",
       "                       0.6302, 0.8030, 0.7014, 1.2564, 1.3981, 1.2509, 0.9444, 1.0753, 0.9766,\n",
       "                       1.0994, 0.9111, 0.6100, 0.9571, 0.9412, 0.9195, 1.0522, 0.9301, 0.6162,\n",
       "                       0.9221, 0.6261, 0.9075, 1.1481, 1.0851, 0.9191, 1.3969, 1.3580, 0.7608,\n",
       "                       1.0898], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-0.0552, -0.0518, -0.0169],\n",
       "                         [-0.1781, -0.2258, -0.0171],\n",
       "                         [ 0.0013, -0.0646,  0.2396]],\n",
       "               \n",
       "                        [[ 0.1803,  0.0029, -0.1969],\n",
       "                         [-0.0629, -0.2176, -0.1148],\n",
       "                         [ 0.1275,  0.2232,  0.4426]],\n",
       "               \n",
       "                        [[-0.1052, -0.1239, -0.2432],\n",
       "                         [ 0.0550, -0.0748, -0.1509],\n",
       "                         [-0.1113, -0.0903, -0.1133]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0363, -0.2232, -0.1212],\n",
       "                         [-0.4827, -0.3564,  0.2465],\n",
       "                         [-0.1428, -0.1749,  0.3656]],\n",
       "               \n",
       "                        [[ 0.1821, -0.0083, -0.0814],\n",
       "                         [ 0.1046,  0.0648, -0.0229],\n",
       "                         [ 0.1235,  0.0611, -0.0104]],\n",
       "               \n",
       "                        [[-0.4950, -0.3531, -0.3406],\n",
       "                         [-0.3248, -0.0720, -0.0386],\n",
       "                         [ 0.2774,  0.5066,  0.3938]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0398,  0.2929, -0.0359],\n",
       "                         [ 0.0497, -0.0617, -0.1706],\n",
       "                         [ 0.0607, -0.0114,  0.2030]],\n",
       "               \n",
       "                        [[ 0.1720,  0.0889,  0.3912],\n",
       "                         [-0.3584, -0.3514, -0.4373],\n",
       "                         [-0.1198,  0.2985,  0.3948]],\n",
       "               \n",
       "                        [[ 0.3818,  0.4086,  0.4432],\n",
       "                         [ 0.2988,  0.2066,  0.3699],\n",
       "                         [ 0.1554,  0.2125,  0.3941]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.3745, -0.2107, -0.2418],\n",
       "                         [ 0.0668,  0.0723, -0.1193],\n",
       "                         [ 0.1658,  0.1627,  0.1326]],\n",
       "               \n",
       "                        [[-0.2157,  0.1014,  0.1957],\n",
       "                         [-0.3181,  0.0211,  0.0886],\n",
       "                         [-0.2779,  0.0960,  0.3412]],\n",
       "               \n",
       "                        [[-0.2660,  0.0486,  0.1427],\n",
       "                         [-0.7072, -0.5646,  0.1724],\n",
       "                         [ 0.0995, -0.0541,  0.3805]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0232, -0.2219, -0.1077],\n",
       "                         [-0.1118, -0.2023,  0.1575],\n",
       "                         [-0.2803, -0.1475,  0.1692]],\n",
       "               \n",
       "                        [[ 0.0268, -0.0475, -0.1938],\n",
       "                         [ 0.0765,  0.0412, -0.0289],\n",
       "                         [ 0.1761,  0.1244,  0.0261]],\n",
       "               \n",
       "                        [[-0.1348, -0.3150, -0.4037],\n",
       "                         [-0.4065, -0.2704, -0.1299],\n",
       "                         [-0.2877, -0.1691, -0.0063]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0530, -0.1342, -0.1449],\n",
       "                         [-0.2913, -0.2642, -0.3326],\n",
       "                         [-0.1452, -0.2391, -0.2708]],\n",
       "               \n",
       "                        [[-0.5667, -0.6442, -0.3987],\n",
       "                         [-0.3698, -0.7739, -0.5180],\n",
       "                         [-0.2811, -0.4799, -0.3650]],\n",
       "               \n",
       "                        [[ 0.0851,  0.2213, -0.0459],\n",
       "                         [-0.0345,  0.0758, -0.0379],\n",
       "                         [ 0.1332, -0.0888, -0.1837]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0603,  0.0942,  0.1174],\n",
       "                         [ 0.3295, -0.0303,  0.2227],\n",
       "                         [ 0.0760, -0.1590, -0.0065]],\n",
       "               \n",
       "                        [[ 0.0974, -0.1822, -0.2166],\n",
       "                         [ 0.0057, -0.1355, -0.1014],\n",
       "                         [ 0.0107, -0.0575,  0.3266]],\n",
       "               \n",
       "                        [[-0.0316, -0.0129,  0.0799],\n",
       "                         [ 0.0446, -0.1721,  0.0043],\n",
       "                         [ 0.1083, -0.1404, -0.1183]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1889,  0.0471,  0.2256],\n",
       "                         [ 0.0098, -0.1343, -0.0266],\n",
       "                         [ 0.0194,  0.0791,  0.0756]],\n",
       "               \n",
       "                        [[-0.1381,  0.0616,  0.0579],\n",
       "                         [ 0.1207,  0.0075, -0.0396],\n",
       "                         [ 0.1610,  0.2292,  0.0773]],\n",
       "               \n",
       "                        [[-0.8341, -1.0011, -0.0856],\n",
       "                         [-0.3181, -0.6751, -0.2187],\n",
       "                         [ 0.0054, -0.0658,  0.2669]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0481,  0.2880,  0.0227],\n",
       "                         [ 0.1393,  0.3853, -0.1632],\n",
       "                         [ 0.1718, -0.0191, -0.2305]],\n",
       "               \n",
       "                        [[ 0.2171, -0.0119,  0.0777],\n",
       "                         [ 0.1122, -0.0118, -0.3004],\n",
       "                         [ 0.2159, -0.1331, -0.2630]],\n",
       "               \n",
       "                        [[ 0.1100,  0.2175,  0.2207],\n",
       "                         [-0.0384,  0.2904,  0.2453],\n",
       "                         [ 0.1433,  0.2702,  0.2972]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0672, -0.3800, -0.2988],\n",
       "                         [-0.1442, -0.2913, -0.1250],\n",
       "                         [-0.0353, -0.2721, -0.3683]],\n",
       "               \n",
       "                        [[-0.1636, -0.2470, -0.2344],\n",
       "                         [-0.3119, -0.2328, -0.1382],\n",
       "                         [-0.2240, -0.1746, -0.0839]],\n",
       "               \n",
       "                        [[ 0.1797, -0.0133, -0.0865],\n",
       "                         [ 0.1685, -0.1825,  0.1089],\n",
       "                         [ 0.2382,  0.0653,  0.1454]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1568,  0.0459, -0.2911],\n",
       "                         [-0.1245,  0.3810,  0.0741],\n",
       "                         [ 0.0992, -0.0885, -0.0811]],\n",
       "               \n",
       "                        [[ 0.0303, -0.0527, -0.1356],\n",
       "                         [ 0.0211,  0.2111, -0.0050],\n",
       "                         [-0.1571,  0.0879,  0.0663]],\n",
       "               \n",
       "                        [[-0.1415,  0.1957,  0.4150],\n",
       "                         [ 0.0401, -0.0891, -0.0463],\n",
       "                         [-0.1121, -0.1589, -0.2461]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.4485, -0.1227,  0.0774],\n",
       "                         [ 0.2247, -0.0581,  0.2494],\n",
       "                         [ 0.1815,  0.0386,  0.2631]],\n",
       "               \n",
       "                        [[ 0.0708, -0.0612, -0.1494],\n",
       "                         [-0.4773, -0.3249,  0.1699],\n",
       "                         [-0.2335, -0.1831, -0.1130]],\n",
       "               \n",
       "                        [[ 0.2165,  0.1236, -0.0844],\n",
       "                         [ 0.4394,  0.1509, -0.1244],\n",
       "                         [ 0.0283, -0.1923,  0.2237]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-0.0087, -0.0092,  0.0071,  0.0222, -0.0091, -0.0087, -0.0070, -0.0127,\n",
       "                        0.0080,  0.0183, -0.0104, -0.0016,  0.0342,  0.0221, -0.0132,  0.0103,\n",
       "                       -0.0004,  0.0005, -0.0046,  0.0048, -0.0297, -0.0077, -0.0106, -0.0171,\n",
       "                       -0.0163,  0.0102, -0.0218, -0.0176,  0.0129, -0.0115,  0.0075,  0.0166,\n",
       "                        0.0260,  0.0119,  0.0018, -0.0074,  0.0103,  0.0019, -0.0238, -0.0043,\n",
       "                        0.0044,  0.0177,  0.0220, -0.0128, -0.0046, -0.0046,  0.0065,  0.0227,\n",
       "                       -0.0189, -0.0592,  0.0047,  0.0046, -0.0014,  0.0279,  0.0152, -0.0207,\n",
       "                       -0.0181,  0.0128,  0.0132,  0.0062,  0.0071,  0.0219,  0.0362,  0.0067],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.7223, -0.5630, -0.5199, -0.6213, -0.5990, -0.5255, -0.6143, -0.3874,\n",
       "                       -0.4352, -0.5472, -0.4839, -0.2642, -0.6268, -0.5430, -0.4818, -0.1934,\n",
       "                       -0.3193, -0.3709, -0.5040, -0.4137, -0.6651, -0.3381, -0.3062, -0.6898,\n",
       "                       -0.5156, -0.3250, -0.5559, -0.4577, -0.5768, -0.3916, -0.7649, -0.6429,\n",
       "                       -0.4531, -0.7730, -0.3959, -0.5099, -0.2668, -0.5467, -0.6896, -1.0173,\n",
       "                       -0.4411, -0.4857, -0.4778, -0.7194, -0.4136, -0.3690, -0.4334, -0.5637,\n",
       "                       -0.6931, -0.9836, -0.8214, -0.3821, -0.4799, -0.5513, -0.4797, -0.5274,\n",
       "                       -0.5071, -0.5073, -0.5482, -0.4414, -0.6790, -0.4656, -0.5285, -0.8491],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.0377, 0.7067, 0.8707, 1.1156, 1.1022, 1.1092, 1.0487, 1.0471, 0.7609,\n",
       "                       0.7529, 0.7082, 1.2359, 0.8646, 0.8491, 0.9485, 0.8643, 0.9119, 1.0798,\n",
       "                       0.8896, 0.9068, 1.0101, 0.9584, 0.8229, 1.0330, 0.8362, 1.0023, 0.8208,\n",
       "                       0.8197, 0.7373, 1.0089, 0.7694, 0.9002, 0.9060, 0.9308, 0.7601, 0.7411,\n",
       "                       1.0108, 1.1356, 0.8865, 1.2111, 0.9471, 0.8133, 0.6516, 1.0893, 0.8124,\n",
       "                       0.6728, 0.7589, 0.8695, 0.9530, 1.4721, 1.1368, 0.7226, 0.9697, 0.7728,\n",
       "                       0.9932, 1.1010, 1.0558, 0.9981, 0.8223, 0.8246, 1.1807, 0.9223, 0.8480,\n",
       "                       0.9001], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-8.8947e-02, -9.6673e-02, -3.9948e-03],\n",
       "                         [-1.5530e-02, -1.1609e-01, -9.4092e-02],\n",
       "                         [ 5.6432e-02, -9.1169e-02,  9.1794e-02]],\n",
       "               \n",
       "                        [[-1.2121e-01, -2.2285e-01, -1.0858e-01],\n",
       "                         [-2.6728e-01, -2.6124e-02,  1.9724e-01],\n",
       "                         [ 5.2824e-02, -7.1375e-02, -8.0869e-02]],\n",
       "               \n",
       "                        [[-1.1086e-03, -3.0246e-02,  1.4816e-01],\n",
       "                         [ 6.0217e-02, -1.4101e-01,  5.9081e-02],\n",
       "                         [ 1.2289e-01, -1.4247e-01,  1.1244e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.8901e-01,  6.8924e-02,  1.0366e-01],\n",
       "                         [-7.8722e-02, -3.6566e-01, -3.8332e-01],\n",
       "                         [ 9.4160e-02, -7.4724e-02, -1.3604e-01]],\n",
       "               \n",
       "                        [[-3.1832e-01, -2.2688e-01,  2.7675e-02],\n",
       "                         [-3.4762e-01, -2.4889e-01,  2.8266e-01],\n",
       "                         [-7.2806e-03,  1.3155e-01,  7.8612e-02]],\n",
       "               \n",
       "                        [[ 2.7913e-01,  2.8868e-01,  2.6588e-01],\n",
       "                         [ 2.5647e-01,  8.9225e-02,  2.4742e-01],\n",
       "                         [ 2.9269e-02,  2.7606e-02,  2.8122e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1088e-01, -7.3156e-01, -1.0811e+00],\n",
       "                         [ 4.5897e-01, -4.2384e-01, -7.5408e-01],\n",
       "                         [ 1.4177e-01, -4.4486e-01, -7.0054e-01]],\n",
       "               \n",
       "                        [[ 9.9812e-02, -2.3903e-01, -1.3354e-01],\n",
       "                         [-1.7921e-01, -2.8784e-01, -2.5782e-01],\n",
       "                         [-8.4262e-02,  1.2109e-01,  3.9641e-02]],\n",
       "               \n",
       "                        [[ 1.4976e-01,  3.2172e-02, -6.3651e-02],\n",
       "                         [ 1.8591e-01,  1.7183e-01,  4.1664e-03],\n",
       "                         [ 8.6497e-02,  2.9568e-01,  2.2874e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0941e-01, -1.0850e-01, -1.1813e-01],\n",
       "                         [-1.5011e-01,  7.3631e-02, -9.9876e-02],\n",
       "                         [-2.1692e-01, -8.1490e-02, -1.9925e-01]],\n",
       "               \n",
       "                        [[-5.9069e-02, -1.2191e-01, -2.1176e-01],\n",
       "                         [-2.2837e-01,  6.0312e-03,  2.9756e-02],\n",
       "                         [-2.8850e-01,  1.5011e-02, -2.1544e-01]],\n",
       "               \n",
       "                        [[ 6.8947e-02,  3.3931e-01,  1.7286e-01],\n",
       "                         [-3.3684e-01, -1.0952e-01,  1.3482e-01],\n",
       "                         [-2.0515e-01,  1.4422e-01,  8.0730e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1866e-01,  5.3614e-02,  6.5315e-02],\n",
       "                         [-2.8851e-01, -2.7413e-01, -1.8471e-01],\n",
       "                         [-2.0930e-01, -1.4442e-01, -2.8129e-01]],\n",
       "               \n",
       "                        [[-2.6725e-02, -2.0578e-01, -1.5601e-01],\n",
       "                         [ 2.1598e-01, -2.1026e-02, -3.5384e-01],\n",
       "                         [ 7.8481e-02, -9.7343e-02, -2.5198e-01]],\n",
       "               \n",
       "                        [[ 1.2725e-01, -1.2808e-01,  2.0151e-01],\n",
       "                         [-2.8450e-02,  5.7556e-02,  2.1256e-01],\n",
       "                         [-5.9615e-02, -1.5819e-01, -9.0206e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.0288e-01, -2.0952e-02,  5.5108e-02],\n",
       "                         [-1.0897e-01, -1.4218e-01, -1.5323e-02],\n",
       "                         [-7.3600e-02, -2.7061e-01, -8.1098e-02]],\n",
       "               \n",
       "                        [[-5.1316e-02,  1.4601e-01,  1.1121e-01],\n",
       "                         [-6.2586e-02, -6.3342e-02, -7.1862e-02],\n",
       "                         [-2.0287e-01, -8.3704e-02, -3.5511e-02]],\n",
       "               \n",
       "                        [[ 5.8441e-02,  2.2368e-01,  2.0253e-01],\n",
       "                         [ 2.0511e-01,  7.8082e-02,  4.3241e-02],\n",
       "                         [-1.7961e-02,  1.1176e-01, -1.1735e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.5865e-03,  2.0013e-01, -1.7829e-01],\n",
       "                         [-6.5540e-02,  2.5779e-03, -1.1012e-01],\n",
       "                         [-1.6716e-01, -7.0334e-01, -1.4243e-01]],\n",
       "               \n",
       "                        [[-4.2519e-01, -4.2180e-01, -2.5056e-01],\n",
       "                         [-4.5262e-01, -3.7827e-01, -5.7711e-02],\n",
       "                         [ 2.5666e-01, -8.7549e-02,  2.2904e-01]],\n",
       "               \n",
       "                        [[ 4.2328e-02, -2.6974e-02,  2.4277e-01],\n",
       "                         [ 4.1234e-02, -9.2512e-02,  1.3896e-01],\n",
       "                         [ 8.6447e-02, -1.2768e-01,  9.8358e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5787e-01,  2.2987e-01, -1.5920e-01],\n",
       "                         [ 2.1384e-01, -3.7154e-01, -4.5396e-01],\n",
       "                         [ 1.4524e-01, -6.8086e-01, -3.3288e-01]],\n",
       "               \n",
       "                        [[-1.8669e-01, -9.2962e-02,  8.9025e-02],\n",
       "                         [ 6.4377e-02,  2.5184e-01,  4.7494e-02],\n",
       "                         [ 1.0999e-01, -9.7002e-02,  3.2453e-01]],\n",
       "               \n",
       "                        [[ 1.1896e-01,  3.6262e-02, -4.5817e-01],\n",
       "                         [ 2.2589e-01, -3.1837e-01, -2.3807e-01],\n",
       "                         [ 1.5791e-01, -4.9304e-01,  9.9954e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.0212e-01, -2.6165e-01, -2.1175e-01],\n",
       "                         [-3.3859e-03,  3.3830e-02,  8.2777e-02],\n",
       "                         [-7.6809e-02,  3.8688e-01,  3.4634e-01]],\n",
       "               \n",
       "                        [[-4.1318e-01,  6.0619e-02,  1.6295e-02],\n",
       "                         [-3.6345e-01, -3.5207e-01,  1.0681e-01],\n",
       "                         [-4.6782e-02, -3.0050e-01,  2.1442e-01]],\n",
       "               \n",
       "                        [[ 1.3472e-01,  2.5350e-01,  1.2164e-01],\n",
       "                         [ 2.2068e-02,  2.6988e-01,  5.3131e-02],\n",
       "                         [-2.3855e-01,  1.7927e-01, -2.5914e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2600e-01,  5.0094e-03, -3.9816e-01],\n",
       "                         [-3.2816e-02,  2.0372e-01, -1.8098e-01],\n",
       "                         [ 6.3464e-02,  2.1729e-01,  9.6025e-03]],\n",
       "               \n",
       "                        [[-6.4930e-02, -1.6863e-01, -8.4929e-02],\n",
       "                         [ 2.3301e-02, -1.4895e-01, -2.9705e-01],\n",
       "                         [-7.1422e-02, -1.4142e-01, -1.2580e-01]],\n",
       "               \n",
       "                        [[ 6.5445e-01,  2.5667e-01,  2.0544e-01],\n",
       "                         [-4.3627e-02, -7.6820e-02, -2.0133e-01],\n",
       "                         [-2.3129e-01, -2.3626e-01,  1.5683e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.9024e-01,  6.8006e-02,  3.1571e-01],\n",
       "                         [ 7.4362e-02, -2.1441e-01,  3.3295e-01],\n",
       "                         [ 4.5818e-02, -1.4404e-01,  1.5520e-02]],\n",
       "               \n",
       "                        [[ 2.5976e-02, -5.8042e-02, -7.5722e-02],\n",
       "                         [-1.3456e-01, -2.2077e-01, -1.3269e-01],\n",
       "                         [-1.5131e-01, -2.2756e-01, -2.7982e-01]],\n",
       "               \n",
       "                        [[-2.9049e-01, -1.1146e-01,  1.5746e-01],\n",
       "                         [ 2.6669e-02, -2.1598e-01, -3.5773e-01],\n",
       "                         [-2.8724e-01, -1.7407e-01, -4.3434e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.1657e-02,  4.6368e-02, -1.5153e-01],\n",
       "                         [-1.5693e-01,  1.7010e-02, -1.2736e-01],\n",
       "                         [-1.2911e-01,  9.1136e-02, -1.0788e-01]],\n",
       "               \n",
       "                        [[-7.6717e-02, -5.1377e-01, -1.9287e-01],\n",
       "                         [-1.6914e-01, -2.1414e-01, -8.6451e-04],\n",
       "                         [-4.1195e-02, -2.0961e-01, -1.8368e-01]],\n",
       "               \n",
       "                        [[ 2.5996e-01,  3.2128e-01,  9.2766e-02],\n",
       "                         [ 3.5690e-01,  4.2198e-01,  1.5806e-01],\n",
       "                         [-3.1720e-02,  1.2629e-01,  1.0463e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 4.1250e-03,  3.7986e-02, -2.2236e-02, -4.9676e-02, -1.4213e-02,\n",
       "                        1.3658e-02, -4.7387e-02, -2.7228e-02, -1.7521e-02, -5.6051e-02,\n",
       "                       -5.9123e-03, -7.0252e-03,  3.1749e-03,  2.9750e-02,  7.7557e-03,\n",
       "                        8.5977e-03, -3.9357e-03, -2.2359e-02, -2.5177e-02, -1.0459e-02,\n",
       "                        5.4773e-03, -3.2566e-02, -1.6284e-02, -2.4308e-02,  1.3500e-02,\n",
       "                       -1.7912e-02,  2.3080e-03,  3.6633e-02, -3.0188e-03,  2.3883e-02,\n",
       "                       -7.3679e-03,  7.3331e-03, -5.1378e-03,  7.0333e-02,  8.1474e-03,\n",
       "                        4.2475e-02, -1.6598e-03, -1.6736e-02, -6.3913e-03,  1.9204e-02,\n",
       "                       -4.9940e-02, -1.2806e-02,  6.7781e-02,  4.2965e-02,  1.2168e-02,\n",
       "                       -4.9881e-02, -1.0214e-01,  1.0897e-02,  3.5558e-02,  1.0061e-01,\n",
       "                       -4.3778e-02, -1.8507e-02, -2.0792e-02,  8.2172e-03,  1.2072e-02,\n",
       "                        6.1623e-02, -1.1852e-02, -1.8381e-02,  3.1606e-03, -3.7534e-03,\n",
       "                        2.8484e-05, -2.8674e-02, -3.4160e-02,  3.2097e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.6017, -0.4780, -0.4892, -0.8492, -1.0702, -0.7887, -0.8295, -0.6969,\n",
       "                       -0.3358, -0.6247, -0.9939, -0.5285, -0.9254, -0.5851, -0.9856, -0.5461,\n",
       "                       -0.8158, -0.4743, -0.8483, -0.6238, -0.6817, -0.4769, -0.4211, -1.0376,\n",
       "                       -0.9802, -0.6869, -0.6768, -0.8752, -0.7253, -0.6898, -0.6401, -0.6538,\n",
       "                       -0.8286, -0.6262, -0.4836, -0.6692, -0.8096, -0.5973, -0.5231, -0.4769,\n",
       "                       -0.7063, -1.5868, -0.6896, -1.8907, -0.7027, -1.0255, -0.7266, -0.5361,\n",
       "                       -0.6326, -0.8311, -1.1398, -0.7910, -0.8678, -0.6213, -0.5821, -1.0058,\n",
       "                       -0.8971, -0.6464, -0.3720, -0.6176, -0.9121, -0.6968, -0.6827, -0.6903],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.6571, 0.7139, 0.7157, 0.8339, 0.9776, 0.8622, 1.0132, 0.7052, 0.6320,\n",
       "                       0.8380, 0.8747, 0.8164, 0.9219, 0.7002, 0.8030, 0.6590, 0.8126, 0.6803,\n",
       "                       0.9719, 0.6998, 0.8286, 0.7268, 0.6386, 1.0455, 0.9501, 0.8360, 0.6325,\n",
       "                       0.9914, 0.8301, 0.9838, 0.8084, 0.7438, 0.9850, 1.0082, 0.7621, 0.7778,\n",
       "                       0.9250, 0.7411, 0.7639, 0.7104, 0.7910, 0.7173, 0.9910, 1.0829, 0.7336,\n",
       "                       0.9676, 0.8398, 0.7753, 0.7975, 0.8226, 0.9543, 0.8884, 0.9038, 0.7794,\n",
       "                       0.8419, 0.9781, 0.9874, 0.7961, 0.6324, 0.8170, 0.7838, 0.7380, 0.9042,\n",
       "                       0.9094], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-1.7067e-01, -1.3066e-02, -1.7701e-01],\n",
       "                         [-1.1037e-01, -9.3368e-02, -1.0649e-01],\n",
       "                         [ 3.1819e-02, -8.6203e-02, -3.0124e-02]],\n",
       "               \n",
       "                        [[-1.6657e-01,  7.2936e-02,  1.0613e-01],\n",
       "                         [-2.4644e-02,  3.7754e-02,  1.1152e-01],\n",
       "                         [ 1.7000e-01,  2.5106e-01,  3.5388e-01]],\n",
       "               \n",
       "                        [[ 9.5263e-02,  1.5322e-01,  1.0976e-01],\n",
       "                         [-2.1412e-02, -3.0098e-02,  2.0796e-02],\n",
       "                         [ 2.3298e-01, -1.3165e-01,  2.1547e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.4961e-01, -1.9217e-01, -4.2393e-01],\n",
       "                         [-1.7386e-01, -1.5587e-01, -2.5135e-01],\n",
       "                         [-1.4756e-01, -1.6373e-01, -2.9216e-01]],\n",
       "               \n",
       "                        [[ 1.9778e-01, -1.4796e-01,  1.2410e-01],\n",
       "                         [ 4.6801e-02, -2.4166e-01,  1.0877e-01],\n",
       "                         [-2.4004e-03, -1.8757e-01, -2.1509e-01]],\n",
       "               \n",
       "                        [[ 2.0079e-01,  1.4096e-01,  2.9325e-01],\n",
       "                         [ 2.2767e-01,  3.1308e-02,  1.7275e-01],\n",
       "                         [ 2.5716e-01,  9.4600e-02,  1.9197e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.0705e-01,  1.3311e-01,  1.1247e-01],\n",
       "                         [-5.5667e-02,  1.1988e-01,  8.3343e-03],\n",
       "                         [-1.5005e-01,  9.3824e-02, -7.1935e-02]],\n",
       "               \n",
       "                        [[-1.6594e-01, -2.0657e-01, -2.3828e-01],\n",
       "                         [ 2.0826e-02, -1.6480e-01, -1.2982e-01],\n",
       "                         [-1.2433e-01, -2.0697e-01, -3.3013e-01]],\n",
       "               \n",
       "                        [[ 1.2100e-01, -6.9613e-02, -3.4620e-02],\n",
       "                         [-5.2852e-02, -7.1432e-02, -3.5363e-02],\n",
       "                         [-2.0287e-02, -2.2322e-01, -2.7510e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7025e-01,  6.0503e-03,  1.5624e-01],\n",
       "                         [ 6.2056e-02,  2.3598e-02,  7.6279e-02],\n",
       "                         [-8.7558e-02, -6.1394e-02, -1.8677e-02]],\n",
       "               \n",
       "                        [[ 1.8419e-01,  2.6918e-01,  6.2228e-01],\n",
       "                         [ 1.6490e-01,  8.7511e-02,  4.5316e-01],\n",
       "                         [ 1.8446e-01,  3.7751e-02,  6.4550e-01]],\n",
       "               \n",
       "                        [[ 1.1532e-01, -4.7652e-03, -1.9973e-01],\n",
       "                         [ 2.4383e-01, -4.9781e-02, -5.2043e-02],\n",
       "                         [-1.1410e-01, -1.9317e-01, -1.5489e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4215e-02,  4.1816e-02, -1.0965e-01],\n",
       "                         [ 2.4125e-01,  2.3535e-01, -6.3894e-02],\n",
       "                         [ 1.2636e-01,  1.6674e-01,  1.4834e-01]],\n",
       "               \n",
       "                        [[ 7.8367e-03,  1.8051e-01, -6.5671e-02],\n",
       "                         [ 9.8078e-03,  1.7806e-01,  1.0109e-01],\n",
       "                         [-2.2050e-01, -2.4391e-01, -1.4126e-01]],\n",
       "               \n",
       "                        [[-5.9175e-02, -2.6013e-01, -2.0994e-01],\n",
       "                         [ 1.0115e-01, -2.5451e-01, -2.1587e-01],\n",
       "                         [-2.5809e-01, -2.3031e-01, -2.7370e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.3602e-02, -9.6916e-02, -1.5441e-01],\n",
       "                         [ 1.5569e-01,  2.3636e-01, -1.3609e-01],\n",
       "                         [-2.9290e-02,  2.1860e-01, -2.6863e-02]],\n",
       "               \n",
       "                        [[-2.3563e-02, -2.7303e-01,  1.0055e-01],\n",
       "                         [-2.7075e-01, -2.7049e-01, -2.1870e-02],\n",
       "                         [-2.5540e-01, -2.9596e-01, -2.5480e-02]],\n",
       "               \n",
       "                        [[-2.3743e-01, -1.8762e-01,  1.3507e-01],\n",
       "                         [ 1.5412e-02, -8.7452e-02,  8.3685e-02],\n",
       "                         [ 1.0808e-01, -2.1368e-01,  1.0315e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3657e-02,  2.1707e-01, -1.1502e-01],\n",
       "                         [ 4.6054e-02, -3.3525e-02, -1.6309e-01],\n",
       "                         [-1.2973e-01, -2.6311e-01, -2.5469e-01]],\n",
       "               \n",
       "                        [[ 1.8950e-01,  1.1237e-01,  1.7514e-01],\n",
       "                         [ 9.3067e-02,  7.5957e-02,  2.4685e-01],\n",
       "                         [ 2.6706e-01,  1.1448e-01,  2.6876e-01]],\n",
       "               \n",
       "                        [[ 1.0918e-01, -7.5168e-02, -2.5756e-01],\n",
       "                         [-1.5880e-01, -1.2850e-01, -4.1996e-01],\n",
       "                         [-1.8728e-01, -5.1427e-01, -4.1481e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.2109e-02, -7.7912e-02, -1.6402e-02],\n",
       "                         [-1.9518e-02,  5.8579e-02,  5.4888e-04],\n",
       "                         [-1.5492e-02,  6.8988e-03,  2.0721e-02]],\n",
       "               \n",
       "                        [[-2.1506e-01, -1.8461e-02,  5.1667e-02],\n",
       "                         [-7.9244e-02, -2.2890e-01, -5.7329e-02],\n",
       "                         [-3.9364e-01, -6.3303e-01, -4.0065e-01]],\n",
       "               \n",
       "                        [[ 2.7630e-01,  1.4794e-01, -1.9473e-02],\n",
       "                         [-4.2248e-02,  5.8114e-02, -1.9232e-01],\n",
       "                         [ 5.0234e-02,  2.0128e-01,  7.8026e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.6807e-01,  3.7591e-02,  2.9190e-01],\n",
       "                         [-3.5815e-01,  1.9374e-01,  7.9991e-02],\n",
       "                         [-3.8051e-01,  5.4600e-02,  2.7299e-01]],\n",
       "               \n",
       "                        [[ 4.9207e-02, -3.2392e-01, -2.3834e-01],\n",
       "                         [ 2.4520e-01,  1.8751e-01,  1.8917e-01],\n",
       "                         [ 3.2089e-04, -5.4633e-02, -2.7677e-02]],\n",
       "               \n",
       "                        [[ 5.5989e-02, -1.9251e-01, -2.9557e-01],\n",
       "                         [ 2.5636e-01, -2.4829e-02, -1.3211e-01],\n",
       "                         [ 1.4171e-01, -2.3610e-02,  5.8787e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-9.2428e-02, -2.0456e-01,  8.3478e-02],\n",
       "                         [-2.3927e-01, -3.9887e-01, -1.5866e-01],\n",
       "                         [-1.1212e-01, -1.1777e-01, -1.2569e-01]],\n",
       "               \n",
       "                        [[ 5.7468e-01,  2.6531e-01,  1.4976e-02],\n",
       "                         [ 7.2242e-01,  4.4700e-01, -1.3288e-01],\n",
       "                         [ 4.9329e-01,  3.5156e-01, -5.7665e-02]],\n",
       "               \n",
       "                        [[ 1.6617e-03,  7.8952e-02, -9.3825e-03],\n",
       "                         [ 1.9070e-01,  2.5640e-01,  9.5074e-02],\n",
       "                         [ 1.7011e-02,  1.3433e-01,  8.2523e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.5604e-01, -3.7159e-01, -2.4709e-01],\n",
       "                         [-3.8478e-01, -1.7239e-01, -3.7673e-01],\n",
       "                         [-7.0394e-01, -4.8626e-01, -6.1311e-01]],\n",
       "               \n",
       "                        [[-1.4100e-01, -1.4097e-01, -8.7667e-02],\n",
       "                         [ 6.1109e-02, -1.6246e-01,  4.6355e-02],\n",
       "                         [-2.1542e-01, -2.9154e-01, -1.4383e-01]],\n",
       "               \n",
       "                        [[-8.5900e-03, -5.7570e-02, -3.8405e-02],\n",
       "                         [-6.1285e-02, -2.2596e-01, -1.1702e-01],\n",
       "                         [ 1.6398e-01, -1.0532e-01, -1.7420e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.6589e-01, -1.7009e-01, -4.8729e-02],\n",
       "                         [ 1.5189e-03, -2.2084e-01, -1.2078e-01],\n",
       "                         [ 1.4541e-01, -1.7820e-01, -7.6363e-02]],\n",
       "               \n",
       "                        [[-1.1031e-01, -2.2752e-01, -3.9630e-02],\n",
       "                         [-7.5092e-02, -3.0361e-03,  1.2115e-01],\n",
       "                         [-1.0173e-01, -3.3766e-01,  6.9194e-02]],\n",
       "               \n",
       "                        [[ 7.6225e-02,  1.3244e-01,  4.0945e-01],\n",
       "                         [-2.5315e-01,  3.1414e-02,  7.7669e-02],\n",
       "                         [-1.7007e-01,  1.5014e-01,  1.8012e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 4.1641e-02, -1.0669e-01, -1.3972e-02,  2.8114e-02, -9.2975e-03,\n",
       "                        2.0619e-03, -1.1585e-01,  3.8805e-02,  1.1243e-02,  2.0440e-02,\n",
       "                        1.9745e-02, -4.5448e-03,  3.0958e-02, -4.6014e-01, -7.8435e-02,\n",
       "                       -8.6226e-02, -3.3750e-02,  4.3969e-02, -3.9211e-03,  1.7982e-02,\n",
       "                        7.4458e-02,  2.0272e-03,  7.9179e-03,  1.1332e-02,  1.9535e-03,\n",
       "                        2.8692e-02,  1.3725e-02,  6.9201e-03,  1.5172e-02,  7.2591e-03,\n",
       "                       -1.0934e-01, -7.1052e-03, -2.1104e-02, -8.9992e-02, -7.2162e-02,\n",
       "                        2.3039e-02,  1.0898e-02, -2.0060e-02,  2.4481e-02,  7.0455e-04,\n",
       "                        1.7038e-03, -1.4990e-01,  7.0736e-02,  8.8229e-03,  1.6725e-01,\n",
       "                       -2.2029e-02, -9.4136e-02,  3.2394e-02,  4.1300e-02, -6.4366e-05,\n",
       "                        4.6533e-02, -7.4872e-03,  3.0531e-02, -3.9437e-02, -1.5623e-02,\n",
       "                        1.2684e-02,  1.7539e-02,  1.3746e-02,  1.2018e-02, -5.1134e-03,\n",
       "                       -4.8543e-03,  1.1518e-02,  9.7071e-03,  1.8686e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-1.0896, -2.1507, -1.8048, -0.8596, -1.0478, -1.0102, -1.4288, -1.4060,\n",
       "                       -0.9663, -1.5368, -1.5156, -0.8173, -1.8204, -2.2456, -0.9085, -1.1909,\n",
       "                       -0.7711, -1.3301, -0.8088, -1.0439, -1.5628, -0.9181, -1.1588, -1.2810,\n",
       "                       -1.0020, -0.8865, -1.0150, -1.0155, -0.9656, -1.5015, -2.3037, -0.9375,\n",
       "                       -1.1398, -1.6902, -1.1948, -1.7713, -0.9115, -1.5968, -1.3182, -0.7613,\n",
       "                       -1.7297, -2.1894, -2.0172, -1.3212, -1.4256, -2.0764, -1.6747, -0.9523,\n",
       "                       -1.5801, -0.9332, -1.1985, -1.2180, -1.6818, -1.5722, -1.6079, -1.3869,\n",
       "                       -0.9416, -0.9241, -1.2179, -1.3441, -0.8788, -1.0140, -1.1072, -0.7275],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([3.7842, 4.4471, 0.7056, 3.7946, 0.2785, 0.2738, 3.4043, 0.4481, 0.2609,\n",
       "                       0.7169, 3.9364, 0.2088, 0.4314, 5.0101, 3.3203, 3.2529, 3.5226, 0.6003,\n",
       "                       0.2146, 0.2651, 0.5497, 0.3230, 0.4740, 3.5539, 0.2871, 0.2918, 0.2972,\n",
       "                       0.3825, 3.2005, 0.3990, 3.9184, 0.2533, 3.2587, 3.6326, 3.3870, 0.8362,\n",
       "                       0.2742, 0.7150, 0.4313, 0.2199, 0.6827, 3.8503, 1.3627, 0.4611, 3.0792,\n",
       "                       3.3972, 3.5064, 0.2283, 0.9893, 0.2625, 3.3467, 3.1768, 0.6578, 3.8792,\n",
       "                       3.3881, 3.4488, 0.3257, 0.2208, 0.3248, 0.5026, 0.2992, 0.3435, 0.3278,\n",
       "                       2.9382], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0348, -0.0104, -0.0042,  ..., -0.0009,  0.0163,  0.0054],\n",
       "                       [-0.0141, -0.0083,  0.0053,  ...,  0.0157,  0.0218,  0.0393],\n",
       "                       [-0.0223,  0.0105,  0.0090,  ...,  0.0104,  0.0185,  0.0036],\n",
       "                       [-0.0093, -0.0106,  0.0012,  ...,  0.0246,  0.0343,  0.0062],\n",
       "                       [-0.0192,  0.0020,  0.0106,  ...,  0.0193,  0.0290, -0.0057]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.2154,  0.3053, -0.2302,  0.4531, -0.2380], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3821220500469207,\n",
       "   1.3021315078735352,\n",
       "   1.222449166893959,\n",
       "   1.1697446973323822,\n",
       "   1.126164562702179,\n",
       "   1.103199450969696,\n",
       "   1.0799383276700973,\n",
       "   1.0510324746370316,\n",
       "   1.0330799340009689,\n",
       "   1.0169329599142074,\n",
       "   0.9822105876207352,\n",
       "   0.9770566730499267,\n",
       "   0.9588655294179916,\n",
       "   0.9414263654947281,\n",
       "   0.9363355795145035,\n",
       "   0.9240059593915939,\n",
       "   0.8977977560758591,\n",
       "   0.8909568021297455,\n",
       "   0.8813100074529647,\n",
       "   0.8655572018027305,\n",
       "   0.8559504778385162,\n",
       "   0.8309543271064759,\n",
       "   0.8299630028605461,\n",
       "   0.8273259307146072,\n",
       "   0.8180275123119354,\n",
       "   0.8168431396484375,\n",
       "   0.7906347794532775,\n",
       "   0.7917161728739739,\n",
       "   0.7819899005889892,\n",
       "   0.7928046478033066,\n",
       "   0.7694970924854279,\n",
       "   0.76863211196661,\n",
       "   0.7561018224954605,\n",
       "   0.7385065902471543,\n",
       "   0.7591250451207161,\n",
       "   0.7319793934226037,\n",
       "   0.7312492675185204,\n",
       "   0.7306893970966339,\n",
       "   0.728378259241581,\n",
       "   0.7039069735407829,\n",
       "   0.7223251054286957,\n",
       "   0.7047018485069275,\n",
       "   0.700988232254982,\n",
       "   0.7032113144397736,\n",
       "   0.7018483471274376,\n",
       "   0.6943742069602012,\n",
       "   0.6895885868668556,\n",
       "   0.6779186080098152,\n",
       "   0.6803666565418244,\n",
       "   0.6810621865987778,\n",
       "   0.6812637149095535,\n",
       "   0.6557760425209999,\n",
       "   0.6690531409978867,\n",
       "   0.6529473427534104,\n",
       "   0.6755649086236953,\n",
       "   0.6613797073364258,\n",
       "   0.651308987736702,\n",
       "   0.6519375024437905,\n",
       "   0.6481107335090637,\n",
       "   0.6441545747518539,\n",
       "   0.6521966953873635,\n",
       "   0.645221735715866,\n",
       "   0.6315558581948281,\n",
       "   0.6255988692045211,\n",
       "   0.6410764248371125,\n",
       "   0.639128813803196,\n",
       "   0.6378846968412399,\n",
       "   0.6167481243610382,\n",
       "   0.6208667230606079,\n",
       "   0.6332033781409263,\n",
       "   0.6127314854860306,\n",
       "   0.6143889218568802,\n",
       "   0.619949030160904,\n",
       "   0.6137805005311966,\n",
       "   0.6032597521543502,\n",
       "   0.6025395417213439,\n",
       "   0.6121120390892029,\n",
       "   0.6128589915633201,\n",
       "   0.6039318307638168,\n",
       "   0.6086812917888165,\n",
       "   0.5973714723587036,\n",
       "   0.5980873594284057,\n",
       "   0.602192023396492,\n",
       "   0.5940385815501213,\n",
       "   0.5822140692174435,\n",
       "   0.5875552316308021,\n",
       "   0.5863675386309624,\n",
       "   0.5852665336132049,\n",
       "   0.5740595889687539,\n",
       "   0.5694547915458679,\n",
       "   0.5712518365979194,\n",
       "   0.5806549159288407,\n",
       "   0.5540645588338375,\n",
       "   0.5712025994658471,\n",
       "   0.5659674203395844,\n",
       "   0.5841565625071525,\n",
       "   0.5712108993530274,\n",
       "   0.5693458061218262,\n",
       "   0.5553330751657486],\n",
       "  'train_loss_std': [0.12801127557390604,\n",
       "   0.12336082410684515,\n",
       "   0.13635924083318587,\n",
       "   0.12554661332111852,\n",
       "   0.1268564065009036,\n",
       "   0.13544711002593973,\n",
       "   0.1497437622719848,\n",
       "   0.13938298808071456,\n",
       "   0.14253826310312293,\n",
       "   0.13639411986157854,\n",
       "   0.14389784873787342,\n",
       "   0.14896825266708055,\n",
       "   0.13228377645501033,\n",
       "   0.1470495758560407,\n",
       "   0.1430183231496096,\n",
       "   0.15262180547142867,\n",
       "   0.13596626441587675,\n",
       "   0.14440958256944372,\n",
       "   0.1449276504620866,\n",
       "   0.15165752260781132,\n",
       "   0.14392671275930444,\n",
       "   0.1333633977546889,\n",
       "   0.14625585779382916,\n",
       "   0.14823901382468152,\n",
       "   0.13968552510589854,\n",
       "   0.1522297783757191,\n",
       "   0.139405518945719,\n",
       "   0.14179371742156666,\n",
       "   0.14704305629774894,\n",
       "   0.152598378794665,\n",
       "   0.1427770380679407,\n",
       "   0.1422912184337323,\n",
       "   0.14663150556614643,\n",
       "   0.14574725713996067,\n",
       "   0.1483526925693327,\n",
       "   0.15155342968202987,\n",
       "   0.138991792476493,\n",
       "   0.15067771600291702,\n",
       "   0.15138181971458303,\n",
       "   0.146991086240644,\n",
       "   0.14832017308575354,\n",
       "   0.1430487216754083,\n",
       "   0.14266094133821608,\n",
       "   0.147983208705066,\n",
       "   0.1494397335129483,\n",
       "   0.14487249153418494,\n",
       "   0.1473558352941536,\n",
       "   0.1469758579229944,\n",
       "   0.14450294436820507,\n",
       "   0.14548173583634944,\n",
       "   0.14596319301365335,\n",
       "   0.13592101273973412,\n",
       "   0.14748962133254923,\n",
       "   0.14327280437489404,\n",
       "   0.15428859628335892,\n",
       "   0.14342698484877522,\n",
       "   0.13279665585419934,\n",
       "   0.14725479781096096,\n",
       "   0.1428647687169155,\n",
       "   0.1430698060436431,\n",
       "   0.14754276981507053,\n",
       "   0.14657789115111505,\n",
       "   0.14556498210199287,\n",
       "   0.13780205059561573,\n",
       "   0.1408331643027919,\n",
       "   0.1440561725858388,\n",
       "   0.1391446882355011,\n",
       "   0.14457839140728956,\n",
       "   0.13772532684458066,\n",
       "   0.14952820124358085,\n",
       "   0.1326991718957292,\n",
       "   0.13927216564107353,\n",
       "   0.13496276646947006,\n",
       "   0.14204750006733052,\n",
       "   0.13878890014447665,\n",
       "   0.13925840098856135,\n",
       "   0.1463542889185346,\n",
       "   0.14727189113373543,\n",
       "   0.13884061377155377,\n",
       "   0.140134462923132,\n",
       "   0.13697797506455173,\n",
       "   0.1421513839630234,\n",
       "   0.1507406849901963,\n",
       "   0.13563639436913083,\n",
       "   0.14141437078255903,\n",
       "   0.13027424436714932,\n",
       "   0.13702290075988652,\n",
       "   0.141461953506257,\n",
       "   0.13455943290498604,\n",
       "   0.1350746401142183,\n",
       "   0.1372223097718178,\n",
       "   0.14106655462301085,\n",
       "   0.1333278437591555,\n",
       "   0.14081110589591758,\n",
       "   0.14252111427568634,\n",
       "   0.15208774617171844,\n",
       "   0.1296614593295469,\n",
       "   0.13488985153868005,\n",
       "   0.13450310665712903],\n",
       "  'train_accuracy_mean': [0.428586667239666,\n",
       "   0.46876000064611434,\n",
       "   0.5082266674637794,\n",
       "   0.5347999989390373,\n",
       "   0.5569733330011368,\n",
       "   0.5652266656160354,\n",
       "   0.5748399984240532,\n",
       "   0.5897466659545898,\n",
       "   0.5980399978160859,\n",
       "   0.6031999994516373,\n",
       "   0.6182266660332679,\n",
       "   0.6209733339548111,\n",
       "   0.6296800004839898,\n",
       "   0.6376800001859665,\n",
       "   0.6394933329820633,\n",
       "   0.643906665623188,\n",
       "   0.6543466662764549,\n",
       "   0.6586666657924652,\n",
       "   0.6621999987959861,\n",
       "   0.6686000021100045,\n",
       "   0.6736666663885117,\n",
       "   0.6834000002741814,\n",
       "   0.6836266658306122,\n",
       "   0.6845733318924904,\n",
       "   0.6883866664171219,\n",
       "   0.6905466667413711,\n",
       "   0.7011066663265229,\n",
       "   0.699306666970253,\n",
       "   0.7040133326053619,\n",
       "   0.6979733331799507,\n",
       "   0.7090266666412354,\n",
       "   0.7082266661524773,\n",
       "   0.7145599992275238,\n",
       "   0.723213332414627,\n",
       "   0.7112800000309945,\n",
       "   0.7249866659641266,\n",
       "   0.7247466676235199,\n",
       "   0.7239200010299682,\n",
       "   0.7231600009799004,\n",
       "   0.7358133323192596,\n",
       "   0.7275066646337509,\n",
       "   0.733600000500679,\n",
       "   0.737253333568573,\n",
       "   0.73394666659832,\n",
       "   0.7349733336567879,\n",
       "   0.7385333330631256,\n",
       "   0.7398000009655953,\n",
       "   0.747106665790081,\n",
       "   0.7452399985790252,\n",
       "   0.7435200006961823,\n",
       "   0.7448666675090789,\n",
       "   0.7541599999666214,\n",
       "   0.7489999997615814,\n",
       "   0.7552933340072632,\n",
       "   0.7470666662454605,\n",
       "   0.7514000004529953,\n",
       "   0.7563733341693878,\n",
       "   0.7560800001621246,\n",
       "   0.7567066668272019,\n",
       "   0.7578666648864746,\n",
       "   0.7545333336591721,\n",
       "   0.7562799990177155,\n",
       "   0.7636000000238419,\n",
       "   0.7673466669321061,\n",
       "   0.7597600008249283,\n",
       "   0.7598533321619034,\n",
       "   0.760466667354107,\n",
       "   0.768413333773613,\n",
       "   0.7678533318042755,\n",
       "   0.762693333029747,\n",
       "   0.771853333234787,\n",
       "   0.76832000041008,\n",
       "   0.7675333330631257,\n",
       "   0.7673066663742065,\n",
       "   0.77378666639328,\n",
       "   0.7763466665744782,\n",
       "   0.770813333272934,\n",
       "   0.7688933311700821,\n",
       "   0.7746933327913285,\n",
       "   0.7726800001859665,\n",
       "   0.7767466663122177,\n",
       "   0.7782399986982346,\n",
       "   0.7753333333730698,\n",
       "   0.7784266669750214,\n",
       "   0.7823733344078064,\n",
       "   0.7794933326244354,\n",
       "   0.7792000001668931,\n",
       "   0.7819199997186661,\n",
       "   0.7860533322095871,\n",
       "   0.7847333332300186,\n",
       "   0.7863866666555405,\n",
       "   0.782346665263176,\n",
       "   0.7938666670322418,\n",
       "   0.7850533335208892,\n",
       "   0.7874933331012726,\n",
       "   0.7814933341145516,\n",
       "   0.7867866657972336,\n",
       "   0.787053332567215,\n",
       "   0.7921199990510941],\n",
       "  'train_accuracy_std': [0.06737921064308171,\n",
       "   0.06546089731424419,\n",
       "   0.07591200735479188,\n",
       "   0.07011723498560915,\n",
       "   0.06977420191150185,\n",
       "   0.07100464496552186,\n",
       "   0.07526041310192337,\n",
       "   0.06868432095195662,\n",
       "   0.07019197900943841,\n",
       "   0.06691075363206772,\n",
       "   0.07250785206299383,\n",
       "   0.07322436897907708,\n",
       "   0.06662922227591263,\n",
       "   0.07164942274759843,\n",
       "   0.06940580430219782,\n",
       "   0.0718933174217093,\n",
       "   0.06660960776813454,\n",
       "   0.06906872676789784,\n",
       "   0.07113964345970662,\n",
       "   0.07332057560832278,\n",
       "   0.06952649420760879,\n",
       "   0.06423633533996384,\n",
       "   0.06960972717209561,\n",
       "   0.0704157190726784,\n",
       "   0.0663577613850617,\n",
       "   0.07172517827477634,\n",
       "   0.06586263243017439,\n",
       "   0.06547779474399565,\n",
       "   0.0666185151056179,\n",
       "   0.06941776521895708,\n",
       "   0.06731969610555617,\n",
       "   0.06555446989055239,\n",
       "   0.06657398393011765,\n",
       "   0.06919013288992641,\n",
       "   0.06936830381008505,\n",
       "   0.06972724431358072,\n",
       "   0.06377357565278177,\n",
       "   0.06774273398247575,\n",
       "   0.07040496401767392,\n",
       "   0.06643346390057893,\n",
       "   0.06753900069055288,\n",
       "   0.06572616708853805,\n",
       "   0.0642986969376107,\n",
       "   0.06556406050065372,\n",
       "   0.06624583245498923,\n",
       "   0.06603689107462543,\n",
       "   0.06738845841963909,\n",
       "   0.067234132523043,\n",
       "   0.06644770786686237,\n",
       "   0.06480370827344462,\n",
       "   0.0679755192991285,\n",
       "   0.061495663828149104,\n",
       "   0.06595604721424432,\n",
       "   0.0648072573083374,\n",
       "   0.0676449547330563,\n",
       "   0.06722099399001859,\n",
       "   0.062083837073735756,\n",
       "   0.06706489578205446,\n",
       "   0.06540606995340235,\n",
       "   0.06512947782046564,\n",
       "   0.06579415680983716,\n",
       "   0.06756943331576573,\n",
       "   0.06590916264878602,\n",
       "   0.06270197530957357,\n",
       "   0.06317478355433129,\n",
       "   0.06662049013487682,\n",
       "   0.0641959152124166,\n",
       "   0.0642084645950181,\n",
       "   0.0633699255823971,\n",
       "   0.06622009844306413,\n",
       "   0.05965259327993763,\n",
       "   0.06364013259978547,\n",
       "   0.05988325642204962,\n",
       "   0.06437072605152275,\n",
       "   0.06192733178884599,\n",
       "   0.06410397822224918,\n",
       "   0.06599633620465825,\n",
       "   0.06694423799991532,\n",
       "   0.06169544590897662,\n",
       "   0.06374546444778599,\n",
       "   0.06177444981938501,\n",
       "   0.06325901069822962,\n",
       "   0.06506680188215687,\n",
       "   0.06094379915776649,\n",
       "   0.061853326116929626,\n",
       "   0.05853858174752532,\n",
       "   0.06114812610247444,\n",
       "   0.06247810472215801,\n",
       "   0.059936089199080715,\n",
       "   0.05936774229448623,\n",
       "   0.06139281197916323,\n",
       "   0.06396530333496596,\n",
       "   0.05951268750344206,\n",
       "   0.06326081301518738,\n",
       "   0.06458366690977631,\n",
       "   0.06706872914991535,\n",
       "   0.05716318765840403,\n",
       "   0.06011937615162815,\n",
       "   0.059697340007060846],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.4313842721780141,\n",
       "   1.3996887254714965,\n",
       "   1.3112757643063864,\n",
       "   1.26250301917394,\n",
       "   1.2876383022467295,\n",
       "   1.2545889856417973,\n",
       "   1.233648990591367,\n",
       "   1.1869175463914872,\n",
       "   1.187744747598966,\n",
       "   1.182660307486852,\n",
       "   1.1714224723974864,\n",
       "   1.1686604909102123,\n",
       "   1.1638708635171255,\n",
       "   1.1429610453049341,\n",
       "   1.1356699309746425,\n",
       "   1.1218904346227645,\n",
       "   1.1724198426802952,\n",
       "   1.091321399609248,\n",
       "   1.113779217004776,\n",
       "   1.0932742349306743,\n",
       "   1.0896364412705104,\n",
       "   1.074805858929952,\n",
       "   1.0746053276459375,\n",
       "   1.0673917526006698,\n",
       "   1.0747260477145513,\n",
       "   1.072311089038849,\n",
       "   1.056406568288803,\n",
       "   1.0801214994986852,\n",
       "   1.0519764280319215,\n",
       "   1.0616912080844243,\n",
       "   1.0431558632850646,\n",
       "   1.0556800566116968,\n",
       "   1.0451338219642639,\n",
       "   1.0661295268932978,\n",
       "   1.0382146974404654,\n",
       "   1.0442357323567073,\n",
       "   1.0441136995951334,\n",
       "   1.0391528894503912,\n",
       "   1.0459215968847275,\n",
       "   1.0423232239484788,\n",
       "   1.0364221445719402,\n",
       "   1.047435855269432,\n",
       "   1.0448797887563706,\n",
       "   1.0483581193288167,\n",
       "   1.0156011585394542,\n",
       "   1.0090239785114925,\n",
       "   1.0318054697910946,\n",
       "   1.0318155618508658,\n",
       "   1.009324775536855,\n",
       "   1.030169877409935,\n",
       "   1.0207442168394725,\n",
       "   1.0271315455436707,\n",
       "   1.0184095988670985,\n",
       "   1.06122403383255,\n",
       "   1.0198205453157425,\n",
       "   1.0205204703410466,\n",
       "   1.0094764437278112,\n",
       "   1.0206693683067958,\n",
       "   1.0017774667342503,\n",
       "   1.0256803411245345,\n",
       "   1.0341172804435095,\n",
       "   1.0220697861909867,\n",
       "   1.0119540995359422,\n",
       "   1.020190665324529,\n",
       "   1.0344882688919703,\n",
       "   1.005111286441485,\n",
       "   1.050649773478508,\n",
       "   1.0339922734101614,\n",
       "   1.042164526383082,\n",
       "   1.021306458711624,\n",
       "   1.0165406602621079,\n",
       "   1.0129109098513922,\n",
       "   1.048312164346377,\n",
       "   1.0098257009188334,\n",
       "   1.0121213106314342,\n",
       "   1.0341343559821448,\n",
       "   1.0300068499644597,\n",
       "   1.0182182582219441,\n",
       "   1.0257631001869838,\n",
       "   1.024888680577278,\n",
       "   1.0162987170616786,\n",
       "   1.0212844596306483,\n",
       "   1.0299896428982416,\n",
       "   1.0076777372757595,\n",
       "   1.0181020182371139,\n",
       "   1.0326851596434912,\n",
       "   1.013904168009758,\n",
       "   1.0162334471940995,\n",
       "   1.0377297514677049,\n",
       "   1.0181487456957499,\n",
       "   1.0156035810708999,\n",
       "   1.0137104294697443,\n",
       "   1.0230013134082159,\n",
       "   1.0133029079437257,\n",
       "   1.0222286359469095,\n",
       "   1.0030337206522624,\n",
       "   1.0063148140907288,\n",
       "   1.0275884489218394,\n",
       "   1.0543068075180053],\n",
       "  'val_loss_std': [0.09205461779074117,\n",
       "   0.10112325803857783,\n",
       "   0.1164816555206192,\n",
       "   0.11684442487868073,\n",
       "   0.12624354610210883,\n",
       "   0.12824916511328138,\n",
       "   0.13190054609615864,\n",
       "   0.13723190192099402,\n",
       "   0.13807000438458453,\n",
       "   0.139339988628521,\n",
       "   0.13426554246896522,\n",
       "   0.13898465266730373,\n",
       "   0.14230000650021354,\n",
       "   0.1437003945085759,\n",
       "   0.14093502381937267,\n",
       "   0.1445453384026066,\n",
       "   0.14431133382674588,\n",
       "   0.13963978126369894,\n",
       "   0.1445965086045647,\n",
       "   0.1473384673962332,\n",
       "   0.14427751914979267,\n",
       "   0.15651336387986003,\n",
       "   0.1504388733701974,\n",
       "   0.1377833643387174,\n",
       "   0.15208553578622375,\n",
       "   0.1511393792146939,\n",
       "   0.15462000489744357,\n",
       "   0.15079747962037007,\n",
       "   0.15380898367491055,\n",
       "   0.14545972001424556,\n",
       "   0.14390736502212356,\n",
       "   0.14739898371032523,\n",
       "   0.14194398915396272,\n",
       "   0.14588002424475452,\n",
       "   0.14871934461862624,\n",
       "   0.15301106322396665,\n",
       "   0.150860339033856,\n",
       "   0.16279383120706814,\n",
       "   0.1613931133569259,\n",
       "   0.1544442296543151,\n",
       "   0.1462521983935629,\n",
       "   0.15418683052514331,\n",
       "   0.1581760828815597,\n",
       "   0.1537545333982123,\n",
       "   0.15045548714138932,\n",
       "   0.14568185742774636,\n",
       "   0.15772383466341564,\n",
       "   0.14845824052159048,\n",
       "   0.1498361445585524,\n",
       "   0.1464824525521518,\n",
       "   0.15481009294325368,\n",
       "   0.1557191555224027,\n",
       "   0.15248729904574643,\n",
       "   0.16376856703765233,\n",
       "   0.15678532315604626,\n",
       "   0.15527713147984473,\n",
       "   0.15353407736482227,\n",
       "   0.16107553013494955,\n",
       "   0.15019259909839439,\n",
       "   0.16347435710670924,\n",
       "   0.15804517369472218,\n",
       "   0.1502543133017246,\n",
       "   0.15369445132971013,\n",
       "   0.15310228893681735,\n",
       "   0.14993041414150787,\n",
       "   0.14353553455377097,\n",
       "   0.15939827731580725,\n",
       "   0.15987908340345744,\n",
       "   0.15769095851692533,\n",
       "   0.15947950931045551,\n",
       "   0.15299200561337614,\n",
       "   0.15248077290781153,\n",
       "   0.15717625634433338,\n",
       "   0.16433322077473966,\n",
       "   0.15634289603947074,\n",
       "   0.1561236866621986,\n",
       "   0.15045094044689802,\n",
       "   0.1492959677872479,\n",
       "   0.16263876700244198,\n",
       "   0.14985152103048288,\n",
       "   0.15523837038001942,\n",
       "   0.14864309446757104,\n",
       "   0.14975653232640035,\n",
       "   0.15363722650206366,\n",
       "   0.14500426139129594,\n",
       "   0.1638719351944222,\n",
       "   0.15360695854519954,\n",
       "   0.15359483738455917,\n",
       "   0.16637769314754855,\n",
       "   0.15317978279037667,\n",
       "   0.15992324554765097,\n",
       "   0.15632972566148234,\n",
       "   0.15912426431886434,\n",
       "   0.15526225850688832,\n",
       "   0.15075307862951706,\n",
       "   0.1573381469391268,\n",
       "   0.15818645666927253,\n",
       "   0.14830270135498172,\n",
       "   0.16267633333612236],\n",
       "  'val_accuracy_mean': [0.40004444509744647,\n",
       "   0.4225333339969317,\n",
       "   0.4581111112733682,\n",
       "   0.48686666518449784,\n",
       "   0.47706666777531304,\n",
       "   0.4963777775565783,\n",
       "   0.5000444449981054,\n",
       "   0.5225999984145164,\n",
       "   0.5207111118237178,\n",
       "   0.527266666094462,\n",
       "   0.5304222198327383,\n",
       "   0.5323111128807068,\n",
       "   0.5304444437225659,\n",
       "   0.5424444445967674,\n",
       "   0.5477777771155039,\n",
       "   0.5516444446643194,\n",
       "   0.5323999987045924,\n",
       "   0.5623999993006389,\n",
       "   0.5551333330074946,\n",
       "   0.5654444433252017,\n",
       "   0.5665333320697149,\n",
       "   0.5747777782877286,\n",
       "   0.5751777780056,\n",
       "   0.5776888875166575,\n",
       "   0.5727777765194575,\n",
       "   0.5741333334644636,\n",
       "   0.5807111101349195,\n",
       "   0.5786222196618716,\n",
       "   0.5828222213188807,\n",
       "   0.5812888873616854,\n",
       "   0.5882222219308217,\n",
       "   0.5859555526574453,\n",
       "   0.585911110540231,\n",
       "   0.578488886654377,\n",
       "   0.5906666655341785,\n",
       "   0.5864444425702096,\n",
       "   0.5875555550058683,\n",
       "   0.5952666650215784,\n",
       "   0.5912444424629212,\n",
       "   0.5931111124157905,\n",
       "   0.5930666654308637,\n",
       "   0.5931777775287628,\n",
       "   0.5911999983588855,\n",
       "   0.591355554163456,\n",
       "   0.6047333331902822,\n",
       "   0.6045555541912715,\n",
       "   0.6015777760744094,\n",
       "   0.596177778840065,\n",
       "   0.6065777761737505,\n",
       "   0.597444442709287,\n",
       "   0.598822219868501,\n",
       "   0.600399999320507,\n",
       "   0.6049777765075366,\n",
       "   0.5903333340088527,\n",
       "   0.6065777761737505,\n",
       "   0.6009999984502792,\n",
       "   0.6096222213904063,\n",
       "   0.6034666643540064,\n",
       "   0.6111333331465721,\n",
       "   0.6009333326419195,\n",
       "   0.5967999984820683,\n",
       "   0.6005555545290311,\n",
       "   0.6077111103137334,\n",
       "   0.6022888872027398,\n",
       "   0.60113333294789,\n",
       "   0.6095999983946482,\n",
       "   0.5930888884266218,\n",
       "   0.5969555552800496,\n",
       "   0.5965333346525828,\n",
       "   0.6035333301623662,\n",
       "   0.6061777769525846,\n",
       "   0.6079777774214744,\n",
       "   0.5964222212632497,\n",
       "   0.6089333349466324,\n",
       "   0.6091333315769831,\n",
       "   0.5997111107905706,\n",
       "   0.6024888890981674,\n",
       "   0.6041333324710528,\n",
       "   0.6009111120303472,\n",
       "   0.6026888893047968,\n",
       "   0.6104888870318731,\n",
       "   0.6038666663567225,\n",
       "   0.6024888880054156,\n",
       "   0.6109333335359891,\n",
       "   0.6040888883670171,\n",
       "   0.6019555548826854,\n",
       "   0.6064222212632497,\n",
       "   0.6051555540164312,\n",
       "   0.6028666660189629,\n",
       "   0.6048444433013598,\n",
       "   0.6116666648785273,\n",
       "   0.6057333334287007,\n",
       "   0.605466666718324,\n",
       "   0.6083777764439583,\n",
       "   0.6010888877511025,\n",
       "   0.6102000002066295,\n",
       "   0.6114222208658854,\n",
       "   0.6065555544694264,\n",
       "   0.5980888879299164],\n",
       "  'val_accuracy_std': [0.05868179908627809,\n",
       "   0.05510485714643319,\n",
       "   0.06277167984344922,\n",
       "   0.06174107176794624,\n",
       "   0.0650582867996088,\n",
       "   0.06243982178396385,\n",
       "   0.06386790305381862,\n",
       "   0.06728311621093111,\n",
       "   0.06469138838995474,\n",
       "   0.06508407161029786,\n",
       "   0.06472711423172837,\n",
       "   0.06630674695412399,\n",
       "   0.06448849098113899,\n",
       "   0.06935647753425962,\n",
       "   0.06666407319079863,\n",
       "   0.06730004319728121,\n",
       "   0.06559823729987052,\n",
       "   0.06762546033436248,\n",
       "   0.06986243479584336,\n",
       "   0.06774343342027654,\n",
       "   0.0690655938373676,\n",
       "   0.06416288843149873,\n",
       "   0.06466327103140679,\n",
       "   0.06287429650511128,\n",
       "   0.06737723044081613,\n",
       "   0.0667862036693972,\n",
       "   0.071066569381169,\n",
       "   0.06587438984708674,\n",
       "   0.06891458743496932,\n",
       "   0.06418351805825863,\n",
       "   0.06417991640387938,\n",
       "   0.0668980269846182,\n",
       "   0.06234212174659084,\n",
       "   0.06813340442264552,\n",
       "   0.0659786716875323,\n",
       "   0.0658951657989604,\n",
       "   0.06462503610634722,\n",
       "   0.06873865646199548,\n",
       "   0.06804197728964713,\n",
       "   0.06782075038569553,\n",
       "   0.06432961374880992,\n",
       "   0.06639593536697386,\n",
       "   0.06719267945568227,\n",
       "   0.06473031996557653,\n",
       "   0.06569713795088973,\n",
       "   0.06292019283010349,\n",
       "   0.06504038203600078,\n",
       "   0.06616185242791073,\n",
       "   0.06592470186517314,\n",
       "   0.06400337432745269,\n",
       "   0.06280320432945151,\n",
       "   0.064700930124911,\n",
       "   0.06363174901732081,\n",
       "   0.06388676238835815,\n",
       "   0.06456457821275267,\n",
       "   0.06652902375333108,\n",
       "   0.06178821201799597,\n",
       "   0.06733760276706922,\n",
       "   0.06547188262972667,\n",
       "   0.0691901512208606,\n",
       "   0.0665987208787667,\n",
       "   0.06517772432585282,\n",
       "   0.06565187373077076,\n",
       "   0.06472006572094625,\n",
       "   0.06678470677825087,\n",
       "   0.06164163957479436,\n",
       "   0.06720956220565541,\n",
       "   0.06672268498490637,\n",
       "   0.0648815219575972,\n",
       "   0.06444605010081442,\n",
       "   0.06566400089159542,\n",
       "   0.06573729788530942,\n",
       "   0.06882570153425312,\n",
       "   0.06610129321431708,\n",
       "   0.06473871244250064,\n",
       "   0.06753044423178259,\n",
       "   0.06534261601371476,\n",
       "   0.06351981902850713,\n",
       "   0.0643870213035903,\n",
       "   0.06284866808422196,\n",
       "   0.06397613789675531,\n",
       "   0.06297686933089675,\n",
       "   0.06634387149727772,\n",
       "   0.06378798316558339,\n",
       "   0.05994707139859857,\n",
       "   0.06714736340778346,\n",
       "   0.06373393498820212,\n",
       "   0.06355701703417364,\n",
       "   0.06824306148040962,\n",
       "   0.06337844739629891,\n",
       "   0.06487623659809585,\n",
       "   0.06657562648520936,\n",
       "   0.06437653190840435,\n",
       "   0.06614600839751872,\n",
       "   0.06484054143281616,\n",
       "   0.062130960546699344,\n",
       "   0.06514410953449053,\n",
       "   0.06320445120708451,\n",
       "   0.0639969284160578],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6818444436788559,\n",
       " 'best_val_iter': 48000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 96,\n",
       " 'train_loss_mean': 0.4526471059322357,\n",
       " 'train_loss_std': 0.12993412983496969,\n",
       " 'train_accuracy_mean': 0.8343333342075347,\n",
       " 'train_accuracy_std': 0.0548158550248203,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.8508474173148474,\n",
       " 'val_loss_std': 0.14179470314582376,\n",
       " 'val_accuracy_mean': 0.6763111112515131,\n",
       " 'val_accuracy_std': 0.05848475377685013,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0115, -0.0778,  0.0524],\n",
       "                         [-0.0305, -0.0349, -0.0020],\n",
       "                         [-0.0158,  0.1034, -0.0275]],\n",
       "               \n",
       "                        [[ 0.0519, -0.0845,  0.0649],\n",
       "                         [-0.0253,  0.0089,  0.0542],\n",
       "                         [-0.0484,  0.0403, -0.0331]],\n",
       "               \n",
       "                        [[ 0.0545, -0.0248, -0.0204],\n",
       "                         [ 0.0301,  0.0602, -0.0543],\n",
       "                         [-0.0532,  0.0362, -0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0481,  0.1008,  0.0398],\n",
       "                         [ 0.0482, -0.0425,  0.0113],\n",
       "                         [-0.0911, -0.0673, -0.0469]],\n",
       "               \n",
       "                        [[-0.0092,  0.0265, -0.0018],\n",
       "                         [ 0.0446, -0.0628, -0.0604],\n",
       "                         [ 0.0496, -0.0558,  0.0711]],\n",
       "               \n",
       "                        [[-0.0320,  0.0804, -0.0688],\n",
       "                         [-0.0160,  0.0558,  0.0703],\n",
       "                         [-0.0326, -0.0326, -0.0280]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0071, -0.0199,  0.0549],\n",
       "                         [-0.0223, -0.0425, -0.0309],\n",
       "                         [ 0.0645,  0.0541, -0.0505]],\n",
       "               \n",
       "                        [[-0.0577, -0.0244,  0.0483],\n",
       "                         [ 0.0787, -0.0048,  0.0167],\n",
       "                         [-0.0691, -0.0398,  0.0441]],\n",
       "               \n",
       "                        [[ 0.0377, -0.0742,  0.0330],\n",
       "                         [ 0.0672,  0.0122, -0.0616],\n",
       "                         [-0.0667,  0.0326,  0.0182]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0424, -0.0622, -0.0432],\n",
       "                         [-0.0184, -0.0636,  0.0860],\n",
       "                         [-0.0358,  0.0483, -0.0402]],\n",
       "               \n",
       "                        [[ 0.0383,  0.0228,  0.0691],\n",
       "                         [ 0.0292, -0.0177, -0.0221],\n",
       "                         [ 0.0055,  0.0494, -0.0077]],\n",
       "               \n",
       "                        [[-0.0062, -0.0760, -0.0401],\n",
       "                         [-0.0594,  0.0223,  0.0496],\n",
       "                         [-0.0420,  0.0299,  0.0492]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0190, -0.0077, -0.0359],\n",
       "                         [ 0.0073,  0.0689, -0.0534],\n",
       "                         [ 0.0444,  0.0745, -0.0492]],\n",
       "               \n",
       "                        [[ 0.0429,  0.0223,  0.0724],\n",
       "                         [ 0.0197, -0.0301, -0.0356],\n",
       "                         [-0.0315, -0.0832,  0.0298]],\n",
       "               \n",
       "                        [[-0.0250, -0.0121, -0.0183],\n",
       "                         [-0.0136, -0.0266,  0.0769],\n",
       "                         [-0.0608, -0.0201,  0.0196]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0678,  0.0252, -0.0223],\n",
       "                         [-0.0489,  0.0072,  0.0349],\n",
       "                         [ 0.0568,  0.0435,  0.0784]],\n",
       "               \n",
       "                        [[-0.0682, -0.0912,  0.0311],\n",
       "                         [ 0.0605, -0.0843, -0.0335],\n",
       "                         [ 0.0032,  0.0237,  0.0296]],\n",
       "               \n",
       "                        [[ 0.0041, -0.0576,  0.0435],\n",
       "                         [ 0.0096, -0.0407, -0.0082],\n",
       "                         [-0.0129, -0.0269,  0.0120]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.0765e-03, -8.0369e-04, -1.8382e-03, -1.1685e-04, -1.5081e-04,\n",
       "                        9.4584e-04,  1.9420e-04,  1.0867e-04,  8.7550e-04, -1.9117e-03,\n",
       "                       -3.8360e-04,  1.2847e-04,  2.6250e-03,  1.7375e-03, -2.0646e-03,\n",
       "                        7.6395e-04, -6.3794e-04, -6.1944e-04, -7.8609e-04, -2.0221e-04,\n",
       "                        1.6218e-03, -2.4587e-04,  2.3333e-03,  2.2809e-03,  3.7578e-04,\n",
       "                        2.2750e-05,  6.9353e-04,  7.8915e-04,  1.0253e-03,  9.2878e-04,\n",
       "                        1.5839e-03, -6.1639e-05,  3.5597e-04, -3.5442e-03, -4.6407e-04,\n",
       "                        6.9834e-04,  1.1986e-03,  6.3443e-04, -1.2857e-05, -8.4554e-04,\n",
       "                        2.7600e-04, -3.0869e-03, -1.1622e-03,  2.0713e-04, -3.6686e-04,\n",
       "                       -7.5862e-05,  2.7620e-04, -1.3584e-03, -2.4717e-03, -1.9970e-05,\n",
       "                        1.1759e-03, -8.4054e-04,  1.2545e-03, -2.8829e-03,  1.1732e-03,\n",
       "                        1.6122e-04,  5.0176e-04,  3.3543e-04, -6.3797e-05, -1.7757e-03,\n",
       "                        2.5158e-04,  1.5920e-03, -5.9205e-04,  7.7316e-04,  3.4142e-04,\n",
       "                        7.5710e-04, -6.7782e-04,  1.7540e-03, -8.1071e-04, -5.7699e-04,\n",
       "                        1.4908e-04, -1.1079e-03, -1.0642e-05, -2.4850e-04,  1.3602e-03,\n",
       "                       -5.1398e-04,  8.3879e-04,  8.4650e-04, -1.5811e-04,  3.1155e-04,\n",
       "                        1.4772e-03, -2.1250e-03,  1.2379e-03,  9.4745e-05, -2.6874e-03,\n",
       "                       -8.9463e-04,  1.0693e-03, -1.5491e-03,  1.8957e-04,  2.8946e-03,\n",
       "                        1.1849e-03,  2.1482e-04,  2.0873e-04, -7.8655e-04,  1.2980e-03,\n",
       "                       -2.6352e-03,  1.9492e-03, -1.8718e-03, -1.6439e-03, -3.3162e-04,\n",
       "                        3.8143e-04, -7.6442e-04,  1.1236e-03, -1.1091e-04, -2.0625e-03,\n",
       "                        1.1619e-03, -4.9918e-04,  1.9502e-06, -1.0099e-03, -8.2773e-04,\n",
       "                        1.4017e-03,  1.5289e-03, -1.8522e-03, -2.8503e-04,  3.0935e-04,\n",
       "                        1.6916e-04, -2.2714e-04, -1.6422e-03,  4.0964e-04, -2.4318e-04,\n",
       "                       -6.1455e-05,  9.1467e-05,  9.6760e-04,  4.0069e-04,  1.5191e-03,\n",
       "                        1.3727e-04,  8.4586e-04, -1.2893e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1601, -0.0269,  0.2146, -0.1223, -0.1164,  0.0143, -0.1938, -0.2474,\n",
       "                       -0.1682, -0.1138, -0.1828, -0.1827,  0.0486, -0.1432, -0.0289, -0.1198,\n",
       "                        0.0172, -0.1315, -0.0307, -0.0695, -0.1784, -0.0808, -0.1548,  0.0150,\n",
       "                       -0.0257, -0.1688, -0.0726, -0.0996, -0.0490,  0.0397, -0.1129, -0.1187,\n",
       "                       -0.1021, -0.1569, -0.1652, -0.2654, -0.1424, -0.0779, -0.1773, -0.1431,\n",
       "                       -0.0401,  0.1688, -0.1125, -0.0631, -0.0056, -0.1462, -0.1993, -0.2671,\n",
       "                        0.1400, -0.1451,  0.0623, -0.2133,  0.2319,  0.0191, -0.0483, -0.0743,\n",
       "                       -0.2520, -0.0044, -0.1727,  0.1165, -0.1224,  0.2687, -0.0037, -0.2654,\n",
       "                       -0.1504, -0.2399, -0.1681, -0.1425, -0.0929, -0.0455,  0.0674, -0.0151,\n",
       "                       -0.1439, -0.2008, -0.0738, -0.0622, -0.1589, -0.0268, -0.0954, -0.1174,\n",
       "                        0.0008, -0.1366, -0.2038, -0.2098, -0.0300,  0.1649, -0.0133, -0.1394,\n",
       "                       -0.1591,  0.0642, -0.0768, -0.1414,  0.0458, -0.0098, -0.0927,  0.0210,\n",
       "                        0.1638,  0.0666, -0.1137, -0.1096, -0.2115, -0.0579,  0.0081,  0.0286,\n",
       "                        0.0164, -0.1907, -0.0867, -0.1441, -0.0205, -0.0744, -0.0806, -0.1129,\n",
       "                       -0.0395, -0.1456, -0.0760, -0.0752, -0.0798,  0.1828, -0.0423,  0.0638,\n",
       "                       -0.1928, -0.2187, -0.1924, -0.1746,  0.1367, -0.0291, -0.2259, -0.1710],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.0777, 1.0709, 1.0801, 0.8988, 0.9437, 1.0149, 0.9128, 0.8826, 0.9809,\n",
       "                       0.9149, 0.9107, 0.9115, 1.0526, 0.8928, 1.0546, 1.0420, 1.0268, 0.9431,\n",
       "                       0.9681, 0.9646, 0.9394, 0.8664, 0.9175, 1.0001, 0.9011, 0.8854, 0.9461,\n",
       "                       0.9155, 0.9748, 0.9375, 1.0159, 0.9061, 1.0190, 1.2193, 0.9277, 0.9384,\n",
       "                       0.9600, 0.9206, 0.9119, 0.9612, 0.9499, 1.0274, 0.9329, 0.9598, 1.0849,\n",
       "                       1.0767, 0.9834, 0.9287, 1.1041, 0.9253, 1.0169, 0.9134, 1.0635, 1.0854,\n",
       "                       1.0065, 0.9070, 1.1188, 1.1279, 0.9739, 1.0818, 0.9082, 1.0035, 0.9521,\n",
       "                       0.9313, 0.9319, 0.8501, 0.8868, 0.9298, 1.0671, 0.9991, 0.9329, 0.9704,\n",
       "                       0.8722, 0.8916, 0.9964, 0.8846, 0.9151, 1.0916, 1.0985, 1.0259, 1.0244,\n",
       "                       0.9868, 0.8907, 0.9007, 1.1403, 1.1089, 1.0677, 1.0212, 0.9175, 0.9868,\n",
       "                       1.0047, 0.9299, 1.0141, 1.0329, 0.9991, 1.0203, 1.1726, 0.9629, 0.9522,\n",
       "                       0.9195, 0.8685, 1.0409, 1.1071, 0.9493, 0.9616, 0.9556, 0.9301, 0.8482,\n",
       "                       1.0131, 0.9587, 1.0176, 1.0275, 0.9890, 1.1160, 0.9377, 1.0863, 0.9138,\n",
       "                       1.0243, 0.9302, 0.9634, 0.8600, 0.8900, 1.2006, 1.0526, 1.0483, 0.9347,\n",
       "                       0.9339, 0.8952], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-3.1068e-02,  1.4934e-02, -2.4708e-02],\n",
       "                         [-4.9840e-02, -2.8395e-02, -7.1771e-02],\n",
       "                         [-2.5563e-02,  2.5884e-02, -1.4416e-02]],\n",
       "               \n",
       "                        [[ 7.1535e-02,  3.6242e-02, -8.3086e-02],\n",
       "                         [-5.5571e-02, -6.0254e-03, -3.9204e-02],\n",
       "                         [-2.6580e-02,  7.6422e-02,  2.6242e-02]],\n",
       "               \n",
       "                        [[ 7.9697e-02,  1.2020e-02, -8.9087e-04],\n",
       "                         [-2.4565e-02,  6.1961e-02,  8.5690e-02],\n",
       "                         [ 1.0954e-02,  5.8338e-02,  8.1456e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3707e-04, -3.0362e-02, -3.2934e-02],\n",
       "                         [-5.1523e-02, -6.6658e-02, -3.6795e-02],\n",
       "                         [ 7.4546e-03,  1.6017e-03, -1.5633e-02]],\n",
       "               \n",
       "                        [[ 1.1219e-02,  3.6779e-02, -7.3502e-03],\n",
       "                         [-9.8202e-03,  2.9635e-02, -1.4038e-02],\n",
       "                         [ 3.2854e-02, -5.6267e-02, -1.8176e-02]],\n",
       "               \n",
       "                        [[-1.0652e-02,  4.8453e-02, -3.1925e-02],\n",
       "                         [-2.8970e-02,  1.5542e-02, -4.6588e-03],\n",
       "                         [-4.3812e-02, -2.1815e-02,  4.8127e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4941e-02, -3.0805e-02,  3.8058e-02],\n",
       "                         [ 3.1757e-02,  3.3032e-02, -4.8846e-02],\n",
       "                         [ 3.0079e-02, -1.2053e-03, -3.8244e-02]],\n",
       "               \n",
       "                        [[ 3.1452e-02,  6.4167e-02, -2.9219e-03],\n",
       "                         [-4.8711e-02,  4.5315e-04,  2.8953e-02],\n",
       "                         [-1.8084e-02, -3.7616e-02,  6.2568e-03]],\n",
       "               \n",
       "                        [[ 6.1893e-02,  2.1873e-02, -4.9432e-02],\n",
       "                         [-1.2321e-02, -1.1952e-03, -2.0872e-02],\n",
       "                         [-2.9053e-02, -1.8110e-02, -4.7141e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1403e-02,  5.6295e-03,  4.0007e-02],\n",
       "                         [-4.6823e-02, -2.6105e-02, -2.8252e-02],\n",
       "                         [-1.2773e-02,  1.2540e-02, -1.2665e-02]],\n",
       "               \n",
       "                        [[-2.8528e-02, -7.3125e-02, -6.5685e-02],\n",
       "                         [ 1.5529e-02,  4.3157e-03,  4.4955e-03],\n",
       "                         [ 1.7821e-02, -4.4237e-02,  1.1965e-02]],\n",
       "               \n",
       "                        [[-1.4344e-02, -6.4940e-02,  1.5923e-02],\n",
       "                         [ 2.7153e-02, -2.8147e-02, -7.4200e-04],\n",
       "                         [-1.2954e-02, -3.2091e-02,  2.8604e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.7807e-02, -2.8579e-02, -4.7183e-02],\n",
       "                         [-2.5427e-03, -3.0682e-03, -4.7967e-02],\n",
       "                         [ 3.1416e-02, -1.7803e-02, -1.8947e-03]],\n",
       "               \n",
       "                        [[-3.5335e-03,  2.9050e-02, -5.2048e-02],\n",
       "                         [ 2.8188e-02,  1.9930e-02, -2.0456e-02],\n",
       "                         [ 3.0739e-02,  4.1309e-02, -4.7110e-02]],\n",
       "               \n",
       "                        [[-3.3564e-02, -4.0410e-02, -8.6391e-02],\n",
       "                         [-2.9952e-02, -1.4456e-02,  2.4784e-02],\n",
       "                         [-4.7339e-02, -4.5708e-02,  7.1623e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.2363e-02,  1.3501e-02, -1.5803e-04],\n",
       "                         [-1.9702e-02, -4.1223e-02, -1.4040e-02],\n",
       "                         [-2.1804e-02, -3.4117e-02, -4.1499e-02]],\n",
       "               \n",
       "                        [[ 5.0175e-02,  4.8530e-02,  6.1841e-02],\n",
       "                         [-5.2386e-02, -3.6055e-02, -2.1579e-02],\n",
       "                         [-6.1513e-02, -3.6027e-03, -4.1555e-02]],\n",
       "               \n",
       "                        [[-3.5838e-02, -6.2272e-02, -8.0042e-03],\n",
       "                         [-3.8722e-02,  7.4427e-03,  1.1058e-02],\n",
       "                         [-1.4307e-02, -3.2467e-02, -3.4531e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3628e-02,  2.7285e-03,  1.5521e-02],\n",
       "                         [ 6.4739e-02, -1.6291e-02,  2.1665e-02],\n",
       "                         [ 6.1771e-02, -5.1059e-02, -4.8901e-02]],\n",
       "               \n",
       "                        [[-2.6963e-02,  1.9296e-02,  5.3938e-02],\n",
       "                         [-5.3194e-02, -1.1404e-02,  1.8829e-02],\n",
       "                         [-7.9276e-02, -2.1864e-02, -3.8196e-02]],\n",
       "               \n",
       "                        [[ 8.4808e-02,  6.7533e-02,  6.2033e-02],\n",
       "                         [ 6.5936e-02, -2.6663e-02,  7.5278e-03],\n",
       "                         [ 2.4147e-02, -4.6637e-02, -4.0440e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.3273e-03,  1.0368e-02,  2.9113e-02],\n",
       "                         [ 4.7940e-02, -2.6338e-02,  3.7946e-02],\n",
       "                         [ 3.2950e-02,  4.6247e-02,  2.5710e-02]],\n",
       "               \n",
       "                        [[-9.7459e-03,  2.0474e-02,  4.2825e-02],\n",
       "                         [ 1.6884e-02,  4.6780e-03, -1.5560e-02],\n",
       "                         [-6.3945e-02,  6.8826e-03,  3.1620e-02]],\n",
       "               \n",
       "                        [[ 1.1923e-02, -2.2587e-02, -1.0285e-03],\n",
       "                         [-6.0087e-03,  9.3974e-05,  8.8612e-03],\n",
       "                         [ 8.7517e-03,  4.7585e-03, -3.5720e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.3675e-02, -4.8650e-02,  1.3389e-02],\n",
       "                         [-4.8180e-02, -1.0838e-01, -1.8075e-02],\n",
       "                         [-6.6564e-02, -4.1220e-02, -3.8812e-02]],\n",
       "               \n",
       "                        [[ 4.3953e-03,  5.0149e-02,  6.3318e-02],\n",
       "                         [ 4.8209e-02,  2.4266e-02,  2.1014e-02],\n",
       "                         [ 1.6416e-02, -3.3700e-02, -4.2817e-02]],\n",
       "               \n",
       "                        [[-2.8837e-02,  2.7409e-03,  2.8331e-02],\n",
       "                         [-5.9724e-03, -5.8118e-02, -7.0318e-02],\n",
       "                         [-6.0833e-02, -5.2656e-02, -5.2772e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.0133e-02,  3.9366e-03,  4.7938e-02],\n",
       "                         [-1.2157e-02,  1.9478e-02, -2.6585e-02],\n",
       "                         [-2.0719e-02, -2.5532e-02, -4.1482e-02]],\n",
       "               \n",
       "                        [[ 2.7662e-02,  5.0188e-02,  5.1921e-02],\n",
       "                         [-7.6517e-03, -5.1657e-02, -2.7490e-02],\n",
       "                         [-2.9292e-02, -3.8551e-02, -2.9929e-03]],\n",
       "               \n",
       "                        [[-2.6770e-02, -3.3468e-02, -3.3695e-02],\n",
       "                         [-5.5176e-02, -2.4818e-02, -5.8880e-02],\n",
       "                         [-1.5056e-02, -6.2207e-02, -4.8363e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.1598e-03, -4.3907e-02,  1.5628e-02],\n",
       "                         [ 1.6393e-02,  3.9933e-02, -4.7980e-03],\n",
       "                         [-9.2348e-03,  8.1859e-02,  3.9698e-02]],\n",
       "               \n",
       "                        [[-3.7246e-02,  1.7502e-02,  3.7163e-02],\n",
       "                         [ 5.7703e-02, -2.4650e-02, -5.0248e-03],\n",
       "                         [-2.8848e-02, -1.8945e-02,  4.7855e-02]],\n",
       "               \n",
       "                        [[ 1.3576e-03, -4.7517e-02,  3.5737e-02],\n",
       "                         [ 1.5863e-02,  3.9620e-02,  9.4381e-03],\n",
       "                         [ 5.0647e-04, -2.3891e-02,  3.4428e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9621e-02, -5.0091e-03, -5.7347e-02],\n",
       "                         [ 6.7124e-02,  8.4320e-02, -2.3866e-02],\n",
       "                         [ 1.6802e-02,  4.7377e-02,  3.4077e-03]],\n",
       "               \n",
       "                        [[-5.5406e-02, -5.9962e-02, -3.6674e-02],\n",
       "                         [ 3.1510e-02,  4.7255e-02,  1.2530e-03],\n",
       "                         [-8.1551e-03,  1.8949e-02, -5.1895e-02]],\n",
       "               \n",
       "                        [[-1.0334e-02,  2.3689e-02, -2.3868e-02],\n",
       "                         [-4.3318e-03,  2.3800e-02,  3.7886e-02],\n",
       "                         [ 2.9135e-02, -6.1549e-03, -1.9589e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 5.9933e-08, -6.1479e-05,  1.1136e-06,  1.1191e-06,  2.5666e-07,\n",
       "                        1.5369e-03, -1.1775e-06, -6.2422e-07, -1.1607e-06, -8.1503e-04,\n",
       "                       -1.8291e-06, -2.3342e-06, -2.2181e-04,  6.1000e-07, -2.3925e-06,\n",
       "                       -3.0629e-07, -3.1087e-06,  8.1563e-07,  9.3452e-08, -5.7189e-04,\n",
       "                       -3.2095e-07, -9.1869e-09,  4.8346e-06, -1.7036e-05,  1.3274e-02,\n",
       "                       -1.3903e-05,  1.6652e-05, -2.7412e-05, -3.4676e-03,  5.0787e-06,\n",
       "                        2.6765e-06,  4.6576e-07,  4.0706e-06,  1.7675e-05, -8.4356e-07,\n",
       "                        2.6364e-06,  4.4227e-09, -1.2458e-05,  2.1915e-05,  4.9682e-07,\n",
       "                        7.0539e-07, -1.6904e-06,  2.0231e-05, -5.5559e-06,  7.0082e-07,\n",
       "                       -3.0102e-06, -1.5954e-04, -3.8790e-06, -1.2362e-06, -3.6231e-06,\n",
       "                        7.5977e-07, -4.2942e-07,  1.9629e-06, -1.0215e-02, -4.5511e-07,\n",
       "                        9.4783e-03, -3.0247e-03, -1.9373e-05,  5.5702e-05, -1.3319e-02,\n",
       "                       -9.0035e-07, -1.1355e-06,  3.0399e-06,  2.3563e-03,  3.7166e-07,\n",
       "                        1.0655e-02,  7.9245e-06,  3.9385e-07, -2.3954e-06,  6.9563e-07,\n",
       "                       -9.1358e-07,  1.7789e-06, -1.0133e-06, -9.4920e-05, -3.5892e-05,\n",
       "                        2.6434e-06, -3.6071e-06,  2.4756e-06,  2.6890e-06,  7.2722e-05,\n",
       "                        2.2625e-07, -9.3507e-07,  2.5611e-06,  1.9175e-06, -6.8006e-06,\n",
       "                        3.2866e-07,  3.1739e-06, -7.8642e-03, -4.1726e-06,  4.7427e-07,\n",
       "                        3.0530e-06,  1.0925e-02,  2.6354e-07,  1.7425e-06, -1.6997e-06,\n",
       "                       -2.4809e-06, -7.8697e-07,  1.2117e-02, -1.6563e-06,  1.2712e-06,\n",
       "                        4.4987e-07,  5.1062e-06,  1.0757e-02, -1.8630e-06,  7.0586e-08,\n",
       "                        5.5715e-07, -1.2244e-04,  1.5775e-05,  1.1858e-02,  9.6294e-03,\n",
       "                       -1.0860e-06,  2.7695e-05, -3.0826e-05, -1.0798e-02,  3.5214e-08,\n",
       "                       -4.0514e-07,  1.3645e-02,  2.0043e-05,  1.9055e-04, -8.7982e-03,\n",
       "                       -1.0212e-06, -1.3086e-05, -2.2918e-08, -2.2165e-05, -1.8928e-06,\n",
       "                       -1.2180e-05, -1.3773e-02, -7.5184e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0872, -0.1572, -0.1664, -0.2343, -0.1549, -0.1571, -0.1078, -0.1666,\n",
       "                       -0.1076, -0.1716, -0.3466, -0.2546, -0.1173, -0.2540, -0.1237, -0.1846,\n",
       "                       -0.2605, -0.0961, -0.0841, -0.2577, -0.1466, -0.1738, -0.2186, -0.2139,\n",
       "                       -0.2324, -0.1326, -0.1120, -0.1670, -0.1783, -0.2671, -0.0948, -0.3858,\n",
       "                       -0.2320, -0.0944, -0.2653, -0.2115, -0.2098, -0.0639, -0.3093, -0.2235,\n",
       "                       -0.1407, -0.1924, -0.2923, -0.1656, -0.2135, -0.2605, -0.1603, -0.1156,\n",
       "                       -0.1902, -0.2313, -0.0166, -0.1515, -0.3205, -0.2022, -0.0888, -0.1692,\n",
       "                       -0.2692, -0.1670, -0.2828, -0.2521, -0.0130, -0.2669, -0.1723, -0.2151,\n",
       "                       -0.1083, -0.0798, -0.1164, -0.2391, -0.1477, -0.1890, -0.2052, -0.0476,\n",
       "                       -0.1590, -0.1589, -0.2383, -0.0808, -0.2007, -0.1882, -0.1486, -0.1086,\n",
       "                       -0.1875, -0.2214, -0.0991,  0.0091, -0.2162, -0.2209, -0.0630, -0.2359,\n",
       "                       -0.1235, -0.1990, -0.2295, -0.2036, -0.2071, -0.2148, -0.1414, -0.0935,\n",
       "                       -0.1874, -0.1651, -0.1903, -0.1393, -0.1805, -0.1963, -0.1423, -0.2635,\n",
       "                       -0.2242, -0.2068, -0.1134, -0.2680, -0.1862, -0.1696, -0.2529, -0.2078,\n",
       "                       -0.1584, -0.2204, -0.2384, -0.1047, -0.1335, -0.2205, -0.1528, -0.2487,\n",
       "                       -0.1355, -0.1146, -0.1673, -0.0136, -0.1558, -0.2546, -0.1811, -0.1617],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.0451, 0.9745, 1.0224, 0.9569, 0.9861, 1.0045, 1.0278, 1.0634, 1.0389,\n",
       "                       1.0216, 0.9842, 1.1099, 1.0157, 0.9824, 1.0965, 1.0307, 0.8674, 0.9835,\n",
       "                       1.0076, 0.9665, 0.9861, 0.9705, 1.0266, 1.0917, 0.9736, 1.0308, 0.9726,\n",
       "                       0.9838, 0.9616, 1.0102, 0.9881, 1.0250, 0.9429, 0.9924, 0.9120, 1.0300,\n",
       "                       0.9982, 1.0233, 0.8428, 0.9264, 0.9836, 0.9378, 1.0010, 0.9307, 1.0222,\n",
       "                       0.9822, 1.0209, 0.9528, 1.0514, 0.9630, 1.0344, 1.0062, 0.9767, 0.9748,\n",
       "                       0.9827, 0.9348, 0.9054, 1.0206, 1.0095, 1.0173, 1.0312, 1.0670, 0.9866,\n",
       "                       0.9704, 0.9777, 0.9320, 0.9356, 0.9721, 0.9230, 1.0510, 0.9052, 1.0517,\n",
       "                       0.9739, 0.9824, 1.0422, 1.0663, 0.9386, 0.9604, 1.0494, 0.9874, 1.0360,\n",
       "                       0.9525, 1.0250, 0.9620, 0.9574, 1.0049, 1.0482, 0.9686, 0.9571, 0.9879,\n",
       "                       0.9952, 0.9969, 0.9531, 0.9737, 0.9241, 1.0071, 0.9846, 0.9989, 1.0562,\n",
       "                       0.9546, 0.9243, 1.0043, 0.9716, 1.0233, 1.0340, 1.0055, 0.9923, 0.9698,\n",
       "                       1.0110, 0.9383, 0.9961, 1.0366, 0.9825, 1.0127, 0.8616, 0.9989, 1.0212,\n",
       "                       0.9751, 0.9882, 1.0225, 0.9810, 1.0087, 0.9795, 1.0189, 0.9698, 1.0015,\n",
       "                       1.0015, 1.0593], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-3.5635e-02,  6.6883e-02,  1.6516e-02],\n",
       "                         [-2.0717e-02,  4.5892e-03, -7.0361e-03],\n",
       "                         [-4.0502e-03, -5.3581e-02, -6.3822e-03]],\n",
       "               \n",
       "                        [[ 2.4649e-03, -2.7850e-02,  9.0671e-03],\n",
       "                         [-4.3892e-02,  2.0201e-02, -2.6540e-02],\n",
       "                         [-6.6467e-04,  1.1619e-02, -5.7028e-02]],\n",
       "               \n",
       "                        [[ 4.4089e-03, -2.2262e-02,  1.7012e-02],\n",
       "                         [ 6.2686e-02,  2.4853e-02, -3.6876e-02],\n",
       "                         [-1.6284e-02,  2.1929e-02, -1.3400e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5534e-02, -3.8601e-02, -6.5782e-02],\n",
       "                         [ 2.6658e-02,  4.9235e-03,  4.0978e-02],\n",
       "                         [ 1.1825e-03, -2.5821e-02, -1.8333e-02]],\n",
       "               \n",
       "                        [[-2.6654e-02, -5.5350e-02,  6.5079e-03],\n",
       "                         [-6.1993e-04, -2.8721e-02, -2.6483e-02],\n",
       "                         [ 2.8866e-02,  1.8139e-02,  4.3261e-03]],\n",
       "               \n",
       "                        [[-3.5204e-03,  5.2470e-02,  3.2148e-02],\n",
       "                         [ 1.7759e-02,  1.0482e-02, -2.4699e-02],\n",
       "                         [ 1.1681e-02,  3.0295e-02,  2.6309e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.6161e-02,  1.5252e-02, -2.2819e-02],\n",
       "                         [ 1.0151e-02, -4.9394e-03, -3.1928e-02],\n",
       "                         [-7.3092e-03, -1.6657e-02,  1.1107e-02]],\n",
       "               \n",
       "                        [[-3.2528e-02,  3.4879e-02,  5.5768e-02],\n",
       "                         [ 2.8912e-02,  8.1838e-02,  7.0347e-02],\n",
       "                         [-5.9509e-02, -3.2777e-02,  1.6557e-03]],\n",
       "               \n",
       "                        [[ 1.0216e-02,  4.4529e-02, -8.3797e-02],\n",
       "                         [ 2.7144e-02, -1.2044e-02, -7.0979e-02],\n",
       "                         [ 2.4772e-02, -3.1583e-02, -4.7915e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.8065e-03,  5.4751e-02, -9.6700e-03],\n",
       "                         [-3.1591e-03, -3.3694e-02, -7.3251e-02],\n",
       "                         [ 7.2923e-05,  4.5793e-03, -5.0973e-02]],\n",
       "               \n",
       "                        [[ 5.8034e-03, -1.8566e-02,  9.1351e-03],\n",
       "                         [-5.1339e-02, -2.9027e-02, -1.2111e-02],\n",
       "                         [-2.9039e-02,  1.3079e-02,  3.4591e-02]],\n",
       "               \n",
       "                        [[ 6.6866e-02, -2.1931e-02, -4.6208e-02],\n",
       "                         [ 1.5051e-02,  3.2506e-02, -1.3522e-02],\n",
       "                         [-1.3544e-02,  3.2223e-02, -5.6732e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.1565e-03, -1.2166e-02, -6.4260e-02],\n",
       "                         [-4.6180e-02, -8.3588e-02, -3.3062e-02],\n",
       "                         [-3.1706e-02, -5.0750e-02, -5.2977e-02]],\n",
       "               \n",
       "                        [[ 1.6955e-03,  1.4892e-03, -2.2856e-02],\n",
       "                         [ 3.9290e-03,  6.5597e-02, -3.6062e-02],\n",
       "                         [-3.7413e-02,  3.7076e-02, -3.6557e-02]],\n",
       "               \n",
       "                        [[ 9.4688e-02, -1.3011e-02,  7.1782e-02],\n",
       "                         [ 1.4776e-02,  7.5849e-02, -1.0182e-02],\n",
       "                         [-5.7237e-03,  2.7586e-02,  4.5924e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.3358e-03,  4.7901e-02, -1.3634e-02],\n",
       "                         [ 2.1187e-02, -3.5819e-02, -3.5869e-02],\n",
       "                         [ 1.4592e-02,  1.6706e-02,  4.2851e-02]],\n",
       "               \n",
       "                        [[-2.2170e-02, -5.4084e-02, -2.6914e-04],\n",
       "                         [ 3.5542e-02, -5.5808e-02, -6.2875e-02],\n",
       "                         [-2.2901e-03, -3.7785e-02,  2.0417e-02]],\n",
       "               \n",
       "                        [[ 1.4934e-02,  2.7684e-02,  8.1969e-02],\n",
       "                         [ 6.0429e-02,  6.1587e-02,  3.9178e-02],\n",
       "                         [-1.8586e-03, -2.7665e-03, -1.9067e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1825e-02,  4.8099e-03, -5.5348e-02],\n",
       "                         [ 5.5376e-02, -2.2714e-02, -1.8907e-02],\n",
       "                         [ 9.4701e-03,  1.6956e-02,  2.8504e-02]],\n",
       "               \n",
       "                        [[-8.7255e-03, -1.3725e-02,  3.0419e-02],\n",
       "                         [ 2.3330e-02,  3.1619e-02, -3.6503e-02],\n",
       "                         [ 1.7630e-02,  1.8091e-02, -5.2045e-03]],\n",
       "               \n",
       "                        [[ 2.6194e-02,  2.7558e-02, -5.5006e-02],\n",
       "                         [-1.1049e-02, -1.7657e-02, -2.4263e-03],\n",
       "                         [-7.5835e-03, -1.4259e-02, -4.7830e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.5579e-02,  5.8151e-02, -2.5747e-02],\n",
       "                         [ 9.2701e-02,  6.5077e-02,  3.7771e-02],\n",
       "                         [ 4.0023e-02,  1.2615e-02, -1.5215e-02]],\n",
       "               \n",
       "                        [[ 3.5414e-02, -3.3109e-02,  3.7701e-02],\n",
       "                         [-2.7612e-02, -2.7967e-02, -5.2714e-02],\n",
       "                         [-3.7227e-02,  3.3004e-02, -8.0880e-02]],\n",
       "               \n",
       "                        [[-1.6259e-02,  4.5828e-03, -7.5952e-03],\n",
       "                         [-4.2413e-02, -9.9031e-03, -3.3439e-02],\n",
       "                         [-4.7989e-02,  6.6052e-02, -1.1792e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.2100e-02,  6.1930e-02, -8.5012e-03],\n",
       "                         [-4.9399e-02, -2.1669e-02, -2.1487e-02],\n",
       "                         [-9.4657e-03,  3.6917e-02, -3.9345e-02]],\n",
       "               \n",
       "                        [[ 1.1505e-02, -3.4558e-02,  3.1325e-02],\n",
       "                         [-1.0512e-04, -2.2429e-02,  3.3865e-02],\n",
       "                         [-1.2944e-02,  3.7145e-02,  1.9752e-02]],\n",
       "               \n",
       "                        [[ 2.9329e-03,  1.7710e-02,  6.5504e-03],\n",
       "                         [ 2.7545e-02,  2.8484e-02,  2.6443e-02],\n",
       "                         [-4.8555e-03, -4.9553e-02,  1.7128e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.9563e-03, -1.2934e-02,  1.1113e-02],\n",
       "                         [-1.8983e-02,  4.7031e-02, -4.0295e-02],\n",
       "                         [-1.3619e-02, -1.9917e-02, -3.2294e-02]],\n",
       "               \n",
       "                        [[-3.2172e-02, -2.8210e-02, -4.2929e-02],\n",
       "                         [ 4.4360e-03, -5.5791e-02,  1.2082e-02],\n",
       "                         [ 4.9856e-02, -1.0001e-02, -4.5965e-02]],\n",
       "               \n",
       "                        [[ 1.4553e-02,  6.3308e-02, -2.4518e-02],\n",
       "                         [-1.3177e-03,  5.6378e-02, -6.8796e-03],\n",
       "                         [-7.8446e-03,  2.0849e-02,  1.5414e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.5729e-02, -3.0278e-02,  3.3063e-04],\n",
       "                         [-3.3246e-02, -2.2090e-02,  2.8049e-02],\n",
       "                         [-9.2174e-02, -6.1077e-02, -5.3846e-03]],\n",
       "               \n",
       "                        [[ 1.3817e-02,  1.4932e-02, -3.2316e-02],\n",
       "                         [ 2.9696e-02,  1.5589e-02, -4.2233e-02],\n",
       "                         [ 3.3876e-02,  2.2564e-02,  1.0178e-02]],\n",
       "               \n",
       "                        [[-1.2799e-02, -1.3293e-02, -3.9625e-02],\n",
       "                         [-4.9859e-02,  3.1562e-02, -1.0173e-02],\n",
       "                         [-4.0080e-02, -3.2473e-02, -3.6179e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 5.4078e-02,  7.9191e-02,  1.7750e-03],\n",
       "                         [ 4.3058e-02,  4.6749e-02, -3.7460e-02],\n",
       "                         [ 1.1480e-03, -1.3536e-02, -6.2119e-02]],\n",
       "               \n",
       "                        [[ 3.5932e-02,  4.2279e-02,  7.7663e-03],\n",
       "                         [-3.6863e-02, -1.1133e-02, -4.4179e-02],\n",
       "                         [-5.6386e-03, -1.8559e-02, -5.7912e-02]],\n",
       "               \n",
       "                        [[-7.8392e-02,  3.8048e-02,  3.1213e-02],\n",
       "                         [-5.2759e-02,  8.1980e-03, -3.1568e-02],\n",
       "                         [-7.3119e-03, -2.0015e-02, -3.6982e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-2.1351e-03,  3.4871e-03,  1.7402e-03,  3.3579e-03,  1.0868e-03,\n",
       "                        7.4822e-04,  3.1768e-03,  6.2910e-04,  6.6134e-04, -1.6108e-03,\n",
       "                       -2.7385e-03,  6.7116e-04, -9.6336e-04,  3.2656e-03,  1.8089e-03,\n",
       "                        3.7092e-04,  2.3694e-03, -1.4622e-03,  1.2253e-03,  1.7384e-03,\n",
       "                        8.2374e-04,  1.4852e-03, -1.9792e-04, -6.7408e-04,  2.5066e-03,\n",
       "                       -1.6606e-03, -7.2716e-04,  2.5755e-03, -1.5390e-03,  1.5680e-03,\n",
       "                       -1.4368e-03,  6.3476e-05, -1.6477e-03,  8.5359e-04, -1.1303e-03,\n",
       "                        1.8436e-03,  2.7225e-03, -9.7316e-04,  3.3027e-03, -4.5836e-03,\n",
       "                        1.0710e-03,  2.4210e-03,  9.4007e-04,  2.7616e-04, -2.6665e-03,\n",
       "                        1.0428e-03, -4.5181e-03, -1.0973e-03,  2.9153e-03,  1.1452e-03,\n",
       "                       -2.6050e-04,  1.7071e-04,  2.0001e-03,  1.2810e-04, -1.9158e-04,\n",
       "                       -4.1856e-03, -3.1744e-03, -7.9482e-04, -3.0404e-04,  3.5577e-03,\n",
       "                        8.4278e-04,  1.1236e-03, -5.2809e-03,  1.9397e-03, -1.0979e-03,\n",
       "                        1.9149e-03, -3.6256e-03,  1.5284e-03, -4.2810e-04, -4.3189e-03,\n",
       "                        7.6343e-04, -1.4412e-03, -1.3658e-03,  2.3325e-03,  2.2334e-03,\n",
       "                        1.4130e-03,  3.4702e-03,  8.2384e-04, -5.8385e-04, -2.3952e-03,\n",
       "                        3.0020e-03, -2.1871e-03, -2.9691e-03, -7.2833e-04, -3.2114e-06,\n",
       "                       -4.5352e-04,  1.9773e-03, -1.5028e-03,  4.9686e-04, -5.7484e-04,\n",
       "                        4.2452e-03,  1.2303e-03, -1.5338e-03, -7.8657e-04,  2.1939e-03,\n",
       "                        3.3835e-03,  3.8203e-03,  3.1000e-03, -3.5266e-03,  2.6436e-03,\n",
       "                        9.1690e-04, -3.8383e-03,  5.2958e-04, -6.9434e-05,  1.7816e-03,\n",
       "                        4.8468e-03,  6.3447e-04,  2.1172e-03,  1.7666e-03, -2.1690e-03,\n",
       "                       -2.0607e-03, -3.0691e-03,  4.9682e-04, -5.3011e-04, -3.4542e-03,\n",
       "                       -1.0074e-03,  2.9607e-04, -2.5081e-03, -1.1018e-03, -3.7656e-04,\n",
       "                       -3.8288e-04,  8.6251e-04, -4.0439e-04,  7.3346e-04, -1.4695e-03,\n",
       "                       -5.3210e-05,  2.3575e-03, -2.5863e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.2745, -0.3432, -0.3225, -0.3161, -0.3458, -0.2965, -0.2656, -0.4619,\n",
       "                       -0.3528, -0.3486, -0.3127, -0.4352, -0.2927, -0.3775, -0.3179, -0.2020,\n",
       "                       -0.2585, -0.2389, -0.4363, -0.3397, -0.2641, -0.3822, -0.2232, -0.3648,\n",
       "                       -0.2792, -0.3051, -0.2956, -0.2948, -0.2251, -0.2499, -0.3535, -0.2657,\n",
       "                       -0.3528, -0.2497, -0.2430, -0.2566, -0.4218, -0.2970, -0.2469, -0.2932,\n",
       "                       -0.2682, -0.2900, -0.2373, -0.2746, -0.2579, -0.3706, -0.2705, -0.2992,\n",
       "                       -0.2342, -0.4011, -0.3201, -0.3366, -0.2704, -0.3693, -0.3214, -0.2929,\n",
       "                       -0.3103, -0.2991, -0.3957, -0.3168, -0.2977, -0.3472, -0.2490, -0.3007,\n",
       "                       -0.2593, -0.2996, -0.3261, -0.2612, -0.2965, -0.3719, -0.3670, -0.3033,\n",
       "                       -0.3026, -0.3622, -0.3271, -0.3308, -0.3027, -0.3465, -0.2958, -0.3535,\n",
       "                       -0.2374, -0.3401, -0.2038, -0.3748, -0.2901, -0.2589, -0.3164, -0.2999,\n",
       "                       -0.2608, -0.2464, -0.2377, -0.2692, -0.3142, -0.3819, -0.3478, -0.3193,\n",
       "                       -0.3010, -0.2188, -0.2880, -0.3890, -0.2799, -0.2614, -0.2786, -0.3609,\n",
       "                       -0.3335, -0.3494, -0.2471, -0.3684, -0.2899, -0.3292, -0.3916, -0.2884,\n",
       "                       -0.2767, -0.3124, -0.2516, -0.3787, -0.2954, -0.3044, -0.3517, -0.3989,\n",
       "                       -0.2850, -0.3227, -0.3362, -0.2426, -0.3302, -0.2835, -0.2408, -0.2675],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.8677, 0.9911, 0.9240, 0.8862, 0.9875, 0.9502, 0.9117, 1.0771, 0.9400,\n",
       "                       0.9616, 0.9689, 1.0767, 0.8404, 1.1268, 0.9214, 0.9644, 1.1083, 0.8781,\n",
       "                       1.0999, 0.9300, 0.8585, 0.9549, 0.8933, 0.9367, 0.9463, 0.9033, 0.9782,\n",
       "                       0.9583, 0.9087, 0.9267, 1.1003, 0.9585, 0.9880, 0.9270, 1.0583, 1.0271,\n",
       "                       1.0955, 0.9693, 0.9092, 0.9284, 0.8522, 0.9795, 0.9794, 0.8672, 0.9084,\n",
       "                       0.9751, 0.8588, 0.8368, 0.9086, 0.9760, 0.9658, 1.0371, 0.9347, 0.9943,\n",
       "                       0.9089, 0.9417, 0.8640, 0.9460, 0.9710, 0.8725, 0.9926, 1.0481, 1.1232,\n",
       "                       0.9393, 0.9935, 0.9556, 0.8583, 0.8728, 0.8861, 1.0107, 1.2039, 0.9133,\n",
       "                       0.8829, 1.0437, 0.9347, 0.9958, 1.0021, 0.8523, 0.9745, 0.9859, 0.9213,\n",
       "                       0.9626, 1.0658, 1.0581, 0.8953, 0.9729, 0.9728, 0.8852, 1.1389, 0.9146,\n",
       "                       0.9648, 0.8962, 1.0027, 1.0481, 0.9987, 0.9533, 0.9388, 0.9414, 0.9178,\n",
       "                       1.0637, 0.9441, 0.9439, 0.8276, 0.9136, 0.8780, 0.8991, 0.8927, 0.9721,\n",
       "                       0.9828, 0.9578, 0.8569, 1.0319, 0.9744, 1.0446, 0.9831, 0.9474, 0.9019,\n",
       "                       1.0095, 0.9606, 1.0142, 0.9484, 1.1066, 1.0178, 0.9520, 0.8968, 0.9898,\n",
       "                       0.9332, 0.8357], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-4.3702e-02, -1.3627e-02, -3.3736e-02],\n",
       "                         [ 5.9619e-02,  1.6159e-02, -2.8567e-02],\n",
       "                         [ 2.7072e-02,  1.9254e-02,  3.9468e-02]],\n",
       "               \n",
       "                        [[ 1.8669e-02, -3.7957e-02,  9.8068e-03],\n",
       "                         [ 4.1648e-03, -3.6611e-02, -4.8710e-02],\n",
       "                         [ 3.7416e-04,  2.0676e-03, -2.5078e-02]],\n",
       "               \n",
       "                        [[ 5.8284e-02,  5.1901e-03,  5.3009e-02],\n",
       "                         [ 7.2872e-04, -1.6713e-02,  3.5374e-02],\n",
       "                         [-1.0131e-02,  5.0062e-02, -1.9479e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.3767e-03, -1.2034e-03, -4.2708e-02],\n",
       "                         [ 3.1162e-02, -5.4115e-02, -1.8587e-02],\n",
       "                         [-3.1021e-04,  2.3716e-02, -1.8927e-02]],\n",
       "               \n",
       "                        [[-2.0851e-02, -4.9040e-02, -1.3251e-02],\n",
       "                         [ 1.1836e-02,  8.5989e-03,  5.9320e-02],\n",
       "                         [-1.5005e-02, -3.3563e-02, -3.0291e-02]],\n",
       "               \n",
       "                        [[ 3.2447e-02, -4.5090e-02,  3.4413e-02],\n",
       "                         [ 1.4186e-02, -8.0078e-03, -5.5526e-02],\n",
       "                         [ 2.3710e-02, -1.7888e-02, -2.6468e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3535e-03, -4.6939e-02, -3.8000e-02],\n",
       "                         [ 4.5195e-02, -6.4326e-02,  7.6383e-03],\n",
       "                         [ 7.2541e-03, -7.9630e-02, -4.7468e-02]],\n",
       "               \n",
       "                        [[ 2.0901e-02,  1.5754e-02,  2.3625e-02],\n",
       "                         [-5.4572e-02, -4.4321e-02, -4.3351e-02],\n",
       "                         [-5.6763e-02,  5.0064e-02, -1.2850e-02]],\n",
       "               \n",
       "                        [[-1.3336e-02,  3.8217e-02,  2.6076e-02],\n",
       "                         [ 3.3369e-02,  2.9154e-02,  2.0140e-02],\n",
       "                         [-2.7863e-03,  4.9059e-02,  2.7505e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.4442e-04, -6.9905e-03,  5.1513e-02],\n",
       "                         [-2.9548e-02, -4.9088e-02,  3.1979e-02],\n",
       "                         [ 3.9791e-02,  1.4876e-02, -1.2177e-02]],\n",
       "               \n",
       "                        [[-8.2850e-04,  5.1148e-02, -1.5545e-02],\n",
       "                         [-1.2785e-02,  7.3915e-02,  5.4139e-02],\n",
       "                         [ 9.3461e-02,  4.4485e-02,  3.1602e-02]],\n",
       "               \n",
       "                        [[-2.1151e-03, -4.5330e-04, -4.4565e-02],\n",
       "                         [-1.0282e-01, -4.2757e-02, -7.6484e-02],\n",
       "                         [-1.8677e-02,  1.2074e-02, -3.7740e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.0233e-03,  3.4157e-02,  4.8908e-02],\n",
       "                         [-1.3790e-02,  3.5846e-02,  5.0121e-02],\n",
       "                         [ 1.1362e-02, -1.8063e-02, -1.4542e-02]],\n",
       "               \n",
       "                        [[ 4.2676e-03, -1.4356e-02,  1.2788e-02],\n",
       "                         [-1.6214e-02,  8.6472e-03,  1.4520e-03],\n",
       "                         [ 1.4571e-02, -2.7827e-02, -3.9042e-02]],\n",
       "               \n",
       "                        [[ 4.9498e-02,  5.5938e-03, -5.5657e-03],\n",
       "                         [-5.8689e-03,  5.9269e-02,  1.8440e-02],\n",
       "                         [ 1.4232e-02,  5.8087e-02,  1.3712e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.9255e-02,  3.7683e-02, -3.5907e-02],\n",
       "                         [ 2.3164e-02, -2.1429e-02, -1.3765e-02],\n",
       "                         [ 4.2311e-03,  5.1843e-02,  4.9206e-02]],\n",
       "               \n",
       "                        [[ 7.8560e-02,  6.9009e-02,  5.1072e-02],\n",
       "                         [ 5.7993e-02,  9.7662e-02,  7.6212e-02],\n",
       "                         [-1.2178e-02,  5.2104e-02,  3.1011e-02]],\n",
       "               \n",
       "                        [[-2.1720e-02,  6.2952e-04,  3.7310e-02],\n",
       "                         [-3.9092e-03, -3.1375e-02, -3.3830e-02],\n",
       "                         [-2.1882e-02,  6.3913e-03,  2.2926e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.4090e-02, -4.3765e-02, -3.4845e-03],\n",
       "                         [ 4.8406e-02,  8.7374e-03, -6.2583e-03],\n",
       "                         [-5.6868e-02, -6.9677e-02, -8.0262e-02]],\n",
       "               \n",
       "                        [[-4.3687e-02,  2.0018e-02, -3.3008e-02],\n",
       "                         [ 1.4243e-02, -4.5496e-02, -2.0843e-02],\n",
       "                         [-3.3093e-02, -2.0732e-02,  1.6793e-02]],\n",
       "               \n",
       "                        [[-1.1730e-02,  2.7783e-02,  5.2310e-02],\n",
       "                         [ 2.8892e-02,  4.5672e-02, -3.0685e-02],\n",
       "                         [-5.3753e-02,  1.4569e-03, -7.2215e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2346e-03,  3.3736e-03, -5.3751e-02],\n",
       "                         [ 8.0512e-03, -4.7253e-03, -2.7187e-02],\n",
       "                         [ 4.6004e-02,  6.3030e-03, -1.2130e-02]],\n",
       "               \n",
       "                        [[-2.0353e-02,  9.5935e-02,  1.7028e-02],\n",
       "                         [ 6.2968e-04,  7.9693e-02, -1.6896e-03],\n",
       "                         [-4.4462e-02,  8.0178e-03, -1.1550e-04]],\n",
       "               \n",
       "                        [[-5.9322e-02,  1.6931e-02, -3.5418e-02],\n",
       "                         [-2.4703e-02,  1.8146e-03,  2.5648e-02],\n",
       "                         [-2.5140e-02, -2.5331e-02, -5.2890e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.6826e-02,  8.7188e-03, -1.4631e-02],\n",
       "                         [-4.2427e-03,  3.1592e-03,  3.4011e-02],\n",
       "                         [ 2.4498e-02, -4.3636e-03,  1.3631e-02]],\n",
       "               \n",
       "                        [[-4.8765e-03, -2.5093e-02, -4.6500e-02],\n",
       "                         [ 1.4969e-02,  4.3267e-02,  4.7689e-02],\n",
       "                         [ 3.1678e-02, -2.9005e-03,  1.8742e-02]],\n",
       "               \n",
       "                        [[-1.6282e-04,  3.7926e-02, -3.3435e-02],\n",
       "                         [-3.7151e-02, -5.9865e-03,  1.1217e-02],\n",
       "                         [-2.7224e-02,  4.4045e-02,  2.3198e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.8922e-02, -5.0440e-02, -2.4127e-02],\n",
       "                         [ 1.3196e-03, -3.4582e-02, -6.3706e-03],\n",
       "                         [-2.4766e-02,  2.4784e-02, -5.1412e-02]],\n",
       "               \n",
       "                        [[-8.5140e-02, -1.1747e-01, -1.0574e-02],\n",
       "                         [-7.6822e-02, -1.8023e-02, -1.2739e-02],\n",
       "                         [-2.4550e-02, -9.7639e-02,  5.0995e-02]],\n",
       "               \n",
       "                        [[ 1.0049e-02,  7.0355e-03, -2.8812e-02],\n",
       "                         [-6.1743e-02,  2.9907e-02, -4.9510e-02],\n",
       "                         [-1.2106e-02, -1.6349e-02, -4.7471e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.3281e-02, -5.9659e-02, -7.4845e-03],\n",
       "                         [ 3.9666e-03, -2.4579e-02, -1.6912e-02],\n",
       "                         [-3.0060e-02, -5.2557e-02, -3.4569e-02]],\n",
       "               \n",
       "                        [[ 1.1601e-02,  2.0093e-02,  1.9330e-02],\n",
       "                         [ 1.1518e-02, -3.4297e-03, -3.9855e-03],\n",
       "                         [ 1.1395e-02,  3.4817e-02,  6.0226e-03]],\n",
       "               \n",
       "                        [[ 3.0721e-02, -1.4320e-02, -2.4393e-02],\n",
       "                         [-7.5937e-02, -3.5532e-02, -2.3386e-02],\n",
       "                         [-4.9772e-02, -6.8097e-02, -2.4774e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7394e-05, -1.8980e-02,  4.6820e-02],\n",
       "                         [ 3.1611e-02, -5.9833e-02,  3.1288e-02],\n",
       "                         [-4.7890e-02, -1.8718e-02, -2.2998e-02]],\n",
       "               \n",
       "                        [[ 1.0739e-02,  4.0940e-02,  4.8134e-02],\n",
       "                         [ 4.7797e-02,  4.2867e-02, -1.0555e-02],\n",
       "                         [ 4.3696e-02,  1.1575e-01,  2.4784e-02]],\n",
       "               \n",
       "                        [[ 2.0929e-02,  5.0720e-02, -3.8524e-03],\n",
       "                         [ 6.9852e-03,  5.0116e-02, -2.8043e-02],\n",
       "                         [-1.1385e-02,  6.5274e-03,  1.4342e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.5599, -0.5552, -0.5566,  0.5535,  0.5584,  0.5569, -0.5565,  0.5590,\n",
       "                       -0.5581, -0.5547,  0.5560, -0.5532, -0.5587,  0.5597, -0.5554, -0.5580,\n",
       "                        0.5588,  0.5554, -0.5597, -0.5538,  0.5582, -0.5558, -0.5580,  0.5537,\n",
       "                        0.5585, -0.5426,  0.5531,  0.5594, -0.5537, -0.5554, -0.5546,  0.5546,\n",
       "                       -0.5588,  0.5417,  0.5531, -0.5571, -0.5582, -0.5558, -0.5579,  0.5563,\n",
       "                       -0.5596, -0.5546,  0.5582, -0.5543, -0.5566,  0.5547, -0.5583, -0.5600,\n",
       "                        0.5525,  0.5590,  0.5591, -0.5529,  0.5544, -0.5542, -0.5560, -0.5583,\n",
       "                        0.5550, -0.5568, -0.5573, -0.5581,  0.5572,  0.5543,  0.5578, -0.5564,\n",
       "                        0.5584,  0.5558,  0.5579,  0.5603,  0.5548,  0.5582,  0.5539,  0.5539,\n",
       "                       -0.5558,  0.5583, -0.5625, -0.5550,  0.5596, -0.5543,  0.5566,  0.5571,\n",
       "                       -0.5600, -0.5554, -0.5594,  0.5592,  0.5555,  0.5557,  0.5542, -0.5608,\n",
       "                       -0.5536,  0.5579, -0.5575,  0.5527, -0.5549,  0.5600,  0.5552,  0.5595,\n",
       "                        0.5599,  0.5586,  0.5556, -0.5573, -0.5529,  0.5576,  0.5554, -0.5570,\n",
       "                       -0.5573, -0.5582, -0.5583, -0.5566, -0.5555, -0.5548, -0.5574, -0.5599,\n",
       "                        0.5589,  0.5589,  0.5558,  0.5593, -0.5566,  0.5566,  0.5548,  0.5579,\n",
       "                       -0.5469, -0.5545, -0.5583,  0.5559, -0.5517,  0.5581, -0.5545,  0.5590],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.0752, -0.2222, -0.2209, -0.0301, -0.1385, -0.2287, -0.1965, -0.1863,\n",
       "                       -0.1171, -0.1848, -0.1981, -0.2096, -0.0811, -0.1206, -0.1761, -0.1511,\n",
       "                       -0.0310, -0.1785, -0.1354, -0.2051, -0.1303, -0.1971, -0.1930, -0.1128,\n",
       "                       -0.2056, -0.1850, -0.2187, -0.0820, -0.1456, -0.1416, -0.1590, -0.0678,\n",
       "                       -0.1887, -0.1743, -0.1259, -0.2086, -0.1691, -0.2446, -0.0669, -0.1646,\n",
       "                       -0.1385, -0.1979, -0.1650, -0.2105, -0.2164, -0.1499, -0.2251, -0.1195,\n",
       "                       -0.0877, -0.0550, -0.1372, -0.1770, -0.2247, -0.2134, -0.1610, -0.1654,\n",
       "                       -0.2468, -0.1107, -0.1712, -0.1072, -0.2446, -0.1986, -0.1468, -0.1368,\n",
       "                       -0.1576, -0.1597, -0.2342, -0.2002, -0.1029, -0.1339, -0.0791, -0.1650,\n",
       "                       -0.1137, -0.1226, -0.2262, -0.1769, -0.1857, -0.1967, -0.1823, -0.1826,\n",
       "                       -0.0848, -0.2136, -0.0045, -0.2291, -0.1357, -0.2389, -0.1607, -0.1593,\n",
       "                       -0.0241, -0.1573, -0.0325, -0.1927, -0.1623, -0.1636, -0.2368, -0.2161,\n",
       "                       -0.1971, -0.1735, -0.1392, -0.1891, -0.2072, -0.1061, -0.2262, -0.1545,\n",
       "                       -0.2028, -0.0613, -0.1243, -0.0735, -0.1948, -0.1561, -0.1174, -0.0502,\n",
       "                       -0.1899, -0.1496, -0.1963, -0.1685, -0.2141, -0.0210, -0.2279, -0.1681,\n",
       "                       -0.2047, -0.1384, -0.1491, -0.2190, -0.0969, -0.2133, -0.0907, -0.1880],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.8093, 0.7495, 0.7997, 0.8392, 0.8219, 0.7918, 0.8508, 0.7993, 0.9625,\n",
       "                       0.7940, 0.7750, 0.7383, 0.8417, 0.7778, 0.7483, 0.9617, 0.8718, 0.8679,\n",
       "                       0.8584, 0.6964, 0.7606, 0.7884, 0.8327, 0.7895, 0.7564, 0.7520, 0.8513,\n",
       "                       0.7525, 0.7667, 0.8095, 1.0495, 0.8087, 0.6924, 0.7637, 0.8062, 0.9847,\n",
       "                       0.8062, 0.8280, 0.9108, 1.0359, 0.7953, 0.7623, 0.7348, 0.7962, 0.7166,\n",
       "                       0.9111, 0.7272, 0.7804, 0.8488, 0.8290, 0.9651, 0.9863, 0.7369, 0.7757,\n",
       "                       0.9778, 0.7277, 0.8836, 0.7600, 0.7946, 0.8953, 0.8346, 0.7383, 0.9778,\n",
       "                       0.8786, 0.8494, 0.8047, 0.8066, 0.8885, 0.8185, 0.7463, 0.7989, 0.7729,\n",
       "                       0.8339, 0.9020, 0.7465, 0.7171, 0.7810, 0.8230, 0.9260, 0.7882, 0.8264,\n",
       "                       0.9635, 0.8743, 0.7630, 0.7732, 0.7391, 0.8376, 0.7785, 0.8861, 1.0193,\n",
       "                       0.8233, 0.9787, 0.8576, 0.8626, 0.8288, 0.8182, 0.7184, 0.7724, 0.8488,\n",
       "                       0.7623, 0.7973, 0.8505, 0.7446, 1.0277, 0.8610, 0.8332, 0.8319, 0.8343,\n",
       "                       0.9999, 0.9363, 0.8064, 0.8397, 0.7709, 0.7834, 0.7463, 0.8550, 0.7467,\n",
       "                       0.8445, 0.8815, 0.7924, 0.7761, 0.9897, 0.7584, 0.7370, 0.8763, 0.6919,\n",
       "                       0.7355, 0.7686], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0053,  0.0019,  0.0046,  ..., -0.0081,  0.0156, -0.0086],\n",
       "                       [-0.0151, -0.0111,  0.0108,  ..., -0.0063,  0.0078, -0.0164],\n",
       "                       [ 0.0050, -0.0080,  0.0010,  ..., -0.0224,  0.0091, -0.0180],\n",
       "                       [ 0.0016, -0.0087,  0.0101,  ...,  0.0078, -0.0055,  0.0038],\n",
       "                       [-0.0126,  0.0035,  0.0124,  ..., -0.0014,  0.0023, -0.0030]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0076,  0.0032,  0.0015, -0.0034, -0.0013], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[ 1.9450e-01, -9.9795e-02,  5.8899e-02,  2.0421e-01, -6.8798e-02,\n",
       "                        -2.0007e-01, -1.7882e-01,  1.9901e-01,  7.3716e-02, -7.7455e-02,\n",
       "                        -1.7553e-01,  1.2926e-01, -2.1960e-01, -5.1855e-02,  1.6968e-01,\n",
       "                        -7.0568e-02, -1.7667e-01,  8.5367e-02, -1.8126e-01, -4.0493e-02],\n",
       "                       [ 3.1346e-01, -2.4672e-01,  3.6882e-01, -2.9854e-01,  4.4060e-01,\n",
       "                        -3.9375e-02,  3.2006e-01,  2.4884e-01,  6.6769e-01, -1.2758e-01,\n",
       "                        -5.8387e-01, -8.7315e-02, -4.9148e-01, -3.9044e-02, -2.8058e-01,\n",
       "                        -1.2313e-01, -6.4514e-01, -3.9344e-01, -7.0918e-01, -4.2825e-01],\n",
       "                       [-1.6792e-01, -9.5964e-03, -4.3481e-02,  6.7062e-03,  1.1711e-01,\n",
       "                        -1.6487e-01, -6.0568e-02, -1.0674e-02,  1.3948e-01,  2.4193e-02,\n",
       "                         3.2932e-02,  1.9114e-01,  1.7074e-01,  1.4639e-01,  1.9790e-01,\n",
       "                        -1.4125e-01,  1.1538e-01,  1.8761e-01,  8.7872e-03, -1.8317e-01],\n",
       "                       [-8.6637e-02, -8.4453e-02, -1.6488e-02, -1.0229e-01, -1.2699e-01,\n",
       "                        -5.9725e-02,  7.7862e-02,  8.2837e-02,  1.6639e-01,  1.4864e-01,\n",
       "                        -6.2536e-02, -6.9908e-02, -7.0712e-02, -5.1099e-02, -1.4010e-01,\n",
       "                        -1.6697e-01,  1.6291e-01,  1.0379e-01, -7.5108e-02,  2.9448e-04],\n",
       "                       [-5.8330e-01, -5.2382e-01,  2.3097e-01, -2.0290e-01,  1.2620e-01,\n",
       "                        -5.2184e-01,  2.3124e-01,  1.6222e-01, -4.6191e-01, -4.8259e-01,\n",
       "                         2.0806e-01, -2.4699e-01, -2.1455e-01, -2.8649e-01, -5.9960e-03,\n",
       "                        -3.3979e-01, -7.1683e-02, -2.2812e-01,  9.2595e-01, -4.0021e-01],\n",
       "                       [-2.2718e-01, -3.8532e-01,  3.6588e-01, -4.9189e-01,  6.1618e-01,\n",
       "                        -4.3076e-01,  6.6215e-01,  1.3517e-01,  2.0993e-01, -6.5571e-01,\n",
       "                         2.4551e-01, -3.9373e-01, -5.2038e-01, -5.6636e-01, -3.1780e-01,\n",
       "                        -4.5481e-01, -3.0907e-01, -5.8442e-01, -1.5079e-01, -4.3089e-01],\n",
       "                       [-4.3577e-01, -2.5230e-01,  3.8415e-01, -3.5953e-01,  5.0516e-01,\n",
       "                        -4.9321e-01,  4.9733e-01, -2.0980e-01, -1.5045e-01, -5.2452e-01,\n",
       "                         1.5987e-01, -1.4204e-01, -7.4178e-02, -3.6433e-01, -4.2967e-01,\n",
       "                        -4.3072e-01, -4.1603e-03, -4.0346e-01,  5.7543e-01, -3.4250e-01],\n",
       "                       [-2.1789e-01,  1.4847e-01, -1.7759e-01, -1.5096e-02,  1.6466e-01,\n",
       "                         1.9650e-01, -2.1852e-02, -1.2478e-02, -1.0809e-02, -1.5789e-01,\n",
       "                        -1.5054e-01, -1.8219e-01,  1.2927e-01, -4.6389e-02,  2.1401e-01,\n",
       "                         1.6270e-01, -2.1683e-01,  3.1173e-02, -1.7408e-01,  5.3669e-02],\n",
       "                       [-2.0245e-01, -2.5325e-01,  2.2297e-01, -3.6453e-01,  2.6300e-01,\n",
       "                        -3.9136e-01,  2.1834e-01, -1.5191e-01, -3.9730e-01, -3.6379e-01,\n",
       "                         2.3120e-01, -4.2832e-02, -2.1437e-01, -1.7172e-01,  1.6827e-01,\n",
       "                        -3.6454e-01,  1.3989e-01, -1.1689e-01,  8.3335e-01, -4.2437e-02],\n",
       "                       [-4.6539e-01, -4.4326e-01, -1.8556e-02, -3.0206e-01,  1.8009e-01,\n",
       "                        -2.9671e-01,  9.9821e-03,  1.4084e-01, -5.4199e-01, -2.9132e-02,\n",
       "                         5.4752e-01, -1.5445e-01, -2.9636e-01, -2.3680e-01, -1.8175e-01,\n",
       "                        -2.4527e-01, -2.0243e-02, -3.0596e-01,  4.4147e-01, -3.4644e-02],\n",
       "                       [-5.1960e-01, -3.0793e-01,  3.9824e-01, -2.8498e-02,  2.7860e-01,\n",
       "                        -2.3621e-01,  1.3768e-01, -1.1015e-03, -1.6295e-01, -2.3110e-01,\n",
       "                         2.8005e-01, -2.1967e-01,  6.6372e-02, -1.7816e-01,  2.1774e-01,\n",
       "                        -2.7426e-01,  3.1498e-01, -1.8950e-01,  4.5967e-01, -2.2480e-01],\n",
       "                       [ 8.6240e-02, -1.0932e-01, -6.6633e-02,  3.1452e-01,  9.8137e-02,\n",
       "                         1.0588e-01, -2.6359e-01,  1.9449e-01, -3.9574e-02, -9.9108e-04,\n",
       "                         1.2918e-01,  1.1637e-01,  2.6810e-01, -7.8607e-02,  2.6053e-01,\n",
       "                         1.5103e-02,  2.4161e-01,  3.2538e-02,  9.5170e-02,  7.7577e-02],\n",
       "                       [ 2.2077e-01, -1.1893e-01,  3.3556e-03,  1.4105e-01,  1.9006e-01,\n",
       "                         5.2740e-02, -1.9452e-01,  2.2726e-02,  1.2223e-01,  4.7893e-02,\n",
       "                         1.0646e-01,  2.3871e-01,  9.1381e-02,  1.1641e-01,  1.0854e-01,\n",
       "                         1.8654e-01, -1.0111e-01,  1.9331e-01,  1.4477e-01, -3.2333e-03],\n",
       "                       [-4.7303e-02, -7.3556e-02, -1.5947e-01,  2.0988e-01, -1.0523e-01,\n",
       "                         1.4511e-01,  3.9058e-02, -2.0894e-01,  1.4523e-01,  1.4160e-01,\n",
       "                         2.1420e-01,  5.6961e-02,  1.9555e-01,  1.8229e-01, -1.8977e-01,\n",
       "                         2.0819e-01,  7.4866e-02, -1.2519e-01, -4.2634e-02, -2.1221e-01],\n",
       "                       [-4.0376e-01, -3.3445e-01,  2.3031e-01, -8.1199e-02, -9.7037e-02,\n",
       "                        -3.8110e-01,  3.1125e-01, -3.0648e-02, -6.1843e-01, -2.2994e-01,\n",
       "                         1.8667e-01, -2.6246e-02,  1.0519e-01, -2.5016e-01, -2.1890e-02,\n",
       "                        -1.8432e-01,  2.6919e-01, -3.7928e-01,  5.7429e-01, -3.0198e-01],\n",
       "                       [ 6.2884e-02,  1.5268e-01, -9.8051e-02, -7.9451e-02,  5.4087e-02,\n",
       "                        -8.8575e-02, -2.0254e-01,  7.8244e-03, -2.1483e-01,  1.6323e-01,\n",
       "                         5.5022e-02,  1.6895e-01,  1.5126e-01, -4.8578e-04, -1.3752e-01,\n",
       "                         1.3657e-02,  8.7678e-02,  7.2777e-02, -2.0556e-01,  1.6001e-01],\n",
       "                       [-1.7184e-01,  1.5199e-01, -1.4167e-01,  4.3341e-02, -1.1432e-01,\n",
       "                         1.8257e-02,  1.5661e-01,  6.9146e-03,  1.4682e-01,  1.5712e-01,\n",
       "                        -1.8079e-01, -2.4925e-02,  9.8675e-02,  1.8632e-01,  7.6713e-02,\n",
       "                        -7.8911e-02, -9.5569e-02, -1.8848e-01,  2.3741e-02, -1.2041e-01],\n",
       "                       [-3.4018e-01, -5.4116e-01,  7.2154e-01, -5.9619e-01,  7.5661e-01,\n",
       "                        -4.8814e-01,  6.5774e-01,  3.5671e-01,  4.0474e-01, -4.2754e-01,\n",
       "                         1.1906e-01, -4.7903e-01, -7.5758e-01, -4.7793e-01, -9.1343e-01,\n",
       "                        -7.0370e-01, -7.4674e-01, -4.1812e-01, -1.0789e-01, -7.1230e-01],\n",
       "                       [-3.7166e-01, -3.5114e-01,  9.7455e-02, -5.1995e-01,  1.1000e-01,\n",
       "                        -4.2867e-01,  2.9503e-01,  8.6841e-02, -1.5219e-01, -2.4102e-01,\n",
       "                         1.7106e-01, -4.9874e-01,  3.9382e-03, -4.8100e-01, -2.7536e-01,\n",
       "                        -5.0917e-01, -2.2812e-01, -5.0284e-01,  4.0104e-01, -5.0062e-01],\n",
       "                       [ 5.7188e-02, -1.6221e-01,  8.6147e-03, -1.6054e-01,  2.3078e-01,\n",
       "                        -2.4389e-01,  3.5177e-01,  4.9625e-01,  8.3814e-02, -1.8604e-01,\n",
       "                        -8.6181e-02,  7.3271e-02, -4.9693e-01, -2.3796e-01, -3.8974e-01,\n",
       "                        -3.0115e-01, -3.0443e-01,  1.8303e-02, -6.3854e-01, -2.5684e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([-0.1579,  0.3830, -0.2280, -0.1649,  0.1597,  0.5916,  0.4159,  0.0164,\n",
       "                        0.1514,  0.3201,  0.2737,  0.1193,  0.1049, -0.1574,  0.2005,  0.1238,\n",
       "                        0.1505,  0.7526,  0.3516,  0.0970], device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 0.1263, -0.0772,  0.1627, -0.0859, -0.2019, -0.2602, -0.4369,  0.1521,\n",
       "                        -0.2212, -0.2299, -0.1524,  0.1932,  0.2122,  0.1275, -0.0518,  0.0328,\n",
       "                         0.0779, -0.4234, -0.0063, -0.1765],\n",
       "                       [ 0.0947, -0.0592, -0.1505,  0.1282,  0.2700,  0.3092,  0.1813, -0.1357,\n",
       "                         0.3183,  0.2896,  0.0864,  0.0237,  0.1133,  0.0974,  0.3301, -0.0959,\n",
       "                        -0.1545,  0.3448,  0.2064, -0.0190],\n",
       "                       [-0.1231, -0.2893, -0.1265,  0.1026, -0.4462, -0.1169, -0.4914, -0.1678,\n",
       "                        -0.3340, -0.3942, -0.2506,  0.0273,  0.0290,  0.0147, -0.4609,  0.0933,\n",
       "                         0.0714, -0.5103, -0.3784, -0.4865],\n",
       "                       [ 0.2137,  0.0238,  0.1494,  0.1903, -0.3170, -0.1248, -0.1031,  0.0262,\n",
       "                        -0.2517,  0.1737, -0.1939, -0.1401, -0.0070,  0.0077, -0.2880,  0.0615,\n",
       "                        -0.1830, -0.2516, -0.0809,  0.1210],\n",
       "                       [-0.1682,  0.3614, -0.1215,  0.1635, -0.1256,  0.2145,  0.3066,  0.2003,\n",
       "                        -0.0317, -0.3850,  0.1504,  0.1634, -0.1465, -0.0818,  0.0308,  0.0231,\n",
       "                         0.0909,  0.0798,  0.2517, -0.0961],\n",
       "                       [ 0.1320,  0.0388,  0.1741,  0.0834,  0.2778,  0.1680, -0.0524,  0.0265,\n",
       "                         0.0478, -0.0274, -0.0982,  0.1646,  0.0666,  0.0608,  0.1382, -0.0616,\n",
       "                         0.2075,  0.3890, -0.0512,  0.0220],\n",
       "                       [ 0.0186,  0.8560, -0.0559,  0.0717,  0.8958,  0.9087,  0.9736, -0.1300,\n",
       "                         0.5675,  0.3810,  0.6347,  0.0049,  0.0567, -0.1425,  0.6163,  0.0342,\n",
       "                         0.1900,  0.8056,  0.9387,  0.3749],\n",
       "                       [-0.0348, -0.0985, -0.2037,  0.0392,  0.0375, -0.1490, -0.1671,  0.2235,\n",
       "                         0.1203,  0.1824,  0.1228, -0.1389,  0.1232,  0.1064, -0.1620, -0.2108,\n",
       "                         0.0877, -0.3298, -0.2546, -0.0387],\n",
       "                       [-0.0676, -0.2835, -0.1881,  0.2059,  0.3445, -0.0828,  0.2131, -0.1769,\n",
       "                         0.2083,  0.2633,  0.2230, -0.0921, -0.1128, -0.0414,  0.2611, -0.1009,\n",
       "                         0.0885, -0.1704,  0.0455, -0.4391],\n",
       "                       [ 0.0187, -0.5333,  0.0174,  0.0651, -0.3656, -0.2181, -0.3764,  0.0620,\n",
       "                        -0.4032,  0.0820, -0.3893,  0.0414, -0.0107,  0.0959, -0.1434, -0.0094,\n",
       "                         0.2114, -0.6247, -0.1515, -0.3881]], device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.2632,  0.2419, -0.3432, -0.0988,  0.0510,  0.0767,  0.6427, -0.1952,\n",
       "                        0.1331, -0.0769], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.483476383447647,\n",
       "   1.3545481526851655,\n",
       "   1.3098033878803252,\n",
       "   1.2686262041330338,\n",
       "   1.218484871506691,\n",
       "   1.1743631842136384,\n",
       "   1.1345976806879043,\n",
       "   1.086325322985649,\n",
       "   1.0543912591934204,\n",
       "   1.0350105865001678,\n",
       "   0.9866085749864578,\n",
       "   0.9846874786615372,\n",
       "   0.9660252624750137,\n",
       "   0.9358155431747437,\n",
       "   0.937087476849556,\n",
       "   0.915649830698967,\n",
       "   0.8967641706466675,\n",
       "   0.8855581641197204,\n",
       "   0.8825522440671921,\n",
       "   0.8663041898012162,\n",
       "   0.8615279206037522,\n",
       "   0.8308090027570725,\n",
       "   0.8249965822696685,\n",
       "   0.8191860826015472,\n",
       "   0.8189492141604423,\n",
       "   0.8036693303585053,\n",
       "   0.7832032409310341,\n",
       "   0.775857702255249,\n",
       "   0.7779861376881599,\n",
       "   0.7800523315668106,\n",
       "   0.7531858184337616,\n",
       "   0.7541596565246582,\n",
       "   0.7361088641285897,\n",
       "   0.7286121693253517,\n",
       "   0.7292022127509117,\n",
       "   0.7095676583051681,\n",
       "   0.7120725163817405,\n",
       "   0.7002220395207405,\n",
       "   0.7061740514039994,\n",
       "   0.6800183079242706,\n",
       "   0.6920564432740212,\n",
       "   0.6727037338614463,\n",
       "   0.6708960717916489,\n",
       "   0.6698056691288948,\n",
       "   0.6596473541855812,\n",
       "   0.6498868864178657,\n",
       "   0.655269834458828,\n",
       "   0.6354625440835953,\n",
       "   0.6332014302015304,\n",
       "   0.6399552510380745,\n",
       "   0.636753729403019,\n",
       "   0.6139424102902412,\n",
       "   0.6182934874892235,\n",
       "   0.6212592880129815,\n",
       "   0.6169081155657768,\n",
       "   0.6051740693747998,\n",
       "   0.6106658101081848,\n",
       "   0.5890947519540787,\n",
       "   0.5899993230104447,\n",
       "   0.5940517761707306,\n",
       "   0.5941763424873352,\n",
       "   0.5757712825536728,\n",
       "   0.5736726365685463,\n",
       "   0.6003453350663185,\n",
       "   0.5694817636013031,\n",
       "   0.5650899789333343,\n",
       "   0.562277569591999,\n",
       "   0.5423234815001488,\n",
       "   0.5434539637863636,\n",
       "   0.5505697911977768,\n",
       "   0.5450181565880775,\n",
       "   0.5424525443017483,\n",
       "   0.5502477190196514,\n",
       "   0.5398116151988507,\n",
       "   0.5268495962321759,\n",
       "   0.5161014630198478,\n",
       "   0.5290606241226197,\n",
       "   0.5290622363686561,\n",
       "   0.5131781410574913,\n",
       "   0.513524123877287,\n",
       "   0.4992736192941666,\n",
       "   0.5125679462850093,\n",
       "   0.5042422689497471,\n",
       "   0.49598163238167764,\n",
       "   0.48699826806783675,\n",
       "   0.4952307816147804,\n",
       "   0.48522328546643256,\n",
       "   0.48399221505224704,\n",
       "   0.4750914240181446,\n",
       "   0.46963882228732107,\n",
       "   0.4760855414271355,\n",
       "   0.4710879309475422,\n",
       "   0.4571245724260807,\n",
       "   0.4625591897964478,\n",
       "   0.4592583292722702,\n",
       "   0.4662577235996723,\n",
       "   0.44587932297587396,\n",
       "   0.4526464014351368,\n",
       "   0.4377901410162449],\n",
       "  'train_loss_std': [0.14395432451489668,\n",
       "   0.11333838373880073,\n",
       "   0.12711974579300753,\n",
       "   0.12105659789091615,\n",
       "   0.13434592199589895,\n",
       "   0.1313440359206019,\n",
       "   0.13860421011488458,\n",
       "   0.13037191019217056,\n",
       "   0.1272481605095346,\n",
       "   0.12974667647832655,\n",
       "   0.13741041014183927,\n",
       "   0.15123350618087675,\n",
       "   0.1418155604692662,\n",
       "   0.1374293410478573,\n",
       "   0.13186008332118565,\n",
       "   0.140305291953603,\n",
       "   0.13605381381361312,\n",
       "   0.13910498653681638,\n",
       "   0.13889903648615332,\n",
       "   0.1434268096153552,\n",
       "   0.13734807948094938,\n",
       "   0.13614374847329086,\n",
       "   0.14652969026418683,\n",
       "   0.14201605577733833,\n",
       "   0.14244231613074756,\n",
       "   0.15172665964938425,\n",
       "   0.1386274556381588,\n",
       "   0.1448365387598719,\n",
       "   0.1387978411864321,\n",
       "   0.1392369955539013,\n",
       "   0.13945563215547616,\n",
       "   0.13514215764376322,\n",
       "   0.14039674919546777,\n",
       "   0.14985599857866935,\n",
       "   0.13834088182754622,\n",
       "   0.14977615913270442,\n",
       "   0.13755955071196108,\n",
       "   0.1426919142423052,\n",
       "   0.14972353920124865,\n",
       "   0.14629833840481885,\n",
       "   0.13974395437761303,\n",
       "   0.13765699316149732,\n",
       "   0.13424311337994166,\n",
       "   0.13918151588458477,\n",
       "   0.1359302521562211,\n",
       "   0.13696987917735345,\n",
       "   0.1381445758943189,\n",
       "   0.13321522538615158,\n",
       "   0.13803707231606446,\n",
       "   0.13306890934295446,\n",
       "   0.1344519116346583,\n",
       "   0.1382546355147701,\n",
       "   0.13382131089253216,\n",
       "   0.1375218830216236,\n",
       "   0.14794151804726205,\n",
       "   0.1303917236588258,\n",
       "   0.13799221257661803,\n",
       "   0.13942923150945669,\n",
       "   0.13231009475445332,\n",
       "   0.13433800276299657,\n",
       "   0.13837884123711383,\n",
       "   0.13936341481167122,\n",
       "   0.14135101436191946,\n",
       "   0.14932812710107446,\n",
       "   0.1299503415927997,\n",
       "   0.1355631097071234,\n",
       "   0.13544314982727282,\n",
       "   0.1397812187860245,\n",
       "   0.1285458448637036,\n",
       "   0.1353943106281065,\n",
       "   0.13053567723681753,\n",
       "   0.13188190368150735,\n",
       "   0.14005317621076105,\n",
       "   0.13371949564274818,\n",
       "   0.13816014847329,\n",
       "   0.1314713657649213,\n",
       "   0.13112349419041336,\n",
       "   0.13471776218016887,\n",
       "   0.13027689970342018,\n",
       "   0.13290112830775985,\n",
       "   0.12413065756293956,\n",
       "   0.1321072014420857,\n",
       "   0.1399392415305865,\n",
       "   0.13140487575785323,\n",
       "   0.13105213940689583,\n",
       "   0.1271608603606777,\n",
       "   0.12232877988984227,\n",
       "   0.1290914488083553,\n",
       "   0.12357111193000987,\n",
       "   0.1265193672330218,\n",
       "   0.12458573172082225,\n",
       "   0.13162288135038194,\n",
       "   0.11534974463449765,\n",
       "   0.12952297854521969,\n",
       "   0.12435834453104028,\n",
       "   0.1389928684385691,\n",
       "   0.12120323266319391,\n",
       "   0.1277518306115412,\n",
       "   0.1216219440238964],\n",
       "  'train_accuracy_mean': [0.37069333359599116,\n",
       "   0.4429600010514259,\n",
       "   0.4647066667675972,\n",
       "   0.4883599992990494,\n",
       "   0.5131999998092651,\n",
       "   0.5351200005412102,\n",
       "   0.5534133326411247,\n",
       "   0.5779733316302299,\n",
       "   0.5907733336687088,\n",
       "   0.5979866657853127,\n",
       "   0.6220266659855842,\n",
       "   0.6233199979066849,\n",
       "   0.6316133319735527,\n",
       "   0.6442533332705498,\n",
       "   0.6442933332920074,\n",
       "   0.6498933331370353,\n",
       "   0.659346665263176,\n",
       "   0.6675466662049293,\n",
       "   0.6657066665887833,\n",
       "   0.6716933337450027,\n",
       "   0.6720133323669434,\n",
       "   0.6882933328151702,\n",
       "   0.6891733328104019,\n",
       "   0.6909999992251397,\n",
       "   0.6918533331155777,\n",
       "   0.6961466667056083,\n",
       "   0.705066666841507,\n",
       "   0.708159999191761,\n",
       "   0.7087200011014938,\n",
       "   0.7072133328318596,\n",
       "   0.717360000371933,\n",
       "   0.7175333334207534,\n",
       "   0.7243199995756149,\n",
       "   0.7285599997043609,\n",
       "   0.7260400002002716,\n",
       "   0.7356933336853981,\n",
       "   0.7333333344459534,\n",
       "   0.7377333332896232,\n",
       "   0.7363199999332428,\n",
       "   0.7475466676354409,\n",
       "   0.7430000010728836,\n",
       "   0.7492400013208389,\n",
       "   0.7498533320426941,\n",
       "   0.7495199999809266,\n",
       "   0.7541600006818772,\n",
       "   0.7580000001192093,\n",
       "   0.7559466677904129,\n",
       "   0.7636399995088577,\n",
       "   0.7658799993991852,\n",
       "   0.7625333334207535,\n",
       "   0.7626399983167649,\n",
       "   0.7732266647815704,\n",
       "   0.7706933327913285,\n",
       "   0.7705733337402344,\n",
       "   0.7721866673231125,\n",
       "   0.7775466668605805,\n",
       "   0.7746533321142197,\n",
       "   0.7821866668462754,\n",
       "   0.7803599991798401,\n",
       "   0.7778133322000503,\n",
       "   0.7796400007009506,\n",
       "   0.7875466661453248,\n",
       "   0.7886400009393693,\n",
       "   0.7777466655969619,\n",
       "   0.7903733334541321,\n",
       "   0.7915733312368393,\n",
       "   0.7917733327150345,\n",
       "   0.7990533335208893,\n",
       "   0.7999999995231628,\n",
       "   0.7966133326292038,\n",
       "   0.7994533331394196,\n",
       "   0.7988000001907348,\n",
       "   0.7957999991178513,\n",
       "   0.8017200000286102,\n",
       "   0.8063333348035813,\n",
       "   0.8085466663837433,\n",
       "   0.804853335261345,\n",
       "   0.8050666663646698,\n",
       "   0.8098933339118958,\n",
       "   0.8110666654109955,\n",
       "   0.8160133337974549,\n",
       "   0.8106666659116745,\n",
       "   0.8153466678857804,\n",
       "   0.817799998998642,\n",
       "   0.821519998908043,\n",
       "   0.8179866662025451,\n",
       "   0.8210000007152557,\n",
       "   0.8225199991464615,\n",
       "   0.8258133324384689,\n",
       "   0.8262800006866455,\n",
       "   0.8266933342218399,\n",
       "   0.8276533333063125,\n",
       "   0.833626668214798,\n",
       "   0.8289600015878678,\n",
       "   0.8305066677331925,\n",
       "   0.8271066660881042,\n",
       "   0.8359733339548111,\n",
       "   0.8337599998712539,\n",
       "   0.8393333332538605],\n",
       "  'train_accuracy_std': [0.0766484566360368,\n",
       "   0.06375887423490642,\n",
       "   0.06983140344978796,\n",
       "   0.06645081097227663,\n",
       "   0.06909561705264296,\n",
       "   0.06746593476190425,\n",
       "   0.07170428686099195,\n",
       "   0.06530189113446383,\n",
       "   0.06611657844767474,\n",
       "   0.0655488958249352,\n",
       "   0.06948943519117827,\n",
       "   0.07238477309039977,\n",
       "   0.06905824857850075,\n",
       "   0.06624096003845513,\n",
       "   0.06401016017503583,\n",
       "   0.06567317884517705,\n",
       "   0.06622533740727694,\n",
       "   0.06528929708091179,\n",
       "   0.06665708840864606,\n",
       "   0.0709408149481394,\n",
       "   0.06510941014946067,\n",
       "   0.061867409215896334,\n",
       "   0.06982092257944582,\n",
       "   0.063668411695221,\n",
       "   0.06475688423526006,\n",
       "   0.06988114262025241,\n",
       "   0.06372105914721576,\n",
       "   0.0669732372169136,\n",
       "   0.06379311623137775,\n",
       "   0.0649701734573289,\n",
       "   0.06280523295637327,\n",
       "   0.06085797585225891,\n",
       "   0.06363789743475273,\n",
       "   0.0669033610935633,\n",
       "   0.06302492042454541,\n",
       "   0.06611662585150434,\n",
       "   0.06259002349539497,\n",
       "   0.062139770798644786,\n",
       "   0.06652628540686055,\n",
       "   0.06532060176314308,\n",
       "   0.06242346371620217,\n",
       "   0.060841872811850016,\n",
       "   0.05806873802044365,\n",
       "   0.06142920633719081,\n",
       "   0.06116521208393779,\n",
       "   0.060884588818522274,\n",
       "   0.060173946170244186,\n",
       "   0.058278978144496695,\n",
       "   0.06011399276989966,\n",
       "   0.05876642937223986,\n",
       "   0.05848729440810904,\n",
       "   0.06153436154620754,\n",
       "   0.05852983513984785,\n",
       "   0.06068593123072759,\n",
       "   0.06457877700587399,\n",
       "   0.0558046494432728,\n",
       "   0.05948026564097738,\n",
       "   0.05974181981651452,\n",
       "   0.05926178628905173,\n",
       "   0.059203759905654935,\n",
       "   0.060292281796880745,\n",
       "   0.05929908234629067,\n",
       "   0.05990376867607915,\n",
       "   0.06180246448648162,\n",
       "   0.056648179731089446,\n",
       "   0.059240492050440206,\n",
       "   0.059980457648278536,\n",
       "   0.05917726179649516,\n",
       "   0.05534859438199707,\n",
       "   0.05959788838774204,\n",
       "   0.056826939943452844,\n",
       "   0.05730836646921678,\n",
       "   0.06117555891421147,\n",
       "   0.05631160789883762,\n",
       "   0.060449979888514506,\n",
       "   0.05730172560835069,\n",
       "   0.05648127201393746,\n",
       "   0.056781217680472944,\n",
       "   0.056481164532692286,\n",
       "   0.056896162071701076,\n",
       "   0.053301822981599106,\n",
       "   0.05656854241906197,\n",
       "   0.057846846322377785,\n",
       "   0.056290161973020725,\n",
       "   0.054676427920292967,\n",
       "   0.05465804694123846,\n",
       "   0.052474545056419096,\n",
       "   0.0540415962790607,\n",
       "   0.05391994874793915,\n",
       "   0.05351578745123087,\n",
       "   0.05204954302900933,\n",
       "   0.05573353128008745,\n",
       "   0.049162797646936346,\n",
       "   0.054824839077500014,\n",
       "   0.05387401950640552,\n",
       "   0.05937064315086252,\n",
       "   0.052577640343184684,\n",
       "   0.05322651995237746,\n",
       "   0.05125362104696331],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.4665240701039632,\n",
       "   1.4171582623322805,\n",
       "   1.3929247081279754,\n",
       "   1.3521079965432485,\n",
       "   1.3207840005556741,\n",
       "   1.2823985087871552,\n",
       "   1.2357146400213241,\n",
       "   1.2074770802259445,\n",
       "   1.16887910703818,\n",
       "   1.1530817288160324,\n",
       "   1.1405859416723252,\n",
       "   1.1206016393502554,\n",
       "   1.1152420143286388,\n",
       "   1.1028757802645366,\n",
       "   1.0881827984253565,\n",
       "   1.0752600787083308,\n",
       "   1.0674547817309696,\n",
       "   1.0507617690165838,\n",
       "   1.0455724114179612,\n",
       "   1.029528054992358,\n",
       "   1.0347131176789601,\n",
       "   1.0308612650632858,\n",
       "   1.0205002764860789,\n",
       "   1.0117877606550854,\n",
       "   0.9991415997346242,\n",
       "   0.9970713939269383,\n",
       "   0.9776921087503433,\n",
       "   0.9844942214091619,\n",
       "   0.971526497801145,\n",
       "   0.9629404856761297,\n",
       "   0.948158764441808,\n",
       "   0.9565264155467351,\n",
       "   0.9395616263151169,\n",
       "   0.9322134967645009,\n",
       "   0.9252809810638428,\n",
       "   0.9278887983163198,\n",
       "   0.9269906002283096,\n",
       "   0.9079518123467764,\n",
       "   0.9162568575143815,\n",
       "   0.9227782398462295,\n",
       "   0.9155866428216298,\n",
       "   0.9006667524576187,\n",
       "   0.9016995388269424,\n",
       "   0.9051644812027614,\n",
       "   0.9001533788442612,\n",
       "   0.9028488105535507,\n",
       "   0.9130249049266179,\n",
       "   0.9138602681954702,\n",
       "   0.8877618835369746,\n",
       "   0.8777974281708399,\n",
       "   0.9076733261346817,\n",
       "   0.8865848875045776,\n",
       "   0.881484682559967,\n",
       "   0.885455636382103,\n",
       "   0.8732743002971013,\n",
       "   0.8691663382450739,\n",
       "   0.8906439789136251,\n",
       "   0.8844351375102997,\n",
       "   0.8689276750882466,\n",
       "   0.8560870597759883,\n",
       "   0.8502876708904902,\n",
       "   0.8479099889596303,\n",
       "   0.8629307077328364,\n",
       "   0.8676115584373474,\n",
       "   0.850867133140564,\n",
       "   0.8470532782872517,\n",
       "   0.8576970545450846,\n",
       "   0.8460821300745011,\n",
       "   0.8614348691701889,\n",
       "   0.8581943772236507,\n",
       "   0.8761467144886652,\n",
       "   0.8548586076498031,\n",
       "   0.8423090946674346,\n",
       "   0.8468090269962947,\n",
       "   0.8592915604511897,\n",
       "   0.846453515291214,\n",
       "   0.8574012688795726,\n",
       "   0.8604395176966985,\n",
       "   0.8488959797223409,\n",
       "   0.8528792436917623,\n",
       "   0.8373437591393789,\n",
       "   0.8498614275455475,\n",
       "   0.8510965873797735,\n",
       "   0.8445712782939275,\n",
       "   0.8561002175013225,\n",
       "   0.8459126494328181,\n",
       "   0.847146147886912,\n",
       "   0.8611347327629725,\n",
       "   0.8484471168120702,\n",
       "   0.8389656205972036,\n",
       "   0.8527462327480316,\n",
       "   0.8448436069488525,\n",
       "   0.8654573482275009,\n",
       "   0.8528101329008738,\n",
       "   0.847724948724111,\n",
       "   0.8371095420916875,\n",
       "   0.853082467118899,\n",
       "   0.8638810996214549,\n",
       "   0.8563886060317357],\n",
       "  'val_loss_std': [0.08925934824660577,\n",
       "   0.0962115106736041,\n",
       "   0.09785377350198164,\n",
       "   0.10308777117396366,\n",
       "   0.10379286137959226,\n",
       "   0.1100652211658379,\n",
       "   0.10810219226848666,\n",
       "   0.115063940231856,\n",
       "   0.11814714321202346,\n",
       "   0.12110769536321783,\n",
       "   0.12240733687056339,\n",
       "   0.12659264634392622,\n",
       "   0.12382647519402466,\n",
       "   0.12968275305346055,\n",
       "   0.12844136788866767,\n",
       "   0.12429290625371377,\n",
       "   0.12871664938497682,\n",
       "   0.1266904178306537,\n",
       "   0.12727888312397284,\n",
       "   0.12889609389107845,\n",
       "   0.1313175871417351,\n",
       "   0.13201137032978313,\n",
       "   0.1259526668741782,\n",
       "   0.1296961244546495,\n",
       "   0.13350331956566044,\n",
       "   0.13289292460184882,\n",
       "   0.13145396794711273,\n",
       "   0.13060179318986692,\n",
       "   0.13357975411905498,\n",
       "   0.1334472919842112,\n",
       "   0.13183806199079628,\n",
       "   0.13489732834192797,\n",
       "   0.13689987353636482,\n",
       "   0.1342893066494463,\n",
       "   0.13439243955069743,\n",
       "   0.135543301869013,\n",
       "   0.13674488682062708,\n",
       "   0.1358373287169887,\n",
       "   0.13662543119044604,\n",
       "   0.1332164788220565,\n",
       "   0.14243757029091045,\n",
       "   0.13396603707063587,\n",
       "   0.1320232131485934,\n",
       "   0.1328392030775204,\n",
       "   0.13551865885521033,\n",
       "   0.13844065492774665,\n",
       "   0.1375059595211697,\n",
       "   0.1386343597346623,\n",
       "   0.13876031742746722,\n",
       "   0.13199150121379633,\n",
       "   0.141161827694547,\n",
       "   0.1368554390194922,\n",
       "   0.14136913080192484,\n",
       "   0.12493857512129981,\n",
       "   0.13582857057996295,\n",
       "   0.13535902917639603,\n",
       "   0.14323822959728608,\n",
       "   0.13862600489213434,\n",
       "   0.13144245184657638,\n",
       "   0.13920814706771145,\n",
       "   0.13557899393781472,\n",
       "   0.13331776433182943,\n",
       "   0.13177768400348586,\n",
       "   0.140342397160721,\n",
       "   0.14008246389525553,\n",
       "   0.13495934427253403,\n",
       "   0.13348690431098875,\n",
       "   0.13508018210927444,\n",
       "   0.1364814989770138,\n",
       "   0.12875634567051716,\n",
       "   0.13803099216237788,\n",
       "   0.13945810556226512,\n",
       "   0.1347022238355996,\n",
       "   0.13466460868966482,\n",
       "   0.1309065270280309,\n",
       "   0.1371298473825938,\n",
       "   0.13512415833957742,\n",
       "   0.13527794369829224,\n",
       "   0.1410121492868729,\n",
       "   0.13858548135255375,\n",
       "   0.13391902094933214,\n",
       "   0.13720964319277953,\n",
       "   0.14410937150890238,\n",
       "   0.1348865303337384,\n",
       "   0.1388979985155829,\n",
       "   0.14354996131970943,\n",
       "   0.13804031083444798,\n",
       "   0.14501041672857284,\n",
       "   0.13926559769320654,\n",
       "   0.133890478818652,\n",
       "   0.14732048299364692,\n",
       "   0.14037577380632232,\n",
       "   0.14494928544447935,\n",
       "   0.1362168295471893,\n",
       "   0.13970073765015756,\n",
       "   0.14332653753978378,\n",
       "   0.146693152060341,\n",
       "   0.14264648011756165,\n",
       "   0.14517821504828038],\n",
       "  'val_accuracy_mean': [0.38444444532195726,\n",
       "   0.4115555561085542,\n",
       "   0.42495555559794107,\n",
       "   0.44615555594364803,\n",
       "   0.4644888890782992,\n",
       "   0.4791555555661519,\n",
       "   0.5022000006834666,\n",
       "   0.5149777763088544,\n",
       "   0.5347777769962947,\n",
       "   0.540666664938132,\n",
       "   0.5482444435358047,\n",
       "   0.5573999987045923,\n",
       "   0.5586222209533056,\n",
       "   0.5657111112276713,\n",
       "   0.5733999998370807,\n",
       "   0.5777333334088326,\n",
       "   0.5819333322842916,\n",
       "   0.5869555561741193,\n",
       "   0.591244444946448,\n",
       "   0.6012444436550141,\n",
       "   0.5970666656891505,\n",
       "   0.6003111113111178,\n",
       "   0.6002888866265614,\n",
       "   0.6054222213228544,\n",
       "   0.6103999996185303,\n",
       "   0.6132666664322217,\n",
       "   0.6210222199559212,\n",
       "   0.6195777772863706,\n",
       "   0.6230222220222156,\n",
       "   0.6269333319862683,\n",
       "   0.6322222207983335,\n",
       "   0.6310666671395302,\n",
       "   0.6359333338340124,\n",
       "   0.6402666673064232,\n",
       "   0.6431333323319753,\n",
       "   0.6444222223758698,\n",
       "   0.6442222221692403,\n",
       "   0.6508888866504033,\n",
       "   0.6474444450934728,\n",
       "   0.6436666671435038,\n",
       "   0.6492888904611269,\n",
       "   0.6522444445888201,\n",
       "   0.6544888892769813,\n",
       "   0.6525555551052094,\n",
       "   0.6522444439927737,\n",
       "   0.6543333328763644,\n",
       "   0.6491777767737706,\n",
       "   0.6498666656017303,\n",
       "   0.6586222206552823,\n",
       "   0.6625555568933487,\n",
       "   0.6531777779261271,\n",
       "   0.6570888869961102,\n",
       "   0.6623777764042219,\n",
       "   0.6622444446881612,\n",
       "   0.6637999998529752,\n",
       "   0.6656444448232651,\n",
       "   0.6576666649182638,\n",
       "   0.6585555551449458,\n",
       "   0.6678666667143504,\n",
       "   0.6745333335796992,\n",
       "   0.6744000005722046,\n",
       "   0.6759333332379659,\n",
       "   0.6685333351294199,\n",
       "   0.6675333335002264,\n",
       "   0.6761333312590917,\n",
       "   0.6757333340247472,\n",
       "   0.670711112121741,\n",
       "   0.6745999991893769,\n",
       "   0.6738444425662359,\n",
       "   0.670933333337307,\n",
       "   0.6647111114859581,\n",
       "   0.6749999994039535,\n",
       "   0.678466666340828,\n",
       "   0.6754666682084401,\n",
       "   0.6697111110885938,\n",
       "   0.6778888899087906,\n",
       "   0.6735333324472109,\n",
       "   0.6703333316246668,\n",
       "   0.674511108994484,\n",
       "   0.6746444447835287,\n",
       "   0.6797777791817983,\n",
       "   0.6743111101786295,\n",
       "   0.6767111106713612,\n",
       "   0.6751333336035411,\n",
       "   0.6725333332022031,\n",
       "   0.6786888877550761,\n",
       "   0.6757999990383784,\n",
       "   0.673799999554952,\n",
       "   0.6739555535713831,\n",
       "   0.6764444426695506,\n",
       "   0.6765333332618078,\n",
       "   0.6791555551687877,\n",
       "   0.6720222214857737,\n",
       "   0.6716888894637426,\n",
       "   0.6760888901352883,\n",
       "   0.6818444436788559,\n",
       "   0.6746666649977366,\n",
       "   0.6732000005245209,\n",
       "   0.6749777751167615],\n",
       "  'val_accuracy_std': [0.05025957484042379,\n",
       "   0.05491092657106505,\n",
       "   0.056014860953133024,\n",
       "   0.057303915876824554,\n",
       "   0.05489921667889171,\n",
       "   0.0579440170032123,\n",
       "   0.05834803579157738,\n",
       "   0.061105550215058924,\n",
       "   0.06031634521410712,\n",
       "   0.061762986879083576,\n",
       "   0.061663598869231936,\n",
       "   0.06523958087297703,\n",
       "   0.0606033678292508,\n",
       "   0.06229301844736194,\n",
       "   0.06340577131165628,\n",
       "   0.06056685322641931,\n",
       "   0.06327104455941784,\n",
       "   0.06075626592125194,\n",
       "   0.06262711152273724,\n",
       "   0.06248383168348306,\n",
       "   0.06393969275864483,\n",
       "   0.06281701437949051,\n",
       "   0.059931363273559926,\n",
       "   0.06202454358468791,\n",
       "   0.06329229072301316,\n",
       "   0.061935416068123285,\n",
       "   0.05996165125627951,\n",
       "   0.060748173305084176,\n",
       "   0.06150501102389181,\n",
       "   0.060213841196828814,\n",
       "   0.06051833851282325,\n",
       "   0.0621864815082362,\n",
       "   0.06237683816852382,\n",
       "   0.06194085544109535,\n",
       "   0.06175546846432876,\n",
       "   0.059390728577605835,\n",
       "   0.0591716491127654,\n",
       "   0.06141741551392941,\n",
       "   0.06180365114969122,\n",
       "   0.05990888315942995,\n",
       "   0.06136905750596059,\n",
       "   0.060035789344127136,\n",
       "   0.05939820142683592,\n",
       "   0.058374165846856924,\n",
       "   0.05840217808412295,\n",
       "   0.06079260435417133,\n",
       "   0.06010908336176229,\n",
       "   0.05962212130719612,\n",
       "   0.059612559863349374,\n",
       "   0.05809432565964007,\n",
       "   0.05699231617511584,\n",
       "   0.05953623024313608,\n",
       "   0.06010957450016752,\n",
       "   0.058450356632963406,\n",
       "   0.060971797022165326,\n",
       "   0.0598503654293905,\n",
       "   0.06133846862200954,\n",
       "   0.0601441699666946,\n",
       "   0.05833781553150552,\n",
       "   0.05941228396370613,\n",
       "   0.05977528211285417,\n",
       "   0.06057425206481595,\n",
       "   0.060714853963637336,\n",
       "   0.06090664298512532,\n",
       "   0.05838198749400176,\n",
       "   0.0596547109791299,\n",
       "   0.05985858400613019,\n",
       "   0.0599631613697989,\n",
       "   0.05993976845198127,\n",
       "   0.059609990782477584,\n",
       "   0.0596237427018188,\n",
       "   0.05967846078212273,\n",
       "   0.0571043630562775,\n",
       "   0.059368629623608796,\n",
       "   0.05840654132492468,\n",
       "   0.05799989487210741,\n",
       "   0.057538047456175696,\n",
       "   0.05779946214836733,\n",
       "   0.0607674827004244,\n",
       "   0.05679514491519087,\n",
       "   0.05904633261034272,\n",
       "   0.0600537363777726,\n",
       "   0.06022302652991291,\n",
       "   0.05911085845608882,\n",
       "   0.05932669264004693,\n",
       "   0.056531075242787124,\n",
       "   0.06114212978223932,\n",
       "   0.0599154102371408,\n",
       "   0.05677687006701778,\n",
       "   0.059882189065888866,\n",
       "   0.05776694063981755,\n",
       "   0.05748454031729291,\n",
       "   0.05736355312258769,\n",
       "   0.0584817127366227,\n",
       "   0.060124250617929295,\n",
       "   0.05574018951959973,\n",
       "   0.0604844655835651,\n",
       "   0.05885560704646691,\n",
       "   0.0588831216038151],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb0c68",
   "metadata": {},
   "source": [
    "### 1.1 MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     #print(key)\n",
    "#     if value.requires_grad:\n",
    "#         print(key)\n",
    "#         print(value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a599c8",
   "metadata": {},
   "source": [
    "### 1.2 Arbiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ebc67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = arbiter_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = arbiter_system.state['best_epoch']\n",
    "\n",
    "state = arbiter_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "arbiter_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "569eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_test_batches(total_batches=int(600/2), augment_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0531d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a86b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "\n",
    "x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "\n",
    "\n",
    "x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task = next(zip(x_support_set,y_support_set,x_target_set, y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cdeb442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "647183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbiter_x_support_set, arbiter_x_target_set, arbiter_y_support_set, arbiter_y_target_set, seed = train_sample\n",
    "\n",
    "arbiter_x_support_set = torch.Tensor(arbiter_x_support_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_x_target_set = torch.Tensor(arbiter_x_target_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_y_support_set = torch.Tensor(arbiter_y_support_set).long().to(device=arbiter_system.model.device)\n",
    "arbiter_y_target_set = torch.Tensor(arbiter_y_target_set).long().to(device=arbiter_system.model.device)\n",
    "\n",
    "\n",
    "arbiter_x_support_set_task, arbiter_y_support_set_task, arbiter_x_target_set_task, arbiter_y_target_set_task = next(zip(arbiter_x_support_set,arbiter_y_support_set,arbiter_x_target_set, arbiter_y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ce1c0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd4d6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = arbiter_system.model.get_inner_loop_parameter_dict(arbiter_system.model.classifier.named_parameters())\n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = arbiter_x_target_set_task.shape\n",
    "\n",
    "arbiter_x_support_set_task = arbiter_x_support_set_task.view(-1, c, h, w)\n",
    "arbiter_y_support_set_task = arbiter_y_support_set_task.view(-1)\n",
    "arbiter_x_target_set_task = arbiter_x_target_set_task.view(-1, c, h, w)\n",
    "arbiter_y_target_set_task = arbiter_y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = arbiter_system.model.net_forward(\n",
    "            x=arbiter_x_support_set_task,\n",
    "            y=arbiter_y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "    \n",
    "    if arbiter_system.model.args.arbiter:\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                retain_graph=True)\n",
    "\n",
    "        names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "\n",
    "        for key, weight in names_weights_copy.items():\n",
    "            weight_norm = torch.norm(weight, p=2)\n",
    "            per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "        for key, grad in names_grads_copy.items():\n",
    "            gradient_l2norm = torch.norm(grad, p=2)\n",
    "            per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "        per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                    per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        generated_gradient_rate = arbiter_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "            g += 1\n",
    "\n",
    "    names_weights_copy,names_grads_copy = arbiter_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                      support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_arbiter.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=arbiter_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in arbiter_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d16650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "y_support_set_task = y_support_set_task.view(-1)\n",
    "x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "y_target_set_task = y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "            x=x_support_set_task,\n",
    "            y=y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "\n",
    "\n",
    "    names_weights_copy,names_grads_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                   support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_maml.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=maml_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575454f0",
   "metadata": {},
   "source": [
    "## landscape 함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aec9618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape==  torch.Size([25, 3, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "tensor([[[[ 1.8434e-04,  1.6267e-04,  4.5384e-04],\n",
      "          [ 3.5390e-04,  2.5260e-04,  3.4887e-04],\n",
      "          [ 4.9057e-04, -2.9846e-05,  5.2275e-04]],\n",
      "\n",
      "         [[ 1.0829e-04,  1.6357e-04, -7.0504e-05],\n",
      "          [-9.8948e-06, -8.3927e-05, -1.1623e-04],\n",
      "          [-8.5773e-05, -1.2296e-04, -2.5848e-04]],\n",
      "\n",
      "         [[-1.9906e-05, -2.6519e-05, -4.5178e-05],\n",
      "          [ 1.0074e-05, -2.2314e-06, -2.1448e-05],\n",
      "          [-3.4122e-05, -4.2992e-05, -6.0114e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.1515e-04,  5.7776e-04,  6.2564e-04],\n",
      "          [-5.3105e-05, -2.0857e-04, -1.1206e-04],\n",
      "          [ 5.1426e-04,  3.5645e-04,  2.7586e-04]],\n",
      "\n",
      "         [[-8.5581e-05, -5.8787e-05, -1.5918e-05],\n",
      "          [-3.0896e-04, -2.2046e-04, -2.3031e-04],\n",
      "          [-2.7069e-04, -3.0435e-04, -3.6678e-04]],\n",
      "\n",
      "         [[ 1.7294e-04,  9.8679e-05, -5.4890e-06],\n",
      "          [ 4.4815e-05, -3.9447e-05, -7.1588e-05],\n",
      "          [ 8.5102e-05,  8.8227e-06, -2.9362e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2178e-04,  4.2238e-04,  2.7163e-04],\n",
      "          [ 1.2730e-04,  1.5916e-04, -1.5307e-05],\n",
      "          [ 1.6724e-04,  5.9660e-05, -1.7811e-04]],\n",
      "\n",
      "         [[-3.9369e-04, -4.1470e-04, -3.9896e-04],\n",
      "          [-2.3555e-04, -1.6132e-04, -1.8357e-04],\n",
      "          [-1.4760e-04,  1.9657e-05,  1.5366e-04]],\n",
      "\n",
      "         [[-8.8173e-05, -8.6344e-05, -8.9532e-05],\n",
      "          [-1.0090e-04, -1.0782e-04, -9.1332e-05],\n",
      "          [-1.1175e-04, -8.5834e-05, -9.0544e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1271e-04,  1.5393e-04,  8.1983e-05],\n",
      "          [ 1.0342e-03,  1.0415e-03,  1.0361e-03],\n",
      "          [ 8.2789e-05,  4.6499e-04,  5.8123e-04]],\n",
      "\n",
      "         [[-2.8656e-04, -1.4201e-04, -2.6697e-04],\n",
      "          [-9.9985e-05, -2.7119e-05, -1.8100e-05],\n",
      "          [-4.0532e-04, -3.0486e-04, -2.3989e-04]],\n",
      "\n",
      "         [[ 6.1019e-06, -3.3762e-05, -1.9753e-04],\n",
      "          [ 2.6011e-04,  2.2032e-04,  1.2905e-04],\n",
      "          [ 7.0337e-04,  8.1156e-04,  9.4839e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8124e-03,  9.4348e-04,  8.3498e-04],\n",
      "          [ 1.4634e-03,  1.4775e-03,  1.5500e-03],\n",
      "          [ 1.3428e-03,  1.9937e-03,  1.7542e-03]],\n",
      "\n",
      "         [[-2.6874e-04, -1.9242e-04, -1.1246e-04],\n",
      "          [ 2.9723e-04,  1.0367e-04,  2.0905e-04],\n",
      "          [ 2.1589e-04, -8.3009e-05,  1.2153e-04]],\n",
      "\n",
      "         [[ 7.4155e-06,  1.2021e-05,  4.9172e-06],\n",
      "          [ 2.4237e-05,  3.4755e-05,  2.7932e-05],\n",
      "          [ 1.4374e-05,  1.7783e-05,  4.2648e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3657e-03,  9.5745e-04,  8.2881e-04],\n",
      "          [ 1.2973e-03,  5.3666e-04,  7.7443e-04],\n",
      "          [-3.6567e-05, -6.9886e-05,  6.3996e-04]],\n",
      "\n",
      "         [[ 1.8707e-04,  6.6766e-05, -8.3947e-05],\n",
      "          [ 4.9415e-05, -9.3001e-05, -2.2115e-04],\n",
      "          [-1.6298e-04, -2.2078e-04, -2.1246e-04]],\n",
      "\n",
      "         [[-1.2723e-04,  5.9091e-05,  1.6360e-05],\n",
      "          [ 4.8273e-04,  3.6960e-04,  2.6085e-04],\n",
      "          [ 2.9596e-04,  3.4356e-04,  3.9897e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.3242e-03,  2.9175e-04,  2.7321e-04],\n",
      "          [ 7.1343e-04,  4.0933e-04, -2.0739e-04],\n",
      "          [-5.8893e-06,  5.7876e-04, -2.8743e-04]],\n",
      "\n",
      "         [[-3.2036e-04, -7.8927e-05, -1.4495e-04],\n",
      "          [-2.9122e-04,  7.7554e-05, -7.9411e-05],\n",
      "          [-2.0667e-04, -1.1664e-04, -2.5673e-04]],\n",
      "\n",
      "         [[-3.0403e-05, -6.5715e-07, -1.4063e-05],\n",
      "          [-5.8273e-05,  1.8477e-05, -3.0110e-05],\n",
      "          [-9.5312e-05, -2.6575e-05, -4.7096e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.9861e-07, -1.4447e-05, -1.0045e-04],\n",
      "          [ 3.3326e-04,  4.0200e-05, -1.6724e-04],\n",
      "          [-2.9436e-05,  3.2517e-05,  8.5242e-05]],\n",
      "\n",
      "         [[-2.9758e-05, -3.6176e-04, -1.8183e-04],\n",
      "          [ 8.2424e-05, -3.0514e-04, -1.2752e-04],\n",
      "          [-3.2575e-05, -3.1167e-04,  7.3925e-05]],\n",
      "\n",
      "         [[-5.8039e-06, -6.5674e-06, -2.1438e-05],\n",
      "          [ 1.0765e-04, -6.2816e-05, -1.5379e-04],\n",
      "          [ 1.1295e-04, -2.0249e-05, -7.7092e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4627e-04,  8.3668e-04,  3.6570e-04],\n",
      "          [ 1.8153e-04,  4.0057e-04,  1.9566e-05],\n",
      "          [-2.8968e-04,  3.7277e-04, -3.6438e-04]],\n",
      "\n",
      "         [[-5.4976e-04, -4.7879e-04, -6.9904e-05],\n",
      "          [-6.3258e-04, -1.9607e-04, -4.6385e-05],\n",
      "          [-2.1916e-04, -5.9809e-05,  2.0105e-05]],\n",
      "\n",
      "         [[-1.6008e-04, -1.5422e-04, -1.2788e-04],\n",
      "          [-1.5463e-04, -1.3447e-04, -9.8746e-05],\n",
      "          [-1.3906e-04, -1.5099e-04, -1.5173e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.1316e-05,  5.3981e-04,  7.5981e-04],\n",
      "          [ 6.0068e-05,  9.5317e-05, -2.3908e-04],\n",
      "          [ 3.4203e-04,  7.1343e-04,  3.3573e-04]],\n",
      "\n",
      "         [[-3.7944e-05, -2.8060e-04,  1.7838e-05],\n",
      "          [-9.6416e-05, -2.7365e-04, -1.5002e-04],\n",
      "          [-2.1815e-04, -1.8765e-04, -1.1446e-04]],\n",
      "\n",
      "         [[-2.1678e-04, -2.3489e-04, -2.6357e-05],\n",
      "          [-1.6256e-04, -4.8252e-06, -1.8839e-05],\n",
      "          [-3.2451e-05,  1.5138e-04,  2.2153e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.5739e-04,  3.8730e-04,  7.1120e-04],\n",
      "          [ 1.6560e-04,  4.7792e-04,  3.3371e-04],\n",
      "          [ 1.0745e-03,  9.9627e-04,  4.1105e-04]],\n",
      "\n",
      "         [[-8.7504e-05,  4.2453e-05, -4.0288e-05],\n",
      "          [ 2.1181e-04,  1.3430e-04, -2.4174e-05],\n",
      "          [ 2.5491e-04, -9.2172e-05, -1.5027e-04]],\n",
      "\n",
      "         [[ 3.3593e-05,  4.5948e-05, -1.0473e-05],\n",
      "          [ 5.0692e-06,  1.7122e-06, -4.9100e-05],\n",
      "          [ 2.2104e-05,  3.7114e-05,  8.0374e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.4645e-04,  1.8164e-04,  6.9069e-04],\n",
      "          [ 3.3981e-04,  3.0001e-04,  2.8309e-04],\n",
      "          [ 1.2153e-03,  1.6135e-03,  1.8486e-03]],\n",
      "\n",
      "         [[ 8.4005e-05,  1.5817e-04,  4.3050e-04],\n",
      "          [ 2.6970e-04,  1.0863e-05,  2.4673e-04],\n",
      "          [ 5.7932e-04,  6.1351e-04,  6.1687e-04]],\n",
      "\n",
      "         [[-1.9054e-04, -2.1654e-04, -1.1859e-04],\n",
      "          [ 7.3561e-05, -7.8255e-06, -7.7194e-05],\n",
      "          [-2.6881e-04, -1.4187e-04, -2.4616e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 8.7311e-11, -2.9104e-11, -5.0932e-11, -1.8917e-10,  1.7462e-10,\n",
      "        -1.1642e-10, -1.2806e-09,  1.3097e-10,  0.0000e+00, -4.3656e-10,\n",
      "         1.2369e-10, -2.9104e-10, -2.9104e-11, -2.3283e-10,  4.6566e-10,\n",
      "        -2.4738e-10,  5.8208e-11,  2.9104e-11, -8.7311e-11, -5.8208e-10,\n",
      "        -5.8208e-11,  1.4552e-10, -8.7311e-11,  2.8376e-10,  1.7462e-10,\n",
      "        -1.1642e-10,  5.8208e-11,  1.7462e-10, -1.6007e-10, -5.8208e-11,\n",
      "         3.6380e-11,  3.0559e-10,  3.7835e-10,  1.3461e-10,  5.5297e-10,\n",
      "         1.1642e-10, -2.9104e-10, -2.2555e-10, -5.8208e-11, -1.0186e-10,\n",
      "        -1.1642e-10,  2.0373e-10, -8.7311e-11,  1.1642e-10,  1.2733e-10,\n",
      "        -1.1642e-10,  0.0000e+00,  1.4552e-10,  3.2014e-10, -1.1642e-10,\n",
      "        -1.4552e-10, -8.0036e-11,  3.2742e-10, -2.4738e-10, -5.8208e-11,\n",
      "         2.3283e-10,  2.9104e-11,  1.4552e-10,  1.0186e-10, -1.4552e-11,\n",
      "         2.3283e-10,  5.8208e-11,  1.0186e-10,  2.3283e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-4.9430e-04, -3.8415e-04, -6.1528e-04],\n",
      "          [-2.6318e-04, -3.3839e-04, -3.2860e-04],\n",
      "          [-3.3334e-04, -4.9341e-04, -3.6388e-04]],\n",
      "\n",
      "         [[ 1.5668e-04,  3.1019e-04,  2.9224e-04],\n",
      "          [ 9.4537e-06,  2.3923e-04,  4.8048e-04],\n",
      "          [ 2.5597e-04,  5.0901e-04,  5.8967e-04]],\n",
      "\n",
      "         [[ 1.6342e-04,  4.1681e-04,  3.1076e-04],\n",
      "          [ 1.3348e-04,  4.4680e-04,  3.5182e-04],\n",
      "          [ 2.1319e-04,  3.0014e-04,  2.3858e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.2252e-04, -6.7625e-04, -5.9616e-04],\n",
      "          [-5.9443e-04, -5.7399e-04, -3.7621e-04],\n",
      "          [-7.9439e-04, -8.9259e-04, -4.7599e-04]],\n",
      "\n",
      "         [[-2.5597e-04, -1.2673e-04,  8.7929e-05],\n",
      "          [-2.6581e-05,  3.9001e-05,  1.2982e-04],\n",
      "          [-1.9036e-04, -1.7630e-04,  1.0497e-05]],\n",
      "\n",
      "         [[-4.6312e-04, -2.2582e-04, -1.1082e-04],\n",
      "          [-1.7482e-04, -5.9088e-06, -1.5120e-05],\n",
      "          [-1.0589e-04,  1.5479e-04,  1.7987e-04]]],\n",
      "\n",
      "\n",
      "        [[[-8.7199e-04, -9.3965e-04, -1.0163e-03],\n",
      "          [-3.2525e-04, -2.1676e-04, -2.6697e-04],\n",
      "          [-4.4978e-04, -3.1340e-04, -4.5197e-04]],\n",
      "\n",
      "         [[-1.9527e-04, -3.4316e-04, -2.5637e-04],\n",
      "          [ 2.4949e-04,  1.9312e-04, -1.3550e-04],\n",
      "          [ 2.3178e-04, -5.8400e-07, -5.6194e-04]],\n",
      "\n",
      "         [[ 1.7339e-04, -3.7368e-04, -1.0246e-03],\n",
      "          [ 1.1111e-04, -8.5909e-04, -1.3919e-03],\n",
      "          [ 2.1997e-04, -7.1893e-04, -9.8405e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.0267e-04, -1.9476e-04, -7.4849e-04],\n",
      "          [ 1.1917e-04,  3.0761e-04, -2.7532e-04],\n",
      "          [-1.7975e-04,  1.2397e-04, -4.9009e-04]],\n",
      "\n",
      "         [[-6.5755e-04, -7.2153e-04, -5.3959e-04],\n",
      "          [-1.0972e-04, -4.2368e-04, -4.1598e-04],\n",
      "          [-3.8544e-04, -7.7871e-04, -9.0405e-04]],\n",
      "\n",
      "         [[-1.3056e-03, -2.1292e-03, -2.0386e-03],\n",
      "          [ 3.4637e-04, -2.3806e-04,  8.5527e-05],\n",
      "          [ 4.9849e-04,  7.9737e-04,  3.5486e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.1273e-04, -4.5955e-04, -3.5762e-04],\n",
      "          [ 4.1375e-05, -1.5237e-04, -3.6197e-05],\n",
      "          [-2.0135e-04, -3.7984e-04, -4.7796e-05]],\n",
      "\n",
      "         [[ 1.6404e-04,  2.6445e-04,  3.4117e-04],\n",
      "          [ 4.6067e-05,  6.5429e-05,  2.3386e-05],\n",
      "          [-2.9885e-04, -2.9105e-04, -3.2968e-04]],\n",
      "\n",
      "         [[-6.7753e-05, -2.9115e-04, -4.1600e-04],\n",
      "          [-1.7792e-04, -2.6393e-04, -2.8593e-04],\n",
      "          [-2.5460e-04, -5.0403e-04, -4.0012e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.9798e-04,  1.6348e-04,  3.7123e-04],\n",
      "          [ 4.9378e-04,  4.5570e-04,  5.6738e-04],\n",
      "          [ 2.9109e-04,  1.7660e-04,  4.5247e-04]],\n",
      "\n",
      "         [[ 5.3276e-04,  5.8010e-04,  8.1637e-04],\n",
      "          [ 5.8865e-04,  5.9550e-04,  3.7656e-04],\n",
      "          [ 6.5022e-04,  7.3620e-04,  6.8857e-04]],\n",
      "\n",
      "         [[ 5.0252e-04, -9.2133e-05, -5.6462e-04],\n",
      "          [ 7.9883e-05, -3.5455e-04, -6.6051e-04],\n",
      "          [ 9.7232e-05, -3.3478e-04, -1.7960e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.6523e-04, -4.6941e-04, -5.5808e-04],\n",
      "          [-2.8220e-04, -6.9616e-04, -7.2141e-04],\n",
      "          [-4.6772e-04, -5.1917e-04, -7.3768e-04]],\n",
      "\n",
      "         [[-8.2369e-04, -1.2869e-03, -4.4351e-04],\n",
      "          [-3.7370e-04, -5.1226e-04, -1.1425e-04],\n",
      "          [ 1.7504e-04,  1.2675e-06, -1.3407e-04]],\n",
      "\n",
      "         [[ 8.4553e-04,  9.3713e-04,  6.7043e-05],\n",
      "          [ 6.0296e-04,  1.9552e-04, -6.2708e-06],\n",
      "          [ 4.0905e-04, -1.5601e-04, -4.3104e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.2512e-04, -3.8987e-04, -5.6439e-04],\n",
      "          [-3.3698e-04, -6.6951e-04, -9.2723e-04],\n",
      "          [-2.0970e-04, -1.9794e-04, -4.1216e-04]],\n",
      "\n",
      "         [[-3.8575e-04, -5.8708e-04, -2.0736e-04],\n",
      "          [-4.9293e-04, -6.3945e-04, -5.5326e-04],\n",
      "          [-6.2146e-04, -7.1556e-04, -3.6106e-04]],\n",
      "\n",
      "         [[-2.9103e-04, -1.0201e-04, -6.7487e-04],\n",
      "          [ 2.7798e-04,  5.6337e-04, -3.4610e-04],\n",
      "          [-1.4998e-04, -5.2530e-04, -4.2235e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5636e-04,  5.9393e-04,  5.6694e-04],\n",
      "          [ 1.1708e-03,  1.2950e-03,  1.0097e-03],\n",
      "          [ 1.2281e-03,  1.1326e-03,  1.4838e-03]],\n",
      "\n",
      "         [[ 1.3364e-04,  5.1966e-04,  5.6733e-04],\n",
      "          [ 7.7343e-07, -2.1330e-04,  1.5337e-04],\n",
      "          [ 2.2031e-04,  3.2275e-04,  4.7538e-04]],\n",
      "\n",
      "         [[-2.6300e-03, -3.0054e-03, -3.0536e-03],\n",
      "          [-3.7237e-03, -3.9656e-03, -3.2123e-03],\n",
      "          [-3.7792e-03, -4.1767e-03, -3.7952e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2476e-03,  1.8959e-03,  1.7290e-03],\n",
      "          [ 2.8283e-03,  2.6497e-03,  1.7569e-03],\n",
      "          [ 2.9236e-03,  2.6690e-03,  2.6397e-03]],\n",
      "\n",
      "         [[ 1.1835e-03,  6.7107e-04,  3.3600e-04],\n",
      "          [ 1.5944e-03,  9.3165e-04,  8.4382e-04],\n",
      "          [ 1.7111e-03,  1.3444e-03,  7.1697e-04]],\n",
      "\n",
      "         [[-7.2562e-04, -1.0640e-04,  1.5275e-03],\n",
      "          [ 3.8012e-04,  3.1574e-04,  7.8247e-04],\n",
      "          [ 8.5348e-04,  2.1129e-03,  2.1039e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6155e-04,  8.0914e-05,  1.5957e-04],\n",
      "          [ 1.8431e-04,  3.7929e-05,  2.1316e-04],\n",
      "          [ 1.7569e-04,  2.0617e-04,  2.1699e-04]],\n",
      "\n",
      "         [[-9.2505e-05,  2.5929e-04,  3.1844e-04],\n",
      "          [ 3.1740e-05,  2.6895e-04,  1.1432e-04],\n",
      "          [ 5.5673e-05, -1.7176e-05,  1.4653e-04]],\n",
      "\n",
      "         [[-9.3230e-04, -2.1651e-04,  3.8291e-05],\n",
      "          [-1.0527e-03,  8.6976e-05,  3.1326e-05],\n",
      "          [-9.8283e-04,  1.5408e-04,  3.3565e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4326e-04,  3.3473e-04,  6.1337e-04],\n",
      "          [ 2.0052e-05, -2.0618e-04,  3.9416e-04],\n",
      "          [ 2.1023e-06, -1.3640e-04,  4.4921e-04]],\n",
      "\n",
      "         [[ 3.2758e-04, -1.6057e-04, -2.1088e-04],\n",
      "          [ 2.4711e-04,  2.0177e-04, -2.2249e-04],\n",
      "          [ 2.6961e-05,  2.3933e-04,  2.0562e-04]],\n",
      "\n",
      "         [[-1.6149e-04,  1.8295e-04,  2.4261e-04],\n",
      "          [-2.1819e-04,  1.9251e-04,  2.1405e-05],\n",
      "          [ 3.1492e-04,  7.0288e-05,  2.5495e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 2.0373e-10, -2.9104e-10, -6.4028e-10,  3.4925e-10,  5.8208e-11,\n",
      "         1.7462e-10,  4.6566e-10, -2.3283e-10,  5.2387e-10, -5.8208e-10,\n",
      "         0.0000e+00, -3.6380e-10,  6.9849e-10, -3.2014e-10,  1.6007e-10,\n",
      "         2.3283e-10,  4.6566e-10,  1.1642e-10,  6.9849e-10, -1.1642e-10,\n",
      "         3.4925e-10,  4.6566e-10,  5.8208e-11, -5.8208e-10,  5.2387e-10,\n",
      "        -2.9104e-10,  1.3097e-10,  5.8208e-11, -2.9104e-11,  2.0373e-10,\n",
      "         2.9104e-10,  4.0745e-10,  1.1642e-10,  3.4925e-10, -1.7462e-10,\n",
      "        -4.6566e-10, -5.8208e-10, -1.1642e-10,  6.1118e-10,  3.6380e-10,\n",
      "        -9.8953e-10, -2.9104e-11,  8.7311e-11, -1.5280e-10, -1.2806e-09,\n",
      "         0.0000e+00,  3.4925e-10,  0.0000e+00,  4.6566e-10, -5.2387e-10,\n",
      "         1.1642e-10,  1.4552e-10,  3.4925e-10, -1.1642e-10, -1.1642e-10,\n",
      "        -6.9849e-10, -2.9104e-10, -2.9104e-10,  8.7311e-11,  4.3656e-10,\n",
      "         8.7311e-11,  2.3283e-10,  1.8626e-09, -1.1642e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[-0.0194,  0.0036,  0.0073,  ..., -0.0027, -0.0137, -0.0334],\n",
      "        [ 0.0045,  0.0011,  0.0006,  ..., -0.0105, -0.0204, -0.0190],\n",
      "        [ 0.0134,  0.0105,  0.0116,  ..., -0.0070,  0.0040,  0.0081],\n",
      "        [ 0.0148,  0.0023, -0.0105,  ..., -0.0016, -0.0050,  0.0102],\n",
      "        [-0.0134, -0.0175, -0.0091,  ...,  0.0218,  0.0351,  0.0341]],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([-0.0025, -0.0115,  0.0093, -0.0051,  0.0099], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-7.3044e-05,  1.2318e-04,  6.6176e-05,  2.0686e-04, -9.4394e-05,\n",
      "        -8.7659e-05, -3.1901e-04, -1.4534e-04,  5.3340e-05,  9.4551e-05,\n",
      "        -3.1337e-04, -1.7767e-04,  2.7972e-04, -1.2147e-04, -1.2926e-04,\n",
      "        -7.6917e-05,  9.5421e-05,  1.5133e-04,  1.1130e-04, -3.5452e-04,\n",
      "        -5.9234e-05, -4.7240e-05,  2.2286e-05, -2.0643e-04,  1.2994e-04,\n",
      "        -3.7191e-05,  2.4259e-04, -1.2098e-04, -1.2256e-04, -4.9012e-06,\n",
      "        -1.9338e-05, -1.0696e-04,  1.0460e-05,  7.2988e-05,  5.0371e-04,\n",
      "        -2.5012e-04,  1.7498e-04,  1.1593e-04,  4.4227e-06,  2.3034e-04,\n",
      "         2.1564e-04,  6.2214e-07, -4.7535e-05,  1.2148e-04,  2.5149e-05,\n",
      "         7.8713e-05,  1.6190e-04, -1.9906e-04,  2.3639e-04, -1.9612e-04,\n",
      "         1.3217e-04,  1.6366e-04,  8.9363e-05, -3.5275e-05,  5.0172e-05,\n",
      "         4.4225e-04, -9.0221e-05,  1.0104e-04, -1.3713e-04,  1.8329e-04,\n",
      "         1.9822e-04,  1.3810e-04,  4.8603e-05, -2.0105e-04,  7.9110e-05,\n",
      "         7.2287e-05,  7.5737e-05, -2.4362e-04,  3.1189e-05,  7.1305e-05,\n",
      "        -4.6957e-05, -2.3884e-05, -1.4298e-04,  4.5841e-05,  1.1080e-04,\n",
      "        -1.7994e-04,  3.5994e-04,  4.1501e-04,  1.3762e-04,  1.6659e-04,\n",
      "        -2.5186e-05, -2.2040e-05, -1.9911e-04, -1.0807e-04, -5.7762e-05,\n",
      "        -5.1260e-05, -4.4362e-05, -2.7799e-04,  5.7223e-04, -1.3600e-04,\n",
      "         1.4806e-04,  5.0371e-04, -1.3982e-05,  1.1605e-04, -3.5917e-05,\n",
      "        -1.5765e-04,  3.9730e-04, -2.0792e-05,  2.6772e-05,  1.8569e-04,\n",
      "        -1.3000e-04, -4.2993e-04,  8.3612e-05, -1.3786e-04, -1.7261e-04,\n",
      "        -2.1565e-04,  1.7414e-04,  1.9797e-04,  9.8224e-05, -3.6077e-04,\n",
      "         7.7123e-05,  1.1106e-04, -2.9758e-04,  1.6157e-04,  1.0355e-04,\n",
      "         8.9457e-05,  4.7037e-04, -2.8246e-05,  1.3891e-05,  6.3619e-05,\n",
      "        -1.3305e-04,  3.0377e-04,  1.8296e-04, -1.2856e-04,  6.4148e-05,\n",
      "         6.2276e-05, -4.0516e-05,  1.5446e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 5.0932e-11,  3.6380e-12,  3.6380e-12,  5.4570e-12,  3.6380e-11,\n",
      "         1.2733e-11,  1.8190e-11, -7.2760e-12, -3.6380e-12,  1.2733e-11,\n",
      "         4.3656e-11, -7.2760e-12,  1.1369e-11, -5.4570e-11, -1.6371e-11,\n",
      "         9.4587e-11,  1.0914e-11, -9.0949e-13,  3.6380e-12,  2.9104e-11,\n",
      "        -2.0009e-11, -2.7285e-11,  1.4552e-11,  4.3656e-11, -1.0914e-11,\n",
      "        -3.6380e-11, -7.2760e-12,  0.0000e+00, -3.5470e-11, -2.1828e-11,\n",
      "        -7.2760e-12,  8.3674e-11, -5.4570e-12,  1.6371e-11,  0.0000e+00,\n",
      "         2.7285e-11,  3.6380e-12,  0.0000e+00, -1.8190e-11,  2.9104e-11,\n",
      "        -2.5466e-11, -8.1855e-12,  7.2760e-12,  4.5475e-11, -2.9104e-11,\n",
      "         4.3656e-11, -7.2760e-12, -4.3656e-11, -1.0914e-11, -8.0036e-11,\n",
      "         3.6380e-11, -2.9104e-11, -3.2742e-11,  4.3656e-11, -7.2760e-12,\n",
      "         8.6402e-12, -2.1828e-11, -5.0932e-11,  2.5466e-11,  2.1828e-11,\n",
      "         4.3656e-11, -3.2742e-11, -2.9104e-11,  7.6398e-11,  4.9113e-11,\n",
      "        -2.3647e-11,  1.8190e-11,  4.1837e-11, -3.8199e-11,  3.2742e-11,\n",
      "        -1.4552e-11,  3.4561e-11, -6.9122e-11, -2.0009e-11,  9.0949e-12,\n",
      "         1.4552e-11, -1.8190e-12, -8.0036e-11,  1.4552e-11, -9.0949e-12,\n",
      "         5.4570e-12, -6.8212e-12, -1.8190e-11,  1.4552e-11,  1.0914e-11,\n",
      "        -2.8194e-11, -2.7285e-11, -1.5461e-11,  1.9099e-11,  2.9104e-11,\n",
      "        -2.1828e-11,  2.5466e-11,  2.3647e-11,  1.8190e-11,  6.3665e-12,\n",
      "        -1.8190e-11, -4.1837e-11,  3.6380e-11,  6.5484e-11, -3.6380e-12,\n",
      "         2.9104e-11, -2.9104e-11,  4.3656e-11, -1.0004e-11,  6.1846e-11,\n",
      "        -2.8194e-11,  1.0914e-11,  3.6380e-11, -2.9104e-11,  2.7285e-11,\n",
      "         1.4552e-11,  2.9104e-11, -2.9104e-11,  1.0914e-11,  2.2737e-11,\n",
      "         0.0000e+00, -2.3647e-11, -2.9104e-11,  1.6371e-11,  1.2733e-11,\n",
      "         3.6380e-11, -3.2742e-11,  3.8199e-11, -3.2742e-11,  3.0923e-11,\n",
      "         4.0018e-11,  0.0000e+00, -3.6380e-12], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-4.0419e-05,  9.2923e-05,  1.7710e-04],\n",
      "          [-1.5402e-04, -1.9159e-04, -1.0374e-04],\n",
      "          [-2.8134e-04, -2.4144e-04, -1.8873e-04]],\n",
      "\n",
      "         [[-5.1350e-04, -6.5431e-04, -7.1513e-04],\n",
      "          [-3.9869e-04, -5.3770e-04, -4.7665e-04],\n",
      "          [-4.5711e-04, -5.3735e-04, -3.7563e-04]],\n",
      "\n",
      "         [[-4.1865e-04, -3.0375e-04, -3.6665e-04],\n",
      "          [-4.3322e-04, -3.6548e-04, -4.1001e-04],\n",
      "          [-6.0040e-04, -4.4382e-04, -5.6456e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.3191e-05,  1.4511e-04,  1.5675e-04],\n",
      "          [ 1.7134e-04,  3.5948e-04,  1.7605e-04],\n",
      "          [ 2.9196e-04,  2.7470e-04,  8.4960e-05]],\n",
      "\n",
      "         [[-2.5621e-04, -2.8565e-04, -3.1727e-04],\n",
      "          [-3.2185e-04, -2.4809e-04, -3.0756e-04],\n",
      "          [-4.3679e-04, -4.2387e-04, -4.3957e-04]],\n",
      "\n",
      "         [[ 1.4964e-05, -1.1080e-05,  1.6703e-04],\n",
      "          [ 2.7885e-05, -1.8162e-04,  1.7864e-04],\n",
      "          [-2.3462e-04, -3.7355e-04,  1.0292e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.1217e-05,  9.6467e-05,  2.5277e-05],\n",
      "          [ 8.7372e-06,  1.0201e-05, -3.7797e-05],\n",
      "          [ 1.2910e-06,  1.8263e-05, -1.1042e-05]],\n",
      "\n",
      "         [[-6.8335e-05,  5.4187e-06, -2.9002e-05],\n",
      "          [-9.2304e-05, -6.7897e-05, -5.6581e-05],\n",
      "          [-5.4152e-05, -3.4591e-05, -4.7481e-05]],\n",
      "\n",
      "         [[-8.8476e-05, -6.6688e-05, -9.0403e-05],\n",
      "          [-1.0225e-04, -7.4926e-05, -8.0753e-05],\n",
      "          [-7.5894e-05, -7.9605e-05, -7.1066e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.1236e-05, -4.5565e-05, -5.7018e-05],\n",
      "          [-2.6802e-05, -5.2851e-05, -4.1717e-05],\n",
      "          [-5.4662e-06, -9.9966e-06, -5.4672e-06]],\n",
      "\n",
      "         [[-2.3866e-05, -3.2324e-05, -3.3974e-05],\n",
      "          [-7.9057e-05, -6.4906e-05, -3.2003e-05],\n",
      "          [-9.1548e-06, -2.7839e-05, -4.8223e-05]],\n",
      "\n",
      "         [[ 3.9289e-05,  2.3638e-05, -3.1465e-05],\n",
      "          [-3.5689e-05, -1.1697e-05, -7.2163e-05],\n",
      "          [-4.0521e-05, -2.0816e-05, -5.1697e-05]]],\n",
      "\n",
      "\n",
      "        [[[-4.5022e-05, -2.5857e-05,  1.1973e-05],\n",
      "          [-6.4955e-05, -1.5399e-05,  2.0884e-05],\n",
      "          [-3.3117e-05,  3.1155e-07,  3.5466e-05]],\n",
      "\n",
      "         [[-7.0090e-05, -3.9723e-06, -2.6487e-05],\n",
      "          [-4.1910e-05, -2.5550e-05, -5.1745e-05],\n",
      "          [-6.2714e-05, -3.8969e-05, -3.7062e-05]],\n",
      "\n",
      "         [[-9.4587e-05, -7.9782e-05, -8.1935e-05],\n",
      "          [-8.3880e-05, -8.4721e-05, -5.2889e-05],\n",
      "          [-8.0642e-05, -4.6927e-05, -2.1364e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7279e-05, -3.9322e-05,  1.9220e-05],\n",
      "          [-2.0345e-05,  5.4735e-06,  4.2336e-05],\n",
      "          [-1.1387e-05,  3.8535e-05,  3.7119e-05]],\n",
      "\n",
      "         [[-4.7053e-05, -4.9006e-05, -3.1626e-05],\n",
      "          [-5.9895e-05, -1.2831e-05, -2.6501e-05],\n",
      "          [-2.6488e-05,  1.2794e-05, -2.4591e-06]],\n",
      "\n",
      "         [[-4.2295e-06, -4.8260e-05,  1.3451e-05],\n",
      "          [-3.6248e-06, -4.0764e-05,  1.5388e-05],\n",
      "          [-2.5788e-05, -3.8610e-05,  1.4020e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.1829e-04,  1.5457e-04,  1.8064e-04],\n",
      "          [ 1.8999e-04,  1.6406e-04,  1.0905e-04],\n",
      "          [ 1.9601e-04,  2.1646e-04,  1.8106e-04]],\n",
      "\n",
      "         [[ 2.3618e-04,  1.9304e-04,  2.3592e-04],\n",
      "          [ 2.9859e-04,  2.5190e-04,  3.1321e-04],\n",
      "          [ 1.5830e-04,  1.7570e-04,  2.1018e-04]],\n",
      "\n",
      "         [[ 4.2332e-04,  5.1681e-04,  5.1581e-04],\n",
      "          [ 5.2210e-04,  5.6968e-04,  5.7527e-04],\n",
      "          [ 5.6816e-04,  5.9460e-04,  5.7482e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9865e-05,  1.5662e-05, -6.2957e-05],\n",
      "          [-3.6426e-05, -3.9689e-05, -1.1617e-04],\n",
      "          [-6.9219e-05, -7.4774e-05, -1.2780e-04]],\n",
      "\n",
      "         [[ 3.4853e-04,  4.1970e-04,  3.6921e-04],\n",
      "          [ 4.1882e-04,  4.3809e-04,  3.8538e-04],\n",
      "          [ 3.6040e-04,  4.0996e-04,  3.5736e-04]],\n",
      "\n",
      "         [[ 1.7132e-04,  4.7033e-05,  1.1727e-04],\n",
      "          [ 2.0377e-04,  2.9926e-05,  1.2687e-04],\n",
      "          [ 1.9888e-04,  7.6421e-05,  1.5660e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.5942e-04, -2.1705e-04, -2.8446e-04],\n",
      "          [-1.8211e-04, -1.9430e-04, -2.3812e-04],\n",
      "          [-2.1372e-04, -2.7688e-04, -3.1081e-04]],\n",
      "\n",
      "         [[ 8.9106e-05, -2.4531e-06,  5.6774e-05],\n",
      "          [ 1.2089e-04,  7.7552e-05,  3.3162e-05],\n",
      "          [ 7.2205e-05,  1.1597e-04,  1.1328e-04]],\n",
      "\n",
      "         [[-1.8848e-04, -3.0974e-04, -3.4672e-04],\n",
      "          [-3.2117e-04, -4.0511e-04, -3.7809e-04],\n",
      "          [-4.0380e-04, -4.0598e-04, -4.1637e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4153e-05, -6.4231e-05, -5.9599e-05],\n",
      "          [-4.1887e-06,  2.2008e-05,  2.3375e-05],\n",
      "          [-3.1943e-05, -6.2981e-05, -8.8150e-05]],\n",
      "\n",
      "         [[-1.5818e-04, -1.9431e-04, -1.7904e-04],\n",
      "          [-1.5828e-04, -1.8310e-04, -1.9528e-04],\n",
      "          [-2.3008e-04, -2.4380e-04, -2.4772e-04]],\n",
      "\n",
      "         [[-3.0092e-05, -5.5698e-05, -1.5794e-04],\n",
      "          [-2.4121e-06, -1.0732e-05, -1.2732e-04],\n",
      "          [-5.7829e-05, -7.2211e-05, -1.9491e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6374e-05,  3.4886e-05,  1.1387e-06],\n",
      "          [-1.8791e-05,  2.2948e-05,  2.7710e-05],\n",
      "          [ 2.1329e-05, -2.3593e-06,  5.6209e-05]],\n",
      "\n",
      "         [[ 4.9605e-05,  8.4404e-05,  3.8648e-05],\n",
      "          [ 4.3049e-06,  2.9437e-05,  1.9361e-05],\n",
      "          [ 4.9036e-05,  4.2268e-06,  1.2754e-05]],\n",
      "\n",
      "         [[ 4.4240e-05,  8.1407e-05,  7.7428e-05],\n",
      "          [ 4.0042e-05,  7.0234e-05,  6.1652e-05],\n",
      "          [ 3.8021e-05,  6.6119e-05,  4.3078e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.6312e-05, -7.1690e-06, -4.9069e-05],\n",
      "          [-3.3422e-05,  1.4040e-05, -4.1703e-05],\n",
      "          [-6.4129e-05,  8.4054e-06, -8.9618e-06]],\n",
      "\n",
      "         [[ 4.5876e-06,  7.4511e-06,  1.9782e-05],\n",
      "          [ 1.3961e-05,  2.3110e-05,  3.2074e-05],\n",
      "          [ 4.4819e-05,  4.5857e-05,  6.7973e-05]],\n",
      "\n",
      "         [[ 2.1886e-05,  6.1899e-05,  2.2326e-05],\n",
      "          [ 5.5951e-05,  6.5584e-05,  3.6900e-05],\n",
      "          [ 2.4671e-05,  1.0197e-04,  3.9776e-05]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-4.3656e-11,  1.2733e-11, -1.0914e-11,  0.0000e+00, -7.2760e-12,\n",
      "        -5.4570e-12,  1.8190e-12, -1.1278e-10,  2.1828e-11,  2.9104e-11,\n",
      "        -2.3647e-11, -6.9122e-11, -2.9104e-11, -2.1828e-11, -5.4570e-12,\n",
      "        -7.2760e-12, -2.1828e-11,  1.2733e-11, -1.4552e-11,  7.2760e-12,\n",
      "         1.8190e-11, -3.6380e-12,  0.0000e+00, -2.9104e-11,  1.6371e-11,\n",
      "        -2.3647e-11,  2.1828e-11,  2.9104e-11,  0.0000e+00, -4.1837e-11,\n",
      "        -8.7311e-11, -1.4552e-11, -3.6380e-12, -3.2742e-11,  3.6380e-12,\n",
      "         1.0914e-11, -1.4552e-11,  1.0914e-11,  2.1828e-11,  2.9104e-11,\n",
      "         2.7285e-11, -4.3656e-11,  2.6193e-10,  1.4552e-11,  5.8208e-11,\n",
      "         1.4552e-11,  9.0949e-12,  1.1823e-11,  1.0914e-11,  1.0914e-11,\n",
      "        -5.0932e-11,  2.9104e-11, -7.2760e-12, -7.2760e-12,  2.9104e-11,\n",
      "         0.0000e+00,  5.0932e-11, -9.0949e-13,  7.2760e-12,  2.1828e-11,\n",
      "         3.0923e-11, -5.8208e-11,  1.0186e-10,  3.2742e-11,  2.1828e-11,\n",
      "         3.6380e-12, -1.4552e-11, -2.0009e-11, -4.3656e-11,  7.2760e-11,\n",
      "         6.9122e-11,  3.2742e-11, -1.4552e-11, -1.6371e-10,  2.9104e-11,\n",
      "         0.0000e+00, -2.1828e-11, -4.9113e-11,  1.5461e-11, -1.4552e-11,\n",
      "         1.9099e-11,  1.2733e-11, -1.4552e-11,  0.0000e+00, -1.1823e-11,\n",
      "        -2.9104e-11,  1.4552e-11,  2.5466e-11,  1.5280e-10,  2.5466e-11,\n",
      "         1.8190e-11,  0.0000e+00, -1.2733e-11,  4.3656e-11,  3.7289e-11,\n",
      "        -7.2760e-12,  5.4570e-12, -1.8190e-11,  1.8190e-11, -2.1828e-11,\n",
      "         9.0949e-12, -3.6380e-12, -3.6380e-11, -5.8208e-11,  4.0018e-11,\n",
      "         1.4552e-11, -6.3665e-11,  3.6380e-12, -2.1828e-11,  2.1828e-11,\n",
      "         3.6380e-12, -5.8208e-11, -1.4552e-11,  1.4552e-11,  1.7462e-10,\n",
      "        -3.2742e-11, -8.5493e-11,  0.0000e+00,  1.0914e-11, -1.6371e-11,\n",
      "         7.2760e-12, -3.2742e-11,  0.0000e+00,  7.2760e-12,  3.6380e-11,\n",
      "         4.0018e-11,  5.0932e-11, -3.6380e-12], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 9.1748e-05,  2.8927e-05,  1.3784e-04, -1.0735e-04, -3.8427e-05,\n",
      "        -7.1714e-05,  5.7825e-07, -5.9094e-04, -1.1122e-04, -1.1440e-04,\n",
      "        -1.2069e-04, -2.9387e-04, -4.7190e-04,  1.4776e-04,  2.1822e-04,\n",
      "         2.4249e-06,  6.1034e-05,  2.1892e-04, -1.9860e-04, -7.8158e-05,\n",
      "        -4.8177e-05, -1.9189e-04, -1.8810e-04, -3.8756e-05,  7.1521e-05,\n",
      "         7.5687e-05, -1.0241e-03,  5.2584e-05, -3.2049e-05, -1.7436e-04,\n",
      "         1.3870e-03,  3.1620e-04, -1.8045e-04,  6.2557e-04,  1.4702e-03,\n",
      "         1.2545e-04, -4.2036e-04, -5.7559e-05, -5.1117e-05,  1.7544e-04,\n",
      "         4.2598e-04, -3.8710e-04,  1.5893e-03,  6.4257e-04, -2.4410e-04,\n",
      "        -5.7792e-05, -1.6809e-05, -1.2542e-04,  5.3447e-06, -1.6438e-04,\n",
      "         5.9475e-04, -1.0225e-04, -2.2211e-04, -1.3464e-04, -1.3396e-04,\n",
      "         6.9109e-04,  2.1822e-04,  1.8354e-04, -8.8841e-05,  7.1955e-05,\n",
      "        -3.9888e-04, -3.4979e-04,  2.1357e-03, -3.1380e-04, -1.3984e-04,\n",
      "        -3.5880e-05,  1.5118e-04, -1.5169e-04, -1.5574e-04, -1.8252e-04,\n",
      "        -6.2830e-04, -5.9049e-05,  2.1572e-04, -8.1126e-04,  5.5246e-05,\n",
      "        -3.6739e-05,  5.1433e-04, -4.9415e-05,  1.4675e-04, -2.9492e-04,\n",
      "        -1.4879e-04, -4.4168e-05,  6.9772e-04,  9.7206e-05, -1.6404e-04,\n",
      "        -5.0935e-04,  1.1235e-04, -1.9296e-05,  7.5484e-04, -1.1044e-04,\n",
      "         1.4155e-06, -9.9102e-05,  8.0505e-05, -3.1445e-04,  4.0132e-05,\n",
      "         3.1711e-04, -1.1376e-05, -6.7420e-05, -8.7717e-04, -3.6417e-04,\n",
      "         7.9296e-05,  2.0459e-04, -1.3827e-04, -2.0655e-04,  9.1526e-06,\n",
      "        -2.5336e-04, -7.7434e-04, -2.7275e-04,  1.3388e-05, -2.6277e-04,\n",
      "         6.5340e-05,  1.7182e-04, -1.4948e-04,  1.7890e-05,  1.5140e-03,\n",
      "        -3.9847e-04, -3.4993e-04,  7.1285e-05, -2.8711e-05,  1.7841e-04,\n",
      "         1.4948e-04, -1.0135e-04, -1.6267e-04, -1.4665e-04,  7.2810e-05,\n",
      "         2.0577e-04, -5.3873e-04, -2.7478e-05], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[-1.4842e-04, -2.1732e-04, -1.1025e-04,  ..., -3.7462e-04,\n",
      "         -3.3375e-04, -4.0458e-04],\n",
      "        [ 2.7058e-04,  3.7980e-04,  3.8048e-04,  ...,  5.4687e-04,\n",
      "          1.7698e-04,  2.7681e-04],\n",
      "        [ 6.4460e-05,  2.0874e-04,  4.4052e-06,  ...,  2.6412e-05,\n",
      "          1.2755e-04,  7.1044e-05],\n",
      "        [-2.0426e-05, -1.5235e-04, -1.2698e-04,  ..., -3.2327e-04,\n",
      "         -1.4694e-04,  1.6107e-05],\n",
      "        [-1.6619e-04, -2.1887e-04, -1.4765e-04,  ...,  1.2461e-04,\n",
      "          1.7617e-04,  4.0624e-05]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([-5.6919e-04,  6.0742e-04,  4.5767e-05, -1.6236e-04,  7.8366e-05],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = loss_landscape_join.landscape(maml_system.model.classifier, arbiter_system.model.classifier, args_arbiter)\n",
    "ls.show_2djoin(x_support_set_task, y_support_set_task, title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
