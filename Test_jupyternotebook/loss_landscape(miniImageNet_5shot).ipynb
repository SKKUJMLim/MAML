{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16249129",
   "metadata": {},
   "source": [
    "## [참고]\n",
    "### https://cocoa-t.tistory.com/entry/PyHessian-Loss-Landscape-%EC%8B%9C%EA%B0%81%ED%99%94-PyHessian-Neural-Networks-Through-the-Lens-of-the-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af476",
   "metadata": {},
   "source": [
    "# 0. Dataset 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "7235fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset=\"mini_imagenet_full_size\"\n",
    "dataset=\"tiered_imagenet\"\n",
    "# dataset=\"CIFAR_FS\"\n",
    "# dataset=\"CUB\"\n",
    "\n",
    "title = 'miniImageNet'\n",
    "# title = 'tieredImageNet'\n",
    "# title = 'CIFAR-FS'\n",
    "# title = 'CUB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005193c",
   "metadata": {},
   "source": [
    "# 1. MAML 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "8f0d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_maml = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_filter128\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_maml.im_shape = (2, 3, args_maml.image_height, args_maml.image_width)\n",
    "\n",
    "args_maml.use_cuda = torch.cuda.is_available()\n",
    "args_maml.seed = 104\n",
    "args_maml.reverse_channels=False\n",
    "args_maml.labels_as_int=False\n",
    "args_maml.reset_stored_filepaths=False\n",
    "args_maml.num_of_gpus=1\n",
    "\n",
    "args_maml.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9052a",
   "metadata": {},
   "source": [
    "## 2. Arbiter 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_arbiter = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 150,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_arbiter.im_shape = (2, 3, args_arbiter.image_height, args_arbiter.image_width)\n",
    "\n",
    "args_arbiter.use_cuda = torch.cuda.is_available()\n",
    "args_arbiter.seed = 104\n",
    "args_arbiter.reverse_channels=False\n",
    "args_arbiter.labels_as_int=False\n",
    "args_arbiter.reset_stored_filepaths=False\n",
    "args_arbiter.num_of_gpus=1\n",
    "\n",
    "args_arbiter.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f7d8",
   "metadata": {},
   "source": [
    "## 3. Model 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803156ee",
   "metadata": {},
   "source": [
    "### 3.1. MAML Model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "f85286c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_filter128\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 206209, 'train': 448695, 'val': 124261}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_maml = MAMLFewShotClassifier(args=args_maml, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_maml.image_height, args_maml.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model_maml, data=data, args=args_maml, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3acf",
   "metadata": {},
   "source": [
    "### 3.2.  Arbiter 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "25651dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 206209, 'train': 448695, 'val': 124261}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_arbiter = MAMLFewShotClassifier(args=args_arbiter, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_arbiter.image_height, args_arbiter.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "arbiter_system = ExperimentBuilder(model=model_arbiter, data=data, args=args_arbiter, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "9a2ff6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6359555553396543,\n",
       " 'best_val_iter': 49000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 98,\n",
       " 'train_loss_mean': 0.6403910377025605,\n",
       " 'train_loss_std': 0.1255260544055785,\n",
       " 'train_accuracy_mean': 0.7639866684675216,\n",
       " 'train_accuracy_std': 0.06118401968549816,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.9505268172423045,\n",
       " 'val_loss_std': 0.15325273037358733,\n",
       " 'val_accuracy_mean': 0.6287999994556109,\n",
       " 'val_accuracy_std': 0.06361374163962585,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-0.0255, -0.0844,  0.0421],\n",
       "                         [-0.0809, -0.0591, -0.0046],\n",
       "                         [-0.0325,  0.1015, -0.0134]],\n",
       "               \n",
       "                        [[ 0.0662, -0.0385,  0.0898],\n",
       "                         [-0.0224,  0.0329,  0.0889],\n",
       "                         [-0.0375,  0.0514, -0.0026]],\n",
       "               \n",
       "                        [[ 0.0667, -0.0065, -0.0467],\n",
       "                         [ 0.0322,  0.0595, -0.0776],\n",
       "                         [-0.0642,  0.0074, -0.0751]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0714,  0.1215,  0.0690],\n",
       "                         [ 0.0657, -0.0497,  0.0189],\n",
       "                         [-0.1058, -0.0985, -0.0763]],\n",
       "               \n",
       "                        [[-0.0225,  0.0162, -0.0007],\n",
       "                         [ 0.0545, -0.0631, -0.0476],\n",
       "                         [ 0.0542, -0.0559,  0.0761]],\n",
       "               \n",
       "                        [[-0.0650,  0.0606, -0.0772],\n",
       "                         [-0.0224,  0.0663,  0.0857],\n",
       "                         [-0.0305, -0.0117, -0.0181]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0324, -0.0386,  0.0566],\n",
       "                         [-0.0143, -0.0344, -0.0204],\n",
       "                         [ 0.0755,  0.0579, -0.0644]],\n",
       "               \n",
       "                        [[-0.0797, -0.0395,  0.0388],\n",
       "                         [ 0.0813, -0.0030,  0.0204],\n",
       "                         [-0.0698, -0.0439,  0.0303]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0722,  0.0254],\n",
       "                         [ 0.0777,  0.0115, -0.0638],\n",
       "                         [-0.0668,  0.0245,  0.0016]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0359, -0.0860, -0.0662],\n",
       "                         [-0.0050, -0.0928,  0.0520],\n",
       "                         [-0.0283,  0.0588, -0.0609]],\n",
       "               \n",
       "                        [[ 0.0648,  0.0180,  0.0776],\n",
       "                         [ 0.0499, -0.0222, -0.0206],\n",
       "                         [ 0.0198,  0.0787,  0.0061]],\n",
       "               \n",
       "                        [[ 0.0096, -0.0855, -0.0410],\n",
       "                         [-0.0569,  0.0049,  0.0344],\n",
       "                         [-0.0491,  0.0319,  0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0077,  0.0151, -0.0113],\n",
       "                         [ 0.0198,  0.0805, -0.0301],\n",
       "                         [ 0.0457,  0.0801, -0.0407]],\n",
       "               \n",
       "                        [[ 0.0811,  0.0580,  0.1031],\n",
       "                         [ 0.0596,  0.0067,  0.0002],\n",
       "                         [-0.0017, -0.0517,  0.0510]],\n",
       "               \n",
       "                        [[-0.0385, -0.0292, -0.0301],\n",
       "                         [-0.0170, -0.0443,  0.0586],\n",
       "                         [-0.0917, -0.0518,  0.0024]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0323,  0.0842, -0.0393],\n",
       "                         [-0.0391,  0.0747,  0.0626],\n",
       "                         [ 0.0786,  0.0831,  0.0922]],\n",
       "               \n",
       "                        [[-0.0269, -0.0427,  0.0134],\n",
       "                         [ 0.0379, -0.0726, -0.0621],\n",
       "                         [-0.0407, -0.0233, -0.0411]],\n",
       "               \n",
       "                        [[ 0.0461, -0.0238,  0.0385],\n",
       "                         [-0.0098, -0.0332, -0.0305],\n",
       "                         [-0.0480, -0.0654, -0.0453]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.0175,  0.0289,  0.0017, -0.0053,  0.0039,  0.0073,  0.0086,  0.0064,\n",
       "                        0.0009, -0.0196,  0.0216,  0.0025, -0.0158,  0.0111,  0.0017,  0.0114,\n",
       "                        0.0064,  0.0143,  0.0169, -0.0116, -0.0180, -0.0081, -0.0033, -0.0074,\n",
       "                       -0.0037,  0.0029,  0.0011,  0.0125,  0.0148,  0.0091,  0.0051, -0.0029,\n",
       "                       -0.0089,  0.0250,  0.0072, -0.0091,  0.0011,  0.0039,  0.0159,  0.0169,\n",
       "                       -0.0124, -0.0267,  0.0020, -0.0068, -0.0071,  0.0006, -0.0248, -0.0077,\n",
       "                        0.0044,  0.0075, -0.0075,  0.0021,  0.0150, -0.0009, -0.0197, -0.0072,\n",
       "                        0.0030,  0.0047, -0.0043, -0.0002, -0.0109, -0.0002,  0.0212,  0.0022,\n",
       "                       -0.0252, -0.0014,  0.0105, -0.0034, -0.0002, -0.0239, -0.0024, -0.0128,\n",
       "                        0.0037,  0.0041,  0.0119, -0.0083,  0.0055, -0.0148,  0.0025, -0.0067,\n",
       "                        0.0161,  0.0038, -0.0007,  0.0080,  0.0148, -0.0060, -0.0045,  0.0282,\n",
       "                       -0.0009, -0.0187, -0.0101, -0.0353,  0.0054,  0.0013, -0.0063, -0.0174,\n",
       "                       -0.0096, -0.0242,  0.0046, -0.0104, -0.0169,  0.0135,  0.0113,  0.0116,\n",
       "                        0.0058, -0.0031, -0.0009, -0.0004,  0.0068, -0.0032, -0.0076,  0.0107,\n",
       "                        0.0072, -0.0025, -0.0163,  0.0089,  0.0125, -0.0108,  0.0024,  0.0226,\n",
       "                       -0.0144,  0.0032, -0.0118, -0.0031,  0.0021, -0.0323,  0.0027, -0.0002],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1631,  0.1944, -0.1138,  0.1166,  0.1172, -0.0240, -0.2111, -0.1388,\n",
       "                       -0.1821, -0.0969,  0.2105, -0.1737,  0.3352, -0.0586,  0.3851,  0.2701,\n",
       "                        0.0005,  0.0484,  0.2549, -0.0579,  0.0446, -0.1371, -0.0903,  0.0135,\n",
       "                        0.0157, -0.0954, -0.0991,  0.3129,  0.2739,  0.1327,  0.0106, -0.0495,\n",
       "                        0.1543, -0.1553, -0.0760, -0.0628,  0.0274,  0.1642, -0.1468, -0.0709,\n",
       "                        0.0016,  0.3324, -0.1130,  0.1172,  0.2649, -0.1730, -0.1429, -0.1181,\n",
       "                       -0.0757,  0.0019,  0.2784, -0.0801,  0.3400,  0.2097, -0.0662, -0.0745,\n",
       "                       -0.0332,  0.2216,  0.0059,  0.3655, -0.1121,  0.2333,  0.0753, -0.1723,\n",
       "                        0.1420, -0.0555,  0.2159, -0.0682,  0.3034, -0.0017,  0.1455,  0.3801,\n",
       "                       -0.0174, -0.1359,  0.2083, -0.1117,  0.0331,  0.3195,  0.1055, -0.1900,\n",
       "                        0.4710, -0.0528, -0.1267, -0.0556,  0.3039,  0.3539,  0.3447,  0.2085,\n",
       "                       -0.1442,  0.1741, -0.1334,  0.0495,  0.0896,  0.1874, -0.0507,  0.2346,\n",
       "                        0.0595,  0.0425, -0.0443, -0.1186,  0.0008,  0.0851, -0.1202, -0.0469,\n",
       "                       -0.0340,  0.0512,  0.1297, -0.1034,  0.0106,  0.2907,  0.2411,  0.4341,\n",
       "                       -0.0890,  0.0216, -0.0916,  0.1807,  0.0130,  0.4608, -0.0359,  0.4015,\n",
       "                       -0.0879, -0.0734, -0.1045, -0.1029,  0.3831, -0.0157, -0.0910, -0.0575],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.9696, 1.1085, 0.8447, 1.1021, 0.9693, 0.9709, 0.9787, 0.9343, 0.8471,\n",
       "                       0.9381, 1.1029, 0.9399, 1.1080, 0.9657, 1.1290, 1.1905, 0.8940, 1.0025,\n",
       "                       0.9887, 0.9973, 0.9491, 0.9175, 0.9180, 0.9997, 0.9615, 0.9512, 0.9187,\n",
       "                       0.9743, 1.0115, 1.0331, 1.0684, 0.9661, 1.1221, 0.9391, 0.9968, 0.9734,\n",
       "                       0.9412, 1.0812, 0.9791, 1.0023, 1.0361, 1.1265, 0.9472, 1.1115, 0.9799,\n",
       "                       0.9660, 0.9552, 0.9922, 0.9178, 1.0445, 0.9584, 0.9479, 1.0931, 0.9649,\n",
       "                       0.9693, 0.9627, 0.9913, 1.0533, 1.0207, 1.0808, 1.0113, 1.0104, 0.9997,\n",
       "                       0.9408, 1.0169, 0.9237, 0.9685, 1.0111, 1.1905, 0.9716, 1.0283, 1.1558,\n",
       "                       0.9849, 0.9728, 0.9640, 0.9509, 1.0301, 1.1699, 1.1360, 0.8957, 1.0791,\n",
       "                       1.0465, 0.9238, 1.0184, 1.0511, 1.1042, 0.9738, 1.0620, 0.8964, 0.9912,\n",
       "                       0.8152, 0.9297, 0.9573, 1.0192, 0.9643, 0.9865, 0.9691, 0.9859, 0.9162,\n",
       "                       1.0091, 0.9118, 1.0460, 0.9582, 1.0327, 0.9797, 1.0288, 1.0011, 0.9599,\n",
       "                       1.0142, 1.0149, 0.9774, 1.1019, 1.0000, 0.9757, 0.9746, 1.1659, 0.9524,\n",
       "                       1.0279, 1.0224, 1.1533, 0.9746, 0.9960, 0.9669, 0.9357, 1.0148, 0.9732,\n",
       "                       0.9627, 0.9500], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 0.0073,  0.0615,  0.0120],\n",
       "                         [-0.0109,  0.0211, -0.0202],\n",
       "                         [ 0.0240,  0.0670, -0.0142]],\n",
       "               \n",
       "                        [[ 0.0408,  0.0463, -0.0002],\n",
       "                         [-0.0267, -0.0258, -0.0401],\n",
       "                         [-0.0322,  0.0412, -0.0157]],\n",
       "               \n",
       "                        [[ 0.0051, -0.0471, -0.0257],\n",
       "                         [-0.0522, -0.0002,  0.0558],\n",
       "                         [ 0.0197,  0.0114, -0.0256]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0189, -0.0542, -0.0090],\n",
       "                         [-0.0584, -0.0753, -0.0285],\n",
       "                         [-0.0501, -0.0588, -0.0454]],\n",
       "               \n",
       "                        [[ 0.0147,  0.0626, -0.0057],\n",
       "                         [-0.0027,  0.0439, -0.0190],\n",
       "                         [ 0.0332, -0.0397,  0.0007]],\n",
       "               \n",
       "                        [[-0.0371,  0.0505, -0.0220],\n",
       "                         [-0.0072,  0.0231,  0.0021],\n",
       "                         [-0.0321, -0.0171,  0.0332]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0149, -0.0397,  0.0341],\n",
       "                         [ 0.0416,  0.0181, -0.0336],\n",
       "                         [ 0.0185,  0.0093,  0.0077]],\n",
       "               \n",
       "                        [[ 0.0276,  0.0237, -0.0054],\n",
       "                         [-0.0764,  0.0186,  0.0145],\n",
       "                         [-0.0378, -0.0316,  0.0317]],\n",
       "               \n",
       "                        [[ 0.0133, -0.0043, -0.0575],\n",
       "                         [-0.0120, -0.0424, -0.0046],\n",
       "                         [-0.0632, -0.0027, -0.0540]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0045, -0.0387, -0.0022],\n",
       "                         [-0.0536, -0.0996, -0.0563],\n",
       "                         [-0.0248,  0.0001, -0.0399]],\n",
       "               \n",
       "                        [[ 0.0384, -0.0114, -0.0219],\n",
       "                         [ 0.0516,  0.0429,  0.0258],\n",
       "                         [ 0.0443, -0.0104,  0.0394]],\n",
       "               \n",
       "                        [[-0.0065, -0.0368,  0.0156],\n",
       "                         [ 0.0126, -0.0725, -0.0404],\n",
       "                         [-0.0613, -0.0766, -0.0209]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0573, -0.0550, -0.0113],\n",
       "                         [-0.0121,  0.0007, -0.0628],\n",
       "                         [ 0.0059,  0.0141, -0.0043]],\n",
       "               \n",
       "                        [[ 0.0305, -0.0125, -0.0485],\n",
       "                         [ 0.0321,  0.0384, -0.0741],\n",
       "                         [ 0.0067,  0.0431, -0.0392]],\n",
       "               \n",
       "                        [[-0.0431,  0.0168, -0.0467],\n",
       "                         [-0.0372,  0.0102,  0.0053],\n",
       "                         [ 0.0024,  0.0143,  0.0398]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0455,  0.0046, -0.0200],\n",
       "                         [-0.0405, -0.0600, -0.0352],\n",
       "                         [-0.0408, -0.0624, -0.0764]],\n",
       "               \n",
       "                        [[ 0.0437,  0.0360,  0.0398],\n",
       "                         [-0.0529, -0.0126, -0.0394],\n",
       "                         [-0.0420,  0.0150, -0.0273]],\n",
       "               \n",
       "                        [[-0.0355, -0.0435,  0.0117],\n",
       "                         [-0.0331,  0.0213,  0.0256],\n",
       "                         [ 0.0059, -0.0061, -0.0219]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0504,  0.0111,  0.0091],\n",
       "                         [ 0.0700,  0.0102, -0.0127],\n",
       "                         [ 0.0472, -0.0750, -0.0749]],\n",
       "               \n",
       "                        [[ 0.0126,  0.0457,  0.0538],\n",
       "                         [-0.0469, -0.0615, -0.0298],\n",
       "                         [-0.0257,  0.0307,  0.0064]],\n",
       "               \n",
       "                        [[ 0.0381,  0.0234,  0.0319],\n",
       "                         [ 0.0427, -0.0436, -0.0046],\n",
       "                         [-0.0133, -0.0470, -0.0157]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0282, -0.0291, -0.0278],\n",
       "                         [ 0.0064, -0.0879, -0.0409],\n",
       "                         [-0.0045, -0.0201, -0.0508]],\n",
       "               \n",
       "                        [[ 0.0012,  0.0317,  0.0562],\n",
       "                         [ 0.0444,  0.0208,  0.0051],\n",
       "                         [-0.0373,  0.0474,  0.0533]],\n",
       "               \n",
       "                        [[-0.0329, -0.0453, -0.0214],\n",
       "                         [-0.0277,  0.0045,  0.0224],\n",
       "                         [-0.0115,  0.0191, -0.0240]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0053, -0.0619,  0.0081],\n",
       "                         [-0.0290, -0.0561,  0.0356],\n",
       "                         [-0.0297,  0.0281,  0.0076]],\n",
       "               \n",
       "                        [[ 0.0019,  0.0412, -0.0531],\n",
       "                         [-0.0054,  0.0489, -0.0376],\n",
       "                         [-0.0378, -0.0377,  0.0578]],\n",
       "               \n",
       "                        [[-0.0227,  0.0314,  0.0516],\n",
       "                         [ 0.0181, -0.0008, -0.0313],\n",
       "                         [-0.0155,  0.0141,  0.0279]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0321, -0.0038,  0.0526],\n",
       "                         [-0.0037,  0.0310, -0.0090],\n",
       "                         [-0.0042, -0.0025, -0.0319]],\n",
       "               \n",
       "                        [[ 0.0066,  0.0332,  0.0456],\n",
       "                         [-0.0186, -0.0539, -0.0037],\n",
       "                         [-0.0198, -0.0442,  0.0125]],\n",
       "               \n",
       "                        [[-0.0379, -0.0151,  0.0192],\n",
       "                         [-0.0437,  0.0384,  0.0344],\n",
       "                         [ 0.0192,  0.0085,  0.0302]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0173, -0.0070, -0.0065],\n",
       "                         [ 0.0314,  0.0438, -0.0579],\n",
       "                         [-0.0199,  0.0290, -0.0254]],\n",
       "               \n",
       "                        [[-0.0348,  0.0351,  0.0423],\n",
       "                         [ 0.0056, -0.0599, -0.0338],\n",
       "                         [-0.0596, -0.0250,  0.0199]],\n",
       "               \n",
       "                        [[-0.0150, -0.0501,  0.0067],\n",
       "                         [-0.0007,  0.0117,  0.0053],\n",
       "                         [ 0.0055, -0.0287,  0.0358]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0159, -0.0148, -0.0628],\n",
       "                         [ 0.0441,  0.0680, -0.0351],\n",
       "                         [ 0.0223,  0.0213, -0.0230]],\n",
       "               \n",
       "                        [[-0.0644, -0.0820, -0.0552],\n",
       "                         [ 0.0076,  0.0235, -0.0232],\n",
       "                         [-0.0340, -0.0002, -0.0556]],\n",
       "               \n",
       "                        [[-0.0215,  0.0316,  0.0109],\n",
       "                         [-0.0233,  0.0090,  0.0530],\n",
       "                         [-0.0053, -0.0364, -0.0194]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-1.6949e-02, -8.4122e-03, -6.4646e-03, -4.2481e-03, -2.1994e-03,\n",
       "                        3.4437e-03, -1.9195e-03,  2.9394e-03, -8.6836e-04,  2.9146e-03,\n",
       "                       -4.8363e-03, -2.9786e-03,  8.2595e-04,  2.0065e-03,  1.7538e-03,\n",
       "                        5.7412e-03, -3.8066e-05,  3.0353e-03, -2.8385e-03,  2.7388e-03,\n",
       "                       -5.7158e-03,  1.9158e-03, -2.4051e-03, -2.9564e-03,  2.5613e-03,\n",
       "                        2.3439e-04, -2.2504e-03,  7.3651e-03, -1.2441e-03, -8.3805e-05,\n",
       "                        8.6782e-03, -3.5325e-03,  3.8182e-03,  1.5983e-03,  1.2204e-03,\n",
       "                       -6.0784e-03, -4.5049e-04,  4.5410e-03,  3.2182e-04, -2.8716e-03,\n",
       "                       -2.2109e-03,  3.7276e-03,  7.1509e-04, -2.3736e-03,  3.0177e-04,\n",
       "                        3.1425e-03,  4.0651e-03, -3.7909e-03, -1.2114e-03, -1.6332e-03,\n",
       "                       -8.7488e-03, -9.9171e-04,  8.2588e-04, -7.7100e-04, -8.6759e-03,\n",
       "                        3.9878e-03, -2.7320e-03, -8.2349e-03,  2.0059e-03,  6.6739e-04,\n",
       "                        2.1313e-03, -2.2211e-03,  1.5898e-03,  6.0315e-04, -7.0056e-03,\n",
       "                        3.8265e-03,  4.4242e-03, -4.5509e-03,  2.2188e-04, -2.7838e-03,\n",
       "                        1.4363e-03,  4.0279e-04, -1.7379e-03, -4.3296e-03,  2.3067e-03,\n",
       "                       -5.1976e-03, -2.9156e-04, -1.3852e-03,  7.8145e-03,  1.6117e-03,\n",
       "                        6.3610e-03, -2.0517e-03, -5.3275e-03,  4.3522e-03, -2.8026e-03,\n",
       "                        3.9732e-03,  5.5070e-03, -7.5503e-05,  1.0788e-03,  2.9781e-03,\n",
       "                       -9.4866e-03,  5.1489e-04, -3.9218e-04, -5.8387e-05, -1.9815e-03,\n",
       "                        6.0945e-04,  1.0402e-03,  9.8662e-04, -5.3993e-04, -2.6299e-04,\n",
       "                        1.4962e-03, -5.3848e-04,  1.7470e-03, -3.8175e-03, -1.7810e-03,\n",
       "                       -1.8436e-03, -2.5974e-03,  2.9317e-03,  2.3702e-03,  2.2829e-04,\n",
       "                        2.0404e-03,  5.7490e-03,  1.5104e-03,  7.5078e-04, -1.6547e-03,\n",
       "                        4.6222e-03,  1.2253e-04,  3.9792e-03,  4.9803e-03,  4.8457e-04,\n",
       "                       -3.8067e-03, -2.3458e-03,  1.2213e-03, -6.6374e-03, -5.1143e-03,\n",
       "                       -1.9425e-03, -1.7261e-03,  2.3845e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0239, -0.1029, -0.1356, -0.1313, -0.1094, -0.1200, -0.0928, -0.1694,\n",
       "                       -0.0216, -0.0378, -0.0961, -0.0686, -0.2043, -0.1061, -0.1571, -0.1857,\n",
       "                       -0.0801,  0.1526, -0.0903, -0.1624, -0.2084, -0.0794, -0.1526, -0.0904,\n",
       "                       -0.1907, -0.0218, -0.1831,  0.0054, -0.0388, -0.1296,  0.0191, -0.1421,\n",
       "                       -0.0195, -0.1232, -0.0756, -0.1430, -0.2357, -0.0668, -0.1038, -0.1295,\n",
       "                       -0.1906, -0.0274, -0.1887, -0.1435, -0.1363, -0.1023, -0.1372, -0.0549,\n",
       "                       -0.0128, -0.1253, -0.1443, -0.0114, -0.0838, -0.1771, -0.0118, -0.0546,\n",
       "                       -0.0105, -0.1224, -0.1915, -0.1396, -0.1079, -0.0798, -0.1026, -0.0963,\n",
       "                       -0.0723, -0.0284, -0.0720, -0.0425, -0.1336, -0.0733, -0.1823, -0.0852,\n",
       "                       -0.0766, -0.0089, -0.0376, -0.1518, -0.0774, -0.1717,  0.0542, -0.1189,\n",
       "                       -0.0779, -0.1253, -0.0342, -0.0763, -0.1555, -0.0973, -0.0166, -0.0855,\n",
       "                       -0.0579, -0.1178, -0.1892, -0.0419, -0.0477, -0.0605, -0.0799, -0.0898,\n",
       "                       -0.0811, -0.0729, -0.1795, -0.1760, -0.1528, -0.1409, -0.1406, -0.1007,\n",
       "                       -0.1355, -0.1034, -0.0196, -0.1602, -0.0788, -0.0995, -0.1055, -0.0866,\n",
       "                       -0.0884, -0.0945, -0.1266, -0.0264, -0.0842, -0.1382, -0.0980, -0.1124,\n",
       "                       -0.1105, -0.0172, -0.1221, -0.0662, -0.0260, -0.0930, -0.0811, -0.1032],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9428, 0.9862, 1.0353, 1.0197, 0.9484, 1.0319, 1.0427, 0.9799, 1.0889,\n",
       "                       0.9699, 1.0409, 0.9796, 0.9760, 1.0362, 1.0216, 1.0216, 1.0361, 0.9781,\n",
       "                       1.0763, 1.0203, 1.0362, 1.0268, 1.0446, 0.9615, 1.0413, 1.0226, 1.0333,\n",
       "                       1.0005, 1.0836, 1.0583, 0.9296, 0.9788, 1.0094, 0.9958, 1.0039, 0.9190,\n",
       "                       1.0555, 0.9978, 0.9221, 0.9902, 1.0340, 0.9762, 1.0726, 1.0702, 0.9993,\n",
       "                       0.9641, 1.0156, 1.0302, 1.0744, 0.9556, 0.9469, 0.9848, 0.9398, 0.9957,\n",
       "                       0.9438, 1.0218, 1.0183, 0.9649, 0.9851, 0.9523, 0.9629, 0.9934, 0.9863,\n",
       "                       0.9597, 0.9350, 1.0053, 1.0129, 1.0250, 0.9279, 1.0033, 0.9282, 0.9721,\n",
       "                       1.0010, 0.9276, 1.0228, 0.9233, 1.0320, 1.0337, 0.9309, 0.9750, 1.0079,\n",
       "                       0.9799, 1.0670, 0.9271, 1.0485, 0.9817, 0.9526, 0.9983, 1.1004, 0.9980,\n",
       "                       1.0839, 1.0081, 0.9850, 1.0833, 0.9840, 0.9629, 0.9918, 0.9759, 1.0162,\n",
       "                       1.0476, 0.9885, 1.0907, 0.9968, 1.0194, 0.9749, 0.9604, 0.9823, 0.9796,\n",
       "                       1.0114, 1.0355, 0.9358, 1.0666, 0.9849, 1.0066, 0.9954, 1.0633, 1.0803,\n",
       "                       0.9390, 0.9113, 1.0046, 0.9884, 1.0092, 1.0470, 0.9643, 0.9522, 1.0208,\n",
       "                       1.0418, 0.9650], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-5.6612e-04,  5.5613e-02, -1.4964e-02],\n",
       "                         [-3.1003e-02, -5.2417e-03, -3.5244e-02],\n",
       "                         [ 1.5153e-02, -2.5045e-02,  8.3373e-03]],\n",
       "               \n",
       "                        [[ 2.6745e-02, -1.6569e-02,  9.6491e-03],\n",
       "                         [-2.3582e-02, -1.7072e-02, -3.0692e-02],\n",
       "                         [ 8.8099e-03,  1.9431e-02, -1.0071e-02]],\n",
       "               \n",
       "                        [[ 2.2886e-02,  1.2657e-02,  2.7444e-02],\n",
       "                         [ 6.7986e-02,  2.3340e-02, -6.3387e-02],\n",
       "                         [-8.0550e-03,  4.3950e-03, -2.6644e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.2784e-02, -4.7080e-02, -5.8843e-02],\n",
       "                         [ 5.8877e-02,  3.3930e-02,  5.3777e-02],\n",
       "                         [-3.0208e-06, -3.6364e-02, -3.0298e-02]],\n",
       "               \n",
       "                        [[-1.8547e-02, -3.3489e-02,  1.4912e-02],\n",
       "                         [ 4.4668e-02,  1.5992e-02, -3.8668e-02],\n",
       "                         [ 1.9676e-02,  3.2789e-02,  3.8013e-02]],\n",
       "               \n",
       "                        [[ 9.6036e-03,  1.6508e-02,  6.3899e-02],\n",
       "                         [-2.2815e-02, -2.2302e-02, -3.1161e-02],\n",
       "                         [ 1.2610e-03,  2.5403e-03,  4.1579e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3809e-03,  3.2775e-03,  3.4006e-02],\n",
       "                         [ 1.2260e-02,  2.4693e-03, -3.2722e-02],\n",
       "                         [-2.4378e-02, -6.5934e-02, -2.4807e-02]],\n",
       "               \n",
       "                        [[-4.2487e-02, -2.5946e-02, -2.8232e-02],\n",
       "                         [-1.6780e-02,  6.6708e-03, -4.2812e-03],\n",
       "                         [-1.7184e-02, -2.3446e-03,  1.5289e-02]],\n",
       "               \n",
       "                        [[ 8.0496e-03,  5.3279e-02,  9.2603e-03],\n",
       "                         [ 3.7587e-02,  3.2692e-02,  6.3311e-03],\n",
       "                         [ 4.8233e-02,  1.8544e-02, -1.1102e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.0004e-02,  3.0527e-02, -4.1664e-02],\n",
       "                         [-5.6780e-02, -4.1942e-02, -5.5847e-02],\n",
       "                         [-4.5592e-02, -2.2262e-02, -7.3276e-02]],\n",
       "               \n",
       "                        [[-2.0432e-02, -1.4052e-02,  1.8676e-03],\n",
       "                         [-9.4970e-02, -1.9388e-02, -1.8318e-02],\n",
       "                         [-6.2985e-02, -3.7837e-02, -3.0712e-02]],\n",
       "               \n",
       "                        [[-9.6093e-03, -8.8685e-02, -7.3937e-02],\n",
       "                         [-2.9326e-02,  7.3677e-04,  8.5064e-03],\n",
       "                         [-4.6906e-02, -1.3926e-02, -2.8744e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.4945e-02, -1.6051e-03, -6.4338e-02],\n",
       "                         [-9.1217e-04, -1.0536e-02,  1.0642e-02],\n",
       "                         [ 5.3609e-02,  2.5719e-02,  1.7490e-02]],\n",
       "               \n",
       "                        [[ 3.2974e-02,  7.0537e-02,  6.3091e-02],\n",
       "                         [-2.3824e-02,  3.8106e-02,  3.0877e-03],\n",
       "                         [-8.1763e-02, -1.9468e-02, -8.9834e-02]],\n",
       "               \n",
       "                        [[ 5.2738e-02,  1.4020e-02,  6.0283e-02],\n",
       "                         [-8.0464e-03,  9.1787e-02,  2.4450e-02],\n",
       "                         [-7.7595e-04,  4.2054e-02, -5.3030e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.1894e-02,  7.4259e-03, -4.8836e-02],\n",
       "                         [-3.9580e-03, -3.1565e-02, -1.5890e-02],\n",
       "                         [-2.4345e-02, -1.7730e-02,  5.6163e-02]],\n",
       "               \n",
       "                        [[-1.3640e-02, -2.9441e-02, -1.1606e-02],\n",
       "                         [ 6.9270e-05, -5.4901e-02, -6.5660e-02],\n",
       "                         [-7.3340e-02, -7.6530e-02, -5.1404e-02]],\n",
       "               \n",
       "                        [[-1.6647e-02,  2.0942e-02,  5.6214e-02],\n",
       "                         [ 1.0635e-02,  3.8857e-02,  2.6654e-02],\n",
       "                         [-1.2568e-02, -2.9424e-04,  1.8442e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.2160e-02, -4.8316e-02, -5.4221e-02],\n",
       "                         [ 3.5441e-02, -3.3133e-02, -1.6483e-02],\n",
       "                         [ 1.4458e-02,  2.3074e-02,  1.1153e-02]],\n",
       "               \n",
       "                        [[-4.4017e-02,  5.9920e-03,  1.8519e-02],\n",
       "                         [-1.0042e-02,  1.2894e-02, -3.1734e-02],\n",
       "                         [-4.1772e-03,  5.4501e-02,  8.8462e-03]],\n",
       "               \n",
       "                        [[-2.9016e-02,  7.5986e-03, -2.8667e-02],\n",
       "                         [ 3.5275e-02,  5.9086e-03,  3.4249e-02],\n",
       "                         [ 6.0786e-02,  2.2129e-02,  1.7514e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.5898e-02,  3.3125e-02,  1.3987e-02],\n",
       "                         [ 1.1372e-02,  4.8750e-03, -5.3042e-03],\n",
       "                         [-1.7770e-02, -3.6008e-02, -4.0320e-02]],\n",
       "               \n",
       "                        [[-8.7624e-03, -1.3168e-02,  6.0018e-02],\n",
       "                         [-6.0902e-03, -3.3698e-03, -2.7294e-02],\n",
       "                         [-3.2872e-02,  3.5987e-02, -7.3005e-02]],\n",
       "               \n",
       "                        [[ 1.0989e-02, -4.6545e-02, -6.0721e-02],\n",
       "                         [-7.2877e-02, -8.1973e-02, -7.7765e-02],\n",
       "                         [-3.0052e-02,  2.0828e-02, -2.5616e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.8925e-02,  4.8917e-02,  2.8418e-02],\n",
       "                         [-2.1285e-02, -2.5500e-02, -7.5975e-03],\n",
       "                         [-2.8078e-03,  2.9815e-02, -1.3520e-02]],\n",
       "               \n",
       "                        [[ 2.1450e-04, -2.8705e-02,  2.4119e-02],\n",
       "                         [-3.6257e-03, -3.2389e-02,  4.0854e-02],\n",
       "                         [-3.2382e-02,  3.5661e-03,  7.8317e-03]],\n",
       "               \n",
       "                        [[-3.0559e-02,  2.0490e-02,  5.0351e-03],\n",
       "                         [ 5.0072e-02,  4.6661e-02, -1.5058e-02],\n",
       "                         [ 1.7608e-02, -1.3727e-02,  2.3573e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.6052e-02, -3.8395e-03,  3.6573e-02],\n",
       "                         [-5.7949e-04,  7.1536e-02, -1.0412e-02],\n",
       "                         [-1.5482e-03,  2.2744e-02, -8.5390e-03]],\n",
       "               \n",
       "                        [[-6.3845e-02, -4.6169e-02, -4.0007e-02],\n",
       "                         [-1.1293e-02, -3.5827e-02,  1.3014e-02],\n",
       "                         [ 5.1841e-02, -1.6890e-02, -7.2922e-02]],\n",
       "               \n",
       "                        [[ 4.1673e-02,  1.7039e-02, -8.0290e-02],\n",
       "                         [-4.1658e-04, -2.3544e-02, -1.0676e-01],\n",
       "                         [ 4.5347e-03, -2.7998e-02, -2.3365e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.9852e-02,  5.9324e-03,  9.6843e-04],\n",
       "                         [-1.5981e-02, -3.1834e-02,  5.2805e-02],\n",
       "                         [-3.6868e-02, -1.7340e-02,  3.9554e-02]],\n",
       "               \n",
       "                        [[ 6.7321e-02,  2.9623e-02,  5.4673e-03],\n",
       "                         [ 2.0529e-02,  6.5971e-03, -2.3391e-02],\n",
       "                         [ 1.7442e-02,  3.5414e-02,  3.6108e-02]],\n",
       "               \n",
       "                        [[-3.3463e-02, -2.1611e-02, -1.9678e-02],\n",
       "                         [-4.8424e-02,  3.5010e-02,  3.9424e-02],\n",
       "                         [-5.9420e-03,  4.3907e-02,  3.3360e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9797e-02,  4.3752e-02,  1.3179e-02],\n",
       "                         [ 4.3918e-02,  3.1366e-02, -2.0022e-02],\n",
       "                         [-3.3028e-02, -2.8256e-02, -3.0205e-02]],\n",
       "               \n",
       "                        [[-1.2938e-02,  1.2830e-02,  2.9501e-02],\n",
       "                         [-1.7726e-02, -7.1185e-03, -3.7811e-02],\n",
       "                         [ 2.8611e-02, -2.0118e-02, -6.1311e-02]],\n",
       "               \n",
       "                        [[-1.0113e-02,  5.4758e-02,  2.1487e-02],\n",
       "                         [ 3.2794e-03,  1.9351e-02, -2.8451e-02],\n",
       "                         [ 8.8019e-02, -1.0072e-02, -3.6928e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 4.6061e-04, -2.6240e-03, -4.8668e-03,  1.3499e-03, -6.7688e-04,\n",
       "                       -1.3051e-03, -3.9467e-04,  1.6521e-03, -7.8187e-03,  4.3409e-03,\n",
       "                       -2.4901e-03, -1.1735e-03, -1.9624e-04, -7.4011e-03,  1.8107e-05,\n",
       "                       -4.4562e-03,  2.1683e-03, -2.7531e-03,  9.8409e-05,  1.1074e-03,\n",
       "                       -5.6326e-03,  6.3480e-03,  5.0846e-03,  1.4609e-03, -3.3035e-03,\n",
       "                       -3.3362e-03,  9.9636e-04, -5.7492e-04, -3.1613e-03, -1.0247e-02,\n",
       "                        4.1812e-03, -1.9299e-03,  4.8126e-03,  6.2209e-03,  9.1613e-03,\n",
       "                       -3.4160e-03,  1.0048e-02, -1.8047e-02,  1.5066e-04, -9.0010e-04,\n",
       "                       -3.3024e-03,  1.1539e-02,  1.5924e-03, -7.8076e-04,  2.7959e-03,\n",
       "                       -3.8437e-03,  5.5420e-04,  5.6630e-03,  1.7700e-03, -1.1816e-03,\n",
       "                       -3.2343e-03, -1.1565e-03,  5.6338e-03, -2.6592e-03, -2.2296e-03,\n",
       "                       -7.1357e-04,  1.0998e-03, -8.9221e-04, -3.3544e-03, -5.9220e-03,\n",
       "                        3.0695e-03, -2.3067e-03,  7.3235e-03,  6.6048e-03,  1.7583e-03,\n",
       "                        1.2300e-03, -8.0600e-04, -1.3774e-02,  2.6487e-03, -2.2824e-03,\n",
       "                       -4.4781e-03, -4.0840e-03, -4.4604e-04, -9.0035e-04,  1.4574e-04,\n",
       "                        2.9544e-03,  1.5378e-03,  3.9039e-03, -1.7558e-03,  3.4783e-03,\n",
       "                       -3.1708e-03,  6.7397e-03,  2.6774e-03, -4.4915e-03,  1.7781e-03,\n",
       "                        1.4992e-03,  2.2443e-03, -9.6971e-04, -1.8576e-03,  2.4945e-03,\n",
       "                        9.1154e-03,  4.9740e-03,  3.8111e-03,  4.0308e-04,  3.2366e-03,\n",
       "                       -1.2821e-03, -6.6860e-04, -6.5602e-03,  2.9548e-03,  3.9707e-03,\n",
       "                        3.1116e-03, -3.0379e-03,  1.4848e-03, -2.9219e-03, -2.7217e-03,\n",
       "                       -3.1725e-03, -2.3843e-03, -2.0663e-04, -3.5932e-03, -2.9701e-03,\n",
       "                        2.3363e-03, -4.0607e-03,  1.1661e-03,  3.8860e-03, -7.5729e-03,\n",
       "                       -2.9014e-03,  7.2572e-04, -6.2508e-03,  1.2011e-04,  2.4776e-03,\n",
       "                        1.8391e-03, -9.3481e-04,  2.6883e-03, -1.7414e-03,  3.9286e-03,\n",
       "                        3.7851e-03,  5.6336e-03,  5.9318e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.1391, -0.2048, -0.1393, -0.1732, -0.1353, -0.1210, -0.1603, -0.2054,\n",
       "                       -0.0353, -0.1253, -0.1125, -0.0369, -0.2840, -0.1378, -0.1706, -0.2121,\n",
       "                       -0.0803, -0.1494, -0.1289, -0.1470, -0.0888, -0.0605, -0.1538, -0.0884,\n",
       "                       -0.0962, -0.0889, -0.1651, -0.1394, -0.2496, -0.1341, -0.1365, -0.1336,\n",
       "                       -0.2978, -0.0906, -0.1493, -0.2138, -0.0082, -0.2035, -0.2011, -0.2006,\n",
       "                       -0.1547, -0.2458, -0.1979, -0.0222, -0.1107, -0.1439, -0.0984, -0.0936,\n",
       "                       -0.1489, -0.1794, -0.1324, -0.1430, -0.2192, -0.1099, -0.1431, -0.1055,\n",
       "                       -0.1645, -0.1377, -0.1245, -0.0560, -0.2493, -0.0998, -0.1132, -0.2081,\n",
       "                       -0.0350, -0.1173, -0.0356, -0.0940, -0.1193, -0.1307, -0.0620, -0.2028,\n",
       "                       -0.1571, -0.1631, -0.0327, -0.1306, -0.0988, -0.1368, -0.0456, -0.0868,\n",
       "                       -0.1458, -0.1827, -0.1800, -0.1469, -0.0483, -0.1266, -0.1537, -0.1395,\n",
       "                       -0.1806, -0.0839, -0.2118, -0.0111, -0.1126, -0.1722, -0.1317, -0.1179,\n",
       "                       -0.1163, -0.1933, -0.1038, -0.1531, -0.1672, -0.1171, -0.0923, -0.0193,\n",
       "                       -0.0547, -0.2908, -0.1291, -0.1831, -0.1212, -0.1551, -0.0190, -0.0806,\n",
       "                       -0.2205, -0.0160,  0.0108, -0.1951, -0.1521, -0.3187, -0.1491, -0.1353,\n",
       "                       -0.0793, -0.1182,  0.0170, -0.1368, -0.1985, -0.1302, -0.1994, -0.1754],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([1.0277, 1.0128, 0.9804, 0.9433, 0.9412, 0.9652, 0.9658, 0.9749, 1.0517,\n",
       "                       1.0016, 0.9643, 1.0066, 1.0405, 0.9912, 0.9841, 0.9218, 1.1151, 1.0733,\n",
       "                       0.9509, 0.9652, 1.0227, 0.9606, 1.0235, 0.9114, 1.0268, 1.0234, 1.0819,\n",
       "                       0.9045, 1.0696, 1.0600, 0.9874, 1.0240, 1.0417, 1.0809, 0.9790, 1.0562,\n",
       "                       0.9363, 1.1194, 0.9735, 0.9849, 0.9253, 1.0497, 1.0019, 0.9294, 0.9515,\n",
       "                       1.0383, 1.0002, 0.9651, 0.9030, 0.9744, 1.0132, 0.9474, 1.0187, 0.9983,\n",
       "                       0.9703, 0.9965, 0.9904, 0.9778, 0.9497, 1.0112, 1.0529, 0.9374, 0.9905,\n",
       "                       1.0483, 1.0328, 1.0214, 0.9460, 0.9351, 0.9422, 0.9163, 0.9332, 1.0385,\n",
       "                       0.9540, 0.9346, 0.9807, 0.9988, 1.0332, 0.9307, 0.9741, 0.9708, 1.0372,\n",
       "                       0.9618, 0.9719, 1.0110, 0.9977, 0.9824, 0.9189, 0.9367, 1.1518, 0.9375,\n",
       "                       1.0127, 0.9777, 0.9833, 1.0089, 0.9567, 0.9944, 0.9539, 0.9537, 0.9933,\n",
       "                       1.0257, 0.9628, 0.9432, 0.9521, 0.9561, 1.0270, 1.0515, 1.0031, 0.9811,\n",
       "                       1.0540, 0.9654, 1.0987, 0.9105, 0.8965, 0.9892, 0.9852, 0.9682, 0.8834,\n",
       "                       1.0933, 1.0380, 0.9390, 0.9505, 1.0529, 1.0772, 1.0426, 0.9539, 1.0445,\n",
       "                       0.9834, 1.0106], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-5.4627e-02,  7.3330e-04,  1.0867e-02],\n",
       "                         [-1.2974e-02,  1.8572e-02, -8.2295e-03],\n",
       "                         [-6.2027e-03,  3.7114e-03,  6.4246e-02]],\n",
       "               \n",
       "                        [[-4.5182e-02, -5.4583e-02, -4.1419e-02],\n",
       "                         [-4.6200e-02, -4.0708e-02, -3.4592e-02],\n",
       "                         [-3.8282e-02,  5.8337e-03, -4.8956e-02]],\n",
       "               \n",
       "                        [[ 6.5746e-02,  1.5244e-02,  8.7147e-02],\n",
       "                         [-3.4472e-02, -2.5536e-02,  2.4144e-02],\n",
       "                         [-3.4479e-02,  4.1504e-02,  2.4551e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.6482e-02, -7.8470e-03, -3.9585e-02],\n",
       "                         [ 2.1164e-02, -1.7489e-02,  1.2096e-02],\n",
       "                         [ 2.4554e-02,  6.7232e-02,  4.6818e-02]],\n",
       "               \n",
       "                        [[-1.3402e-02, -3.8527e-03, -2.4207e-02],\n",
       "                         [ 1.4606e-02, -7.9428e-03, -1.4672e-02],\n",
       "                         [ 2.4145e-02, -2.7772e-02, -3.5707e-02]],\n",
       "               \n",
       "                        [[ 5.1828e-02, -9.1739e-03,  8.9307e-03],\n",
       "                         [ 3.5144e-03, -6.4678e-03, -4.3382e-02],\n",
       "                         [-9.9676e-03, -3.8737e-02, -2.5570e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4494e-03, -8.8308e-03, -2.3047e-02],\n",
       "                         [ 5.3864e-03, -3.2619e-02, -1.8617e-02],\n",
       "                         [ 3.7532e-03, -3.0453e-03, -3.3701e-02]],\n",
       "               \n",
       "                        [[-1.7086e-02, -1.4788e-02, -3.8940e-03],\n",
       "                         [-6.3392e-02, -3.3190e-02, -4.4878e-02],\n",
       "                         [-3.9955e-02,  1.3318e-02,  1.3630e-02]],\n",
       "               \n",
       "                        [[-2.2971e-02,  1.3972e-02,  4.7967e-02],\n",
       "                         [ 2.2433e-02,  3.2503e-03, -3.8789e-04],\n",
       "                         [ 1.0756e-02,  1.0665e-02,  4.0786e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.7207e-02, -3.5612e-02,  3.2990e-02],\n",
       "                         [-1.0289e-02, -5.2001e-02,  2.2325e-02],\n",
       "                         [ 4.3186e-02, -5.0887e-03, -3.1521e-02]],\n",
       "               \n",
       "                        [[-7.2463e-03,  2.2586e-02, -4.4914e-02],\n",
       "                         [-2.0083e-02, -1.9633e-04, -3.0193e-02],\n",
       "                         [ 3.4482e-02, -5.3447e-03,  2.6395e-02]],\n",
       "               \n",
       "                        [[ 2.2625e-02, -2.5691e-03, -5.2997e-02],\n",
       "                         [-3.9718e-02, -5.9920e-02, -7.0250e-02],\n",
       "                         [-3.1720e-02, -5.6156e-03, -3.6102e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.1116e-02,  4.1400e-02,  7.7896e-02],\n",
       "                         [-5.2314e-03,  5.7153e-02,  6.5850e-02],\n",
       "                         [ 6.1248e-02,  3.1912e-02,  8.1214e-03]],\n",
       "               \n",
       "                        [[-1.7594e-02, -2.7667e-02,  6.5020e-03],\n",
       "                         [-2.5013e-02,  3.1521e-02,  1.0809e-02],\n",
       "                         [-4.3111e-03, -8.4143e-05, -1.1235e-02]],\n",
       "               \n",
       "                        [[ 2.0829e-02,  8.6329e-03, -4.2440e-02],\n",
       "                         [-2.5366e-02,  2.7901e-02, -9.4518e-03],\n",
       "                         [ 8.4475e-03,  5.2818e-02, -3.1964e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2820e-02, -1.8504e-02, -6.6513e-02],\n",
       "                         [-1.3898e-03, -2.1544e-02, -1.7567e-02],\n",
       "                         [-5.2538e-02,  1.3536e-02,  1.3664e-03]],\n",
       "               \n",
       "                        [[ 5.7266e-02,  3.3359e-02,  2.5032e-03],\n",
       "                         [ 6.3723e-02,  6.3739e-02,  5.4753e-02],\n",
       "                         [-1.3828e-02,  1.9169e-02,  1.1911e-02]],\n",
       "               \n",
       "                        [[-2.6761e-02,  1.5723e-02, -1.2717e-02],\n",
       "                         [-2.4329e-02,  3.9293e-03, -3.9389e-02],\n",
       "                         [ 1.6449e-02,  1.2233e-02, -1.2991e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0369e-02,  2.0749e-02,  7.4456e-02],\n",
       "                         [ 7.0774e-02,  4.0400e-02,  2.0487e-02],\n",
       "                         [ 3.9115e-02,  3.1699e-02,  2.0401e-02]],\n",
       "               \n",
       "                        [[-2.9395e-02,  2.5912e-02, -1.5670e-02],\n",
       "                         [ 3.6106e-02, -5.6428e-03, -1.2668e-02],\n",
       "                         [ 1.1267e-02, -1.9298e-02,  1.0444e-02]],\n",
       "               \n",
       "                        [[ 6.6017e-02,  9.4842e-02,  1.0661e-01],\n",
       "                         [ 5.6030e-02,  4.9520e-02, -1.6742e-02],\n",
       "                         [ 3.1501e-02,  5.3863e-02,  6.5480e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.5279e-03,  2.6637e-02, -1.5511e-02],\n",
       "                         [ 1.0471e-02, -2.2111e-02, -1.6427e-02],\n",
       "                         [ 6.6035e-02,  2.6296e-02,  9.1037e-04]],\n",
       "               \n",
       "                        [[-1.9806e-02, -5.3623e-03, -2.7325e-02],\n",
       "                         [-2.5083e-02, -1.0296e-02, -1.6746e-02],\n",
       "                         [ 2.9134e-02, -5.8477e-03, -2.6059e-03]],\n",
       "               \n",
       "                        [[-3.5956e-02,  9.9449e-03,  7.3160e-03],\n",
       "                         [-3.3580e-02, -1.2026e-02,  1.8119e-02],\n",
       "                         [-5.9664e-02, -2.7382e-02, -9.3631e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0678e-02,  2.4934e-02,  2.0218e-02],\n",
       "                         [ 4.2124e-02,  6.6200e-03,  6.8485e-02],\n",
       "                         [ 4.8555e-02,  3.4361e-03, -4.3516e-03]],\n",
       "               \n",
       "                        [[ 2.7361e-02, -1.3040e-02,  1.2497e-02],\n",
       "                         [ 2.7245e-02,  7.9802e-03,  6.1445e-02],\n",
       "                         [ 2.1408e-02, -2.5271e-02, -1.4236e-02]],\n",
       "               \n",
       "                        [[ 2.6388e-02,  1.0168e-02,  5.2068e-03],\n",
       "                         [-1.7016e-02, -1.5717e-02, -2.3489e-02],\n",
       "                         [ 2.1516e-03,  2.9778e-02, -4.5113e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0460e-02, -1.1264e-02,  1.8085e-02],\n",
       "                         [ 6.7404e-02, -4.3560e-03,  7.5595e-02],\n",
       "                         [ 3.1166e-02,  8.9611e-02,  2.6672e-02]],\n",
       "               \n",
       "                        [[ 3.6279e-03, -4.4067e-02,  2.6130e-02],\n",
       "                         [-2.2956e-02,  3.1149e-02,  1.0416e-02],\n",
       "                         [ 2.2577e-02, -3.9788e-02,  4.6509e-02]],\n",
       "               \n",
       "                        [[ 1.1394e-02,  2.5146e-03, -3.5653e-02],\n",
       "                         [-4.5091e-02,  1.8573e-02, -5.8179e-02],\n",
       "                         [-2.0710e-02, -1.1970e-02, -3.7716e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.0215e-02, -2.5592e-02, -7.2966e-03],\n",
       "                         [-6.4089e-03,  8.9794e-03,  4.4473e-03],\n",
       "                         [-5.4829e-02, -2.1084e-02,  1.4473e-03]],\n",
       "               \n",
       "                        [[ 5.4302e-02,  6.5507e-02,  8.2479e-02],\n",
       "                         [ 5.4617e-02,  6.6363e-02,  1.0264e-01],\n",
       "                         [ 6.9296e-02,  1.1446e-01,  1.0007e-01]],\n",
       "               \n",
       "                        [[-2.8678e-02, -2.0243e-02, -2.5346e-02],\n",
       "                         [-5.8157e-02, -1.5913e-02, -2.3890e-02],\n",
       "                         [ 6.0083e-03, -2.9671e-02, -3.1314e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5507e-02, -1.0065e-02,  3.0881e-02],\n",
       "                         [ 3.1270e-03, -3.6857e-02,  1.6174e-02],\n",
       "                         [-4.4302e-02, -1.9763e-02, -2.2433e-02]],\n",
       "               \n",
       "                        [[ 4.3493e-02,  8.6395e-02,  3.7174e-02],\n",
       "                         [ 2.9114e-02,  2.4833e-02, -1.4103e-02],\n",
       "                         [ 3.0108e-02,  8.4913e-02,  2.5513e-02]],\n",
       "               \n",
       "                        [[ 5.9403e-03,  9.7578e-03, -2.9943e-02],\n",
       "                         [ 1.4497e-03,  2.5015e-02, -1.4867e-02],\n",
       "                         [ 3.0806e-02,  3.3726e-02,  1.3615e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.0357,  0.0166,  0.0038, -0.0068, -0.0011, -0.0061, -0.0103, -0.0034,\n",
       "                       -0.0166,  0.0006,  0.0040, -0.0227, -0.0109, -0.0218, -0.0041,  0.0159,\n",
       "                        0.0249,  0.0085,  0.0003, -0.0096, -0.0051,  0.0175, -0.0161,  0.0572,\n",
       "                       -0.0335,  0.0107,  0.0247,  0.0042,  0.0377, -0.0171, -0.1015, -0.0228,\n",
       "                       -0.0087, -0.0797, -0.0014,  0.0367, -0.0072, -0.0147,  0.0137, -0.0163,\n",
       "                       -0.0089,  0.0034, -0.0174, -0.0188, -0.0105,  0.0368, -0.0123, -0.0034,\n",
       "                        0.0002,  0.0054, -0.0139,  0.0104, -0.0051, -0.0017,  0.0151, -0.0027,\n",
       "                        0.0474,  0.0014,  0.0009,  0.0025, -0.0011, -0.0130,  0.0087, -0.0031,\n",
       "                       -0.0616, -0.0055, -0.0170, -0.0149, -0.0131, -0.0219, -0.0087,  0.0045,\n",
       "                        0.0055,  0.0209, -0.0233, -0.0038, -0.0206, -0.0121,  0.0124, -0.0020,\n",
       "                       -0.0793,  0.0078,  0.0091, -0.0271,  0.0183, -0.0040, -0.0276, -0.0200,\n",
       "                       -0.0018, -0.0120,  0.0062,  0.0035, -0.0193, -0.0043,  0.0060, -0.0033,\n",
       "                        0.0979,  0.0046,  0.0230, -0.0159,  0.0214,  0.0229, -0.0133,  0.0055,\n",
       "                        0.0117, -0.0413,  0.0519, -0.0069, -0.0121,  0.0104,  0.0603,  0.0050,\n",
       "                       -0.0129, -0.0009,  0.0235,  0.0443, -0.0130, -0.0091, -0.0486,  0.0067,\n",
       "                       -0.0302,  0.0049, -0.0119, -0.0003,  0.0028, -0.0006, -0.0140, -0.0030],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([ 0.0080, -0.1051, -0.0583, -0.2114, -0.0156, -0.3025, -0.1431, -0.1690,\n",
       "                       -0.1414, -0.1600, -0.0668, -0.2989, -0.3124, -0.2248, -0.1399, -0.2922,\n",
       "                       -0.1752, -0.2401, -0.0152, -0.2591, -0.2447, -0.1701, -0.1470, -0.2224,\n",
       "                       -0.0362, -0.2455, -0.1750, -0.0396, -0.2098, -0.1359, -0.3048, -0.0343,\n",
       "                        0.0020, -0.1888, -0.2021, -0.0549, -0.1270, -0.1581, -0.1407, -0.0158,\n",
       "                       -0.0772, -0.0932, -0.1724, -0.2669, -0.1154, -0.2852, -0.1887, -0.0445,\n",
       "                       -0.1951, -0.1330, -0.0634, -0.0719, -0.0925, -0.0487, -0.1158, -0.0116,\n",
       "                       -0.3001, -0.2889, -0.1159, -0.0192, -0.1558, -0.1853, -0.1915, -0.1553,\n",
       "                       -0.1455, -0.2951, -0.2013,  0.0113, -0.0357, -0.2242, -0.0636, -0.3184,\n",
       "                       -0.2581, -0.1903, -0.0680, -0.1121, -0.2960, -0.1810, -0.0241, -0.2858,\n",
       "                       -0.0688, -0.1451, -0.0308, -0.1205, -0.3220, -0.3186, -0.0910, -0.1209,\n",
       "                       -0.0116, -0.1675, -0.0689, -0.1094, -0.1885, -0.1492,  0.0643, -0.2327,\n",
       "                       -0.1868, -0.1117, -0.1298, -0.1076, -0.2204, -0.2048, -0.1689, -0.1174,\n",
       "                       -0.1029, -0.1313, -0.2718, -0.1771, -0.0555, -0.1106, -0.2870, -0.0997,\n",
       "                       -0.1625, -0.1487, -0.3023, -0.2512, -0.2352, -0.0299, -0.0183, -0.0160,\n",
       "                       -0.2046, -0.2446, -0.2953, -0.1546, -0.1913, -0.0767, -0.0154, -0.0886],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([1.3516, 1.1140, 1.1335, 1.0717, 1.3636, 0.7596, 1.1063, 1.2343, 1.0519,\n",
       "                       1.5841, 1.0131, 1.0762, 1.1994, 1.4031, 1.1075, 0.9533, 1.0498, 1.0185,\n",
       "                       1.1805, 1.0876, 0.7843, 1.0940, 1.0523, 0.8384, 1.0683, 0.7602, 0.9357,\n",
       "                       1.2673, 0.9396, 1.0553, 0.9735, 1.3504, 1.2847, 1.0209, 1.1235, 1.0936,\n",
       "                       1.1686, 0.9953, 1.0376, 1.3279, 1.3441, 1.0842, 1.2170, 1.0637, 1.1351,\n",
       "                       1.0148, 1.0655, 1.2049, 1.2501, 1.0279, 1.1874, 1.2345, 1.1056, 1.6950,\n",
       "                       1.0704, 1.3799, 0.9915, 1.0065, 1.1655, 1.3675, 1.2508, 0.9559, 0.8257,\n",
       "                       1.2192, 1.0724, 1.2390, 0.9456, 1.2447, 1.2510, 0.8256, 1.2509, 0.9981,\n",
       "                       1.2127, 1.0307, 1.1309, 1.1541, 1.0804, 1.2754, 1.1647, 0.7533, 1.2607,\n",
       "                       1.0583, 1.2901, 0.9747, 0.7526, 1.0185, 1.1555, 1.1468, 1.3335, 1.1562,\n",
       "                       1.3142, 1.2976, 1.2818, 1.0301, 1.4043, 0.9505, 0.9716, 1.0872, 1.1282,\n",
       "                       1.0112, 0.9712, 0.9378, 1.1418, 1.2569, 1.0719, 1.2371, 0.8701, 1.1466,\n",
       "                       1.2526, 1.2469, 0.7931, 1.2396, 1.0986, 1.1581, 0.7296, 0.8709, 1.0038,\n",
       "                       1.3437, 1.2107, 1.2521, 0.9667, 1.2116, 0.9963, 1.1073, 1.0488, 1.2081,\n",
       "                       1.1719, 1.4677], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-4.9095e-03, -1.0411e-02,  5.6891e-03,  ..., -4.5805e-03,\n",
       "                         3.6054e-03,  7.6953e-04],\n",
       "                       [-1.1041e-02, -4.2250e-03,  1.1841e-02,  ..., -7.0961e-03,\n",
       "                         1.0293e-02, -1.3303e-02],\n",
       "                       [ 1.2086e-02,  3.3581e-03,  1.2667e-05,  ..., -2.6210e-03,\n",
       "                         6.9780e-03, -2.0183e-02],\n",
       "                       [-1.2930e-02, -5.7542e-03,  8.0124e-03,  ..., -9.6367e-03,\n",
       "                         2.3009e-04, -4.6410e-03],\n",
       "                       [-7.4690e-03, -9.8778e-03,  1.0700e-02,  ..., -4.4659e-03,\n",
       "                         9.0888e-03, -5.3043e-03]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0318,  0.0158,  0.0506, -0.1405,  0.1046], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.481154550075531,\n",
       "   1.3577819538116456,\n",
       "   1.3073870079517365,\n",
       "   1.2560416444540023,\n",
       "   1.2139344412088393,\n",
       "   1.1738374639749527,\n",
       "   1.1513489400148391,\n",
       "   1.1156441736221314,\n",
       "   1.1000408185720443,\n",
       "   1.1022559398412703,\n",
       "   1.0600120342969894,\n",
       "   1.0610931652784348,\n",
       "   1.0509943833351136,\n",
       "   1.0297512702941896,\n",
       "   1.0259157729148864,\n",
       "   1.0170646781921386,\n",
       "   1.0025434201955796,\n",
       "   0.9979545515775681,\n",
       "   0.9981109420061112,\n",
       "   0.983209489941597,\n",
       "   0.9851244906187058,\n",
       "   0.9585923129320144,\n",
       "   0.9533718949556351,\n",
       "   0.9537534577846527,\n",
       "   0.9565720618963242,\n",
       "   0.9413936160802842,\n",
       "   0.9310720965862275,\n",
       "   0.9261977025270463,\n",
       "   0.927066831946373,\n",
       "   0.9329067448377609,\n",
       "   0.912413406252861,\n",
       "   0.9153839558362961,\n",
       "   0.8964474000930787,\n",
       "   0.8877046661376953,\n",
       "   0.8930694156885147,\n",
       "   0.8728554663658142,\n",
       "   0.8791505703926087,\n",
       "   0.8727824296951294,\n",
       "   0.876187061548233,\n",
       "   0.8546114403605461,\n",
       "   0.8619972977638245,\n",
       "   0.8517441082596778,\n",
       "   0.8415768438577652,\n",
       "   0.8492840473651886,\n",
       "   0.843542504310608,\n",
       "   0.8249881924390793,\n",
       "   0.8268159502148629,\n",
       "   0.8218312242031097,\n",
       "   0.8166390725374222,\n",
       "   0.8181124683618546,\n",
       "   0.8208748137354851,\n",
       "   0.8045872502923012,\n",
       "   0.8046765422224998,\n",
       "   0.7958034241199493,\n",
       "   0.8077864516973495,\n",
       "   0.7975693554878235,\n",
       "   0.790548490345478,\n",
       "   0.7801121383309364,\n",
       "   0.7803864649534226,\n",
       "   0.7814394291639328,\n",
       "   0.7828030265569687,\n",
       "   0.7666941681504249,\n",
       "   0.7606288375258445,\n",
       "   0.7601050273180008,\n",
       "   0.7594932028055191,\n",
       "   0.7595013257265091,\n",
       "   0.7552100366353989,\n",
       "   0.7360448144674301,\n",
       "   0.7414670512080193,\n",
       "   0.7494295625090599,\n",
       "   0.7371460669636727,\n",
       "   0.7371414548754692,\n",
       "   0.7387767916321755,\n",
       "   0.7346621882319451,\n",
       "   0.729343874335289,\n",
       "   0.7154981326460839,\n",
       "   0.7241853189468384,\n",
       "   0.7200521076917649,\n",
       "   0.7168885003328324,\n",
       "   0.7137106335759162,\n",
       "   0.704644744694233,\n",
       "   0.712933917939663,\n",
       "   0.7101412235498429,\n",
       "   0.6976962233185768,\n",
       "   0.6835261573195457,\n",
       "   0.6959152084589004,\n",
       "   0.6946925252676011,\n",
       "   0.6936626036167145,\n",
       "   0.6855437688231468,\n",
       "   0.6757366685271263,\n",
       "   0.6853866739869118,\n",
       "   0.6831811745166778,\n",
       "   0.669134651184082,\n",
       "   0.674112578690052,\n",
       "   0.6681912115812302,\n",
       "   0.6822096589207649,\n",
       "   0.6632646207809448,\n",
       "   0.6614208935499192,\n",
       "   0.6562968887090683],\n",
       "  'train_loss_std': [0.1826323852296664,\n",
       "   0.1304803816692784,\n",
       "   0.13473082044900705,\n",
       "   0.1264148518281296,\n",
       "   0.1309927541645151,\n",
       "   0.13041765483289183,\n",
       "   0.13595038986421248,\n",
       "   0.1282007766301599,\n",
       "   0.125523515884704,\n",
       "   0.12920474467392193,\n",
       "   0.13149506488701052,\n",
       "   0.14458368235504027,\n",
       "   0.13563396875998307,\n",
       "   0.1340685289431707,\n",
       "   0.1269818281368983,\n",
       "   0.13682861508570374,\n",
       "   0.13270618698630288,\n",
       "   0.13596521509053586,\n",
       "   0.13223582941469023,\n",
       "   0.14225810632161273,\n",
       "   0.13308502067756578,\n",
       "   0.12679021754557895,\n",
       "   0.1413390589588845,\n",
       "   0.1359445235222156,\n",
       "   0.13791269515262905,\n",
       "   0.14443757268196517,\n",
       "   0.13392421180560674,\n",
       "   0.13951165945190877,\n",
       "   0.13877931301643015,\n",
       "   0.1358298267473773,\n",
       "   0.13501797264604687,\n",
       "   0.1349164184967577,\n",
       "   0.13695523673551566,\n",
       "   0.1451483073167463,\n",
       "   0.13982310347214305,\n",
       "   0.14314590149939144,\n",
       "   0.1363050076063552,\n",
       "   0.14002422585038526,\n",
       "   0.14759149039856237,\n",
       "   0.14450472539140388,\n",
       "   0.13188158767524813,\n",
       "   0.1350245778423253,\n",
       "   0.12905263484849128,\n",
       "   0.13764389183831624,\n",
       "   0.13881268250807685,\n",
       "   0.1354287122386498,\n",
       "   0.13985902688178561,\n",
       "   0.13178799312143952,\n",
       "   0.1342801378552925,\n",
       "   0.13451919669575796,\n",
       "   0.12829729237562648,\n",
       "   0.14118260486342377,\n",
       "   0.1308724887873948,\n",
       "   0.13919309404204383,\n",
       "   0.14872840812009974,\n",
       "   0.12976774577852346,\n",
       "   0.12731262007723013,\n",
       "   0.13881869922137072,\n",
       "   0.13462943655004367,\n",
       "   0.13770271167993803,\n",
       "   0.13847298443940023,\n",
       "   0.140252588668378,\n",
       "   0.1382651704000421,\n",
       "   0.13250643508855578,\n",
       "   0.13567018807668704,\n",
       "   0.14114834201387394,\n",
       "   0.13345703945506426,\n",
       "   0.13655771799823885,\n",
       "   0.12798300171630722,\n",
       "   0.1373892975461676,\n",
       "   0.12939987675503944,\n",
       "   0.1371566591935981,\n",
       "   0.1380250530490947,\n",
       "   0.13158649704415568,\n",
       "   0.13477428405938588,\n",
       "   0.13420257553948292,\n",
       "   0.1407853177902386,\n",
       "   0.14061222669207435,\n",
       "   0.13677513237132138,\n",
       "   0.13573551411253723,\n",
       "   0.13190207288199612,\n",
       "   0.132529215541455,\n",
       "   0.13839148092956385,\n",
       "   0.130502342266507,\n",
       "   0.1302888656064939,\n",
       "   0.1315539211756657,\n",
       "   0.12965038840868331,\n",
       "   0.13434221155013185,\n",
       "   0.13612430655629618,\n",
       "   0.13094215138925933,\n",
       "   0.13315066921480567,\n",
       "   0.13470950451381783,\n",
       "   0.1279483889866256,\n",
       "   0.13657829005808028,\n",
       "   0.12884267597012028,\n",
       "   0.14187728639938374,\n",
       "   0.12587805437370733,\n",
       "   0.13249774895751423,\n",
       "   0.13068641180660404],\n",
       "  'train_accuracy_mean': [0.4261333337724209,\n",
       "   0.4493066667318344,\n",
       "   0.4682266671061516,\n",
       "   0.4951066664457321,\n",
       "   0.5127066660523415,\n",
       "   0.5336266663074494,\n",
       "   0.5442933322191238,\n",
       "   0.5627199993133545,\n",
       "   0.5695866670012474,\n",
       "   0.565373331964016,\n",
       "   0.588639999628067,\n",
       "   0.5888799980282784,\n",
       "   0.5933466667532921,\n",
       "   0.601453332722187,\n",
       "   0.6016533324122428,\n",
       "   0.603493331849575,\n",
       "   0.6135866670608521,\n",
       "   0.6145866674780845,\n",
       "   0.6122266656756401,\n",
       "   0.6214533323645591,\n",
       "   0.6191599994301796,\n",
       "   0.6309333310723305,\n",
       "   0.6338266662359238,\n",
       "   0.6329333322644234,\n",
       "   0.6336533327102661,\n",
       "   0.6381600015163421,\n",
       "   0.6439333313703537,\n",
       "   0.6469600001573562,\n",
       "   0.6427999994754792,\n",
       "   0.6396266660690307,\n",
       "   0.6505066667199135,\n",
       "   0.6491199991106987,\n",
       "   0.6559199989438057,\n",
       "   0.6606533325314522,\n",
       "   0.6594800000190735,\n",
       "   0.6673466663360595,\n",
       "   0.6655200003981591,\n",
       "   0.6673333329558373,\n",
       "   0.6650399996638298,\n",
       "   0.6748266662359238,\n",
       "   0.6705600000619888,\n",
       "   0.6772666668891907,\n",
       "   0.6799200012087822,\n",
       "   0.6772000007033349,\n",
       "   0.6801466667056084,\n",
       "   0.6869466667175292,\n",
       "   0.686026665687561,\n",
       "   0.6902533336877823,\n",
       "   0.6920133324265481,\n",
       "   0.6886266642808914,\n",
       "   0.68794666659832,\n",
       "   0.6986266648769379,\n",
       "   0.6940266666412354,\n",
       "   0.699613334774971,\n",
       "   0.6942933332920075,\n",
       "   0.7006933341026306,\n",
       "   0.7033600001335144,\n",
       "   0.7074799988865852,\n",
       "   0.7057733334302903,\n",
       "   0.7049866656064987,\n",
       "   0.7036266678571701,\n",
       "   0.7119333344697952,\n",
       "   0.7138933347463607,\n",
       "   0.715853333234787,\n",
       "   0.7141999994516373,\n",
       "   0.7140400002002716,\n",
       "   0.7168533338308334,\n",
       "   0.725626667380333,\n",
       "   0.7227066665887832,\n",
       "   0.7193200001716614,\n",
       "   0.725413333773613,\n",
       "   0.7234800004959107,\n",
       "   0.723253332734108,\n",
       "   0.7253733327388764,\n",
       "   0.7260933326482772,\n",
       "   0.7316666649580001,\n",
       "   0.7288933338522912,\n",
       "   0.7306133338212967,\n",
       "   0.7303333345651627,\n",
       "   0.7314266653060914,\n",
       "   0.735506667137146,\n",
       "   0.7332399996519089,\n",
       "   0.7349199990034103,\n",
       "   0.7404933340549469,\n",
       "   0.7425599994659424,\n",
       "   0.7415066667795182,\n",
       "   0.7392666659355164,\n",
       "   0.7413466668128967,\n",
       "   0.7460399987697601,\n",
       "   0.746693333029747,\n",
       "   0.7457199994325637,\n",
       "   0.7445866672992706,\n",
       "   0.7506400004625321,\n",
       "   0.7486000003814697,\n",
       "   0.7513066667318344,\n",
       "   0.7477466658353805,\n",
       "   0.7541600004434585,\n",
       "   0.7561466666460037,\n",
       "   0.7561733330488205],\n",
       "  'train_accuracy_std': [0.06591361322123487,\n",
       "   0.06227793756495563,\n",
       "   0.06835308367811276,\n",
       "   0.06538203616534438,\n",
       "   0.06831468476400906,\n",
       "   0.06832798748275302,\n",
       "   0.07193369262052633,\n",
       "   0.06729224365959229,\n",
       "   0.06510100054305314,\n",
       "   0.06479930119070763,\n",
       "   0.06795959040083484,\n",
       "   0.07165900098586855,\n",
       "   0.06741513048744034,\n",
       "   0.0680922502859073,\n",
       "   0.06673413008510871,\n",
       "   0.06553351161599054,\n",
       "   0.06747923416971312,\n",
       "   0.06650268395691467,\n",
       "   0.0683710270020473,\n",
       "   0.07152263884476014,\n",
       "   0.06612652072070295,\n",
       "   0.06206032981381363,\n",
       "   0.0703668251657764,\n",
       "   0.06648789418471994,\n",
       "   0.06728701246625754,\n",
       "   0.07155660559517912,\n",
       "   0.066334473167138,\n",
       "   0.06834229755388486,\n",
       "   0.06930627723079129,\n",
       "   0.06537307174004116,\n",
       "   0.06784433944616584,\n",
       "   0.06691306983620103,\n",
       "   0.0668841311562931,\n",
       "   0.069106165892124,\n",
       "   0.06901591305095696,\n",
       "   0.06915219765764169,\n",
       "   0.0647862887261049,\n",
       "   0.06623258767813986,\n",
       "   0.0718419459488728,\n",
       "   0.07169497800973891,\n",
       "   0.06508761424464551,\n",
       "   0.0657706116016513,\n",
       "   0.06296801911217323,\n",
       "   0.06590080222099384,\n",
       "   0.06840842025300041,\n",
       "   0.06627492772510031,\n",
       "   0.06543352288977157,\n",
       "   0.06403907092916886,\n",
       "   0.06840104002154876,\n",
       "   0.06458743119987016,\n",
       "   0.06211642687526532,\n",
       "   0.06738927239924632,\n",
       "   0.06253929844881151,\n",
       "   0.06659951138714797,\n",
       "   0.07149553758333171,\n",
       "   0.06295719321740238,\n",
       "   0.06128475736175209,\n",
       "   0.06637372883572769,\n",
       "   0.06555880956323262,\n",
       "   0.0679099067738081,\n",
       "   0.06434354386270719,\n",
       "   0.06817295001884771,\n",
       "   0.06506917563911233,\n",
       "   0.061723078532587426,\n",
       "   0.0640697890322431,\n",
       "   0.0658114347887569,\n",
       "   0.06567976711477352,\n",
       "   0.06569548135653552,\n",
       "   0.061942325345967345,\n",
       "   0.06522289835833038,\n",
       "   0.06401411539999399,\n",
       "   0.06662449371062536,\n",
       "   0.06412274824441129,\n",
       "   0.06401123695914206,\n",
       "   0.06404671884218813,\n",
       "   0.06506594910096727,\n",
       "   0.06656206531730184,\n",
       "   0.06710573129074256,\n",
       "   0.06630150024425562,\n",
       "   0.06420529746841241,\n",
       "   0.06424250044563738,\n",
       "   0.06427503455762233,\n",
       "   0.06539770887594931,\n",
       "   0.06324433107517077,\n",
       "   0.06368971393939064,\n",
       "   0.06288010494075127,\n",
       "   0.06242716640396464,\n",
       "   0.06472409529633813,\n",
       "   0.06565402924516457,\n",
       "   0.0637313745094133,\n",
       "   0.06218461308944313,\n",
       "   0.06251565245503438,\n",
       "   0.06131912101622321,\n",
       "   0.06498748440065961,\n",
       "   0.062471889729398905,\n",
       "   0.06593726125482527,\n",
       "   0.059265363121513114,\n",
       "   0.06267656471033763,\n",
       "   0.061985131336572095],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.471176369190216,\n",
       "   1.4203386040528616,\n",
       "   1.3806963209311167,\n",
       "   1.3533262141545614,\n",
       "   1.3228069829940796,\n",
       "   1.2923235789934795,\n",
       "   1.2658109680811565,\n",
       "   1.2479879687229791,\n",
       "   1.22375756641229,\n",
       "   1.2312382558981578,\n",
       "   1.2062380532423655,\n",
       "   1.201440934141477,\n",
       "   1.1956076474984487,\n",
       "   1.193373558918635,\n",
       "   1.1765884820620218,\n",
       "   1.17311465660731,\n",
       "   1.187868253191312,\n",
       "   1.1736191300551098,\n",
       "   1.1559246160586676,\n",
       "   1.1491659792264302,\n",
       "   1.1453562692801158,\n",
       "   1.1545843807856242,\n",
       "   1.1417370017369588,\n",
       "   1.1338486299912134,\n",
       "   1.1240334182977676,\n",
       "   1.1281356239318847,\n",
       "   1.1178909635543823,\n",
       "   1.1150680551926295,\n",
       "   1.103643309076627,\n",
       "   1.1039624404907227,\n",
       "   1.0983565264940263,\n",
       "   1.0944552268584569,\n",
       "   1.0878954009215036,\n",
       "   1.0826534553368887,\n",
       "   1.084435091416041,\n",
       "   1.0796767693758011,\n",
       "   1.0691722273826598,\n",
       "   1.075462252298991,\n",
       "   1.0627307752768198,\n",
       "   1.069868769844373,\n",
       "   1.058972169359525,\n",
       "   1.0591596617301304,\n",
       "   1.056293647289276,\n",
       "   1.0516797292232514,\n",
       "   1.0464338860909144,\n",
       "   1.04096601943175,\n",
       "   1.0390473582347235,\n",
       "   1.0437529691060383,\n",
       "   1.0289837962388992,\n",
       "   1.0326292578379312,\n",
       "   1.0349435208241144,\n",
       "   1.03252683142821,\n",
       "   1.0310525498787562,\n",
       "   1.0104590525229773,\n",
       "   1.0139344509442647,\n",
       "   1.0106956521670023,\n",
       "   1.032102631131808,\n",
       "   1.0090903135140736,\n",
       "   1.0105778209368388,\n",
       "   1.0125532406568527,\n",
       "   1.0054045470555624,\n",
       "   0.9964819796880087,\n",
       "   0.9941920403639476,\n",
       "   0.9932256497939428,\n",
       "   0.9877218504746755,\n",
       "   0.9872963384787241,\n",
       "   0.9900266283750534,\n",
       "   0.9877198950449626,\n",
       "   0.9829322874546051,\n",
       "   0.9856233690182368,\n",
       "   0.9900106473763783,\n",
       "   0.9901331992944081,\n",
       "   0.9795691275596619,\n",
       "   0.9728402330478032,\n",
       "   0.9765372564395268,\n",
       "   0.9615845106045405,\n",
       "   0.9775967270135879,\n",
       "   0.9674115592241287,\n",
       "   0.9670706542332967,\n",
       "   0.9565932681163152,\n",
       "   0.9645783323049545,\n",
       "   0.954222569068273,\n",
       "   0.9548276960849762,\n",
       "   0.9615878440936406,\n",
       "   0.9601044146219889,\n",
       "   0.9499993185202281,\n",
       "   0.951361569960912,\n",
       "   0.9399580576022466,\n",
       "   0.9446669640143712,\n",
       "   0.9545165171225866,\n",
       "   0.9450459994872411,\n",
       "   0.9451093810796738,\n",
       "   0.9568006992340088,\n",
       "   0.9583671281735102,\n",
       "   0.9470258406798044,\n",
       "   0.9410540425777435,\n",
       "   0.9342471953233084,\n",
       "   0.9384942634900411,\n",
       "   0.9439093967278799],\n",
       "  'val_loss_std': [0.11203740130864129,\n",
       "   0.10715929112252705,\n",
       "   0.09827341826141245,\n",
       "   0.10125932640351137,\n",
       "   0.10043295801446125,\n",
       "   0.10740659524183253,\n",
       "   0.11049734687425121,\n",
       "   0.11983441107245077,\n",
       "   0.11930227996812569,\n",
       "   0.11529669617151428,\n",
       "   0.11472055706061447,\n",
       "   0.11461747819445803,\n",
       "   0.1188223925285219,\n",
       "   0.12176546787710572,\n",
       "   0.11885147128588246,\n",
       "   0.11581106846402765,\n",
       "   0.11909995483656824,\n",
       "   0.12196800945397059,\n",
       "   0.12398465967775586,\n",
       "   0.12429102030429559,\n",
       "   0.12626138369322082,\n",
       "   0.12399722233293942,\n",
       "   0.12768290815715796,\n",
       "   0.12657912390755227,\n",
       "   0.1270241844349698,\n",
       "   0.12868818947660365,\n",
       "   0.1277192766153749,\n",
       "   0.1314605584614048,\n",
       "   0.13090726697788443,\n",
       "   0.1326811614541339,\n",
       "   0.12793874582377707,\n",
       "   0.1295033536529318,\n",
       "   0.13508594559952292,\n",
       "   0.1256389745632251,\n",
       "   0.1276599462085666,\n",
       "   0.13297767868085042,\n",
       "   0.13163224099730259,\n",
       "   0.1336244294053933,\n",
       "   0.13202423489457363,\n",
       "   0.1308216289214086,\n",
       "   0.1334729536791329,\n",
       "   0.13540187290704866,\n",
       "   0.1331894646012306,\n",
       "   0.13424498999450782,\n",
       "   0.13375660790981328,\n",
       "   0.13291695676870965,\n",
       "   0.13276573331422253,\n",
       "   0.13617518701132503,\n",
       "   0.13315697343223637,\n",
       "   0.131602270458581,\n",
       "   0.13268724730948828,\n",
       "   0.133248200070377,\n",
       "   0.1334498839268254,\n",
       "   0.13341375022411908,\n",
       "   0.1332544375230059,\n",
       "   0.13522877668616448,\n",
       "   0.13646125097249184,\n",
       "   0.13827714189175325,\n",
       "   0.13632873247995536,\n",
       "   0.1397483090422933,\n",
       "   0.14107534466995766,\n",
       "   0.13191791023486107,\n",
       "   0.13609270302166718,\n",
       "   0.1373436265325103,\n",
       "   0.13782754732955282,\n",
       "   0.13881340803437905,\n",
       "   0.13607888968501025,\n",
       "   0.13811387686912519,\n",
       "   0.13513214094033638,\n",
       "   0.13763618903021407,\n",
       "   0.14120659189166673,\n",
       "   0.13886795139057656,\n",
       "   0.13668628195830615,\n",
       "   0.13594511421080996,\n",
       "   0.1339807457042434,\n",
       "   0.13636625945424435,\n",
       "   0.14274674896351155,\n",
       "   0.13894214011758702,\n",
       "   0.13304951145628094,\n",
       "   0.13509201511513172,\n",
       "   0.14027035249475067,\n",
       "   0.13705290740066123,\n",
       "   0.13767863220396992,\n",
       "   0.1387881382663321,\n",
       "   0.13586516452145064,\n",
       "   0.14269873294848856,\n",
       "   0.13649695558837954,\n",
       "   0.13155420116093455,\n",
       "   0.13704813390960915,\n",
       "   0.14400514639874926,\n",
       "   0.14107233424250729,\n",
       "   0.14060849573919304,\n",
       "   0.14711724698209286,\n",
       "   0.1410907367007103,\n",
       "   0.13930975593546616,\n",
       "   0.14175003598064614,\n",
       "   0.14033359981714238,\n",
       "   0.13476484911151734,\n",
       "   0.13750004130782806],\n",
       "  'val_accuracy_mean': [0.4045111114283403,\n",
       "   0.41722222248713176,\n",
       "   0.4324888893961906,\n",
       "   0.4456444451212883,\n",
       "   0.45973333438237507,\n",
       "   0.47355555643637975,\n",
       "   0.485844445625941,\n",
       "   0.49588888804117837,\n",
       "   0.5084666676322619,\n",
       "   0.5044666656851768,\n",
       "   0.5158888885378837,\n",
       "   0.5172444433967273,\n",
       "   0.5201333321134249,\n",
       "   0.5220222216844559,\n",
       "   0.5298222202062607,\n",
       "   0.53033333192269,\n",
       "   0.5256666652361552,\n",
       "   0.5339777773618698,\n",
       "   0.5385111107428868,\n",
       "   0.5424444432059924,\n",
       "   0.543244443833828,\n",
       "   0.5373111102978388,\n",
       "   0.5459999985496203,\n",
       "   0.550755555431048,\n",
       "   0.553444446225961,\n",
       "   0.5526666683952014,\n",
       "   0.5570666654904683,\n",
       "   0.5590888892610868,\n",
       "   0.5632888871431351,\n",
       "   0.563711110452811,\n",
       "   0.5653777765234311,\n",
       "   0.568422221938769,\n",
       "   0.5713999994595845,\n",
       "   0.5731777773300807,\n",
       "   0.5676222208142281,\n",
       "   0.5737999984622002,\n",
       "   0.5795555543899537,\n",
       "   0.5761999988555908,\n",
       "   0.5801555547118187,\n",
       "   0.5775555542111397,\n",
       "   0.584933332502842,\n",
       "   0.584577779173851,\n",
       "   0.5832666645447413,\n",
       "   0.5870222210884094,\n",
       "   0.5891777774691582,\n",
       "   0.5897111116846403,\n",
       "   0.5926888887087504,\n",
       "   0.5913777764638265,\n",
       "   0.5966444445649782,\n",
       "   0.5947777767976125,\n",
       "   0.593177777826786,\n",
       "   0.5961777770519257,\n",
       "   0.5967333329717318,\n",
       "   0.6039111100633939,\n",
       "   0.5999999997019768,\n",
       "   0.6032444436351458,\n",
       "   0.5947777771949768,\n",
       "   0.6045777769883474,\n",
       "   0.6029333333174388,\n",
       "   0.6017555550734202,\n",
       "   0.6071777763962746,\n",
       "   0.6112444439530372,\n",
       "   0.6100222223997116,\n",
       "   0.6110222214460372,\n",
       "   0.6153333310286204,\n",
       "   0.6140222209692001,\n",
       "   0.6119555546840032,\n",
       "   0.6158888883392016,\n",
       "   0.6154888861378034,\n",
       "   0.6144222230712573,\n",
       "   0.612022221883138,\n",
       "   0.6133555539449056,\n",
       "   0.6153111106157303,\n",
       "   0.6194888902703921,\n",
       "   0.616088885863622,\n",
       "   0.6242444427808126,\n",
       "   0.6175777745246888,\n",
       "   0.6214222213625908,\n",
       "   0.6218444431821505,\n",
       "   0.6247555573781332,\n",
       "   0.622711109717687,\n",
       "   0.6277111102143923,\n",
       "   0.6264444435636203,\n",
       "   0.6237333337465922,\n",
       "   0.6238444447517395,\n",
       "   0.6310888887445132,\n",
       "   0.6279999990264574,\n",
       "   0.6320666670799255,\n",
       "   0.6319333316882452,\n",
       "   0.630444445113341,\n",
       "   0.6305555550257365,\n",
       "   0.6290666659673055,\n",
       "   0.6284222219387691,\n",
       "   0.6274222214023272,\n",
       "   0.6303555554151535,\n",
       "   0.6347555540998777,\n",
       "   0.6359333327412605,\n",
       "   0.6359555553396543,\n",
       "   0.6325333312153816],\n",
       "  'val_accuracy_std': [0.05353799162002322,\n",
       "   0.0524720981914053,\n",
       "   0.05174179064930801,\n",
       "   0.0545594439699906,\n",
       "   0.05402814078556591,\n",
       "   0.05838780569726321,\n",
       "   0.058177808417806676,\n",
       "   0.059355695041157855,\n",
       "   0.061534849233953345,\n",
       "   0.05942979616153935,\n",
       "   0.06035072230962436,\n",
       "   0.057592111503469996,\n",
       "   0.05651335922892677,\n",
       "   0.06100806981501532,\n",
       "   0.06108741345146006,\n",
       "   0.05955669406457232,\n",
       "   0.058520333169778685,\n",
       "   0.06090453432575346,\n",
       "   0.06073382762738213,\n",
       "   0.0630527898408561,\n",
       "   0.06264413916069356,\n",
       "   0.0613480611002943,\n",
       "   0.06201911090369558,\n",
       "   0.0629110434808522,\n",
       "   0.062440392068660824,\n",
       "   0.06351319518242336,\n",
       "   0.06256483259522035,\n",
       "   0.06216751088082387,\n",
       "   0.06503216901734982,\n",
       "   0.063530729283033,\n",
       "   0.06337751299008496,\n",
       "   0.06272361094956662,\n",
       "   0.06447941994903779,\n",
       "   0.06387816796777328,\n",
       "   0.06152727250719634,\n",
       "   0.06408735869920205,\n",
       "   0.06256571892982157,\n",
       "   0.06175642809294948,\n",
       "   0.06404262086135958,\n",
       "   0.06290223682003454,\n",
       "   0.0639579955819006,\n",
       "   0.06262474690427144,\n",
       "   0.06504126291026241,\n",
       "   0.06563552119824073,\n",
       "   0.06647131003645682,\n",
       "   0.06358946703232453,\n",
       "   0.06259830473072665,\n",
       "   0.06436952090747526,\n",
       "   0.06365764528204357,\n",
       "   0.06382953621904496,\n",
       "   0.06245690339964687,\n",
       "   0.06149903542250107,\n",
       "   0.06350382894868392,\n",
       "   0.064204242829744,\n",
       "   0.06414104917256028,\n",
       "   0.06333795125922861,\n",
       "   0.06269611338101945,\n",
       "   0.06368271627047645,\n",
       "   0.06318921597259596,\n",
       "   0.0649081370904879,\n",
       "   0.0637927615753104,\n",
       "   0.06103617205479102,\n",
       "   0.06279065355775032,\n",
       "   0.06318924604310566,\n",
       "   0.06395484523229095,\n",
       "   0.06394186781302506,\n",
       "   0.06258764194025813,\n",
       "   0.06308949158158543,\n",
       "   0.06251712203552993,\n",
       "   0.06230509942086057,\n",
       "   0.06541423204172483,\n",
       "   0.061592445558287426,\n",
       "   0.06380363086714479,\n",
       "   0.06302408409497127,\n",
       "   0.06400549391932762,\n",
       "   0.06282414847505599,\n",
       "   0.0629419452437504,\n",
       "   0.06382191338091253,\n",
       "   0.061903194602919105,\n",
       "   0.061412204716973555,\n",
       "   0.06305833634416222,\n",
       "   0.06295163999330895,\n",
       "   0.06317485019100758,\n",
       "   0.06443362498009035,\n",
       "   0.06333829041430022,\n",
       "   0.06522325974022641,\n",
       "   0.06261463607075052,\n",
       "   0.060682747250622435,\n",
       "   0.06417772774643522,\n",
       "   0.06386289312281059,\n",
       "   0.06522998233880463,\n",
       "   0.06278136106601999,\n",
       "   0.06413203697901643,\n",
       "   0.06247979930146581,\n",
       "   0.06308154220281362,\n",
       "   0.06139410959916963,\n",
       "   0.06245279278670863,\n",
       "   0.06198397255791851,\n",
       "   0.06265919627346102],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6818444436788559,\n",
       " 'best_val_iter': 48000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 96,\n",
       " 'train_loss_mean': 0.4526471059322357,\n",
       " 'train_loss_std': 0.12993412983496969,\n",
       " 'train_accuracy_mean': 0.8343333342075347,\n",
       " 'train_accuracy_std': 0.0548158550248203,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.8508474173148474,\n",
       " 'val_loss_std': 0.14179470314582376,\n",
       " 'val_accuracy_mean': 0.6763111112515131,\n",
       " 'val_accuracy_std': 0.05848475377685013,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0115, -0.0778,  0.0524],\n",
       "                         [-0.0305, -0.0349, -0.0020],\n",
       "                         [-0.0158,  0.1034, -0.0275]],\n",
       "               \n",
       "                        [[ 0.0519, -0.0845,  0.0649],\n",
       "                         [-0.0253,  0.0089,  0.0542],\n",
       "                         [-0.0484,  0.0403, -0.0331]],\n",
       "               \n",
       "                        [[ 0.0545, -0.0248, -0.0204],\n",
       "                         [ 0.0301,  0.0602, -0.0543],\n",
       "                         [-0.0532,  0.0362, -0.0425]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0481,  0.1008,  0.0398],\n",
       "                         [ 0.0482, -0.0425,  0.0113],\n",
       "                         [-0.0911, -0.0673, -0.0469]],\n",
       "               \n",
       "                        [[-0.0092,  0.0265, -0.0018],\n",
       "                         [ 0.0446, -0.0628, -0.0604],\n",
       "                         [ 0.0496, -0.0558,  0.0711]],\n",
       "               \n",
       "                        [[-0.0320,  0.0804, -0.0688],\n",
       "                         [-0.0160,  0.0558,  0.0703],\n",
       "                         [-0.0326, -0.0326, -0.0280]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0071, -0.0199,  0.0549],\n",
       "                         [-0.0223, -0.0425, -0.0309],\n",
       "                         [ 0.0645,  0.0541, -0.0505]],\n",
       "               \n",
       "                        [[-0.0577, -0.0244,  0.0483],\n",
       "                         [ 0.0787, -0.0048,  0.0167],\n",
       "                         [-0.0691, -0.0398,  0.0441]],\n",
       "               \n",
       "                        [[ 0.0377, -0.0742,  0.0330],\n",
       "                         [ 0.0672,  0.0122, -0.0616],\n",
       "                         [-0.0667,  0.0326,  0.0182]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0424, -0.0622, -0.0432],\n",
       "                         [-0.0184, -0.0636,  0.0860],\n",
       "                         [-0.0358,  0.0483, -0.0402]],\n",
       "               \n",
       "                        [[ 0.0383,  0.0228,  0.0691],\n",
       "                         [ 0.0292, -0.0177, -0.0221],\n",
       "                         [ 0.0055,  0.0494, -0.0077]],\n",
       "               \n",
       "                        [[-0.0062, -0.0760, -0.0401],\n",
       "                         [-0.0594,  0.0223,  0.0496],\n",
       "                         [-0.0420,  0.0299,  0.0492]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0190, -0.0077, -0.0359],\n",
       "                         [ 0.0073,  0.0689, -0.0534],\n",
       "                         [ 0.0444,  0.0745, -0.0492]],\n",
       "               \n",
       "                        [[ 0.0429,  0.0223,  0.0724],\n",
       "                         [ 0.0197, -0.0301, -0.0356],\n",
       "                         [-0.0315, -0.0832,  0.0298]],\n",
       "               \n",
       "                        [[-0.0250, -0.0121, -0.0183],\n",
       "                         [-0.0136, -0.0266,  0.0769],\n",
       "                         [-0.0608, -0.0201,  0.0196]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0678,  0.0252, -0.0223],\n",
       "                         [-0.0489,  0.0072,  0.0349],\n",
       "                         [ 0.0568,  0.0435,  0.0784]],\n",
       "               \n",
       "                        [[-0.0682, -0.0912,  0.0311],\n",
       "                         [ 0.0605, -0.0843, -0.0335],\n",
       "                         [ 0.0032,  0.0237,  0.0296]],\n",
       "               \n",
       "                        [[ 0.0041, -0.0576,  0.0435],\n",
       "                         [ 0.0096, -0.0407, -0.0082],\n",
       "                         [-0.0129, -0.0269,  0.0120]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.0765e-03, -8.0369e-04, -1.8382e-03, -1.1685e-04, -1.5081e-04,\n",
       "                        9.4584e-04,  1.9420e-04,  1.0867e-04,  8.7550e-04, -1.9117e-03,\n",
       "                       -3.8360e-04,  1.2847e-04,  2.6250e-03,  1.7375e-03, -2.0646e-03,\n",
       "                        7.6395e-04, -6.3794e-04, -6.1944e-04, -7.8609e-04, -2.0221e-04,\n",
       "                        1.6218e-03, -2.4587e-04,  2.3333e-03,  2.2809e-03,  3.7578e-04,\n",
       "                        2.2750e-05,  6.9353e-04,  7.8915e-04,  1.0253e-03,  9.2878e-04,\n",
       "                        1.5839e-03, -6.1639e-05,  3.5597e-04, -3.5442e-03, -4.6407e-04,\n",
       "                        6.9834e-04,  1.1986e-03,  6.3443e-04, -1.2857e-05, -8.4554e-04,\n",
       "                        2.7600e-04, -3.0869e-03, -1.1622e-03,  2.0713e-04, -3.6686e-04,\n",
       "                       -7.5862e-05,  2.7620e-04, -1.3584e-03, -2.4717e-03, -1.9970e-05,\n",
       "                        1.1759e-03, -8.4054e-04,  1.2545e-03, -2.8829e-03,  1.1732e-03,\n",
       "                        1.6122e-04,  5.0176e-04,  3.3543e-04, -6.3797e-05, -1.7757e-03,\n",
       "                        2.5158e-04,  1.5920e-03, -5.9205e-04,  7.7316e-04,  3.4142e-04,\n",
       "                        7.5710e-04, -6.7782e-04,  1.7540e-03, -8.1071e-04, -5.7699e-04,\n",
       "                        1.4908e-04, -1.1079e-03, -1.0642e-05, -2.4850e-04,  1.3602e-03,\n",
       "                       -5.1398e-04,  8.3879e-04,  8.4650e-04, -1.5811e-04,  3.1155e-04,\n",
       "                        1.4772e-03, -2.1250e-03,  1.2379e-03,  9.4745e-05, -2.6874e-03,\n",
       "                       -8.9463e-04,  1.0693e-03, -1.5491e-03,  1.8957e-04,  2.8946e-03,\n",
       "                        1.1849e-03,  2.1482e-04,  2.0873e-04, -7.8655e-04,  1.2980e-03,\n",
       "                       -2.6352e-03,  1.9492e-03, -1.8718e-03, -1.6439e-03, -3.3162e-04,\n",
       "                        3.8143e-04, -7.6442e-04,  1.1236e-03, -1.1091e-04, -2.0625e-03,\n",
       "                        1.1619e-03, -4.9918e-04,  1.9502e-06, -1.0099e-03, -8.2773e-04,\n",
       "                        1.4017e-03,  1.5289e-03, -1.8522e-03, -2.8503e-04,  3.0935e-04,\n",
       "                        1.6916e-04, -2.2714e-04, -1.6422e-03,  4.0964e-04, -2.4318e-04,\n",
       "                       -6.1455e-05,  9.1467e-05,  9.6760e-04,  4.0069e-04,  1.5191e-03,\n",
       "                        1.3727e-04,  8.4586e-04, -1.2893e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1601, -0.0269,  0.2146, -0.1223, -0.1164,  0.0143, -0.1938, -0.2474,\n",
       "                       -0.1682, -0.1138, -0.1828, -0.1827,  0.0486, -0.1432, -0.0289, -0.1198,\n",
       "                        0.0172, -0.1315, -0.0307, -0.0695, -0.1784, -0.0808, -0.1548,  0.0150,\n",
       "                       -0.0257, -0.1688, -0.0726, -0.0996, -0.0490,  0.0397, -0.1129, -0.1187,\n",
       "                       -0.1021, -0.1569, -0.1652, -0.2654, -0.1424, -0.0779, -0.1773, -0.1431,\n",
       "                       -0.0401,  0.1688, -0.1125, -0.0631, -0.0056, -0.1462, -0.1993, -0.2671,\n",
       "                        0.1400, -0.1451,  0.0623, -0.2133,  0.2319,  0.0191, -0.0483, -0.0743,\n",
       "                       -0.2520, -0.0044, -0.1727,  0.1165, -0.1224,  0.2687, -0.0037, -0.2654,\n",
       "                       -0.1504, -0.2399, -0.1681, -0.1425, -0.0929, -0.0455,  0.0674, -0.0151,\n",
       "                       -0.1439, -0.2008, -0.0738, -0.0622, -0.1589, -0.0268, -0.0954, -0.1174,\n",
       "                        0.0008, -0.1366, -0.2038, -0.2098, -0.0300,  0.1649, -0.0133, -0.1394,\n",
       "                       -0.1591,  0.0642, -0.0768, -0.1414,  0.0458, -0.0098, -0.0927,  0.0210,\n",
       "                        0.1638,  0.0666, -0.1137, -0.1096, -0.2115, -0.0579,  0.0081,  0.0286,\n",
       "                        0.0164, -0.1907, -0.0867, -0.1441, -0.0205, -0.0744, -0.0806, -0.1129,\n",
       "                       -0.0395, -0.1456, -0.0760, -0.0752, -0.0798,  0.1828, -0.0423,  0.0638,\n",
       "                       -0.1928, -0.2187, -0.1924, -0.1746,  0.1367, -0.0291, -0.2259, -0.1710],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.0777, 1.0709, 1.0801, 0.8988, 0.9437, 1.0149, 0.9128, 0.8826, 0.9809,\n",
       "                       0.9149, 0.9107, 0.9115, 1.0526, 0.8928, 1.0546, 1.0420, 1.0268, 0.9431,\n",
       "                       0.9681, 0.9646, 0.9394, 0.8664, 0.9175, 1.0001, 0.9011, 0.8854, 0.9461,\n",
       "                       0.9155, 0.9748, 0.9375, 1.0159, 0.9061, 1.0190, 1.2193, 0.9277, 0.9384,\n",
       "                       0.9600, 0.9206, 0.9119, 0.9612, 0.9499, 1.0274, 0.9329, 0.9598, 1.0849,\n",
       "                       1.0767, 0.9834, 0.9287, 1.1041, 0.9253, 1.0169, 0.9134, 1.0635, 1.0854,\n",
       "                       1.0065, 0.9070, 1.1188, 1.1279, 0.9739, 1.0818, 0.9082, 1.0035, 0.9521,\n",
       "                       0.9313, 0.9319, 0.8501, 0.8868, 0.9298, 1.0671, 0.9991, 0.9329, 0.9704,\n",
       "                       0.8722, 0.8916, 0.9964, 0.8846, 0.9151, 1.0916, 1.0985, 1.0259, 1.0244,\n",
       "                       0.9868, 0.8907, 0.9007, 1.1403, 1.1089, 1.0677, 1.0212, 0.9175, 0.9868,\n",
       "                       1.0047, 0.9299, 1.0141, 1.0329, 0.9991, 1.0203, 1.1726, 0.9629, 0.9522,\n",
       "                       0.9195, 0.8685, 1.0409, 1.1071, 0.9493, 0.9616, 0.9556, 0.9301, 0.8482,\n",
       "                       1.0131, 0.9587, 1.0176, 1.0275, 0.9890, 1.1160, 0.9377, 1.0863, 0.9138,\n",
       "                       1.0243, 0.9302, 0.9634, 0.8600, 0.8900, 1.2006, 1.0526, 1.0483, 0.9347,\n",
       "                       0.9339, 0.8952], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-3.1068e-02,  1.4934e-02, -2.4708e-02],\n",
       "                         [-4.9840e-02, -2.8395e-02, -7.1771e-02],\n",
       "                         [-2.5563e-02,  2.5884e-02, -1.4416e-02]],\n",
       "               \n",
       "                        [[ 7.1535e-02,  3.6242e-02, -8.3086e-02],\n",
       "                         [-5.5571e-02, -6.0254e-03, -3.9204e-02],\n",
       "                         [-2.6580e-02,  7.6422e-02,  2.6242e-02]],\n",
       "               \n",
       "                        [[ 7.9697e-02,  1.2020e-02, -8.9087e-04],\n",
       "                         [-2.4565e-02,  6.1961e-02,  8.5690e-02],\n",
       "                         [ 1.0954e-02,  5.8338e-02,  8.1456e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3707e-04, -3.0362e-02, -3.2934e-02],\n",
       "                         [-5.1523e-02, -6.6658e-02, -3.6795e-02],\n",
       "                         [ 7.4546e-03,  1.6017e-03, -1.5633e-02]],\n",
       "               \n",
       "                        [[ 1.1219e-02,  3.6779e-02, -7.3502e-03],\n",
       "                         [-9.8202e-03,  2.9635e-02, -1.4038e-02],\n",
       "                         [ 3.2854e-02, -5.6267e-02, -1.8176e-02]],\n",
       "               \n",
       "                        [[-1.0652e-02,  4.8453e-02, -3.1925e-02],\n",
       "                         [-2.8970e-02,  1.5542e-02, -4.6588e-03],\n",
       "                         [-4.3812e-02, -2.1815e-02,  4.8127e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4941e-02, -3.0805e-02,  3.8058e-02],\n",
       "                         [ 3.1757e-02,  3.3032e-02, -4.8846e-02],\n",
       "                         [ 3.0079e-02, -1.2053e-03, -3.8244e-02]],\n",
       "               \n",
       "                        [[ 3.1452e-02,  6.4167e-02, -2.9219e-03],\n",
       "                         [-4.8711e-02,  4.5315e-04,  2.8953e-02],\n",
       "                         [-1.8084e-02, -3.7616e-02,  6.2568e-03]],\n",
       "               \n",
       "                        [[ 6.1893e-02,  2.1873e-02, -4.9432e-02],\n",
       "                         [-1.2321e-02, -1.1952e-03, -2.0872e-02],\n",
       "                         [-2.9053e-02, -1.8110e-02, -4.7141e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1403e-02,  5.6295e-03,  4.0007e-02],\n",
       "                         [-4.6823e-02, -2.6105e-02, -2.8252e-02],\n",
       "                         [-1.2773e-02,  1.2540e-02, -1.2665e-02]],\n",
       "               \n",
       "                        [[-2.8528e-02, -7.3125e-02, -6.5685e-02],\n",
       "                         [ 1.5529e-02,  4.3157e-03,  4.4955e-03],\n",
       "                         [ 1.7821e-02, -4.4237e-02,  1.1965e-02]],\n",
       "               \n",
       "                        [[-1.4344e-02, -6.4940e-02,  1.5923e-02],\n",
       "                         [ 2.7153e-02, -2.8147e-02, -7.4200e-04],\n",
       "                         [-1.2954e-02, -3.2091e-02,  2.8604e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.7807e-02, -2.8579e-02, -4.7183e-02],\n",
       "                         [-2.5427e-03, -3.0682e-03, -4.7967e-02],\n",
       "                         [ 3.1416e-02, -1.7803e-02, -1.8947e-03]],\n",
       "               \n",
       "                        [[-3.5335e-03,  2.9050e-02, -5.2048e-02],\n",
       "                         [ 2.8188e-02,  1.9930e-02, -2.0456e-02],\n",
       "                         [ 3.0739e-02,  4.1309e-02, -4.7110e-02]],\n",
       "               \n",
       "                        [[-3.3564e-02, -4.0410e-02, -8.6391e-02],\n",
       "                         [-2.9952e-02, -1.4456e-02,  2.4784e-02],\n",
       "                         [-4.7339e-02, -4.5708e-02,  7.1623e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.2363e-02,  1.3501e-02, -1.5803e-04],\n",
       "                         [-1.9702e-02, -4.1223e-02, -1.4040e-02],\n",
       "                         [-2.1804e-02, -3.4117e-02, -4.1499e-02]],\n",
       "               \n",
       "                        [[ 5.0175e-02,  4.8530e-02,  6.1841e-02],\n",
       "                         [-5.2386e-02, -3.6055e-02, -2.1579e-02],\n",
       "                         [-6.1513e-02, -3.6027e-03, -4.1555e-02]],\n",
       "               \n",
       "                        [[-3.5838e-02, -6.2272e-02, -8.0042e-03],\n",
       "                         [-3.8722e-02,  7.4427e-03,  1.1058e-02],\n",
       "                         [-1.4307e-02, -3.2467e-02, -3.4531e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3628e-02,  2.7285e-03,  1.5521e-02],\n",
       "                         [ 6.4739e-02, -1.6291e-02,  2.1665e-02],\n",
       "                         [ 6.1771e-02, -5.1059e-02, -4.8901e-02]],\n",
       "               \n",
       "                        [[-2.6963e-02,  1.9296e-02,  5.3938e-02],\n",
       "                         [-5.3194e-02, -1.1404e-02,  1.8829e-02],\n",
       "                         [-7.9276e-02, -2.1864e-02, -3.8196e-02]],\n",
       "               \n",
       "                        [[ 8.4808e-02,  6.7533e-02,  6.2033e-02],\n",
       "                         [ 6.5936e-02, -2.6663e-02,  7.5278e-03],\n",
       "                         [ 2.4147e-02, -4.6637e-02, -4.0440e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.3273e-03,  1.0368e-02,  2.9113e-02],\n",
       "                         [ 4.7940e-02, -2.6338e-02,  3.7946e-02],\n",
       "                         [ 3.2950e-02,  4.6247e-02,  2.5710e-02]],\n",
       "               \n",
       "                        [[-9.7459e-03,  2.0474e-02,  4.2825e-02],\n",
       "                         [ 1.6884e-02,  4.6780e-03, -1.5560e-02],\n",
       "                         [-6.3945e-02,  6.8826e-03,  3.1620e-02]],\n",
       "               \n",
       "                        [[ 1.1923e-02, -2.2587e-02, -1.0285e-03],\n",
       "                         [-6.0087e-03,  9.3974e-05,  8.8612e-03],\n",
       "                         [ 8.7517e-03,  4.7585e-03, -3.5720e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.3675e-02, -4.8650e-02,  1.3389e-02],\n",
       "                         [-4.8180e-02, -1.0838e-01, -1.8075e-02],\n",
       "                         [-6.6564e-02, -4.1220e-02, -3.8812e-02]],\n",
       "               \n",
       "                        [[ 4.3953e-03,  5.0149e-02,  6.3318e-02],\n",
       "                         [ 4.8209e-02,  2.4266e-02,  2.1014e-02],\n",
       "                         [ 1.6416e-02, -3.3700e-02, -4.2817e-02]],\n",
       "               \n",
       "                        [[-2.8837e-02,  2.7409e-03,  2.8331e-02],\n",
       "                         [-5.9724e-03, -5.8118e-02, -7.0318e-02],\n",
       "                         [-6.0833e-02, -5.2656e-02, -5.2772e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.0133e-02,  3.9366e-03,  4.7938e-02],\n",
       "                         [-1.2157e-02,  1.9478e-02, -2.6585e-02],\n",
       "                         [-2.0719e-02, -2.5532e-02, -4.1482e-02]],\n",
       "               \n",
       "                        [[ 2.7662e-02,  5.0188e-02,  5.1921e-02],\n",
       "                         [-7.6517e-03, -5.1657e-02, -2.7490e-02],\n",
       "                         [-2.9292e-02, -3.8551e-02, -2.9929e-03]],\n",
       "               \n",
       "                        [[-2.6770e-02, -3.3468e-02, -3.3695e-02],\n",
       "                         [-5.5176e-02, -2.4818e-02, -5.8880e-02],\n",
       "                         [-1.5056e-02, -6.2207e-02, -4.8363e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.1598e-03, -4.3907e-02,  1.5628e-02],\n",
       "                         [ 1.6393e-02,  3.9933e-02, -4.7980e-03],\n",
       "                         [-9.2348e-03,  8.1859e-02,  3.9698e-02]],\n",
       "               \n",
       "                        [[-3.7246e-02,  1.7502e-02,  3.7163e-02],\n",
       "                         [ 5.7703e-02, -2.4650e-02, -5.0248e-03],\n",
       "                         [-2.8848e-02, -1.8945e-02,  4.7855e-02]],\n",
       "               \n",
       "                        [[ 1.3576e-03, -4.7517e-02,  3.5737e-02],\n",
       "                         [ 1.5863e-02,  3.9620e-02,  9.4381e-03],\n",
       "                         [ 5.0647e-04, -2.3891e-02,  3.4428e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.9621e-02, -5.0091e-03, -5.7347e-02],\n",
       "                         [ 6.7124e-02,  8.4320e-02, -2.3866e-02],\n",
       "                         [ 1.6802e-02,  4.7377e-02,  3.4077e-03]],\n",
       "               \n",
       "                        [[-5.5406e-02, -5.9962e-02, -3.6674e-02],\n",
       "                         [ 3.1510e-02,  4.7255e-02,  1.2530e-03],\n",
       "                         [-8.1551e-03,  1.8949e-02, -5.1895e-02]],\n",
       "               \n",
       "                        [[-1.0334e-02,  2.3689e-02, -2.3868e-02],\n",
       "                         [-4.3318e-03,  2.3800e-02,  3.7886e-02],\n",
       "                         [ 2.9135e-02, -6.1549e-03, -1.9589e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 5.9933e-08, -6.1479e-05,  1.1136e-06,  1.1191e-06,  2.5666e-07,\n",
       "                        1.5369e-03, -1.1775e-06, -6.2422e-07, -1.1607e-06, -8.1503e-04,\n",
       "                       -1.8291e-06, -2.3342e-06, -2.2181e-04,  6.1000e-07, -2.3925e-06,\n",
       "                       -3.0629e-07, -3.1087e-06,  8.1563e-07,  9.3452e-08, -5.7189e-04,\n",
       "                       -3.2095e-07, -9.1869e-09,  4.8346e-06, -1.7036e-05,  1.3274e-02,\n",
       "                       -1.3903e-05,  1.6652e-05, -2.7412e-05, -3.4676e-03,  5.0787e-06,\n",
       "                        2.6765e-06,  4.6576e-07,  4.0706e-06,  1.7675e-05, -8.4356e-07,\n",
       "                        2.6364e-06,  4.4227e-09, -1.2458e-05,  2.1915e-05,  4.9682e-07,\n",
       "                        7.0539e-07, -1.6904e-06,  2.0231e-05, -5.5559e-06,  7.0082e-07,\n",
       "                       -3.0102e-06, -1.5954e-04, -3.8790e-06, -1.2362e-06, -3.6231e-06,\n",
       "                        7.5977e-07, -4.2942e-07,  1.9629e-06, -1.0215e-02, -4.5511e-07,\n",
       "                        9.4783e-03, -3.0247e-03, -1.9373e-05,  5.5702e-05, -1.3319e-02,\n",
       "                       -9.0035e-07, -1.1355e-06,  3.0399e-06,  2.3563e-03,  3.7166e-07,\n",
       "                        1.0655e-02,  7.9245e-06,  3.9385e-07, -2.3954e-06,  6.9563e-07,\n",
       "                       -9.1358e-07,  1.7789e-06, -1.0133e-06, -9.4920e-05, -3.5892e-05,\n",
       "                        2.6434e-06, -3.6071e-06,  2.4756e-06,  2.6890e-06,  7.2722e-05,\n",
       "                        2.2625e-07, -9.3507e-07,  2.5611e-06,  1.9175e-06, -6.8006e-06,\n",
       "                        3.2866e-07,  3.1739e-06, -7.8642e-03, -4.1726e-06,  4.7427e-07,\n",
       "                        3.0530e-06,  1.0925e-02,  2.6354e-07,  1.7425e-06, -1.6997e-06,\n",
       "                       -2.4809e-06, -7.8697e-07,  1.2117e-02, -1.6563e-06,  1.2712e-06,\n",
       "                        4.4987e-07,  5.1062e-06,  1.0757e-02, -1.8630e-06,  7.0586e-08,\n",
       "                        5.5715e-07, -1.2244e-04,  1.5775e-05,  1.1858e-02,  9.6294e-03,\n",
       "                       -1.0860e-06,  2.7695e-05, -3.0826e-05, -1.0798e-02,  3.5214e-08,\n",
       "                       -4.0514e-07,  1.3645e-02,  2.0043e-05,  1.9055e-04, -8.7982e-03,\n",
       "                       -1.0212e-06, -1.3086e-05, -2.2918e-08, -2.2165e-05, -1.8928e-06,\n",
       "                       -1.2180e-05, -1.3773e-02, -7.5184e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.0872, -0.1572, -0.1664, -0.2343, -0.1549, -0.1571, -0.1078, -0.1666,\n",
       "                       -0.1076, -0.1716, -0.3466, -0.2546, -0.1173, -0.2540, -0.1237, -0.1846,\n",
       "                       -0.2605, -0.0961, -0.0841, -0.2577, -0.1466, -0.1738, -0.2186, -0.2139,\n",
       "                       -0.2324, -0.1326, -0.1120, -0.1670, -0.1783, -0.2671, -0.0948, -0.3858,\n",
       "                       -0.2320, -0.0944, -0.2653, -0.2115, -0.2098, -0.0639, -0.3093, -0.2235,\n",
       "                       -0.1407, -0.1924, -0.2923, -0.1656, -0.2135, -0.2605, -0.1603, -0.1156,\n",
       "                       -0.1902, -0.2313, -0.0166, -0.1515, -0.3205, -0.2022, -0.0888, -0.1692,\n",
       "                       -0.2692, -0.1670, -0.2828, -0.2521, -0.0130, -0.2669, -0.1723, -0.2151,\n",
       "                       -0.1083, -0.0798, -0.1164, -0.2391, -0.1477, -0.1890, -0.2052, -0.0476,\n",
       "                       -0.1590, -0.1589, -0.2383, -0.0808, -0.2007, -0.1882, -0.1486, -0.1086,\n",
       "                       -0.1875, -0.2214, -0.0991,  0.0091, -0.2162, -0.2209, -0.0630, -0.2359,\n",
       "                       -0.1235, -0.1990, -0.2295, -0.2036, -0.2071, -0.2148, -0.1414, -0.0935,\n",
       "                       -0.1874, -0.1651, -0.1903, -0.1393, -0.1805, -0.1963, -0.1423, -0.2635,\n",
       "                       -0.2242, -0.2068, -0.1134, -0.2680, -0.1862, -0.1696, -0.2529, -0.2078,\n",
       "                       -0.1584, -0.2204, -0.2384, -0.1047, -0.1335, -0.2205, -0.1528, -0.2487,\n",
       "                       -0.1355, -0.1146, -0.1673, -0.0136, -0.1558, -0.2546, -0.1811, -0.1617],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.0451, 0.9745, 1.0224, 0.9569, 0.9861, 1.0045, 1.0278, 1.0634, 1.0389,\n",
       "                       1.0216, 0.9842, 1.1099, 1.0157, 0.9824, 1.0965, 1.0307, 0.8674, 0.9835,\n",
       "                       1.0076, 0.9665, 0.9861, 0.9705, 1.0266, 1.0917, 0.9736, 1.0308, 0.9726,\n",
       "                       0.9838, 0.9616, 1.0102, 0.9881, 1.0250, 0.9429, 0.9924, 0.9120, 1.0300,\n",
       "                       0.9982, 1.0233, 0.8428, 0.9264, 0.9836, 0.9378, 1.0010, 0.9307, 1.0222,\n",
       "                       0.9822, 1.0209, 0.9528, 1.0514, 0.9630, 1.0344, 1.0062, 0.9767, 0.9748,\n",
       "                       0.9827, 0.9348, 0.9054, 1.0206, 1.0095, 1.0173, 1.0312, 1.0670, 0.9866,\n",
       "                       0.9704, 0.9777, 0.9320, 0.9356, 0.9721, 0.9230, 1.0510, 0.9052, 1.0517,\n",
       "                       0.9739, 0.9824, 1.0422, 1.0663, 0.9386, 0.9604, 1.0494, 0.9874, 1.0360,\n",
       "                       0.9525, 1.0250, 0.9620, 0.9574, 1.0049, 1.0482, 0.9686, 0.9571, 0.9879,\n",
       "                       0.9952, 0.9969, 0.9531, 0.9737, 0.9241, 1.0071, 0.9846, 0.9989, 1.0562,\n",
       "                       0.9546, 0.9243, 1.0043, 0.9716, 1.0233, 1.0340, 1.0055, 0.9923, 0.9698,\n",
       "                       1.0110, 0.9383, 0.9961, 1.0366, 0.9825, 1.0127, 0.8616, 0.9989, 1.0212,\n",
       "                       0.9751, 0.9882, 1.0225, 0.9810, 1.0087, 0.9795, 1.0189, 0.9698, 1.0015,\n",
       "                       1.0015, 1.0593], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-3.5635e-02,  6.6883e-02,  1.6516e-02],\n",
       "                         [-2.0717e-02,  4.5892e-03, -7.0361e-03],\n",
       "                         [-4.0502e-03, -5.3581e-02, -6.3822e-03]],\n",
       "               \n",
       "                        [[ 2.4649e-03, -2.7850e-02,  9.0671e-03],\n",
       "                         [-4.3892e-02,  2.0201e-02, -2.6540e-02],\n",
       "                         [-6.6467e-04,  1.1619e-02, -5.7028e-02]],\n",
       "               \n",
       "                        [[ 4.4089e-03, -2.2262e-02,  1.7012e-02],\n",
       "                         [ 6.2686e-02,  2.4853e-02, -3.6876e-02],\n",
       "                         [-1.6284e-02,  2.1929e-02, -1.3400e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5534e-02, -3.8601e-02, -6.5782e-02],\n",
       "                         [ 2.6658e-02,  4.9235e-03,  4.0978e-02],\n",
       "                         [ 1.1825e-03, -2.5821e-02, -1.8333e-02]],\n",
       "               \n",
       "                        [[-2.6654e-02, -5.5350e-02,  6.5079e-03],\n",
       "                         [-6.1993e-04, -2.8721e-02, -2.6483e-02],\n",
       "                         [ 2.8866e-02,  1.8139e-02,  4.3261e-03]],\n",
       "               \n",
       "                        [[-3.5204e-03,  5.2470e-02,  3.2148e-02],\n",
       "                         [ 1.7759e-02,  1.0482e-02, -2.4699e-02],\n",
       "                         [ 1.1681e-02,  3.0295e-02,  2.6309e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.6161e-02,  1.5252e-02, -2.2819e-02],\n",
       "                         [ 1.0151e-02, -4.9394e-03, -3.1928e-02],\n",
       "                         [-7.3092e-03, -1.6657e-02,  1.1107e-02]],\n",
       "               \n",
       "                        [[-3.2528e-02,  3.4879e-02,  5.5768e-02],\n",
       "                         [ 2.8912e-02,  8.1838e-02,  7.0347e-02],\n",
       "                         [-5.9509e-02, -3.2777e-02,  1.6557e-03]],\n",
       "               \n",
       "                        [[ 1.0216e-02,  4.4529e-02, -8.3797e-02],\n",
       "                         [ 2.7144e-02, -1.2044e-02, -7.0979e-02],\n",
       "                         [ 2.4772e-02, -3.1583e-02, -4.7915e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.8065e-03,  5.4751e-02, -9.6700e-03],\n",
       "                         [-3.1591e-03, -3.3694e-02, -7.3251e-02],\n",
       "                         [ 7.2923e-05,  4.5793e-03, -5.0973e-02]],\n",
       "               \n",
       "                        [[ 5.8034e-03, -1.8566e-02,  9.1351e-03],\n",
       "                         [-5.1339e-02, -2.9027e-02, -1.2111e-02],\n",
       "                         [-2.9039e-02,  1.3079e-02,  3.4591e-02]],\n",
       "               \n",
       "                        [[ 6.6866e-02, -2.1931e-02, -4.6208e-02],\n",
       "                         [ 1.5051e-02,  3.2506e-02, -1.3522e-02],\n",
       "                         [-1.3544e-02,  3.2223e-02, -5.6732e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.1565e-03, -1.2166e-02, -6.4260e-02],\n",
       "                         [-4.6180e-02, -8.3588e-02, -3.3062e-02],\n",
       "                         [-3.1706e-02, -5.0750e-02, -5.2977e-02]],\n",
       "               \n",
       "                        [[ 1.6955e-03,  1.4892e-03, -2.2856e-02],\n",
       "                         [ 3.9290e-03,  6.5597e-02, -3.6062e-02],\n",
       "                         [-3.7413e-02,  3.7076e-02, -3.6557e-02]],\n",
       "               \n",
       "                        [[ 9.4688e-02, -1.3011e-02,  7.1782e-02],\n",
       "                         [ 1.4776e-02,  7.5849e-02, -1.0182e-02],\n",
       "                         [-5.7237e-03,  2.7586e-02,  4.5924e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.3358e-03,  4.7901e-02, -1.3634e-02],\n",
       "                         [ 2.1187e-02, -3.5819e-02, -3.5869e-02],\n",
       "                         [ 1.4592e-02,  1.6706e-02,  4.2851e-02]],\n",
       "               \n",
       "                        [[-2.2170e-02, -5.4084e-02, -2.6914e-04],\n",
       "                         [ 3.5542e-02, -5.5808e-02, -6.2875e-02],\n",
       "                         [-2.2901e-03, -3.7785e-02,  2.0417e-02]],\n",
       "               \n",
       "                        [[ 1.4934e-02,  2.7684e-02,  8.1969e-02],\n",
       "                         [ 6.0429e-02,  6.1587e-02,  3.9178e-02],\n",
       "                         [-1.8586e-03, -2.7665e-03, -1.9067e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1825e-02,  4.8099e-03, -5.5348e-02],\n",
       "                         [ 5.5376e-02, -2.2714e-02, -1.8907e-02],\n",
       "                         [ 9.4701e-03,  1.6956e-02,  2.8504e-02]],\n",
       "               \n",
       "                        [[-8.7255e-03, -1.3725e-02,  3.0419e-02],\n",
       "                         [ 2.3330e-02,  3.1619e-02, -3.6503e-02],\n",
       "                         [ 1.7630e-02,  1.8091e-02, -5.2045e-03]],\n",
       "               \n",
       "                        [[ 2.6194e-02,  2.7558e-02, -5.5006e-02],\n",
       "                         [-1.1049e-02, -1.7657e-02, -2.4263e-03],\n",
       "                         [-7.5835e-03, -1.4259e-02, -4.7830e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.5579e-02,  5.8151e-02, -2.5747e-02],\n",
       "                         [ 9.2701e-02,  6.5077e-02,  3.7771e-02],\n",
       "                         [ 4.0023e-02,  1.2615e-02, -1.5215e-02]],\n",
       "               \n",
       "                        [[ 3.5414e-02, -3.3109e-02,  3.7701e-02],\n",
       "                         [-2.7612e-02, -2.7967e-02, -5.2714e-02],\n",
       "                         [-3.7227e-02,  3.3004e-02, -8.0880e-02]],\n",
       "               \n",
       "                        [[-1.6259e-02,  4.5828e-03, -7.5952e-03],\n",
       "                         [-4.2413e-02, -9.9031e-03, -3.3439e-02],\n",
       "                         [-4.7989e-02,  6.6052e-02, -1.1792e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.2100e-02,  6.1930e-02, -8.5012e-03],\n",
       "                         [-4.9399e-02, -2.1669e-02, -2.1487e-02],\n",
       "                         [-9.4657e-03,  3.6917e-02, -3.9345e-02]],\n",
       "               \n",
       "                        [[ 1.1505e-02, -3.4558e-02,  3.1325e-02],\n",
       "                         [-1.0512e-04, -2.2429e-02,  3.3865e-02],\n",
       "                         [-1.2944e-02,  3.7145e-02,  1.9752e-02]],\n",
       "               \n",
       "                        [[ 2.9329e-03,  1.7710e-02,  6.5504e-03],\n",
       "                         [ 2.7545e-02,  2.8484e-02,  2.6443e-02],\n",
       "                         [-4.8555e-03, -4.9553e-02,  1.7128e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.9563e-03, -1.2934e-02,  1.1113e-02],\n",
       "                         [-1.8983e-02,  4.7031e-02, -4.0295e-02],\n",
       "                         [-1.3619e-02, -1.9917e-02, -3.2294e-02]],\n",
       "               \n",
       "                        [[-3.2172e-02, -2.8210e-02, -4.2929e-02],\n",
       "                         [ 4.4360e-03, -5.5791e-02,  1.2082e-02],\n",
       "                         [ 4.9856e-02, -1.0001e-02, -4.5965e-02]],\n",
       "               \n",
       "                        [[ 1.4553e-02,  6.3308e-02, -2.4518e-02],\n",
       "                         [-1.3177e-03,  5.6378e-02, -6.8796e-03],\n",
       "                         [-7.8446e-03,  2.0849e-02,  1.5414e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.5729e-02, -3.0278e-02,  3.3063e-04],\n",
       "                         [-3.3246e-02, -2.2090e-02,  2.8049e-02],\n",
       "                         [-9.2174e-02, -6.1077e-02, -5.3846e-03]],\n",
       "               \n",
       "                        [[ 1.3817e-02,  1.4932e-02, -3.2316e-02],\n",
       "                         [ 2.9696e-02,  1.5589e-02, -4.2233e-02],\n",
       "                         [ 3.3876e-02,  2.2564e-02,  1.0178e-02]],\n",
       "               \n",
       "                        [[-1.2799e-02, -1.3293e-02, -3.9625e-02],\n",
       "                         [-4.9859e-02,  3.1562e-02, -1.0173e-02],\n",
       "                         [-4.0080e-02, -3.2473e-02, -3.6179e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 5.4078e-02,  7.9191e-02,  1.7750e-03],\n",
       "                         [ 4.3058e-02,  4.6749e-02, -3.7460e-02],\n",
       "                         [ 1.1480e-03, -1.3536e-02, -6.2119e-02]],\n",
       "               \n",
       "                        [[ 3.5932e-02,  4.2279e-02,  7.7663e-03],\n",
       "                         [-3.6863e-02, -1.1133e-02, -4.4179e-02],\n",
       "                         [-5.6386e-03, -1.8559e-02, -5.7912e-02]],\n",
       "               \n",
       "                        [[-7.8392e-02,  3.8048e-02,  3.1213e-02],\n",
       "                         [-5.2759e-02,  8.1980e-03, -3.1568e-02],\n",
       "                         [-7.3119e-03, -2.0015e-02, -3.6982e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-2.1351e-03,  3.4871e-03,  1.7402e-03,  3.3579e-03,  1.0868e-03,\n",
       "                        7.4822e-04,  3.1768e-03,  6.2910e-04,  6.6134e-04, -1.6108e-03,\n",
       "                       -2.7385e-03,  6.7116e-04, -9.6336e-04,  3.2656e-03,  1.8089e-03,\n",
       "                        3.7092e-04,  2.3694e-03, -1.4622e-03,  1.2253e-03,  1.7384e-03,\n",
       "                        8.2374e-04,  1.4852e-03, -1.9792e-04, -6.7408e-04,  2.5066e-03,\n",
       "                       -1.6606e-03, -7.2716e-04,  2.5755e-03, -1.5390e-03,  1.5680e-03,\n",
       "                       -1.4368e-03,  6.3476e-05, -1.6477e-03,  8.5359e-04, -1.1303e-03,\n",
       "                        1.8436e-03,  2.7225e-03, -9.7316e-04,  3.3027e-03, -4.5836e-03,\n",
       "                        1.0710e-03,  2.4210e-03,  9.4007e-04,  2.7616e-04, -2.6665e-03,\n",
       "                        1.0428e-03, -4.5181e-03, -1.0973e-03,  2.9153e-03,  1.1452e-03,\n",
       "                       -2.6050e-04,  1.7071e-04,  2.0001e-03,  1.2810e-04, -1.9158e-04,\n",
       "                       -4.1856e-03, -3.1744e-03, -7.9482e-04, -3.0404e-04,  3.5577e-03,\n",
       "                        8.4278e-04,  1.1236e-03, -5.2809e-03,  1.9397e-03, -1.0979e-03,\n",
       "                        1.9149e-03, -3.6256e-03,  1.5284e-03, -4.2810e-04, -4.3189e-03,\n",
       "                        7.6343e-04, -1.4412e-03, -1.3658e-03,  2.3325e-03,  2.2334e-03,\n",
       "                        1.4130e-03,  3.4702e-03,  8.2384e-04, -5.8385e-04, -2.3952e-03,\n",
       "                        3.0020e-03, -2.1871e-03, -2.9691e-03, -7.2833e-04, -3.2114e-06,\n",
       "                       -4.5352e-04,  1.9773e-03, -1.5028e-03,  4.9686e-04, -5.7484e-04,\n",
       "                        4.2452e-03,  1.2303e-03, -1.5338e-03, -7.8657e-04,  2.1939e-03,\n",
       "                        3.3835e-03,  3.8203e-03,  3.1000e-03, -3.5266e-03,  2.6436e-03,\n",
       "                        9.1690e-04, -3.8383e-03,  5.2958e-04, -6.9434e-05,  1.7816e-03,\n",
       "                        4.8468e-03,  6.3447e-04,  2.1172e-03,  1.7666e-03, -2.1690e-03,\n",
       "                       -2.0607e-03, -3.0691e-03,  4.9682e-04, -5.3011e-04, -3.4542e-03,\n",
       "                       -1.0074e-03,  2.9607e-04, -2.5081e-03, -1.1018e-03, -3.7656e-04,\n",
       "                       -3.8288e-04,  8.6251e-04, -4.0439e-04,  7.3346e-04, -1.4695e-03,\n",
       "                       -5.3210e-05,  2.3575e-03, -2.5863e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.2745, -0.3432, -0.3225, -0.3161, -0.3458, -0.2965, -0.2656, -0.4619,\n",
       "                       -0.3528, -0.3486, -0.3127, -0.4352, -0.2927, -0.3775, -0.3179, -0.2020,\n",
       "                       -0.2585, -0.2389, -0.4363, -0.3397, -0.2641, -0.3822, -0.2232, -0.3648,\n",
       "                       -0.2792, -0.3051, -0.2956, -0.2948, -0.2251, -0.2499, -0.3535, -0.2657,\n",
       "                       -0.3528, -0.2497, -0.2430, -0.2566, -0.4218, -0.2970, -0.2469, -0.2932,\n",
       "                       -0.2682, -0.2900, -0.2373, -0.2746, -0.2579, -0.3706, -0.2705, -0.2992,\n",
       "                       -0.2342, -0.4011, -0.3201, -0.3366, -0.2704, -0.3693, -0.3214, -0.2929,\n",
       "                       -0.3103, -0.2991, -0.3957, -0.3168, -0.2977, -0.3472, -0.2490, -0.3007,\n",
       "                       -0.2593, -0.2996, -0.3261, -0.2612, -0.2965, -0.3719, -0.3670, -0.3033,\n",
       "                       -0.3026, -0.3622, -0.3271, -0.3308, -0.3027, -0.3465, -0.2958, -0.3535,\n",
       "                       -0.2374, -0.3401, -0.2038, -0.3748, -0.2901, -0.2589, -0.3164, -0.2999,\n",
       "                       -0.2608, -0.2464, -0.2377, -0.2692, -0.3142, -0.3819, -0.3478, -0.3193,\n",
       "                       -0.3010, -0.2188, -0.2880, -0.3890, -0.2799, -0.2614, -0.2786, -0.3609,\n",
       "                       -0.3335, -0.3494, -0.2471, -0.3684, -0.2899, -0.3292, -0.3916, -0.2884,\n",
       "                       -0.2767, -0.3124, -0.2516, -0.3787, -0.2954, -0.3044, -0.3517, -0.3989,\n",
       "                       -0.2850, -0.3227, -0.3362, -0.2426, -0.3302, -0.2835, -0.2408, -0.2675],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.8677, 0.9911, 0.9240, 0.8862, 0.9875, 0.9502, 0.9117, 1.0771, 0.9400,\n",
       "                       0.9616, 0.9689, 1.0767, 0.8404, 1.1268, 0.9214, 0.9644, 1.1083, 0.8781,\n",
       "                       1.0999, 0.9300, 0.8585, 0.9549, 0.8933, 0.9367, 0.9463, 0.9033, 0.9782,\n",
       "                       0.9583, 0.9087, 0.9267, 1.1003, 0.9585, 0.9880, 0.9270, 1.0583, 1.0271,\n",
       "                       1.0955, 0.9693, 0.9092, 0.9284, 0.8522, 0.9795, 0.9794, 0.8672, 0.9084,\n",
       "                       0.9751, 0.8588, 0.8368, 0.9086, 0.9760, 0.9658, 1.0371, 0.9347, 0.9943,\n",
       "                       0.9089, 0.9417, 0.8640, 0.9460, 0.9710, 0.8725, 0.9926, 1.0481, 1.1232,\n",
       "                       0.9393, 0.9935, 0.9556, 0.8583, 0.8728, 0.8861, 1.0107, 1.2039, 0.9133,\n",
       "                       0.8829, 1.0437, 0.9347, 0.9958, 1.0021, 0.8523, 0.9745, 0.9859, 0.9213,\n",
       "                       0.9626, 1.0658, 1.0581, 0.8953, 0.9729, 0.9728, 0.8852, 1.1389, 0.9146,\n",
       "                       0.9648, 0.8962, 1.0027, 1.0481, 0.9987, 0.9533, 0.9388, 0.9414, 0.9178,\n",
       "                       1.0637, 0.9441, 0.9439, 0.8276, 0.9136, 0.8780, 0.8991, 0.8927, 0.9721,\n",
       "                       0.9828, 0.9578, 0.8569, 1.0319, 0.9744, 1.0446, 0.9831, 0.9474, 0.9019,\n",
       "                       1.0095, 0.9606, 1.0142, 0.9484, 1.1066, 1.0178, 0.9520, 0.8968, 0.9898,\n",
       "                       0.9332, 0.8357], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-4.3702e-02, -1.3627e-02, -3.3736e-02],\n",
       "                         [ 5.9619e-02,  1.6159e-02, -2.8567e-02],\n",
       "                         [ 2.7072e-02,  1.9254e-02,  3.9468e-02]],\n",
       "               \n",
       "                        [[ 1.8669e-02, -3.7957e-02,  9.8068e-03],\n",
       "                         [ 4.1648e-03, -3.6611e-02, -4.8710e-02],\n",
       "                         [ 3.7416e-04,  2.0676e-03, -2.5078e-02]],\n",
       "               \n",
       "                        [[ 5.8284e-02,  5.1901e-03,  5.3009e-02],\n",
       "                         [ 7.2872e-04, -1.6713e-02,  3.5374e-02],\n",
       "                         [-1.0131e-02,  5.0062e-02, -1.9479e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.3767e-03, -1.2034e-03, -4.2708e-02],\n",
       "                         [ 3.1162e-02, -5.4115e-02, -1.8587e-02],\n",
       "                         [-3.1021e-04,  2.3716e-02, -1.8927e-02]],\n",
       "               \n",
       "                        [[-2.0851e-02, -4.9040e-02, -1.3251e-02],\n",
       "                         [ 1.1836e-02,  8.5989e-03,  5.9320e-02],\n",
       "                         [-1.5005e-02, -3.3563e-02, -3.0291e-02]],\n",
       "               \n",
       "                        [[ 3.2447e-02, -4.5090e-02,  3.4413e-02],\n",
       "                         [ 1.4186e-02, -8.0078e-03, -5.5526e-02],\n",
       "                         [ 2.3710e-02, -1.7888e-02, -2.6468e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.3535e-03, -4.6939e-02, -3.8000e-02],\n",
       "                         [ 4.5195e-02, -6.4326e-02,  7.6383e-03],\n",
       "                         [ 7.2541e-03, -7.9630e-02, -4.7468e-02]],\n",
       "               \n",
       "                        [[ 2.0901e-02,  1.5754e-02,  2.3625e-02],\n",
       "                         [-5.4572e-02, -4.4321e-02, -4.3351e-02],\n",
       "                         [-5.6763e-02,  5.0064e-02, -1.2850e-02]],\n",
       "               \n",
       "                        [[-1.3336e-02,  3.8217e-02,  2.6076e-02],\n",
       "                         [ 3.3369e-02,  2.9154e-02,  2.0140e-02],\n",
       "                         [-2.7863e-03,  4.9059e-02,  2.7505e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.4442e-04, -6.9905e-03,  5.1513e-02],\n",
       "                         [-2.9548e-02, -4.9088e-02,  3.1979e-02],\n",
       "                         [ 3.9791e-02,  1.4876e-02, -1.2177e-02]],\n",
       "               \n",
       "                        [[-8.2850e-04,  5.1148e-02, -1.5545e-02],\n",
       "                         [-1.2785e-02,  7.3915e-02,  5.4139e-02],\n",
       "                         [ 9.3461e-02,  4.4485e-02,  3.1602e-02]],\n",
       "               \n",
       "                        [[-2.1151e-03, -4.5330e-04, -4.4565e-02],\n",
       "                         [-1.0282e-01, -4.2757e-02, -7.6484e-02],\n",
       "                         [-1.8677e-02,  1.2074e-02, -3.7740e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.0233e-03,  3.4157e-02,  4.8908e-02],\n",
       "                         [-1.3790e-02,  3.5846e-02,  5.0121e-02],\n",
       "                         [ 1.1362e-02, -1.8063e-02, -1.4542e-02]],\n",
       "               \n",
       "                        [[ 4.2676e-03, -1.4356e-02,  1.2788e-02],\n",
       "                         [-1.6214e-02,  8.6472e-03,  1.4520e-03],\n",
       "                         [ 1.4571e-02, -2.7827e-02, -3.9042e-02]],\n",
       "               \n",
       "                        [[ 4.9498e-02,  5.5938e-03, -5.5657e-03],\n",
       "                         [-5.8689e-03,  5.9269e-02,  1.8440e-02],\n",
       "                         [ 1.4232e-02,  5.8087e-02,  1.3712e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.9255e-02,  3.7683e-02, -3.5907e-02],\n",
       "                         [ 2.3164e-02, -2.1429e-02, -1.3765e-02],\n",
       "                         [ 4.2311e-03,  5.1843e-02,  4.9206e-02]],\n",
       "               \n",
       "                        [[ 7.8560e-02,  6.9009e-02,  5.1072e-02],\n",
       "                         [ 5.7993e-02,  9.7662e-02,  7.6212e-02],\n",
       "                         [-1.2178e-02,  5.2104e-02,  3.1011e-02]],\n",
       "               \n",
       "                        [[-2.1720e-02,  6.2952e-04,  3.7310e-02],\n",
       "                         [-3.9092e-03, -3.1375e-02, -3.3830e-02],\n",
       "                         [-2.1882e-02,  6.3913e-03,  2.2926e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.4090e-02, -4.3765e-02, -3.4845e-03],\n",
       "                         [ 4.8406e-02,  8.7374e-03, -6.2583e-03],\n",
       "                         [-5.6868e-02, -6.9677e-02, -8.0262e-02]],\n",
       "               \n",
       "                        [[-4.3687e-02,  2.0018e-02, -3.3008e-02],\n",
       "                         [ 1.4243e-02, -4.5496e-02, -2.0843e-02],\n",
       "                         [-3.3093e-02, -2.0732e-02,  1.6793e-02]],\n",
       "               \n",
       "                        [[-1.1730e-02,  2.7783e-02,  5.2310e-02],\n",
       "                         [ 2.8892e-02,  4.5672e-02, -3.0685e-02],\n",
       "                         [-5.3753e-02,  1.4569e-03, -7.2215e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2346e-03,  3.3736e-03, -5.3751e-02],\n",
       "                         [ 8.0512e-03, -4.7253e-03, -2.7187e-02],\n",
       "                         [ 4.6004e-02,  6.3030e-03, -1.2130e-02]],\n",
       "               \n",
       "                        [[-2.0353e-02,  9.5935e-02,  1.7028e-02],\n",
       "                         [ 6.2968e-04,  7.9693e-02, -1.6896e-03],\n",
       "                         [-4.4462e-02,  8.0178e-03, -1.1550e-04]],\n",
       "               \n",
       "                        [[-5.9322e-02,  1.6931e-02, -3.5418e-02],\n",
       "                         [-2.4703e-02,  1.8146e-03,  2.5648e-02],\n",
       "                         [-2.5140e-02, -2.5331e-02, -5.2890e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.6826e-02,  8.7188e-03, -1.4631e-02],\n",
       "                         [-4.2427e-03,  3.1592e-03,  3.4011e-02],\n",
       "                         [ 2.4498e-02, -4.3636e-03,  1.3631e-02]],\n",
       "               \n",
       "                        [[-4.8765e-03, -2.5093e-02, -4.6500e-02],\n",
       "                         [ 1.4969e-02,  4.3267e-02,  4.7689e-02],\n",
       "                         [ 3.1678e-02, -2.9005e-03,  1.8742e-02]],\n",
       "               \n",
       "                        [[-1.6282e-04,  3.7926e-02, -3.3435e-02],\n",
       "                         [-3.7151e-02, -5.9865e-03,  1.1217e-02],\n",
       "                         [-2.7224e-02,  4.4045e-02,  2.3198e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.8922e-02, -5.0440e-02, -2.4127e-02],\n",
       "                         [ 1.3196e-03, -3.4582e-02, -6.3706e-03],\n",
       "                         [-2.4766e-02,  2.4784e-02, -5.1412e-02]],\n",
       "               \n",
       "                        [[-8.5140e-02, -1.1747e-01, -1.0574e-02],\n",
       "                         [-7.6822e-02, -1.8023e-02, -1.2739e-02],\n",
       "                         [-2.4550e-02, -9.7639e-02,  5.0995e-02]],\n",
       "               \n",
       "                        [[ 1.0049e-02,  7.0355e-03, -2.8812e-02],\n",
       "                         [-6.1743e-02,  2.9907e-02, -4.9510e-02],\n",
       "                         [-1.2106e-02, -1.6349e-02, -4.7471e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.3281e-02, -5.9659e-02, -7.4845e-03],\n",
       "                         [ 3.9666e-03, -2.4579e-02, -1.6912e-02],\n",
       "                         [-3.0060e-02, -5.2557e-02, -3.4569e-02]],\n",
       "               \n",
       "                        [[ 1.1601e-02,  2.0093e-02,  1.9330e-02],\n",
       "                         [ 1.1518e-02, -3.4297e-03, -3.9855e-03],\n",
       "                         [ 1.1395e-02,  3.4817e-02,  6.0226e-03]],\n",
       "               \n",
       "                        [[ 3.0721e-02, -1.4320e-02, -2.4393e-02],\n",
       "                         [-7.5937e-02, -3.5532e-02, -2.3386e-02],\n",
       "                         [-4.9772e-02, -6.8097e-02, -2.4774e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7394e-05, -1.8980e-02,  4.6820e-02],\n",
       "                         [ 3.1611e-02, -5.9833e-02,  3.1288e-02],\n",
       "                         [-4.7890e-02, -1.8718e-02, -2.2998e-02]],\n",
       "               \n",
       "                        [[ 1.0739e-02,  4.0940e-02,  4.8134e-02],\n",
       "                         [ 4.7797e-02,  4.2867e-02, -1.0555e-02],\n",
       "                         [ 4.3696e-02,  1.1575e-01,  2.4784e-02]],\n",
       "               \n",
       "                        [[ 2.0929e-02,  5.0720e-02, -3.8524e-03],\n",
       "                         [ 6.9852e-03,  5.0116e-02, -2.8043e-02],\n",
       "                         [-1.1385e-02,  6.5274e-03,  1.4342e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-0.5599, -0.5552, -0.5566,  0.5535,  0.5584,  0.5569, -0.5565,  0.5590,\n",
       "                       -0.5581, -0.5547,  0.5560, -0.5532, -0.5587,  0.5597, -0.5554, -0.5580,\n",
       "                        0.5588,  0.5554, -0.5597, -0.5538,  0.5582, -0.5558, -0.5580,  0.5537,\n",
       "                        0.5585, -0.5426,  0.5531,  0.5594, -0.5537, -0.5554, -0.5546,  0.5546,\n",
       "                       -0.5588,  0.5417,  0.5531, -0.5571, -0.5582, -0.5558, -0.5579,  0.5563,\n",
       "                       -0.5596, -0.5546,  0.5582, -0.5543, -0.5566,  0.5547, -0.5583, -0.5600,\n",
       "                        0.5525,  0.5590,  0.5591, -0.5529,  0.5544, -0.5542, -0.5560, -0.5583,\n",
       "                        0.5550, -0.5568, -0.5573, -0.5581,  0.5572,  0.5543,  0.5578, -0.5564,\n",
       "                        0.5584,  0.5558,  0.5579,  0.5603,  0.5548,  0.5582,  0.5539,  0.5539,\n",
       "                       -0.5558,  0.5583, -0.5625, -0.5550,  0.5596, -0.5543,  0.5566,  0.5571,\n",
       "                       -0.5600, -0.5554, -0.5594,  0.5592,  0.5555,  0.5557,  0.5542, -0.5608,\n",
       "                       -0.5536,  0.5579, -0.5575,  0.5527, -0.5549,  0.5600,  0.5552,  0.5595,\n",
       "                        0.5599,  0.5586,  0.5556, -0.5573, -0.5529,  0.5576,  0.5554, -0.5570,\n",
       "                       -0.5573, -0.5582, -0.5583, -0.5566, -0.5555, -0.5548, -0.5574, -0.5599,\n",
       "                        0.5589,  0.5589,  0.5558,  0.5593, -0.5566,  0.5566,  0.5548,  0.5579,\n",
       "                       -0.5469, -0.5545, -0.5583,  0.5559, -0.5517,  0.5581, -0.5545,  0.5590],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.0752, -0.2222, -0.2209, -0.0301, -0.1385, -0.2287, -0.1965, -0.1863,\n",
       "                       -0.1171, -0.1848, -0.1981, -0.2096, -0.0811, -0.1206, -0.1761, -0.1511,\n",
       "                       -0.0310, -0.1785, -0.1354, -0.2051, -0.1303, -0.1971, -0.1930, -0.1128,\n",
       "                       -0.2056, -0.1850, -0.2187, -0.0820, -0.1456, -0.1416, -0.1590, -0.0678,\n",
       "                       -0.1887, -0.1743, -0.1259, -0.2086, -0.1691, -0.2446, -0.0669, -0.1646,\n",
       "                       -0.1385, -0.1979, -0.1650, -0.2105, -0.2164, -0.1499, -0.2251, -0.1195,\n",
       "                       -0.0877, -0.0550, -0.1372, -0.1770, -0.2247, -0.2134, -0.1610, -0.1654,\n",
       "                       -0.2468, -0.1107, -0.1712, -0.1072, -0.2446, -0.1986, -0.1468, -0.1368,\n",
       "                       -0.1576, -0.1597, -0.2342, -0.2002, -0.1029, -0.1339, -0.0791, -0.1650,\n",
       "                       -0.1137, -0.1226, -0.2262, -0.1769, -0.1857, -0.1967, -0.1823, -0.1826,\n",
       "                       -0.0848, -0.2136, -0.0045, -0.2291, -0.1357, -0.2389, -0.1607, -0.1593,\n",
       "                       -0.0241, -0.1573, -0.0325, -0.1927, -0.1623, -0.1636, -0.2368, -0.2161,\n",
       "                       -0.1971, -0.1735, -0.1392, -0.1891, -0.2072, -0.1061, -0.2262, -0.1545,\n",
       "                       -0.2028, -0.0613, -0.1243, -0.0735, -0.1948, -0.1561, -0.1174, -0.0502,\n",
       "                       -0.1899, -0.1496, -0.1963, -0.1685, -0.2141, -0.0210, -0.2279, -0.1681,\n",
       "                       -0.2047, -0.1384, -0.1491, -0.2190, -0.0969, -0.2133, -0.0907, -0.1880],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.8093, 0.7495, 0.7997, 0.8392, 0.8219, 0.7918, 0.8508, 0.7993, 0.9625,\n",
       "                       0.7940, 0.7750, 0.7383, 0.8417, 0.7778, 0.7483, 0.9617, 0.8718, 0.8679,\n",
       "                       0.8584, 0.6964, 0.7606, 0.7884, 0.8327, 0.7895, 0.7564, 0.7520, 0.8513,\n",
       "                       0.7525, 0.7667, 0.8095, 1.0495, 0.8087, 0.6924, 0.7637, 0.8062, 0.9847,\n",
       "                       0.8062, 0.8280, 0.9108, 1.0359, 0.7953, 0.7623, 0.7348, 0.7962, 0.7166,\n",
       "                       0.9111, 0.7272, 0.7804, 0.8488, 0.8290, 0.9651, 0.9863, 0.7369, 0.7757,\n",
       "                       0.9778, 0.7277, 0.8836, 0.7600, 0.7946, 0.8953, 0.8346, 0.7383, 0.9778,\n",
       "                       0.8786, 0.8494, 0.8047, 0.8066, 0.8885, 0.8185, 0.7463, 0.7989, 0.7729,\n",
       "                       0.8339, 0.9020, 0.7465, 0.7171, 0.7810, 0.8230, 0.9260, 0.7882, 0.8264,\n",
       "                       0.9635, 0.8743, 0.7630, 0.7732, 0.7391, 0.8376, 0.7785, 0.8861, 1.0193,\n",
       "                       0.8233, 0.9787, 0.8576, 0.8626, 0.8288, 0.8182, 0.7184, 0.7724, 0.8488,\n",
       "                       0.7623, 0.7973, 0.8505, 0.7446, 1.0277, 0.8610, 0.8332, 0.8319, 0.8343,\n",
       "                       0.9999, 0.9363, 0.8064, 0.8397, 0.7709, 0.7834, 0.7463, 0.8550, 0.7467,\n",
       "                       0.8445, 0.8815, 0.7924, 0.7761, 0.9897, 0.7584, 0.7370, 0.8763, 0.6919,\n",
       "                       0.7355, 0.7686], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0053,  0.0019,  0.0046,  ..., -0.0081,  0.0156, -0.0086],\n",
       "                       [-0.0151, -0.0111,  0.0108,  ..., -0.0063,  0.0078, -0.0164],\n",
       "                       [ 0.0050, -0.0080,  0.0010,  ..., -0.0224,  0.0091, -0.0180],\n",
       "                       [ 0.0016, -0.0087,  0.0101,  ...,  0.0078, -0.0055,  0.0038],\n",
       "                       [-0.0126,  0.0035,  0.0124,  ..., -0.0014,  0.0023, -0.0030]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0076,  0.0032,  0.0015, -0.0034, -0.0013], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[ 1.9450e-01, -9.9795e-02,  5.8899e-02,  2.0421e-01, -6.8798e-02,\n",
       "                        -2.0007e-01, -1.7882e-01,  1.9901e-01,  7.3716e-02, -7.7455e-02,\n",
       "                        -1.7553e-01,  1.2926e-01, -2.1960e-01, -5.1855e-02,  1.6968e-01,\n",
       "                        -7.0568e-02, -1.7667e-01,  8.5367e-02, -1.8126e-01, -4.0493e-02],\n",
       "                       [ 3.1346e-01, -2.4672e-01,  3.6882e-01, -2.9854e-01,  4.4060e-01,\n",
       "                        -3.9375e-02,  3.2006e-01,  2.4884e-01,  6.6769e-01, -1.2758e-01,\n",
       "                        -5.8387e-01, -8.7315e-02, -4.9148e-01, -3.9044e-02, -2.8058e-01,\n",
       "                        -1.2313e-01, -6.4514e-01, -3.9344e-01, -7.0918e-01, -4.2825e-01],\n",
       "                       [-1.6792e-01, -9.5964e-03, -4.3481e-02,  6.7062e-03,  1.1711e-01,\n",
       "                        -1.6487e-01, -6.0568e-02, -1.0674e-02,  1.3948e-01,  2.4193e-02,\n",
       "                         3.2932e-02,  1.9114e-01,  1.7074e-01,  1.4639e-01,  1.9790e-01,\n",
       "                        -1.4125e-01,  1.1538e-01,  1.8761e-01,  8.7872e-03, -1.8317e-01],\n",
       "                       [-8.6637e-02, -8.4453e-02, -1.6488e-02, -1.0229e-01, -1.2699e-01,\n",
       "                        -5.9725e-02,  7.7862e-02,  8.2837e-02,  1.6639e-01,  1.4864e-01,\n",
       "                        -6.2536e-02, -6.9908e-02, -7.0712e-02, -5.1099e-02, -1.4010e-01,\n",
       "                        -1.6697e-01,  1.6291e-01,  1.0379e-01, -7.5108e-02,  2.9448e-04],\n",
       "                       [-5.8330e-01, -5.2382e-01,  2.3097e-01, -2.0290e-01,  1.2620e-01,\n",
       "                        -5.2184e-01,  2.3124e-01,  1.6222e-01, -4.6191e-01, -4.8259e-01,\n",
       "                         2.0806e-01, -2.4699e-01, -2.1455e-01, -2.8649e-01, -5.9960e-03,\n",
       "                        -3.3979e-01, -7.1683e-02, -2.2812e-01,  9.2595e-01, -4.0021e-01],\n",
       "                       [-2.2718e-01, -3.8532e-01,  3.6588e-01, -4.9189e-01,  6.1618e-01,\n",
       "                        -4.3076e-01,  6.6215e-01,  1.3517e-01,  2.0993e-01, -6.5571e-01,\n",
       "                         2.4551e-01, -3.9373e-01, -5.2038e-01, -5.6636e-01, -3.1780e-01,\n",
       "                        -4.5481e-01, -3.0907e-01, -5.8442e-01, -1.5079e-01, -4.3089e-01],\n",
       "                       [-4.3577e-01, -2.5230e-01,  3.8415e-01, -3.5953e-01,  5.0516e-01,\n",
       "                        -4.9321e-01,  4.9733e-01, -2.0980e-01, -1.5045e-01, -5.2452e-01,\n",
       "                         1.5987e-01, -1.4204e-01, -7.4178e-02, -3.6433e-01, -4.2967e-01,\n",
       "                        -4.3072e-01, -4.1603e-03, -4.0346e-01,  5.7543e-01, -3.4250e-01],\n",
       "                       [-2.1789e-01,  1.4847e-01, -1.7759e-01, -1.5096e-02,  1.6466e-01,\n",
       "                         1.9650e-01, -2.1852e-02, -1.2478e-02, -1.0809e-02, -1.5789e-01,\n",
       "                        -1.5054e-01, -1.8219e-01,  1.2927e-01, -4.6389e-02,  2.1401e-01,\n",
       "                         1.6270e-01, -2.1683e-01,  3.1173e-02, -1.7408e-01,  5.3669e-02],\n",
       "                       [-2.0245e-01, -2.5325e-01,  2.2297e-01, -3.6453e-01,  2.6300e-01,\n",
       "                        -3.9136e-01,  2.1834e-01, -1.5191e-01, -3.9730e-01, -3.6379e-01,\n",
       "                         2.3120e-01, -4.2832e-02, -2.1437e-01, -1.7172e-01,  1.6827e-01,\n",
       "                        -3.6454e-01,  1.3989e-01, -1.1689e-01,  8.3335e-01, -4.2437e-02],\n",
       "                       [-4.6539e-01, -4.4326e-01, -1.8556e-02, -3.0206e-01,  1.8009e-01,\n",
       "                        -2.9671e-01,  9.9821e-03,  1.4084e-01, -5.4199e-01, -2.9132e-02,\n",
       "                         5.4752e-01, -1.5445e-01, -2.9636e-01, -2.3680e-01, -1.8175e-01,\n",
       "                        -2.4527e-01, -2.0243e-02, -3.0596e-01,  4.4147e-01, -3.4644e-02],\n",
       "                       [-5.1960e-01, -3.0793e-01,  3.9824e-01, -2.8498e-02,  2.7860e-01,\n",
       "                        -2.3621e-01,  1.3768e-01, -1.1015e-03, -1.6295e-01, -2.3110e-01,\n",
       "                         2.8005e-01, -2.1967e-01,  6.6372e-02, -1.7816e-01,  2.1774e-01,\n",
       "                        -2.7426e-01,  3.1498e-01, -1.8950e-01,  4.5967e-01, -2.2480e-01],\n",
       "                       [ 8.6240e-02, -1.0932e-01, -6.6633e-02,  3.1452e-01,  9.8137e-02,\n",
       "                         1.0588e-01, -2.6359e-01,  1.9449e-01, -3.9574e-02, -9.9108e-04,\n",
       "                         1.2918e-01,  1.1637e-01,  2.6810e-01, -7.8607e-02,  2.6053e-01,\n",
       "                         1.5103e-02,  2.4161e-01,  3.2538e-02,  9.5170e-02,  7.7577e-02],\n",
       "                       [ 2.2077e-01, -1.1893e-01,  3.3556e-03,  1.4105e-01,  1.9006e-01,\n",
       "                         5.2740e-02, -1.9452e-01,  2.2726e-02,  1.2223e-01,  4.7893e-02,\n",
       "                         1.0646e-01,  2.3871e-01,  9.1381e-02,  1.1641e-01,  1.0854e-01,\n",
       "                         1.8654e-01, -1.0111e-01,  1.9331e-01,  1.4477e-01, -3.2333e-03],\n",
       "                       [-4.7303e-02, -7.3556e-02, -1.5947e-01,  2.0988e-01, -1.0523e-01,\n",
       "                         1.4511e-01,  3.9058e-02, -2.0894e-01,  1.4523e-01,  1.4160e-01,\n",
       "                         2.1420e-01,  5.6961e-02,  1.9555e-01,  1.8229e-01, -1.8977e-01,\n",
       "                         2.0819e-01,  7.4866e-02, -1.2519e-01, -4.2634e-02, -2.1221e-01],\n",
       "                       [-4.0376e-01, -3.3445e-01,  2.3031e-01, -8.1199e-02, -9.7037e-02,\n",
       "                        -3.8110e-01,  3.1125e-01, -3.0648e-02, -6.1843e-01, -2.2994e-01,\n",
       "                         1.8667e-01, -2.6246e-02,  1.0519e-01, -2.5016e-01, -2.1890e-02,\n",
       "                        -1.8432e-01,  2.6919e-01, -3.7928e-01,  5.7429e-01, -3.0198e-01],\n",
       "                       [ 6.2884e-02,  1.5268e-01, -9.8051e-02, -7.9451e-02,  5.4087e-02,\n",
       "                        -8.8575e-02, -2.0254e-01,  7.8244e-03, -2.1483e-01,  1.6323e-01,\n",
       "                         5.5022e-02,  1.6895e-01,  1.5126e-01, -4.8578e-04, -1.3752e-01,\n",
       "                         1.3657e-02,  8.7678e-02,  7.2777e-02, -2.0556e-01,  1.6001e-01],\n",
       "                       [-1.7184e-01,  1.5199e-01, -1.4167e-01,  4.3341e-02, -1.1432e-01,\n",
       "                         1.8257e-02,  1.5661e-01,  6.9146e-03,  1.4682e-01,  1.5712e-01,\n",
       "                        -1.8079e-01, -2.4925e-02,  9.8675e-02,  1.8632e-01,  7.6713e-02,\n",
       "                        -7.8911e-02, -9.5569e-02, -1.8848e-01,  2.3741e-02, -1.2041e-01],\n",
       "                       [-3.4018e-01, -5.4116e-01,  7.2154e-01, -5.9619e-01,  7.5661e-01,\n",
       "                        -4.8814e-01,  6.5774e-01,  3.5671e-01,  4.0474e-01, -4.2754e-01,\n",
       "                         1.1906e-01, -4.7903e-01, -7.5758e-01, -4.7793e-01, -9.1343e-01,\n",
       "                        -7.0370e-01, -7.4674e-01, -4.1812e-01, -1.0789e-01, -7.1230e-01],\n",
       "                       [-3.7166e-01, -3.5114e-01,  9.7455e-02, -5.1995e-01,  1.1000e-01,\n",
       "                        -4.2867e-01,  2.9503e-01,  8.6841e-02, -1.5219e-01, -2.4102e-01,\n",
       "                         1.7106e-01, -4.9874e-01,  3.9382e-03, -4.8100e-01, -2.7536e-01,\n",
       "                        -5.0917e-01, -2.2812e-01, -5.0284e-01,  4.0104e-01, -5.0062e-01],\n",
       "                       [ 5.7188e-02, -1.6221e-01,  8.6147e-03, -1.6054e-01,  2.3078e-01,\n",
       "                        -2.4389e-01,  3.5177e-01,  4.9625e-01,  8.3814e-02, -1.8604e-01,\n",
       "                        -8.6181e-02,  7.3271e-02, -4.9693e-01, -2.3796e-01, -3.8974e-01,\n",
       "                        -3.0115e-01, -3.0443e-01,  1.8303e-02, -6.3854e-01, -2.5684e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([-0.1579,  0.3830, -0.2280, -0.1649,  0.1597,  0.5916,  0.4159,  0.0164,\n",
       "                        0.1514,  0.3201,  0.2737,  0.1193,  0.1049, -0.1574,  0.2005,  0.1238,\n",
       "                        0.1505,  0.7526,  0.3516,  0.0970], device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 0.1263, -0.0772,  0.1627, -0.0859, -0.2019, -0.2602, -0.4369,  0.1521,\n",
       "                        -0.2212, -0.2299, -0.1524,  0.1932,  0.2122,  0.1275, -0.0518,  0.0328,\n",
       "                         0.0779, -0.4234, -0.0063, -0.1765],\n",
       "                       [ 0.0947, -0.0592, -0.1505,  0.1282,  0.2700,  0.3092,  0.1813, -0.1357,\n",
       "                         0.3183,  0.2896,  0.0864,  0.0237,  0.1133,  0.0974,  0.3301, -0.0959,\n",
       "                        -0.1545,  0.3448,  0.2064, -0.0190],\n",
       "                       [-0.1231, -0.2893, -0.1265,  0.1026, -0.4462, -0.1169, -0.4914, -0.1678,\n",
       "                        -0.3340, -0.3942, -0.2506,  0.0273,  0.0290,  0.0147, -0.4609,  0.0933,\n",
       "                         0.0714, -0.5103, -0.3784, -0.4865],\n",
       "                       [ 0.2137,  0.0238,  0.1494,  0.1903, -0.3170, -0.1248, -0.1031,  0.0262,\n",
       "                        -0.2517,  0.1737, -0.1939, -0.1401, -0.0070,  0.0077, -0.2880,  0.0615,\n",
       "                        -0.1830, -0.2516, -0.0809,  0.1210],\n",
       "                       [-0.1682,  0.3614, -0.1215,  0.1635, -0.1256,  0.2145,  0.3066,  0.2003,\n",
       "                        -0.0317, -0.3850,  0.1504,  0.1634, -0.1465, -0.0818,  0.0308,  0.0231,\n",
       "                         0.0909,  0.0798,  0.2517, -0.0961],\n",
       "                       [ 0.1320,  0.0388,  0.1741,  0.0834,  0.2778,  0.1680, -0.0524,  0.0265,\n",
       "                         0.0478, -0.0274, -0.0982,  0.1646,  0.0666,  0.0608,  0.1382, -0.0616,\n",
       "                         0.2075,  0.3890, -0.0512,  0.0220],\n",
       "                       [ 0.0186,  0.8560, -0.0559,  0.0717,  0.8958,  0.9087,  0.9736, -0.1300,\n",
       "                         0.5675,  0.3810,  0.6347,  0.0049,  0.0567, -0.1425,  0.6163,  0.0342,\n",
       "                         0.1900,  0.8056,  0.9387,  0.3749],\n",
       "                       [-0.0348, -0.0985, -0.2037,  0.0392,  0.0375, -0.1490, -0.1671,  0.2235,\n",
       "                         0.1203,  0.1824,  0.1228, -0.1389,  0.1232,  0.1064, -0.1620, -0.2108,\n",
       "                         0.0877, -0.3298, -0.2546, -0.0387],\n",
       "                       [-0.0676, -0.2835, -0.1881,  0.2059,  0.3445, -0.0828,  0.2131, -0.1769,\n",
       "                         0.2083,  0.2633,  0.2230, -0.0921, -0.1128, -0.0414,  0.2611, -0.1009,\n",
       "                         0.0885, -0.1704,  0.0455, -0.4391],\n",
       "                       [ 0.0187, -0.5333,  0.0174,  0.0651, -0.3656, -0.2181, -0.3764,  0.0620,\n",
       "                        -0.4032,  0.0820, -0.3893,  0.0414, -0.0107,  0.0959, -0.1434, -0.0094,\n",
       "                         0.2114, -0.6247, -0.1515, -0.3881]], device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.2632,  0.2419, -0.3432, -0.0988,  0.0510,  0.0767,  0.6427, -0.1952,\n",
       "                        0.1331, -0.0769], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.483476383447647,\n",
       "   1.3545481526851655,\n",
       "   1.3098033878803252,\n",
       "   1.2686262041330338,\n",
       "   1.218484871506691,\n",
       "   1.1743631842136384,\n",
       "   1.1345976806879043,\n",
       "   1.086325322985649,\n",
       "   1.0543912591934204,\n",
       "   1.0350105865001678,\n",
       "   0.9866085749864578,\n",
       "   0.9846874786615372,\n",
       "   0.9660252624750137,\n",
       "   0.9358155431747437,\n",
       "   0.937087476849556,\n",
       "   0.915649830698967,\n",
       "   0.8967641706466675,\n",
       "   0.8855581641197204,\n",
       "   0.8825522440671921,\n",
       "   0.8663041898012162,\n",
       "   0.8615279206037522,\n",
       "   0.8308090027570725,\n",
       "   0.8249965822696685,\n",
       "   0.8191860826015472,\n",
       "   0.8189492141604423,\n",
       "   0.8036693303585053,\n",
       "   0.7832032409310341,\n",
       "   0.775857702255249,\n",
       "   0.7779861376881599,\n",
       "   0.7800523315668106,\n",
       "   0.7531858184337616,\n",
       "   0.7541596565246582,\n",
       "   0.7361088641285897,\n",
       "   0.7286121693253517,\n",
       "   0.7292022127509117,\n",
       "   0.7095676583051681,\n",
       "   0.7120725163817405,\n",
       "   0.7002220395207405,\n",
       "   0.7061740514039994,\n",
       "   0.6800183079242706,\n",
       "   0.6920564432740212,\n",
       "   0.6727037338614463,\n",
       "   0.6708960717916489,\n",
       "   0.6698056691288948,\n",
       "   0.6596473541855812,\n",
       "   0.6498868864178657,\n",
       "   0.655269834458828,\n",
       "   0.6354625440835953,\n",
       "   0.6332014302015304,\n",
       "   0.6399552510380745,\n",
       "   0.636753729403019,\n",
       "   0.6139424102902412,\n",
       "   0.6182934874892235,\n",
       "   0.6212592880129815,\n",
       "   0.6169081155657768,\n",
       "   0.6051740693747998,\n",
       "   0.6106658101081848,\n",
       "   0.5890947519540787,\n",
       "   0.5899993230104447,\n",
       "   0.5940517761707306,\n",
       "   0.5941763424873352,\n",
       "   0.5757712825536728,\n",
       "   0.5736726365685463,\n",
       "   0.6003453350663185,\n",
       "   0.5694817636013031,\n",
       "   0.5650899789333343,\n",
       "   0.562277569591999,\n",
       "   0.5423234815001488,\n",
       "   0.5434539637863636,\n",
       "   0.5505697911977768,\n",
       "   0.5450181565880775,\n",
       "   0.5424525443017483,\n",
       "   0.5502477190196514,\n",
       "   0.5398116151988507,\n",
       "   0.5268495962321759,\n",
       "   0.5161014630198478,\n",
       "   0.5290606241226197,\n",
       "   0.5290622363686561,\n",
       "   0.5131781410574913,\n",
       "   0.513524123877287,\n",
       "   0.4992736192941666,\n",
       "   0.5125679462850093,\n",
       "   0.5042422689497471,\n",
       "   0.49598163238167764,\n",
       "   0.48699826806783675,\n",
       "   0.4952307816147804,\n",
       "   0.48522328546643256,\n",
       "   0.48399221505224704,\n",
       "   0.4750914240181446,\n",
       "   0.46963882228732107,\n",
       "   0.4760855414271355,\n",
       "   0.4710879309475422,\n",
       "   0.4571245724260807,\n",
       "   0.4625591897964478,\n",
       "   0.4592583292722702,\n",
       "   0.4662577235996723,\n",
       "   0.44587932297587396,\n",
       "   0.4526464014351368,\n",
       "   0.4377901410162449],\n",
       "  'train_loss_std': [0.14395432451489668,\n",
       "   0.11333838373880073,\n",
       "   0.12711974579300753,\n",
       "   0.12105659789091615,\n",
       "   0.13434592199589895,\n",
       "   0.1313440359206019,\n",
       "   0.13860421011488458,\n",
       "   0.13037191019217056,\n",
       "   0.1272481605095346,\n",
       "   0.12974667647832655,\n",
       "   0.13741041014183927,\n",
       "   0.15123350618087675,\n",
       "   0.1418155604692662,\n",
       "   0.1374293410478573,\n",
       "   0.13186008332118565,\n",
       "   0.140305291953603,\n",
       "   0.13605381381361312,\n",
       "   0.13910498653681638,\n",
       "   0.13889903648615332,\n",
       "   0.1434268096153552,\n",
       "   0.13734807948094938,\n",
       "   0.13614374847329086,\n",
       "   0.14652969026418683,\n",
       "   0.14201605577733833,\n",
       "   0.14244231613074756,\n",
       "   0.15172665964938425,\n",
       "   0.1386274556381588,\n",
       "   0.1448365387598719,\n",
       "   0.1387978411864321,\n",
       "   0.1392369955539013,\n",
       "   0.13945563215547616,\n",
       "   0.13514215764376322,\n",
       "   0.14039674919546777,\n",
       "   0.14985599857866935,\n",
       "   0.13834088182754622,\n",
       "   0.14977615913270442,\n",
       "   0.13755955071196108,\n",
       "   0.1426919142423052,\n",
       "   0.14972353920124865,\n",
       "   0.14629833840481885,\n",
       "   0.13974395437761303,\n",
       "   0.13765699316149732,\n",
       "   0.13424311337994166,\n",
       "   0.13918151588458477,\n",
       "   0.1359302521562211,\n",
       "   0.13696987917735345,\n",
       "   0.1381445758943189,\n",
       "   0.13321522538615158,\n",
       "   0.13803707231606446,\n",
       "   0.13306890934295446,\n",
       "   0.1344519116346583,\n",
       "   0.1382546355147701,\n",
       "   0.13382131089253216,\n",
       "   0.1375218830216236,\n",
       "   0.14794151804726205,\n",
       "   0.1303917236588258,\n",
       "   0.13799221257661803,\n",
       "   0.13942923150945669,\n",
       "   0.13231009475445332,\n",
       "   0.13433800276299657,\n",
       "   0.13837884123711383,\n",
       "   0.13936341481167122,\n",
       "   0.14135101436191946,\n",
       "   0.14932812710107446,\n",
       "   0.1299503415927997,\n",
       "   0.1355631097071234,\n",
       "   0.13544314982727282,\n",
       "   0.1397812187860245,\n",
       "   0.1285458448637036,\n",
       "   0.1353943106281065,\n",
       "   0.13053567723681753,\n",
       "   0.13188190368150735,\n",
       "   0.14005317621076105,\n",
       "   0.13371949564274818,\n",
       "   0.13816014847329,\n",
       "   0.1314713657649213,\n",
       "   0.13112349419041336,\n",
       "   0.13471776218016887,\n",
       "   0.13027689970342018,\n",
       "   0.13290112830775985,\n",
       "   0.12413065756293956,\n",
       "   0.1321072014420857,\n",
       "   0.1399392415305865,\n",
       "   0.13140487575785323,\n",
       "   0.13105213940689583,\n",
       "   0.1271608603606777,\n",
       "   0.12232877988984227,\n",
       "   0.1290914488083553,\n",
       "   0.12357111193000987,\n",
       "   0.1265193672330218,\n",
       "   0.12458573172082225,\n",
       "   0.13162288135038194,\n",
       "   0.11534974463449765,\n",
       "   0.12952297854521969,\n",
       "   0.12435834453104028,\n",
       "   0.1389928684385691,\n",
       "   0.12120323266319391,\n",
       "   0.1277518306115412,\n",
       "   0.1216219440238964],\n",
       "  'train_accuracy_mean': [0.37069333359599116,\n",
       "   0.4429600010514259,\n",
       "   0.4647066667675972,\n",
       "   0.4883599992990494,\n",
       "   0.5131999998092651,\n",
       "   0.5351200005412102,\n",
       "   0.5534133326411247,\n",
       "   0.5779733316302299,\n",
       "   0.5907733336687088,\n",
       "   0.5979866657853127,\n",
       "   0.6220266659855842,\n",
       "   0.6233199979066849,\n",
       "   0.6316133319735527,\n",
       "   0.6442533332705498,\n",
       "   0.6442933332920074,\n",
       "   0.6498933331370353,\n",
       "   0.659346665263176,\n",
       "   0.6675466662049293,\n",
       "   0.6657066665887833,\n",
       "   0.6716933337450027,\n",
       "   0.6720133323669434,\n",
       "   0.6882933328151702,\n",
       "   0.6891733328104019,\n",
       "   0.6909999992251397,\n",
       "   0.6918533331155777,\n",
       "   0.6961466667056083,\n",
       "   0.705066666841507,\n",
       "   0.708159999191761,\n",
       "   0.7087200011014938,\n",
       "   0.7072133328318596,\n",
       "   0.717360000371933,\n",
       "   0.7175333334207534,\n",
       "   0.7243199995756149,\n",
       "   0.7285599997043609,\n",
       "   0.7260400002002716,\n",
       "   0.7356933336853981,\n",
       "   0.7333333344459534,\n",
       "   0.7377333332896232,\n",
       "   0.7363199999332428,\n",
       "   0.7475466676354409,\n",
       "   0.7430000010728836,\n",
       "   0.7492400013208389,\n",
       "   0.7498533320426941,\n",
       "   0.7495199999809266,\n",
       "   0.7541600006818772,\n",
       "   0.7580000001192093,\n",
       "   0.7559466677904129,\n",
       "   0.7636399995088577,\n",
       "   0.7658799993991852,\n",
       "   0.7625333334207535,\n",
       "   0.7626399983167649,\n",
       "   0.7732266647815704,\n",
       "   0.7706933327913285,\n",
       "   0.7705733337402344,\n",
       "   0.7721866673231125,\n",
       "   0.7775466668605805,\n",
       "   0.7746533321142197,\n",
       "   0.7821866668462754,\n",
       "   0.7803599991798401,\n",
       "   0.7778133322000503,\n",
       "   0.7796400007009506,\n",
       "   0.7875466661453248,\n",
       "   0.7886400009393693,\n",
       "   0.7777466655969619,\n",
       "   0.7903733334541321,\n",
       "   0.7915733312368393,\n",
       "   0.7917733327150345,\n",
       "   0.7990533335208893,\n",
       "   0.7999999995231628,\n",
       "   0.7966133326292038,\n",
       "   0.7994533331394196,\n",
       "   0.7988000001907348,\n",
       "   0.7957999991178513,\n",
       "   0.8017200000286102,\n",
       "   0.8063333348035813,\n",
       "   0.8085466663837433,\n",
       "   0.804853335261345,\n",
       "   0.8050666663646698,\n",
       "   0.8098933339118958,\n",
       "   0.8110666654109955,\n",
       "   0.8160133337974549,\n",
       "   0.8106666659116745,\n",
       "   0.8153466678857804,\n",
       "   0.817799998998642,\n",
       "   0.821519998908043,\n",
       "   0.8179866662025451,\n",
       "   0.8210000007152557,\n",
       "   0.8225199991464615,\n",
       "   0.8258133324384689,\n",
       "   0.8262800006866455,\n",
       "   0.8266933342218399,\n",
       "   0.8276533333063125,\n",
       "   0.833626668214798,\n",
       "   0.8289600015878678,\n",
       "   0.8305066677331925,\n",
       "   0.8271066660881042,\n",
       "   0.8359733339548111,\n",
       "   0.8337599998712539,\n",
       "   0.8393333332538605],\n",
       "  'train_accuracy_std': [0.0766484566360368,\n",
       "   0.06375887423490642,\n",
       "   0.06983140344978796,\n",
       "   0.06645081097227663,\n",
       "   0.06909561705264296,\n",
       "   0.06746593476190425,\n",
       "   0.07170428686099195,\n",
       "   0.06530189113446383,\n",
       "   0.06611657844767474,\n",
       "   0.0655488958249352,\n",
       "   0.06948943519117827,\n",
       "   0.07238477309039977,\n",
       "   0.06905824857850075,\n",
       "   0.06624096003845513,\n",
       "   0.06401016017503583,\n",
       "   0.06567317884517705,\n",
       "   0.06622533740727694,\n",
       "   0.06528929708091179,\n",
       "   0.06665708840864606,\n",
       "   0.0709408149481394,\n",
       "   0.06510941014946067,\n",
       "   0.061867409215896334,\n",
       "   0.06982092257944582,\n",
       "   0.063668411695221,\n",
       "   0.06475688423526006,\n",
       "   0.06988114262025241,\n",
       "   0.06372105914721576,\n",
       "   0.0669732372169136,\n",
       "   0.06379311623137775,\n",
       "   0.0649701734573289,\n",
       "   0.06280523295637327,\n",
       "   0.06085797585225891,\n",
       "   0.06363789743475273,\n",
       "   0.0669033610935633,\n",
       "   0.06302492042454541,\n",
       "   0.06611662585150434,\n",
       "   0.06259002349539497,\n",
       "   0.062139770798644786,\n",
       "   0.06652628540686055,\n",
       "   0.06532060176314308,\n",
       "   0.06242346371620217,\n",
       "   0.060841872811850016,\n",
       "   0.05806873802044365,\n",
       "   0.06142920633719081,\n",
       "   0.06116521208393779,\n",
       "   0.060884588818522274,\n",
       "   0.060173946170244186,\n",
       "   0.058278978144496695,\n",
       "   0.06011399276989966,\n",
       "   0.05876642937223986,\n",
       "   0.05848729440810904,\n",
       "   0.06153436154620754,\n",
       "   0.05852983513984785,\n",
       "   0.06068593123072759,\n",
       "   0.06457877700587399,\n",
       "   0.0558046494432728,\n",
       "   0.05948026564097738,\n",
       "   0.05974181981651452,\n",
       "   0.05926178628905173,\n",
       "   0.059203759905654935,\n",
       "   0.060292281796880745,\n",
       "   0.05929908234629067,\n",
       "   0.05990376867607915,\n",
       "   0.06180246448648162,\n",
       "   0.056648179731089446,\n",
       "   0.059240492050440206,\n",
       "   0.059980457648278536,\n",
       "   0.05917726179649516,\n",
       "   0.05534859438199707,\n",
       "   0.05959788838774204,\n",
       "   0.056826939943452844,\n",
       "   0.05730836646921678,\n",
       "   0.06117555891421147,\n",
       "   0.05631160789883762,\n",
       "   0.060449979888514506,\n",
       "   0.05730172560835069,\n",
       "   0.05648127201393746,\n",
       "   0.056781217680472944,\n",
       "   0.056481164532692286,\n",
       "   0.056896162071701076,\n",
       "   0.053301822981599106,\n",
       "   0.05656854241906197,\n",
       "   0.057846846322377785,\n",
       "   0.056290161973020725,\n",
       "   0.054676427920292967,\n",
       "   0.05465804694123846,\n",
       "   0.052474545056419096,\n",
       "   0.0540415962790607,\n",
       "   0.05391994874793915,\n",
       "   0.05351578745123087,\n",
       "   0.05204954302900933,\n",
       "   0.05573353128008745,\n",
       "   0.049162797646936346,\n",
       "   0.054824839077500014,\n",
       "   0.05387401950640552,\n",
       "   0.05937064315086252,\n",
       "   0.052577640343184684,\n",
       "   0.05322651995237746,\n",
       "   0.05125362104696331],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.4665240701039632,\n",
       "   1.4171582623322805,\n",
       "   1.3929247081279754,\n",
       "   1.3521079965432485,\n",
       "   1.3207840005556741,\n",
       "   1.2823985087871552,\n",
       "   1.2357146400213241,\n",
       "   1.2074770802259445,\n",
       "   1.16887910703818,\n",
       "   1.1530817288160324,\n",
       "   1.1405859416723252,\n",
       "   1.1206016393502554,\n",
       "   1.1152420143286388,\n",
       "   1.1028757802645366,\n",
       "   1.0881827984253565,\n",
       "   1.0752600787083308,\n",
       "   1.0674547817309696,\n",
       "   1.0507617690165838,\n",
       "   1.0455724114179612,\n",
       "   1.029528054992358,\n",
       "   1.0347131176789601,\n",
       "   1.0308612650632858,\n",
       "   1.0205002764860789,\n",
       "   1.0117877606550854,\n",
       "   0.9991415997346242,\n",
       "   0.9970713939269383,\n",
       "   0.9776921087503433,\n",
       "   0.9844942214091619,\n",
       "   0.971526497801145,\n",
       "   0.9629404856761297,\n",
       "   0.948158764441808,\n",
       "   0.9565264155467351,\n",
       "   0.9395616263151169,\n",
       "   0.9322134967645009,\n",
       "   0.9252809810638428,\n",
       "   0.9278887983163198,\n",
       "   0.9269906002283096,\n",
       "   0.9079518123467764,\n",
       "   0.9162568575143815,\n",
       "   0.9227782398462295,\n",
       "   0.9155866428216298,\n",
       "   0.9006667524576187,\n",
       "   0.9016995388269424,\n",
       "   0.9051644812027614,\n",
       "   0.9001533788442612,\n",
       "   0.9028488105535507,\n",
       "   0.9130249049266179,\n",
       "   0.9138602681954702,\n",
       "   0.8877618835369746,\n",
       "   0.8777974281708399,\n",
       "   0.9076733261346817,\n",
       "   0.8865848875045776,\n",
       "   0.881484682559967,\n",
       "   0.885455636382103,\n",
       "   0.8732743002971013,\n",
       "   0.8691663382450739,\n",
       "   0.8906439789136251,\n",
       "   0.8844351375102997,\n",
       "   0.8689276750882466,\n",
       "   0.8560870597759883,\n",
       "   0.8502876708904902,\n",
       "   0.8479099889596303,\n",
       "   0.8629307077328364,\n",
       "   0.8676115584373474,\n",
       "   0.850867133140564,\n",
       "   0.8470532782872517,\n",
       "   0.8576970545450846,\n",
       "   0.8460821300745011,\n",
       "   0.8614348691701889,\n",
       "   0.8581943772236507,\n",
       "   0.8761467144886652,\n",
       "   0.8548586076498031,\n",
       "   0.8423090946674346,\n",
       "   0.8468090269962947,\n",
       "   0.8592915604511897,\n",
       "   0.846453515291214,\n",
       "   0.8574012688795726,\n",
       "   0.8604395176966985,\n",
       "   0.8488959797223409,\n",
       "   0.8528792436917623,\n",
       "   0.8373437591393789,\n",
       "   0.8498614275455475,\n",
       "   0.8510965873797735,\n",
       "   0.8445712782939275,\n",
       "   0.8561002175013225,\n",
       "   0.8459126494328181,\n",
       "   0.847146147886912,\n",
       "   0.8611347327629725,\n",
       "   0.8484471168120702,\n",
       "   0.8389656205972036,\n",
       "   0.8527462327480316,\n",
       "   0.8448436069488525,\n",
       "   0.8654573482275009,\n",
       "   0.8528101329008738,\n",
       "   0.847724948724111,\n",
       "   0.8371095420916875,\n",
       "   0.853082467118899,\n",
       "   0.8638810996214549,\n",
       "   0.8563886060317357],\n",
       "  'val_loss_std': [0.08925934824660577,\n",
       "   0.0962115106736041,\n",
       "   0.09785377350198164,\n",
       "   0.10308777117396366,\n",
       "   0.10379286137959226,\n",
       "   0.1100652211658379,\n",
       "   0.10810219226848666,\n",
       "   0.115063940231856,\n",
       "   0.11814714321202346,\n",
       "   0.12110769536321783,\n",
       "   0.12240733687056339,\n",
       "   0.12659264634392622,\n",
       "   0.12382647519402466,\n",
       "   0.12968275305346055,\n",
       "   0.12844136788866767,\n",
       "   0.12429290625371377,\n",
       "   0.12871664938497682,\n",
       "   0.1266904178306537,\n",
       "   0.12727888312397284,\n",
       "   0.12889609389107845,\n",
       "   0.1313175871417351,\n",
       "   0.13201137032978313,\n",
       "   0.1259526668741782,\n",
       "   0.1296961244546495,\n",
       "   0.13350331956566044,\n",
       "   0.13289292460184882,\n",
       "   0.13145396794711273,\n",
       "   0.13060179318986692,\n",
       "   0.13357975411905498,\n",
       "   0.1334472919842112,\n",
       "   0.13183806199079628,\n",
       "   0.13489732834192797,\n",
       "   0.13689987353636482,\n",
       "   0.1342893066494463,\n",
       "   0.13439243955069743,\n",
       "   0.135543301869013,\n",
       "   0.13674488682062708,\n",
       "   0.1358373287169887,\n",
       "   0.13662543119044604,\n",
       "   0.1332164788220565,\n",
       "   0.14243757029091045,\n",
       "   0.13396603707063587,\n",
       "   0.1320232131485934,\n",
       "   0.1328392030775204,\n",
       "   0.13551865885521033,\n",
       "   0.13844065492774665,\n",
       "   0.1375059595211697,\n",
       "   0.1386343597346623,\n",
       "   0.13876031742746722,\n",
       "   0.13199150121379633,\n",
       "   0.141161827694547,\n",
       "   0.1368554390194922,\n",
       "   0.14136913080192484,\n",
       "   0.12493857512129981,\n",
       "   0.13582857057996295,\n",
       "   0.13535902917639603,\n",
       "   0.14323822959728608,\n",
       "   0.13862600489213434,\n",
       "   0.13144245184657638,\n",
       "   0.13920814706771145,\n",
       "   0.13557899393781472,\n",
       "   0.13331776433182943,\n",
       "   0.13177768400348586,\n",
       "   0.140342397160721,\n",
       "   0.14008246389525553,\n",
       "   0.13495934427253403,\n",
       "   0.13348690431098875,\n",
       "   0.13508018210927444,\n",
       "   0.1364814989770138,\n",
       "   0.12875634567051716,\n",
       "   0.13803099216237788,\n",
       "   0.13945810556226512,\n",
       "   0.1347022238355996,\n",
       "   0.13466460868966482,\n",
       "   0.1309065270280309,\n",
       "   0.1371298473825938,\n",
       "   0.13512415833957742,\n",
       "   0.13527794369829224,\n",
       "   0.1410121492868729,\n",
       "   0.13858548135255375,\n",
       "   0.13391902094933214,\n",
       "   0.13720964319277953,\n",
       "   0.14410937150890238,\n",
       "   0.1348865303337384,\n",
       "   0.1388979985155829,\n",
       "   0.14354996131970943,\n",
       "   0.13804031083444798,\n",
       "   0.14501041672857284,\n",
       "   0.13926559769320654,\n",
       "   0.133890478818652,\n",
       "   0.14732048299364692,\n",
       "   0.14037577380632232,\n",
       "   0.14494928544447935,\n",
       "   0.1362168295471893,\n",
       "   0.13970073765015756,\n",
       "   0.14332653753978378,\n",
       "   0.146693152060341,\n",
       "   0.14264648011756165,\n",
       "   0.14517821504828038],\n",
       "  'val_accuracy_mean': [0.38444444532195726,\n",
       "   0.4115555561085542,\n",
       "   0.42495555559794107,\n",
       "   0.44615555594364803,\n",
       "   0.4644888890782992,\n",
       "   0.4791555555661519,\n",
       "   0.5022000006834666,\n",
       "   0.5149777763088544,\n",
       "   0.5347777769962947,\n",
       "   0.540666664938132,\n",
       "   0.5482444435358047,\n",
       "   0.5573999987045923,\n",
       "   0.5586222209533056,\n",
       "   0.5657111112276713,\n",
       "   0.5733999998370807,\n",
       "   0.5777333334088326,\n",
       "   0.5819333322842916,\n",
       "   0.5869555561741193,\n",
       "   0.591244444946448,\n",
       "   0.6012444436550141,\n",
       "   0.5970666656891505,\n",
       "   0.6003111113111178,\n",
       "   0.6002888866265614,\n",
       "   0.6054222213228544,\n",
       "   0.6103999996185303,\n",
       "   0.6132666664322217,\n",
       "   0.6210222199559212,\n",
       "   0.6195777772863706,\n",
       "   0.6230222220222156,\n",
       "   0.6269333319862683,\n",
       "   0.6322222207983335,\n",
       "   0.6310666671395302,\n",
       "   0.6359333338340124,\n",
       "   0.6402666673064232,\n",
       "   0.6431333323319753,\n",
       "   0.6444222223758698,\n",
       "   0.6442222221692403,\n",
       "   0.6508888866504033,\n",
       "   0.6474444450934728,\n",
       "   0.6436666671435038,\n",
       "   0.6492888904611269,\n",
       "   0.6522444445888201,\n",
       "   0.6544888892769813,\n",
       "   0.6525555551052094,\n",
       "   0.6522444439927737,\n",
       "   0.6543333328763644,\n",
       "   0.6491777767737706,\n",
       "   0.6498666656017303,\n",
       "   0.6586222206552823,\n",
       "   0.6625555568933487,\n",
       "   0.6531777779261271,\n",
       "   0.6570888869961102,\n",
       "   0.6623777764042219,\n",
       "   0.6622444446881612,\n",
       "   0.6637999998529752,\n",
       "   0.6656444448232651,\n",
       "   0.6576666649182638,\n",
       "   0.6585555551449458,\n",
       "   0.6678666667143504,\n",
       "   0.6745333335796992,\n",
       "   0.6744000005722046,\n",
       "   0.6759333332379659,\n",
       "   0.6685333351294199,\n",
       "   0.6675333335002264,\n",
       "   0.6761333312590917,\n",
       "   0.6757333340247472,\n",
       "   0.670711112121741,\n",
       "   0.6745999991893769,\n",
       "   0.6738444425662359,\n",
       "   0.670933333337307,\n",
       "   0.6647111114859581,\n",
       "   0.6749999994039535,\n",
       "   0.678466666340828,\n",
       "   0.6754666682084401,\n",
       "   0.6697111110885938,\n",
       "   0.6778888899087906,\n",
       "   0.6735333324472109,\n",
       "   0.6703333316246668,\n",
       "   0.674511108994484,\n",
       "   0.6746444447835287,\n",
       "   0.6797777791817983,\n",
       "   0.6743111101786295,\n",
       "   0.6767111106713612,\n",
       "   0.6751333336035411,\n",
       "   0.6725333332022031,\n",
       "   0.6786888877550761,\n",
       "   0.6757999990383784,\n",
       "   0.673799999554952,\n",
       "   0.6739555535713831,\n",
       "   0.6764444426695506,\n",
       "   0.6765333332618078,\n",
       "   0.6791555551687877,\n",
       "   0.6720222214857737,\n",
       "   0.6716888894637426,\n",
       "   0.6760888901352883,\n",
       "   0.6818444436788559,\n",
       "   0.6746666649977366,\n",
       "   0.6732000005245209,\n",
       "   0.6749777751167615],\n",
       "  'val_accuracy_std': [0.05025957484042379,\n",
       "   0.05491092657106505,\n",
       "   0.056014860953133024,\n",
       "   0.057303915876824554,\n",
       "   0.05489921667889171,\n",
       "   0.0579440170032123,\n",
       "   0.05834803579157738,\n",
       "   0.061105550215058924,\n",
       "   0.06031634521410712,\n",
       "   0.061762986879083576,\n",
       "   0.061663598869231936,\n",
       "   0.06523958087297703,\n",
       "   0.0606033678292508,\n",
       "   0.06229301844736194,\n",
       "   0.06340577131165628,\n",
       "   0.06056685322641931,\n",
       "   0.06327104455941784,\n",
       "   0.06075626592125194,\n",
       "   0.06262711152273724,\n",
       "   0.06248383168348306,\n",
       "   0.06393969275864483,\n",
       "   0.06281701437949051,\n",
       "   0.059931363273559926,\n",
       "   0.06202454358468791,\n",
       "   0.06329229072301316,\n",
       "   0.061935416068123285,\n",
       "   0.05996165125627951,\n",
       "   0.060748173305084176,\n",
       "   0.06150501102389181,\n",
       "   0.060213841196828814,\n",
       "   0.06051833851282325,\n",
       "   0.0621864815082362,\n",
       "   0.06237683816852382,\n",
       "   0.06194085544109535,\n",
       "   0.06175546846432876,\n",
       "   0.059390728577605835,\n",
       "   0.0591716491127654,\n",
       "   0.06141741551392941,\n",
       "   0.06180365114969122,\n",
       "   0.05990888315942995,\n",
       "   0.06136905750596059,\n",
       "   0.060035789344127136,\n",
       "   0.05939820142683592,\n",
       "   0.058374165846856924,\n",
       "   0.05840217808412295,\n",
       "   0.06079260435417133,\n",
       "   0.06010908336176229,\n",
       "   0.05962212130719612,\n",
       "   0.059612559863349374,\n",
       "   0.05809432565964007,\n",
       "   0.05699231617511584,\n",
       "   0.05953623024313608,\n",
       "   0.06010957450016752,\n",
       "   0.058450356632963406,\n",
       "   0.060971797022165326,\n",
       "   0.0598503654293905,\n",
       "   0.06133846862200954,\n",
       "   0.0601441699666946,\n",
       "   0.05833781553150552,\n",
       "   0.05941228396370613,\n",
       "   0.05977528211285417,\n",
       "   0.06057425206481595,\n",
       "   0.060714853963637336,\n",
       "   0.06090664298512532,\n",
       "   0.05838198749400176,\n",
       "   0.0596547109791299,\n",
       "   0.05985858400613019,\n",
       "   0.0599631613697989,\n",
       "   0.05993976845198127,\n",
       "   0.059609990782477584,\n",
       "   0.0596237427018188,\n",
       "   0.05967846078212273,\n",
       "   0.0571043630562775,\n",
       "   0.059368629623608796,\n",
       "   0.05840654132492468,\n",
       "   0.05799989487210741,\n",
       "   0.057538047456175696,\n",
       "   0.05779946214836733,\n",
       "   0.0607674827004244,\n",
       "   0.05679514491519087,\n",
       "   0.05904633261034272,\n",
       "   0.0600537363777726,\n",
       "   0.06022302652991291,\n",
       "   0.05911085845608882,\n",
       "   0.05932669264004693,\n",
       "   0.056531075242787124,\n",
       "   0.06114212978223932,\n",
       "   0.0599154102371408,\n",
       "   0.05677687006701778,\n",
       "   0.059882189065888866,\n",
       "   0.05776694063981755,\n",
       "   0.05748454031729291,\n",
       "   0.05736355312258769,\n",
       "   0.0584817127366227,\n",
       "   0.060124250617929295,\n",
       "   0.05574018951959973,\n",
       "   0.0604844655835651,\n",
       "   0.05885560704646691,\n",
       "   0.0588831216038151],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb0c68",
   "metadata": {},
   "source": [
    "### 1.1 MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     #print(key)\n",
    "#     if value.requires_grad:\n",
    "#         print(key)\n",
    "#         print(value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a599c8",
   "metadata": {},
   "source": [
    "### 1.2 Arbiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "9ebc67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = arbiter_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = arbiter_system.state['best_epoch']\n",
    "\n",
    "state = arbiter_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "arbiter_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "569eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "0531d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "a86b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "\n",
    "x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "\n",
    "\n",
    "x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task = next(zip(x_support_set,y_support_set,x_target_set, y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "cdeb442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "647183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbiter_x_support_set, arbiter_x_target_set, arbiter_y_support_set, arbiter_y_target_set, seed = train_sample\n",
    "\n",
    "arbiter_x_support_set = torch.Tensor(arbiter_x_support_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_x_target_set = torch.Tensor(arbiter_x_target_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_y_support_set = torch.Tensor(arbiter_y_support_set).long().to(device=arbiter_system.model.device)\n",
    "arbiter_y_target_set = torch.Tensor(arbiter_y_target_set).long().to(device=arbiter_system.model.device)\n",
    "\n",
    "\n",
    "arbiter_x_support_set_task, arbiter_y_support_set_task, arbiter_x_target_set_task, arbiter_y_target_set_task = next(zip(arbiter_x_support_set,arbiter_y_support_set,arbiter_x_target_set, arbiter_y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "ce1c0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "fd4d6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = arbiter_system.model.get_inner_loop_parameter_dict(arbiter_system.model.classifier.named_parameters())\n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = arbiter_x_target_set_task.shape\n",
    "\n",
    "arbiter_x_support_set_task = arbiter_x_support_set_task.view(-1, c, h, w)\n",
    "arbiter_y_support_set_task = arbiter_y_support_set_task.view(-1)\n",
    "arbiter_x_target_set_task = arbiter_x_target_set_task.view(-1, c, h, w)\n",
    "arbiter_y_target_set_task = arbiter_y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = arbiter_system.model.net_forward(\n",
    "            x=arbiter_x_support_set_task,\n",
    "            y=arbiter_y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "    \n",
    "    if arbiter_system.model.args.arbiter:\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                retain_graph=True)\n",
    "\n",
    "        names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "\n",
    "        for key, weight in names_weights_copy.items():\n",
    "            weight_norm = torch.norm(weight, p=2)\n",
    "            per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "        for key, grad in names_grads_copy.items():\n",
    "            gradient_l2norm = torch.norm(grad, p=2)\n",
    "            per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "        per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                    per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        generated_gradient_rate = arbiter_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "            g += 1\n",
    "\n",
    "    names_weights_copy,names_grads_copy = arbiter_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                      support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_arbiter.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=arbiter_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in arbiter_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "d16650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "y_support_set_task = y_support_set_task.view(-1)\n",
    "x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "y_target_set_task = y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "            x=x_support_set_task,\n",
    "            y=y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "\n",
    "\n",
    "    names_weights_copy,names_grads_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                   support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_maml.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=maml_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575454f0",
   "metadata": {},
   "source": [
    "## landscape 함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "aec9618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape==  torch.Size([25, 3, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "tensor([[[[-1.4513e-02, -3.9422e-03, -1.7162e-03],\n",
      "          [-9.1207e-03, -8.1244e-04,  5.3192e-04],\n",
      "          [-8.4783e-03,  3.4759e-05,  2.7000e-03]],\n",
      "\n",
      "         [[-1.1269e-02, -8.5102e-04,  5.0615e-04],\n",
      "          [-5.2378e-03,  2.8049e-03,  2.4038e-03],\n",
      "          [-3.8324e-03,  4.5017e-03,  4.3436e-03]],\n",
      "\n",
      "         [[-1.1182e-03,  8.0143e-03,  8.4442e-03],\n",
      "          [ 3.5240e-03,  1.0223e-02,  9.3669e-03],\n",
      "          [ 5.2550e-03,  1.1795e-02,  1.0335e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1179e-03,  5.6974e-03,  9.6673e-03],\n",
      "          [-1.6549e-03,  4.6006e-03,  1.1504e-02],\n",
      "          [-4.5817e-03,  4.3792e-03,  1.0826e-02]],\n",
      "\n",
      "         [[ 1.0713e-02,  1.5069e-02,  1.9473e-02],\n",
      "          [ 5.9756e-03,  1.3212e-02,  1.9859e-02],\n",
      "          [ 2.6845e-03,  1.2456e-02,  1.8250e-02]],\n",
      "\n",
      "         [[ 1.8694e-02,  2.2974e-02,  2.7105e-02],\n",
      "          [ 1.5027e-02,  2.0937e-02,  2.6131e-02],\n",
      "          [ 1.0639e-02,  1.8903e-02,  2.3504e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.2711e-04, -1.0085e-03,  1.0877e-03],\n",
      "          [-2.3405e-03, -2.0332e-03,  2.1525e-04],\n",
      "          [-1.6059e-03, -3.7506e-03, -2.0221e-03]],\n",
      "\n",
      "         [[ 7.0432e-04,  2.8905e-04,  2.2610e-03],\n",
      "          [-8.6424e-04, -8.6224e-04,  1.4885e-03],\n",
      "          [ 4.4919e-04, -2.3668e-03, -8.5393e-04]],\n",
      "\n",
      "         [[ 7.4053e-03,  7.0681e-03,  8.6952e-03],\n",
      "          [ 6.4268e-03,  6.5691e-03,  8.9848e-03],\n",
      "          [ 8.9131e-03,  5.9242e-03,  7.5233e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.0010e-03, -4.1890e-03, -6.5599e-03],\n",
      "          [-5.5837e-03, -4.9693e-03, -1.0757e-02],\n",
      "          [-9.9322e-03, -1.2091e-02, -1.7007e-02]],\n",
      "\n",
      "         [[-7.4559e-03, -4.3926e-03, -5.8175e-03],\n",
      "          [-6.4507e-03, -5.3952e-03, -1.0645e-02],\n",
      "          [-1.0507e-02, -1.2360e-02, -1.7304e-02]],\n",
      "\n",
      "         [[-1.6259e-02, -1.2453e-02, -1.2968e-02],\n",
      "          [-1.6171e-02, -1.4076e-02, -1.8054e-02],\n",
      "          [-2.0486e-02, -2.1306e-02, -2.4908e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9112e-03,  7.3496e-04, -1.3482e-03],\n",
      "          [ 2.6596e-03,  5.9585e-04, -9.7871e-04],\n",
      "          [ 3.4605e-03,  9.4690e-04, -1.0716e-03]],\n",
      "\n",
      "         [[-8.8871e-04, -2.2949e-03, -4.5232e-03],\n",
      "          [-1.6929e-04, -2.4588e-03, -4.2325e-03],\n",
      "          [ 5.3332e-04, -2.0419e-03, -4.1282e-03]],\n",
      "\n",
      "         [[-1.8492e-03, -2.9105e-03, -4.7744e-03],\n",
      "          [-1.6134e-03, -3.2906e-03, -4.4956e-03],\n",
      "          [-9.1194e-04, -2.8457e-03, -4.2823e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2917e-03,  5.5680e-03,  4.3690e-03],\n",
      "          [ 6.4931e-03,  6.7303e-03,  6.8844e-03],\n",
      "          [ 7.0737e-03,  6.7169e-03,  7.5152e-03]],\n",
      "\n",
      "         [[ 3.7999e-03,  4.1451e-03,  3.1323e-03],\n",
      "          [ 5.7390e-03,  6.3187e-03,  6.4012e-03],\n",
      "          [ 6.4215e-03,  6.6729e-03,  7.0515e-03]],\n",
      "\n",
      "         [[ 2.6395e-04,  6.1392e-04,  4.3886e-04],\n",
      "          [ 2.8996e-03,  3.4918e-03,  4.5791e-03],\n",
      "          [ 4.1940e-03,  4.4155e-03,  5.3723e-03]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.1332e-03, -3.6543e-04, -1.1442e-04, -4.2247e-04, -1.2164e-03,\n",
      "        -1.5529e-03,  1.2145e-03,  3.8108e-04,  1.3190e-03,  8.1559e-04,\n",
      "         4.5604e-05, -2.8829e-04, -9.4690e-05, -4.9664e-04, -5.3244e-05,\n",
      "        -1.6540e-04, -1.4078e-04,  7.8835e-04, -1.9689e-04,  2.5927e-04,\n",
      "         5.0820e-04, -3.2375e-03, -9.5054e-04, -5.9110e-04,  2.0406e-04,\n",
      "         2.9882e-03, -1.9950e-04,  1.2237e-03, -2.2899e-04,  8.0741e-04,\n",
      "         4.5383e-04, -2.2067e-03, -3.8964e-05, -1.9315e-04,  1.6296e-03,\n",
      "         1.1054e-04,  1.6120e-03,  1.2557e-04, -9.5821e-04,  5.7879e-04,\n",
      "         7.6483e-05,  6.6188e-04,  1.0199e-03, -1.2368e-03, -5.6136e-04,\n",
      "         1.1551e-03,  7.3650e-05, -9.4351e-04,  1.0055e-03, -1.3417e-04,\n",
      "         7.1041e-05,  1.2634e-04,  2.0360e-06,  1.2439e-04, -1.5711e-04,\n",
      "         2.4125e-03,  2.3238e-04, -1.1731e-04, -8.3986e-05,  5.3769e-04,\n",
      "         3.0449e-04, -2.7191e-04, -1.2038e-04, -6.0287e-04, -4.3179e-04,\n",
      "         1.4832e-03, -1.1622e-03, -8.1683e-04, -6.7919e-04, -3.3698e-04,\n",
      "        -1.2925e-03,  8.9794e-05, -2.0930e-03, -1.5222e-03,  3.2234e-04,\n",
      "        -4.5648e-04,  2.2456e-04, -6.1671e-04, -1.1690e-03, -1.1669e-04,\n",
      "        -4.0588e-05,  2.2304e-04, -3.8085e-04, -1.3391e-04, -3.4695e-04,\n",
      "         4.4908e-04, -5.0698e-05,  7.5716e-04,  2.5428e-04,  1.0531e-03,\n",
      "         8.6401e-04, -7.8557e-04, -3.8239e-04,  2.2998e-04,  6.3750e-04,\n",
      "        -5.5597e-04,  7.1402e-04, -1.0299e-04,  6.5622e-04, -1.1755e-03,\n",
      "         7.3881e-04,  2.6570e-04,  1.2987e-04,  1.0351e-03, -5.5632e-04,\n",
      "         1.7135e-03,  3.5053e-04,  6.3891e-04,  5.0281e-04,  5.7081e-04,\n",
      "        -7.4202e-04, -3.7262e-04, -1.2479e-03, -1.0781e-03,  1.1188e-03,\n",
      "         3.0433e-04, -7.8745e-04, -1.1793e-04, -3.3185e-04,  2.8249e-04,\n",
      "         7.2108e-04,  4.5263e-03,  5.6771e-04, -1.3784e-03,  3.4447e-04,\n",
      "         8.3019e-04, -2.0487e-03, -1.9800e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-3.0536e-04,  3.3387e-04,  4.5947e-04,  2.3553e-03, -3.2858e-04,\n",
      "         4.4892e-04,  1.5560e-03,  7.6657e-04,  3.3170e-04,  1.1809e-03,\n",
      "         4.6153e-05, -1.0808e-03,  1.5994e-04, -8.4956e-04,  1.2336e-03,\n",
      "         1.6729e-03, -1.2393e-04, -2.2400e-05, -1.0958e-03,  8.0185e-04,\n",
      "         6.5516e-04, -3.5143e-03, -1.4311e-03,  1.0609e-03,  4.1388e-04,\n",
      "         2.4388e-03, -8.1036e-04, -1.5244e-03,  3.6764e-04,  1.5787e-03,\n",
      "         8.5863e-04, -4.4295e-03, -2.1518e-03,  4.5041e-04,  1.0404e-03,\n",
      "         1.0870e-03,  2.2767e-03,  7.7121e-04, -2.4267e-04,  1.2822e-03,\n",
      "         1.2295e-03, -9.9639e-04,  2.3000e-03, -1.6035e-03, -7.2734e-04,\n",
      "         2.5985e-03, -1.7538e-03, -1.7040e-03,  2.0974e-03, -2.2748e-03,\n",
      "         1.4913e-03,  1.7298e-03,  3.2356e-04, -1.1898e-03,  1.4652e-03,\n",
      "         1.0144e-03,  1.0663e-03,  5.2536e-04, -4.9880e-04, -1.5090e-03,\n",
      "         1.0117e-03, -6.3238e-04, -1.8134e-03, -5.6115e-04, -9.2194e-04,\n",
      "         1.0077e-03, -1.7434e-03, -8.6985e-04,  1.5678e-03, -1.6465e-04,\n",
      "        -6.8197e-04, -2.0650e-05, -2.1884e-03, -5.8244e-04, -6.7564e-04,\n",
      "        -1.5735e-03, -8.0789e-04, -1.3873e-03,  6.4698e-04, -1.0898e-04,\n",
      "        -4.3414e-04, -1.1616e-03, -2.4021e-04,  2.1710e-05,  3.2425e-04,\n",
      "        -1.0502e-03,  5.0512e-04,  1.4567e-03, -6.0975e-04, -6.1963e-05,\n",
      "         1.2990e-03, -7.5123e-04,  2.8086e-04, -1.9632e-03,  1.0510e-03,\n",
      "        -1.4298e-03,  5.6279e-04,  9.6792e-04,  1.3399e-04, -7.5472e-04,\n",
      "        -3.3505e-04, -1.7321e-03,  1.0307e-03,  1.2722e-03,  1.0005e-03,\n",
      "         7.2243e-04, -2.2781e-04,  6.3882e-04, -5.7219e-04, -1.2643e-03,\n",
      "        -2.2165e-04, -1.7358e-03, -1.2154e-03, -4.1462e-04,  1.2285e-03,\n",
      "         1.4795e-03, -2.7045e-04, -8.2517e-04, -6.2733e-04,  4.8761e-04,\n",
      "         9.4212e-04,  4.8364e-03,  4.9513e-04, -1.4945e-03, -1.0744e-03,\n",
      "         1.0146e-03, -2.1030e-03,  7.0119e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-8.4200e-04, -1.4592e-03, -9.0341e-05,  4.9080e-03, -1.9397e-03,\n",
      "        -2.1200e-03, -1.5818e-03,  1.3908e-03,  4.9006e-04,  5.2663e-04,\n",
      "         9.4391e-04,  2.2390e-03, -8.2562e-04, -6.3769e-04,  4.4170e-03,\n",
      "        -1.4984e-03, -3.7989e-04,  1.8717e-04,  2.1295e-03,  6.7743e-04,\n",
      "        -3.6126e-04,  4.5318e-03, -2.6590e-03, -1.7694e-03,  8.3460e-04,\n",
      "        -2.0552e-03, -2.8055e-03,  1.0663e-03, -2.9108e-04,  5.0212e-04,\n",
      "        -1.0315e-03,  1.6813e-03,  3.4068e-04, -4.0139e-05,  1.7493e-03,\n",
      "         2.4045e-03, -1.6604e-03, -3.1098e-04, -2.0099e-03,  6.1940e-04,\n",
      "         1.1414e-03,  1.2433e-03, -2.3555e-03,  1.1402e-03, -2.1687e-03,\n",
      "         7.3156e-04,  1.8841e-03,  2.2692e-04,  2.2180e-03, -8.0388e-04,\n",
      "         1.7784e-03,  1.8857e-04, -1.4082e-03, -3.2416e-04,  1.2470e-03,\n",
      "        -6.0279e-05, -1.3298e-03,  1.9557e-03, -5.2673e-04,  3.1004e-04,\n",
      "        -5.7334e-03, -6.4298e-04,  3.3229e-04, -9.1904e-04,  2.5918e-05,\n",
      "        -2.4698e-04,  2.2264e-03,  2.1980e-03, -2.2586e-03,  2.3047e-03,\n",
      "         8.0880e-04,  1.0695e-03, -3.5770e-04,  1.0045e-03, -1.1828e-03,\n",
      "        -3.9463e-04, -4.8452e-05,  7.2272e-04, -6.6529e-04, -1.0054e-03,\n",
      "         2.9668e-03, -1.1069e-03,  1.0572e-03, -3.8794e-04,  2.9929e-05,\n",
      "        -1.1624e-03,  1.0607e-03, -4.8604e-04,  1.3364e-03, -1.8851e-03,\n",
      "        -4.4640e-03, -3.8950e-04, -3.5790e-03,  6.5665e-03,  1.7118e-03,\n",
      "         1.2956e-03,  6.0773e-04,  4.8039e-03, -1.0218e-03, -1.8392e-03,\n",
      "         2.5943e-05, -6.2352e-04, -8.1250e-05, -1.6128e-03,  2.0893e-03,\n",
      "         1.7549e-03,  1.9534e-03,  7.9676e-04,  1.5223e-04, -5.4590e-03,\n",
      "         3.1459e-03,  1.8015e-03,  2.8580e-03,  3.2258e-03,  1.4046e-03,\n",
      "         1.2871e-03, -1.3005e-04,  1.2481e-03, -2.1980e-04,  1.0702e-03,\n",
      "        -1.0343e-03,  6.7527e-04,  3.7013e-04,  4.7062e-06,  1.9509e-03,\n",
      "         1.5894e-04, -3.1019e-04,  4.8983e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 7.5665e-04, -1.7252e-03, -4.0200e-05,  3.6769e-03, -4.2316e-04,\n",
      "        -2.7320e-03, -2.8403e-03,  2.4660e-03, -1.0447e-04, -2.3705e-03,\n",
      "         1.4712e-03,  2.5836e-03, -2.6380e-03, -1.1138e-03,  4.5886e-03,\n",
      "        -8.5841e-04, -1.2051e-03,  6.8922e-04,  2.6651e-03,  7.4268e-04,\n",
      "        -3.6453e-04,  5.0893e-03, -4.3164e-03, -1.2841e-03,  2.7140e-03,\n",
      "        -2.1935e-03, -1.4630e-03, -2.1780e-04, -1.4141e-03, -1.2552e-03,\n",
      "        -4.3058e-03, -1.1082e-04, -1.7470e-03,  3.5502e-03,  1.6146e-03,\n",
      "         3.3093e-03, -5.5786e-03,  2.3425e-04, -1.0199e-03,  8.4557e-04,\n",
      "         8.5948e-04,  1.9257e-03, -9.9276e-05, -2.5261e-03, -5.8262e-03,\n",
      "        -9.2526e-04,  9.6594e-04,  5.1744e-04,  2.4036e-03, -1.4295e-03,\n",
      "         4.2570e-03, -3.1496e-03, -1.3429e-03, -1.3756e-03,  7.1382e-04,\n",
      "         9.2327e-04, -2.6183e-03, -4.5954e-04,  2.9776e-04, -1.5803e-03,\n",
      "        -3.5962e-03, -1.7795e-03, -3.8507e-03,  8.8674e-04,  9.6629e-04,\n",
      "         5.5356e-04,  2.8903e-03,  4.2918e-03, -1.7246e-03,  1.5357e-03,\n",
      "         1.2457e-03,  2.0557e-04, -2.3283e-04, -1.2254e-03, -2.2011e-03,\n",
      "         2.0699e-03,  1.8032e-03,  6.9525e-04, -2.6210e-03, -1.4078e-03,\n",
      "         3.3808e-03, -7.2477e-04,  1.4169e-03, -4.0546e-03, -5.5390e-04,\n",
      "        -2.1825e-03,  8.5442e-04, -1.0846e-03,  2.2219e-03, -3.2843e-03,\n",
      "        -4.6176e-03, -2.5346e-03, -2.3004e-03,  5.4703e-03, -2.1660e-03,\n",
      "         3.5787e-03,  6.7587e-04,  3.1710e-03,  2.4633e-03,  8.0286e-04,\n",
      "        -9.9011e-04, -1.3408e-03, -3.3130e-04, -3.7419e-03,  3.6294e-03,\n",
      "         1.0997e-03,  2.7786e-03,  3.0319e-03,  9.6416e-04, -7.2429e-03,\n",
      "         2.4754e-03,  9.1332e-05,  4.6746e-03,  4.0477e-03,  2.5024e-03,\n",
      "         2.0570e-03,  9.2916e-05,  1.0794e-03, -1.3896e-03, -4.7122e-04,\n",
      "        -8.1048e-04, -8.8505e-04,  1.0312e-03, -2.8411e-04,  2.1675e-03,\n",
      "        -2.6601e-05, -1.2961e-03,  1.5710e-03], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 4.0307e-05,  9.0452e-04, -3.9088e-05],\n",
      "          [-6.5251e-04, -5.6091e-04,  2.1747e-03],\n",
      "          [ 1.0560e-03,  3.3251e-03,  2.3759e-03]],\n",
      "\n",
      "         [[-6.9858e-05,  4.9116e-04,  1.6383e-03],\n",
      "          [ 1.2634e-03,  1.7013e-04, -1.5782e-04],\n",
      "          [ 1.4977e-04,  8.1581e-04,  9.9417e-05]],\n",
      "\n",
      "         [[-3.3796e-04,  1.8622e-03, -7.1571e-04],\n",
      "          [-2.6560e-04,  1.5592e-03, -1.4308e-03],\n",
      "          [-1.2283e-03,  1.7631e-03, -6.2014e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5687e-03, -7.1481e-04, -1.1382e-03],\n",
      "          [-2.2660e-03,  2.0475e-03, -2.3942e-03],\n",
      "          [ 9.7688e-04,  3.7943e-03,  1.7718e-03]],\n",
      "\n",
      "         [[-6.6525e-04, -7.2435e-05, -2.3092e-04],\n",
      "          [-1.5267e-04, -4.3275e-04, -9.4629e-04],\n",
      "          [-6.8752e-04,  7.2714e-04, -6.5980e-04]],\n",
      "\n",
      "         [[-1.0401e-04,  8.1604e-04, -9.4424e-04],\n",
      "          [ 6.1488e-04,  1.2426e-04, -1.6539e-03],\n",
      "          [ 7.7871e-05,  2.4682e-03, -1.1356e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7995e-04,  1.2329e-03,  1.7623e-03],\n",
      "          [ 3.4211e-04,  1.3318e-04, -5.3081e-05],\n",
      "          [ 6.4936e-04, -3.7460e-04, -5.1462e-04]],\n",
      "\n",
      "         [[-9.6118e-04, -8.2094e-04,  1.6417e-05],\n",
      "          [-1.6029e-03, -1.4350e-03, -1.4723e-03],\n",
      "          [-8.3015e-04, -1.0711e-03, -1.5048e-03]],\n",
      "\n",
      "         [[ 2.3279e-04,  1.2636e-03,  1.4492e-03],\n",
      "          [-1.3989e-04,  9.4251e-04,  1.5364e-03],\n",
      "          [-3.9265e-04, -6.0519e-04,  2.4204e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3791e-03,  4.9271e-04,  1.3619e-03],\n",
      "          [-2.6302e-04, -9.1957e-04, -1.5199e-03],\n",
      "          [ 2.2848e-04, -1.8703e-04,  7.0145e-04]],\n",
      "\n",
      "         [[-4.0508e-04,  5.1307e-04,  6.9517e-05],\n",
      "          [-1.1583e-03, -6.2536e-04, -5.0352e-04],\n",
      "          [-1.7550e-04, -5.0564e-04, -4.1085e-04]],\n",
      "\n",
      "         [[ 6.6942e-04,  1.0895e-03, -9.9836e-04],\n",
      "          [-2.9855e-04, -5.1388e-04, -3.4571e-04],\n",
      "          [-3.1845e-04, -1.2082e-03, -7.7502e-04]]],\n",
      "\n",
      "\n",
      "        [[[-4.4317e-04, -4.1297e-04, -4.0569e-04],\n",
      "          [ 4.8596e-04,  2.6074e-04, -5.0528e-04],\n",
      "          [-2.6376e-04, -2.2678e-04, -3.7596e-04]],\n",
      "\n",
      "         [[-2.9411e-04, -6.6775e-04, -9.8085e-04],\n",
      "          [-1.1161e-04,  1.1913e-04, -6.2190e-05],\n",
      "          [-3.6481e-04,  1.0234e-04, -4.3820e-04]],\n",
      "\n",
      "         [[-1.0418e-03, -3.1194e-04, -5.6061e-04],\n",
      "          [-6.8985e-04,  4.1051e-04,  9.6272e-06],\n",
      "          [-7.3236e-04, -3.5648e-04, -9.1601e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0818e-03, -7.8562e-04, -6.5923e-04],\n",
      "          [ 4.1628e-04,  9.3906e-04, -1.1418e-04],\n",
      "          [ 2.7486e-04,  3.7826e-04,  5.2459e-04]],\n",
      "\n",
      "         [[-1.9851e-04,  5.2059e-04,  8.0563e-06],\n",
      "          [ 1.0572e-03,  8.6985e-04,  5.2656e-05],\n",
      "          [ 5.9878e-04,  7.0434e-04,  5.8620e-04]],\n",
      "\n",
      "         [[-1.5557e-04,  4.2816e-06, -2.7456e-04],\n",
      "          [-5.6461e-04,  1.7499e-04, -1.2195e-03],\n",
      "          [ 8.3073e-04,  5.8568e-04, -7.0412e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.8668e-03, -2.5081e-03, -1.6105e-03],\n",
      "          [-1.6289e-03, -1.4147e-03, -1.7173e-03],\n",
      "          [-1.5496e-03, -2.3022e-03,  7.9584e-04]],\n",
      "\n",
      "         [[ 2.6418e-04,  7.7358e-04,  8.7771e-04],\n",
      "          [-7.9894e-04,  2.1325e-04,  1.1262e-03],\n",
      "          [-5.7890e-04, -1.5027e-04,  6.6020e-04]],\n",
      "\n",
      "         [[ 5.1997e-04,  5.8856e-04,  2.3080e-04],\n",
      "          [-4.4282e-05,  1.3921e-03,  8.2066e-04],\n",
      "          [-8.0202e-04,  7.9232e-04,  9.5983e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7332e-05, -3.3810e-04, -8.4792e-04],\n",
      "          [-7.9943e-04, -1.4614e-03, -1.0539e-03],\n",
      "          [-1.7517e-04,  2.5378e-04, -2.3397e-04]],\n",
      "\n",
      "         [[-4.3863e-04, -4.3717e-04, -4.6824e-04],\n",
      "          [-1.5125e-03, -2.9434e-04, -1.0279e-03],\n",
      "          [-6.3767e-04, -8.2861e-04, -5.9783e-04]],\n",
      "\n",
      "         [[ 5.3580e-04, -1.0727e-03,  6.0260e-04],\n",
      "          [-7.4907e-04, -1.2306e-03, -4.7813e-04],\n",
      "          [-3.5341e-04, -1.2279e-03,  6.4204e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3798e-04,  5.2990e-05,  1.1125e-03],\n",
      "          [ 3.0126e-04, -1.5254e-04,  1.0305e-03],\n",
      "          [ 2.3569e-03,  1.1959e-03,  9.0768e-04]],\n",
      "\n",
      "         [[-7.9578e-05,  5.4574e-04,  5.7886e-04],\n",
      "          [ 4.6533e-04, -8.7149e-04, -9.7466e-04],\n",
      "          [ 1.2769e-04, -1.4711e-03, -1.0112e-03]],\n",
      "\n",
      "         [[-7.9969e-04,  5.4455e-04,  2.0321e-03],\n",
      "          [-1.3445e-03, -1.5307e-04,  1.8795e-03],\n",
      "          [-6.6540e-04, -1.5214e-04,  1.1202e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6201e-04,  3.3507e-04,  1.3800e-03],\n",
      "          [-9.2606e-04, -1.1274e-03, -2.3450e-04],\n",
      "          [-5.3248e-04, -5.8594e-04,  9.5444e-04]],\n",
      "\n",
      "         [[-1.0491e-03, -6.1633e-04,  2.4681e-04],\n",
      "          [-6.6949e-04, -6.0962e-04,  1.1936e-04],\n",
      "          [-5.2193e-04, -3.7379e-04,  7.1898e-05]],\n",
      "\n",
      "         [[ 2.8509e-04, -4.8593e-04, -4.5088e-04],\n",
      "          [ 1.3555e-03, -2.5360e-04, -5.1916e-05],\n",
      "          [ 2.5692e-04, -2.9691e-04,  5.8764e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6160e-03, -7.1473e-04, -6.1810e-04],\n",
      "          [ 1.0843e-03,  1.2170e-03, -1.7763e-03],\n",
      "          [-1.1788e-03, -6.4812e-04, -1.0221e-03]],\n",
      "\n",
      "         [[-1.7584e-03, -9.4078e-04, -3.4359e-04],\n",
      "          [ 3.3550e-04, -2.7523e-04, -1.5465e-03],\n",
      "          [ 6.8599e-04,  7.3728e-05, -1.4499e-04]],\n",
      "\n",
      "         [[ 6.7673e-04, -4.7723e-04, -5.9636e-04],\n",
      "          [ 9.4602e-04,  3.0731e-04, -4.6570e-04],\n",
      "          [-6.0300e-05,  2.6731e-04, -2.0087e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0296e-03,  1.1832e-03, -1.2528e-03],\n",
      "          [ 3.2993e-03,  2.1941e-03, -2.1568e-03],\n",
      "          [ 6.1532e-04, -7.6945e-04, -2.2779e-04]],\n",
      "\n",
      "         [[-9.8769e-04, -7.8134e-04, -7.5605e-04],\n",
      "          [ 1.7712e-03,  9.6386e-04, -1.1694e-03],\n",
      "          [-1.5869e-04, -2.9688e-04, -5.9426e-04]],\n",
      "\n",
      "         [[ 1.5808e-03, -1.1517e-03,  4.4770e-04],\n",
      "          [ 1.8548e-03, -1.0049e-03, -3.4435e-04],\n",
      "          [ 1.1121e-03,  1.7205e-04,  5.7805e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 0.0000e+00, -5.8208e-11, -2.6193e-10,  1.1642e-10,  1.4552e-10,\n",
      "        -1.3097e-10,  0.0000e+00,  3.4925e-10, -1.7462e-10,  2.3283e-10,\n",
      "         2.9104e-11,  1.1642e-10, -8.7311e-11, -2.3283e-10, -2.9104e-11,\n",
      "        -5.8208e-11,  2.3283e-10, -1.1642e-10,  8.7311e-11,  1.1642e-10,\n",
      "        -7.2760e-11,  3.4925e-10, -1.4552e-10,  0.0000e+00, -2.7649e-10,\n",
      "        -1.6007e-10, -2.3283e-10,  2.6193e-10,  5.8208e-11,  6.5484e-11,\n",
      "        -4.6566e-10, -5.3842e-10, -5.8208e-10,  1.1642e-10,  2.0373e-10,\n",
      "         1.1642e-10,  2.9104e-11,  5.8208e-11,  1.7462e-10,  0.0000e+00,\n",
      "        -2.3283e-10, -5.8208e-11, -5.8208e-10,  2.3283e-10, -1.1642e-10,\n",
      "         0.0000e+00, -4.6566e-10,  2.1828e-10,  2.9104e-11,  1.1642e-10,\n",
      "         0.0000e+00, -8.7311e-11,  5.8208e-11, -2.9104e-10,  1.8626e-09,\n",
      "        -1.1642e-10,  3.4925e-10, -7.2760e-11, -5.8208e-11,  1.1642e-10,\n",
      "         5.8208e-11, -2.7649e-10,  0.0000e+00, -7.2760e-11,  0.0000e+00,\n",
      "        -1.1642e-10, -5.8208e-11,  2.6193e-10, -4.5111e-10, -8.7311e-11,\n",
      "        -2.3283e-10,  0.0000e+00,  1.4552e-11,  8.7311e-10, -2.9104e-10,\n",
      "         5.8208e-11, -1.4552e-10,  2.1828e-10, -5.8208e-11,  5.8208e-11,\n",
      "         1.7462e-10, -1.7462e-10,  4.6566e-10,  2.9104e-11,  5.8208e-10,\n",
      "        -1.1642e-10,  1.4552e-10, -3.4925e-10,  1.1642e-10, -3.2014e-10,\n",
      "         0.0000e+00, -3.4925e-10,  2.3283e-10, -2.1828e-11,  5.8208e-11,\n",
      "         1.4552e-10,  0.0000e+00,  2.9104e-11, -1.4552e-10, -2.3283e-10,\n",
      "         1.3824e-10,  1.1642e-10,  4.0745e-10, -2.3283e-10, -8.7311e-11,\n",
      "         3.4925e-10,  0.0000e+00,  8.0036e-11, -4.3656e-10,  0.0000e+00,\n",
      "         2.3283e-10, -5.8208e-11, -7.2760e-12,  1.1642e-10,  2.3283e-10,\n",
      "         1.1642e-10,  0.0000e+00,  8.7311e-11,  2.3283e-10,  1.1642e-10,\n",
      "        -4.6566e-10, -1.1642e-10,  0.0000e+00, -2.3283e-10, -1.1642e-10,\n",
      "         0.0000e+00, -3.7835e-10, -5.8208e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[ 0.0058,  0.0346,  0.0427,  ...,  0.0525,  0.0585,  0.0263],\n",
      "        [ 0.0103,  0.0140,  0.0238,  ...,  0.0038,  0.0119,  0.0231],\n",
      "        [-0.0009, -0.0015,  0.0068,  ...,  0.0014, -0.0037, -0.0133],\n",
      "        [-0.0163, -0.0550, -0.0921,  ..., -0.0754, -0.0718, -0.0377],\n",
      "        [ 0.0011,  0.0079,  0.0188,  ...,  0.0178,  0.0051,  0.0016]],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([ 0.0319,  0.0177, -0.0009, -0.0573,  0.0086], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.5709e-04, -1.4566e-05, -2.6225e-04],\n",
      "          [-7.2068e-05,  1.1748e-04,  3.1795e-06],\n",
      "          [ 5.4846e-04,  6.5255e-04,  5.2674e-04]],\n",
      "\n",
      "         [[ 2.5165e-03,  2.6160e-03,  2.3441e-03],\n",
      "          [ 2.7055e-03,  2.8533e-03,  2.7365e-03],\n",
      "          [ 3.3239e-03,  3.3689e-03,  3.2300e-03]],\n",
      "\n",
      "         [[ 1.5914e-03,  1.6286e-03,  1.4330e-03],\n",
      "          [ 1.8102e-03,  1.9039e-03,  1.9030e-03],\n",
      "          [ 2.3205e-03,  2.3843e-03,  2.3298e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4271e-03,  1.8346e-03,  1.6988e-03],\n",
      "          [ 1.2566e-03,  1.6625e-03,  1.7635e-03],\n",
      "          [ 1.3894e-03,  1.7295e-03,  1.7827e-03]],\n",
      "\n",
      "         [[ 1.9874e-03,  2.3797e-03,  2.1997e-03],\n",
      "          [ 1.7658e-03,  2.1106e-03,  2.2106e-03],\n",
      "          [ 1.8563e-03,  2.1010e-03,  2.1574e-03]],\n",
      "\n",
      "         [[ 2.9935e-04,  6.7520e-04,  5.3403e-04],\n",
      "          [ 2.6478e-04,  5.9573e-04,  6.5379e-04],\n",
      "          [ 3.5801e-04,  6.2111e-04,  5.9201e-04]]],\n",
      "\n",
      "\n",
      "        [[[-2.1211e-03, -1.8960e-03, -1.8020e-03],\n",
      "          [-2.0038e-03, -1.6852e-03, -1.6590e-03],\n",
      "          [-1.6665e-03, -1.1991e-03, -1.2406e-03]],\n",
      "\n",
      "         [[-1.3164e-03, -9.4582e-04, -1.0644e-03],\n",
      "          [-1.4077e-03, -9.6408e-04, -1.0782e-03],\n",
      "          [-8.1250e-04, -3.7437e-04, -6.2733e-04]],\n",
      "\n",
      "         [[-7.4245e-04, -7.5588e-04, -5.8304e-04],\n",
      "          [-2.7729e-04, -1.1541e-04, -4.7442e-05],\n",
      "          [-5.3732e-04, -2.5494e-04, -3.0060e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.1435e-04,  4.5356e-05, -3.4879e-05],\n",
      "          [ 1.7624e-04, -3.2991e-05, -8.6575e-05],\n",
      "          [ 1.8991e-04, -5.8047e-05, -4.0623e-05]],\n",
      "\n",
      "         [[ 3.2020e-04,  9.4628e-06, -1.7467e-04],\n",
      "          [ 2.0061e-04, -5.1386e-05, -1.5381e-04],\n",
      "          [ 2.5878e-04, -3.6189e-05, -6.2502e-05]],\n",
      "\n",
      "         [[ 1.8093e-04, -9.3068e-06,  1.9869e-04],\n",
      "          [ 1.2807e-04,  7.6340e-05,  3.2144e-04],\n",
      "          [ 6.7052e-05,  7.4135e-05,  3.3087e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8482e-03,  2.7695e-03,  2.5011e-03],\n",
      "          [ 2.9145e-03,  2.8460e-03,  2.6707e-03],\n",
      "          [ 2.9639e-03,  2.9420e-03,  2.8898e-03]],\n",
      "\n",
      "         [[ 2.9842e-03,  2.8029e-03,  2.4974e-03],\n",
      "          [ 3.1252e-03,  2.9594e-03,  2.7538e-03],\n",
      "          [ 3.2493e-03,  3.1460e-03,  3.0380e-03]],\n",
      "\n",
      "         [[ 9.7735e-04,  7.3548e-04,  4.0151e-04],\n",
      "          [ 8.0219e-04,  5.6418e-04,  3.1184e-04],\n",
      "          [ 7.1506e-04,  5.5256e-04,  4.6002e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 7.3549e-06,  2.4635e-05, -3.2640e-05],\n",
      "          [ 1.0328e-04,  1.5289e-04,  3.7998e-05],\n",
      "          [ 1.4113e-04,  1.8965e-04,  4.3452e-05]],\n",
      "\n",
      "         [[-6.2409e-05, -3.4127e-05, -9.9825e-05],\n",
      "          [ 1.9141e-05,  7.0901e-05, -5.2717e-05],\n",
      "          [ 5.2064e-05,  1.2089e-04, -1.7565e-05]],\n",
      "\n",
      "         [[ 1.2897e-04,  1.5308e-04,  1.1978e-04],\n",
      "          [ 2.1119e-04,  2.4983e-04,  1.7845e-04],\n",
      "          [ 1.8834e-04,  2.5545e-04,  1.7411e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 2.6708e-05,  3.5249e-05,  4.5622e-05,  6.6936e-05,  1.3653e-04,\n",
      "        -1.3301e-05,  4.2072e-06,  8.1109e-05,  4.9059e-05,  5.2277e-05,\n",
      "         7.6830e-05,  6.6530e-05, -1.5187e-05, -2.8383e-05,  1.4751e-04,\n",
      "         3.3373e-04, -8.4211e-06,  1.0638e-04,  7.2048e-05,  3.0062e-05,\n",
      "        -5.8008e-05, -1.4490e-04, -3.5176e-05,  3.3668e-05, -4.6210e-05,\n",
      "         7.9607e-05, -3.5418e-05, -1.6970e-05,  2.8142e-05,  6.7770e-05,\n",
      "        -1.2875e-05,  1.4772e-06,  1.0978e-04,  1.4517e-04, -6.3622e-05,\n",
      "         1.6476e-04, -1.7755e-05,  9.1139e-05, -1.8224e-04, -1.1863e-04,\n",
      "        -1.1386e-04, -4.9473e-05, -9.7404e-05,  8.9491e-05,  1.4187e-04,\n",
      "        -2.1185e-05, -1.5479e-06,  9.8076e-05, -4.4609e-05, -4.3666e-05,\n",
      "         5.4337e-05, -1.1919e-04,  2.1847e-05, -1.6708e-04, -6.5264e-07,\n",
      "         8.1227e-05,  2.0761e-04,  1.3199e-04, -1.4571e-05, -1.2937e-04,\n",
      "        -7.8846e-06,  4.1808e-06, -1.9302e-05,  1.2123e-04,  7.7857e-05,\n",
      "         1.7067e-04,  3.1765e-05,  3.8783e-05,  3.8266e-04, -7.2582e-05,\n",
      "         1.1250e-04, -3.1245e-05,  5.3890e-05, -5.7135e-05,  6.1994e-05,\n",
      "         1.0845e-05, -2.6964e-05, -2.3056e-04,  5.4533e-04,  1.4511e-04,\n",
      "        -1.1083e-04,  2.7391e-05, -2.7814e-05,  6.7923e-05, -2.0044e-05,\n",
      "        -1.3359e-04,  1.1666e-05,  7.9452e-05, -4.1599e-05, -1.9981e-04,\n",
      "        -5.1732e-05, -1.6085e-05, -2.4130e-05,  6.8336e-05,  1.2319e-04,\n",
      "         3.9258e-06, -1.9163e-04, -1.6526e-06,  6.6067e-05,  4.1570e-05,\n",
      "         9.7239e-05, -1.0736e-05, -7.1199e-05,  4.4805e-05,  1.1558e-04,\n",
      "         5.2875e-05,  2.2826e-05,  2.1880e-04, -1.3150e-05, -1.4216e-04,\n",
      "         3.2392e-04, -2.7385e-05, -3.5219e-05, -6.6347e-05, -1.3053e-04,\n",
      "         4.7395e-05,  4.6648e-05,  9.2538e-05,  1.3308e-04,  9.1074e-05,\n",
      "         1.9316e-05,  1.1014e-04, -1.5690e-04, -1.3127e-04,  6.4067e-06,\n",
      "         5.0461e-05,  1.0650e-04, -8.5632e-06], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-2.1646e-05,  2.4677e-05, -2.9855e-05,  4.0750e-05,  1.3353e-04,\n",
      "         2.6409e-05, -1.8870e-07,  7.6693e-05,  2.0710e-05,  1.6209e-05,\n",
      "         2.7719e-05,  5.0220e-05,  1.5769e-05, -5.0393e-05,  1.0442e-04,\n",
      "         3.2393e-04, -6.4406e-05,  1.0991e-04,  3.5589e-05, -9.0558e-06,\n",
      "        -6.6502e-06, -1.3512e-04, -6.7650e-05,  1.8379e-04, -9.3601e-05,\n",
      "         4.6030e-05, -5.6776e-05, -1.3715e-05, -1.5527e-05,  9.3059e-05,\n",
      "        -2.3957e-05,  4.2526e-05,  7.7837e-05,  2.0958e-04, -9.1191e-05,\n",
      "         1.5947e-04,  1.8133e-05,  1.1855e-04, -2.6927e-04, -5.4780e-04,\n",
      "        -4.5642e-04, -1.7250e-04, -7.8538e-05,  9.0090e-05, -3.2989e-04,\n",
      "         1.9303e-04,  2.5645e-05,  4.8144e-05, -1.1617e-04, -6.1369e-05,\n",
      "        -5.9613e-05, -1.0444e-04, -5.2566e-05, -2.4166e-04,  3.8718e-06,\n",
      "         1.0776e-04,  2.3100e-04, -6.2678e-06, -3.1102e-05, -1.2769e-04,\n",
      "         1.8692e-05, -3.5905e-05,  9.6097e-06,  1.4166e-04,  7.2861e-05,\n",
      "         1.4167e-04,  9.0585e-05,  7.9220e-06,  2.7662e-04, -3.2671e-04,\n",
      "         2.7350e-04, -3.2177e-05,  8.1844e-05, -2.9515e-05,  1.2764e-05,\n",
      "        -2.8762e-06, -5.1375e-05,  1.2333e-04,  4.0512e-04,  1.0187e-04,\n",
      "        -1.0103e-04, -4.4356e-05, -1.4774e-05,  6.2264e-05,  1.1345e-04,\n",
      "        -2.3224e-04, -9.1380e-05,  1.4384e-04, -1.7401e-05, -1.3268e-04,\n",
      "        -7.4197e-05, -8.1637e-06, -1.2599e-04, -1.8015e-05,  1.4409e-04,\n",
      "        -1.3477e-04, -1.9200e-05, -9.7686e-05,  3.1120e-05, -2.9846e-06,\n",
      "         8.7174e-05, -3.4357e-06,  1.0701e-05,  6.3297e-05,  6.9515e-05,\n",
      "         5.8330e-05,  1.9766e-04,  2.3997e-04,  3.2940e-05, -9.1357e-05,\n",
      "         1.3636e-04, -3.3637e-05, -4.5883e-05, -1.7664e-04, -2.8978e-04,\n",
      "         3.6544e-04, -4.3954e-06,  9.8887e-06, -8.1574e-05,  6.9431e-05,\n",
      "         2.7396e-05,  6.5099e-05, -1.7683e-04, -1.7423e-04,  4.9654e-05,\n",
      "         3.4441e-05,  1.0817e-04,  1.4093e-05], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 5.6618e-05,  4.1722e-05,  1.2175e-04],\n",
      "          [ 1.8121e-05,  2.6849e-06, -2.2408e-05],\n",
      "          [ 2.8918e-05, -1.3850e-06, -1.2923e-05]],\n",
      "\n",
      "         [[-5.5981e-05, -3.7903e-05, -1.7878e-05],\n",
      "          [-9.6764e-05, -1.0090e-04, -1.1088e-04],\n",
      "          [-7.0621e-05, -8.0012e-05, -9.6312e-05]],\n",
      "\n",
      "         [[-1.2419e-04, -1.5171e-04, -1.7495e-04],\n",
      "          [-1.9894e-04, -1.8692e-04, -2.3229e-04],\n",
      "          [-2.0911e-04, -2.1186e-04, -2.8114e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5778e-04,  1.4022e-04,  1.6336e-04],\n",
      "          [ 1.4240e-04,  1.6310e-04,  1.6369e-04],\n",
      "          [ 1.4786e-04,  1.7944e-04,  1.6522e-04]],\n",
      "\n",
      "         [[ 9.5767e-05,  1.1212e-04,  1.1281e-04],\n",
      "          [ 8.0892e-05,  1.1953e-04,  1.0521e-04],\n",
      "          [ 9.5683e-05,  1.3800e-04,  1.2202e-04]],\n",
      "\n",
      "         [[-1.3295e-04, -1.4635e-04, -1.5490e-04],\n",
      "          [-1.1362e-04, -1.1771e-04, -1.3452e-04],\n",
      "          [-1.4845e-04, -1.3936e-04, -1.6292e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5978e-05,  6.0873e-05,  8.1001e-05],\n",
      "          [-1.9753e-06,  1.0678e-04,  2.3404e-05],\n",
      "          [-5.2176e-05,  1.0256e-05,  3.3582e-06]],\n",
      "\n",
      "         [[ 9.9038e-05,  1.4162e-04,  8.8723e-05],\n",
      "          [ 8.9122e-05,  1.0717e-04,  1.4161e-04],\n",
      "          [ 5.2494e-05,  1.2291e-04,  1.1922e-04]],\n",
      "\n",
      "         [[ 2.3873e-04,  2.5513e-04,  2.3867e-04],\n",
      "          [ 2.3031e-04,  2.8413e-04,  2.4633e-04],\n",
      "          [ 1.9145e-04,  2.1393e-04,  2.1192e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.4760e-05, -2.1800e-05, -3.7683e-05],\n",
      "          [-7.2651e-05, -5.4951e-06, -6.8643e-05],\n",
      "          [-1.2419e-04, -9.8860e-05, -1.0886e-04]],\n",
      "\n",
      "         [[-2.4271e-04, -2.1530e-04, -2.2560e-04],\n",
      "          [-2.5608e-04, -2.2473e-04, -2.4501e-04],\n",
      "          [-2.5792e-04, -2.3243e-04, -2.4628e-04]],\n",
      "\n",
      "         [[ 7.4440e-05,  8.0551e-05,  8.8089e-05],\n",
      "          [ 6.2473e-05,  7.5436e-05,  6.1743e-05],\n",
      "          [ 1.3944e-05,  1.8406e-05,  2.3397e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.0411e-04, -7.3885e-05, -9.4741e-05],\n",
      "          [-6.1547e-05, -2.9947e-05, -6.9651e-05],\n",
      "          [-3.8966e-05, -7.8877e-05, -3.6290e-05]],\n",
      "\n",
      "         [[-4.3662e-07,  8.1128e-06, -5.2085e-06],\n",
      "          [-9.9435e-06, -1.8631e-05,  7.5049e-06],\n",
      "          [-7.9649e-06, -1.8368e-05, -2.6542e-05]],\n",
      "\n",
      "         [[ 3.3468e-05,  3.7480e-05,  6.3833e-05],\n",
      "          [ 8.9115e-05,  1.0130e-04,  1.1653e-04],\n",
      "          [ 9.9848e-05,  9.9886e-05,  1.1737e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9665e-04, -1.6019e-04, -1.5470e-04],\n",
      "          [-1.3071e-04, -8.4423e-05, -9.7843e-05],\n",
      "          [-1.4001e-04, -1.0024e-04, -1.0558e-04]],\n",
      "\n",
      "         [[-2.5045e-04, -2.2373e-04, -2.0573e-04],\n",
      "          [-2.3319e-04, -2.1901e-04, -2.0406e-04],\n",
      "          [-2.4180e-04, -2.2515e-04, -2.0898e-04]],\n",
      "\n",
      "         [[-2.2868e-05, -8.9812e-06, -5.4662e-06],\n",
      "          [ 1.6516e-05,  3.4106e-05,  3.3049e-05],\n",
      "          [ 5.1873e-06,  2.3197e-05,  1.2329e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.5152e-05,  1.4421e-06,  4.7821e-05],\n",
      "          [ 1.5958e-05,  1.1042e-05,  4.2007e-05],\n",
      "          [ 3.4607e-05,  2.4874e-05,  7.4915e-05]],\n",
      "\n",
      "         [[-9.0679e-05, -1.3485e-04, -1.4589e-04],\n",
      "          [-1.4573e-04, -1.3442e-04, -1.1474e-04],\n",
      "          [-9.6983e-05, -6.0838e-05, -5.0797e-05]],\n",
      "\n",
      "         [[-3.1041e-04, -3.5693e-04, -2.9392e-04],\n",
      "          [-3.1823e-04, -3.7827e-04, -2.7666e-04],\n",
      "          [-2.9554e-04, -3.3834e-04, -2.9884e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.1146e-04,  2.7549e-04,  2.8181e-04],\n",
      "          [ 2.9988e-04,  2.6108e-04,  2.4964e-04],\n",
      "          [ 2.7422e-04,  2.6055e-04,  2.5253e-04]],\n",
      "\n",
      "         [[ 4.1625e-04,  3.8520e-04,  3.7525e-04],\n",
      "          [ 3.9403e-04,  3.8780e-04,  3.9617e-04],\n",
      "          [ 4.1178e-04,  4.1089e-04,  4.0314e-04]],\n",
      "\n",
      "         [[-1.0544e-04, -1.2092e-04, -9.7736e-05],\n",
      "          [-7.9852e-05, -1.1412e-04, -1.0961e-04],\n",
      "          [-1.0661e-04, -1.2920e-04, -1.1408e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3451e-06, -1.5131e-05,  1.0398e-05],\n",
      "          [-3.7872e-06, -2.4964e-05, -1.3271e-05],\n",
      "          [ 5.4939e-06, -9.9979e-06, -1.4601e-05]],\n",
      "\n",
      "         [[-2.9627e-05, -7.8750e-06,  3.8169e-06],\n",
      "          [ 3.0578e-06,  9.4555e-06,  2.5142e-05],\n",
      "          [ 2.8339e-06, -9.9952e-06, -6.4923e-06]],\n",
      "\n",
      "         [[ 8.1458e-05,  4.4912e-05,  6.9629e-05],\n",
      "          [ 4.4858e-05,  2.2448e-05,  3.2140e-05],\n",
      "          [ 3.4885e-05,  1.9068e-05,  5.4048e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.2006e-05, -7.4928e-05, -5.5685e-05],\n",
      "          [-6.4130e-05, -7.0713e-05, -6.3250e-05],\n",
      "          [-6.2276e-05, -6.6968e-05, -5.5822e-05]],\n",
      "\n",
      "         [[-7.6239e-05, -7.5049e-05, -5.5346e-05],\n",
      "          [-5.8165e-05, -5.3129e-05, -4.3567e-05],\n",
      "          [-4.9503e-05, -5.4929e-05, -4.6208e-05]],\n",
      "\n",
      "         [[ 1.4079e-05,  5.0984e-06,  1.3615e-05],\n",
      "          [-2.0262e-06, -1.7663e-06,  3.6670e-06],\n",
      "          [-3.1737e-06, -3.6155e-06,  2.8554e-06]]],\n",
      "\n",
      "\n",
      "        [[[-1.3080e-04, -9.0778e-05, -1.1071e-04],\n",
      "          [-7.9898e-05, -9.5228e-05, -1.2271e-04],\n",
      "          [-1.6762e-04, -2.0403e-06, -6.8580e-05]],\n",
      "\n",
      "         [[ 1.3248e-04,  1.6941e-04,  1.1482e-04],\n",
      "          [ 2.2083e-04,  2.1695e-04,  1.3199e-04],\n",
      "          [ 2.1742e-04,  1.9546e-04,  1.3483e-04]],\n",
      "\n",
      "         [[ 3.7996e-04,  4.1802e-04,  3.8260e-04],\n",
      "          [ 4.2050e-04,  4.8328e-04,  3.4861e-04],\n",
      "          [ 3.3888e-04,  4.9921e-04,  3.9903e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5094e-04, -3.5073e-04, -3.4903e-04],\n",
      "          [-3.3906e-04, -2.8043e-04, -3.0203e-04],\n",
      "          [-3.3570e-04, -2.4593e-04, -3.2286e-04]],\n",
      "\n",
      "         [[-4.8149e-04, -5.1142e-04, -4.6824e-04],\n",
      "          [-4.6282e-04, -4.6839e-04, -4.8585e-04],\n",
      "          [-4.4905e-04, -4.5897e-04, -4.9760e-04]],\n",
      "\n",
      "         [[ 1.7277e-04,  2.0099e-04,  1.8058e-04],\n",
      "          [ 1.7296e-04,  2.2192e-04,  2.0145e-04],\n",
      "          [ 1.5057e-04,  2.2457e-04,  1.7206e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-1.1940e-04, -1.4305e-04, -1.7562e-04, -4.1462e-04,  2.5994e-05,\n",
      "        -1.7101e-04,  9.7108e-05,  2.3734e-04,  6.6142e-05, -1.4385e-04,\n",
      "         2.1376e-04, -2.5864e-04,  5.6594e-05,  2.7205e-04,  8.7636e-05,\n",
      "        -1.7921e-04,  1.0154e-04,  2.6337e-05, -6.7259e-05, -6.3156e-05,\n",
      "        -2.2086e-04,  2.1295e-05,  8.0620e-06, -1.8420e-04,  2.0327e-04,\n",
      "         2.1493e-04, -1.2267e-04,  5.8355e-05, -2.0944e-04,  1.9886e-04,\n",
      "         3.4449e-05,  9.4690e-05,  7.1092e-05,  7.3265e-05,  1.2650e-04,\n",
      "        -4.1862e-05, -2.4535e-04,  7.7987e-05,  3.4602e-05,  9.0599e-05,\n",
      "         1.2212e-04,  1.1337e-04,  9.6170e-05, -1.7843e-04,  2.4523e-05,\n",
      "        -1.3850e-04, -7.2321e-05,  3.3723e-04,  6.1018e-05,  8.6346e-05,\n",
      "        -9.0761e-05,  2.1030e-04,  2.2127e-04,  1.5660e-04, -9.2887e-05,\n",
      "         7.3493e-05, -7.1044e-05,  2.4473e-04, -1.4753e-04,  9.3767e-06,\n",
      "         1.1135e-04, -1.5735e-04,  3.0616e-05, -1.1462e-04, -1.5508e-04,\n",
      "         1.8758e-04, -3.5056e-05,  2.7512e-04,  1.4524e-04, -4.6820e-05,\n",
      "        -4.6847e-05,  2.3985e-04, -3.8876e-04,  2.5814e-05, -1.8992e-04,\n",
      "        -8.5836e-05, -2.6476e-04,  4.1431e-04,  1.1731e-04,  7.0629e-05,\n",
      "        -8.9172e-05, -6.3440e-05,  2.1269e-04, -8.2977e-06, -1.0727e-04,\n",
      "         4.6082e-04, -1.0561e-04,  1.6943e-04,  9.1489e-05,  5.4200e-05,\n",
      "         2.7633e-04,  3.7012e-05,  1.2717e-04,  8.7003e-05, -1.6753e-04,\n",
      "         2.3493e-04,  1.5365e-04,  1.0367e-04,  9.9672e-05, -8.9928e-06,\n",
      "        -1.1219e-04,  6.7417e-05, -2.8764e-04, -5.5600e-05,  7.0979e-05,\n",
      "         1.5431e-04, -3.0562e-04, -2.0329e-04, -1.9471e-04,  1.0068e-04,\n",
      "         4.5128e-05,  2.0920e-04, -2.2411e-04,  4.3814e-05, -2.6916e-05,\n",
      "        -8.3365e-07, -6.9841e-05,  1.9103e-04, -9.7589e-05, -4.5503e-05,\n",
      "         4.1477e-04,  7.3659e-05, -9.0968e-05, -2.0993e-06, -1.9796e-04,\n",
      "         2.3345e-04,  8.7808e-05, -3.4950e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-2.8487e-04, -2.2753e-04, -6.0422e-04, -4.5453e-04, -4.6180e-07,\n",
      "        -2.8697e-04,  7.5040e-05,  2.6934e-04,  3.1890e-04, -4.1039e-04,\n",
      "         1.8244e-04, -1.0900e-04,  1.3029e-04,  3.8919e-04,  9.6135e-05,\n",
      "        -4.6946e-04,  4.0403e-05,  1.1705e-04, -1.2312e-04, -9.7841e-05,\n",
      "        -2.0708e-04, -1.9996e-04,  9.9506e-05, -9.0211e-05, -2.5266e-05,\n",
      "        -1.0384e-04, -1.6030e-04, -3.4672e-05, -1.9924e-04,  2.6092e-04,\n",
      "        -1.6210e-04,  1.7451e-04,  1.0627e-04,  4.6641e-05,  5.6499e-05,\n",
      "         5.9612e-05, -2.2960e-04,  1.5923e-04, -4.1447e-05,  6.2960e-05,\n",
      "         9.7434e-05,  1.8438e-05,  1.0298e-04, -4.6814e-05,  1.8173e-04,\n",
      "        -3.2409e-05, -3.0581e-05,  5.7947e-04, -1.8180e-05,  8.1891e-05,\n",
      "        -1.4412e-04,  3.3414e-04,  2.9398e-04,  1.9103e-04, -4.3235e-05,\n",
      "         1.2459e-04, -1.0631e-04,  2.1454e-04, -2.1600e-04, -1.0896e-04,\n",
      "        -1.4692e-04, -4.0767e-04,  1.2399e-04, -1.4034e-04, -1.7186e-04,\n",
      "        -7.3250e-05,  4.7123e-06,  2.6218e-04, -7.0208e-05,  7.9521e-05,\n",
      "        -7.7054e-05, -1.3523e-04, -2.7463e-04,  3.8145e-05, -1.6522e-05,\n",
      "        -1.0806e-04, -2.5261e-04,  3.9391e-04, -3.6213e-05, -1.8492e-05,\n",
      "        -2.4530e-05, -8.8996e-05,  9.9670e-04,  2.0335e-05, -2.4620e-04,\n",
      "         3.2894e-04, -9.2228e-05,  2.0890e-04, -1.3320e-04,  1.2971e-04,\n",
      "         3.5379e-04,  9.3126e-05, -6.1227e-05,  6.8675e-05, -1.7405e-04,\n",
      "         2.1133e-04,  3.4740e-04,  3.3371e-04,  1.2644e-04, -1.0264e-04,\n",
      "        -2.1396e-04,  4.7734e-05, -2.1181e-04, -3.2917e-05, -1.5682e-05,\n",
      "         3.7984e-04, -8.2106e-05, -1.2909e-04, -1.9976e-04,  1.0841e-04,\n",
      "        -1.3743e-05,  1.4445e-04, -2.2459e-04,  3.8923e-06, -3.8906e-05,\n",
      "        -5.6672e-05, -9.8647e-05,  5.4736e-05, -6.3332e-05,  3.4932e-06,\n",
      "         3.8159e-04,  1.0211e-04, -6.7127e-05, -7.8352e-05, -7.1037e-05,\n",
      "         1.1226e-06,  1.1854e-04, -2.3513e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-9.4354e-05, -8.0077e-05, -1.2424e-04],\n",
      "          [-1.4916e-04, -1.8648e-04, -1.2921e-04],\n",
      "          [-5.4783e-05, -1.1276e-04, -1.1723e-04]],\n",
      "\n",
      "         [[ 1.4346e-04,  1.9016e-04,  1.0672e-04],\n",
      "          [ 5.1472e-06,  1.3330e-04,  1.0408e-04],\n",
      "          [ 9.1912e-05,  1.1844e-04,  1.0382e-04]],\n",
      "\n",
      "         [[ 2.0090e-04,  2.4110e-04,  2.3487e-04],\n",
      "          [ 1.5139e-04,  1.2477e-04,  1.0629e-04],\n",
      "          [ 5.1568e-05,  1.3458e-04,  1.4635e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.9229e-05, -7.5604e-06,  1.1118e-05],\n",
      "          [-9.0177e-05, -4.2234e-05, -3.4776e-05],\n",
      "          [-9.6538e-05, -4.2233e-05, -1.0462e-06]],\n",
      "\n",
      "         [[-6.2362e-05, -5.2284e-05, -4.4052e-05],\n",
      "          [-1.3563e-04, -1.1230e-04, -7.4920e-05],\n",
      "          [-8.4687e-05, -1.0813e-04, -4.2080e-05]],\n",
      "\n",
      "         [[ 8.4665e-06,  6.4083e-06, -1.1569e-04],\n",
      "          [ 7.0604e-06, -8.9661e-05, -2.6829e-04],\n",
      "          [ 6.1125e-05,  6.1116e-05, -1.4305e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.3651e-05,  5.0058e-07,  1.9455e-05],\n",
      "          [ 1.9581e-05,  1.0814e-05,  3.5603e-05],\n",
      "          [ 1.7948e-06,  1.9418e-06,  2.1893e-05]],\n",
      "\n",
      "         [[-4.2570e-05, -3.0492e-05, -2.6448e-05],\n",
      "          [-1.9015e-05, -2.0216e-05, -2.2700e-05],\n",
      "          [-7.4936e-06, -1.1682e-05, -8.3838e-06]],\n",
      "\n",
      "         [[-2.5171e-05, -5.3659e-05, -4.2655e-05],\n",
      "          [-3.9470e-05, -7.3776e-05, -6.8687e-05],\n",
      "          [-1.4112e-05, -3.2498e-05, -3.9720e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.4891e-06,  1.7508e-05,  1.5182e-05],\n",
      "          [ 2.3589e-06,  1.4182e-05,  1.5344e-05],\n",
      "          [ 3.0294e-06,  4.0701e-06,  8.5996e-06]],\n",
      "\n",
      "         [[-8.9391e-06, -1.7663e-05, -1.6546e-05],\n",
      "          [-2.0039e-06, -3.9744e-06, -1.2528e-05],\n",
      "          [ 2.6089e-06,  2.5959e-06, -1.2373e-05]],\n",
      "\n",
      "         [[-2.6657e-05,  3.7923e-05,  7.6369e-06],\n",
      "          [-4.0903e-05,  1.1599e-05,  7.1318e-06],\n",
      "          [-1.1549e-05,  1.1114e-05,  1.3546e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4281e-05, -1.5036e-05, -2.3218e-06],\n",
      "          [-4.3644e-06, -2.8563e-05, -6.1900e-06],\n",
      "          [-1.0002e-05, -1.2754e-05,  5.9588e-06]],\n",
      "\n",
      "         [[ 3.1713e-05,  1.2963e-05, -3.1715e-07],\n",
      "          [-9.4043e-06, -1.8019e-05, -1.3095e-05],\n",
      "          [-6.6934e-05, -4.4933e-05, -2.2781e-05]],\n",
      "\n",
      "         [[ 5.2370e-06, -4.4457e-05, -2.6223e-06],\n",
      "          [ 1.4256e-05, -3.7898e-05, -6.9498e-06],\n",
      "          [ 1.1533e-05, -4.0279e-05, -3.0854e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.4789e-06, -1.7840e-05, -2.0216e-05],\n",
      "          [ 5.8896e-06, -2.1741e-05, -3.6799e-05],\n",
      "          [-6.4073e-05, -2.0369e-05, -2.3045e-05]],\n",
      "\n",
      "         [[-3.6809e-05, -4.9076e-05, -2.1008e-05],\n",
      "          [ 6.1798e-08,  7.6723e-07,  4.8911e-06],\n",
      "          [-1.7070e-05, -4.6096e-06, -3.2402e-06]],\n",
      "\n",
      "         [[ 4.7111e-05,  1.6495e-05,  1.1493e-04],\n",
      "          [ 2.2025e-05, -2.9364e-05,  5.1381e-05],\n",
      "          [ 1.1756e-05, -3.5610e-05,  3.3744e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.0570e-06,  2.9012e-05,  2.8330e-05],\n",
      "          [ 9.8939e-06,  7.3065e-05,  6.2686e-05],\n",
      "          [-6.5796e-06,  3.7816e-05,  4.3002e-05]],\n",
      "\n",
      "         [[-1.3026e-05,  6.1329e-05,  7.1726e-05],\n",
      "          [-3.9366e-05,  4.6526e-05,  8.0056e-06],\n",
      "          [ 3.8185e-05,  1.3237e-04,  1.3750e-04]],\n",
      "\n",
      "         [[ 2.1517e-05,  6.7460e-05,  8.1051e-05],\n",
      "          [ 3.2845e-06,  1.1281e-04,  1.0244e-04],\n",
      "          [ 2.9849e-05,  1.2729e-04,  9.7257e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.7572e-06,  4.7205e-05,  6.1076e-05],\n",
      "          [ 2.0311e-05,  9.2922e-05,  3.7352e-05],\n",
      "          [ 3.5859e-05,  7.0171e-05,  4.8574e-05]],\n",
      "\n",
      "         [[ 5.5313e-05,  7.8326e-05,  8.4956e-05],\n",
      "          [ 7.5361e-05,  9.2671e-05,  8.6089e-05],\n",
      "          [ 8.8494e-05,  1.1214e-04,  8.4369e-05]],\n",
      "\n",
      "         [[-3.0007e-05,  3.3584e-05,  3.2912e-05],\n",
      "          [-3.5180e-05, -2.4520e-05,  4.9590e-05],\n",
      "          [ 3.5545e-06,  2.0133e-05,  9.7324e-05]]],\n",
      "\n",
      "\n",
      "        [[[-5.4308e-06,  4.9653e-05,  9.2060e-05],\n",
      "          [ 9.3137e-05,  5.8403e-05,  1.2150e-05],\n",
      "          [-9.1936e-06,  1.5364e-05,  4.8553e-05]],\n",
      "\n",
      "         [[-3.3425e-04, -3.0055e-04, -2.8284e-04],\n",
      "          [-2.7064e-04, -2.1957e-04, -1.5865e-04],\n",
      "          [-1.8890e-04, -1.8675e-04, -2.1483e-04]],\n",
      "\n",
      "         [[-3.3435e-04, -3.8944e-04, -5.5149e-04],\n",
      "          [-3.9409e-04, -4.4853e-04, -5.2125e-04],\n",
      "          [-5.4052e-04, -6.6588e-04, -7.1391e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.5744e-05, -1.6000e-05,  1.0456e-04],\n",
      "          [ 1.0258e-04,  1.6187e-05,  1.8003e-04],\n",
      "          [ 7.5240e-05,  3.9677e-05,  1.2013e-04]],\n",
      "\n",
      "         [[ 3.8983e-05,  1.9217e-05, -3.2216e-05],\n",
      "          [ 6.0168e-05,  3.3155e-06,  7.4935e-06],\n",
      "          [ 8.7533e-05,  1.3216e-05, -5.5228e-05]],\n",
      "\n",
      "         [[-2.6998e-04, -5.7701e-05, -1.7554e-04],\n",
      "          [-3.0558e-04,  7.8043e-05, -4.9364e-05],\n",
      "          [-2.4554e-04,  2.0951e-05, -7.9930e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6294e-05,  2.2651e-05,  1.7828e-05],\n",
      "          [ 2.2291e-05,  2.5972e-05,  2.8071e-05],\n",
      "          [ 1.8516e-05,  3.7652e-05,  3.5215e-05]],\n",
      "\n",
      "         [[-3.0283e-06, -2.5555e-06,  1.9419e-06],\n",
      "          [-2.0849e-05, -1.2135e-05, -2.0654e-06],\n",
      "          [-3.3178e-05, -1.1345e-05, -1.2975e-05]],\n",
      "\n",
      "         [[ 4.1233e-06,  1.4308e-05,  4.0781e-05],\n",
      "          [ 1.2570e-05,  1.9553e-05,  2.1731e-05],\n",
      "          [ 1.6748e-05,  1.8507e-05,  1.6453e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.1130e-06,  8.7637e-07, -4.2271e-06],\n",
      "          [-7.7365e-06, -8.0282e-06, -1.7915e-05],\n",
      "          [ 5.1853e-06,  6.6673e-07, -3.7024e-06]],\n",
      "\n",
      "         [[ 4.1566e-05,  6.5474e-05,  5.7350e-05],\n",
      "          [ 1.9399e-05,  3.6054e-05,  2.9419e-05],\n",
      "          [ 2.0739e-05,  4.0631e-05,  2.9063e-05]],\n",
      "\n",
      "         [[ 9.2024e-06,  1.4800e-06,  3.3386e-05],\n",
      "          [ 3.3808e-06, -9.5856e-06,  1.2306e-05],\n",
      "          [ 1.5067e-05,  9.8559e-06, -1.2445e-06]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-3.4106e-12,  7.2760e-12,  0.0000e+00,  0.0000e+00, -9.0949e-13,\n",
      "        -1.4552e-11, -2.5466e-11, -3.6380e-12, -2.9104e-11, -5.4570e-12,\n",
      "        -2.7285e-12,  4.5475e-12,  3.2742e-11, -5.4570e-12,  5.4570e-12,\n",
      "         1.3097e-10, -1.8190e-12,  0.0000e+00, -2.5466e-11, -7.2760e-12,\n",
      "         3.6380e-12, -7.2760e-12,  2.1828e-11, -1.0914e-11,  2.7285e-12,\n",
      "         7.2760e-12,  0.0000e+00,  2.2737e-13,  0.0000e+00, -4.3656e-11,\n",
      "        -2.9104e-11,  2.7285e-12, -5.4570e-12,  4.5475e-13,  3.6380e-12,\n",
      "         5.4570e-12, -2.0009e-11,  2.9104e-11,  1.4552e-11,  8.7311e-11,\n",
      "         1.6371e-11,  7.2760e-12, -7.2760e-12,  1.0914e-11,  9.0949e-13,\n",
      "        -2.1828e-11,  1.4552e-11,  8.1855e-12,  1.0914e-11,  4.5475e-12,\n",
      "         2.1828e-11, -5.4570e-11,  2.0009e-11,  5.0022e-12, -9.4587e-11,\n",
      "        -3.6380e-12,  1.2733e-11,  9.0949e-13, -3.1832e-12,  9.0949e-13,\n",
      "         7.2760e-11, -5.4570e-12,  3.6380e-11, -7.7307e-12, -7.2760e-12,\n",
      "        -1.4552e-11,  7.2760e-12, -5.0477e-11,  7.2760e-12, -2.2737e-12,\n",
      "         4.5475e-12,  7.2760e-12,  5.4570e-12,  7.2760e-12, -3.6380e-12,\n",
      "         7.2760e-12, -3.6380e-12,  0.0000e+00,  3.2742e-11, -3.6380e-12,\n",
      "         5.4570e-12,  3.6380e-12,  2.2737e-12, -7.2760e-12, -3.6380e-12,\n",
      "         0.0000e+00,  3.6380e-12, -7.2760e-12,  1.3642e-11,  2.9104e-11,\n",
      "        -4.5475e-12,  4.3656e-11,  0.0000e+00,  5.4570e-11, -1.4097e-11,\n",
      "         1.4552e-11,  7.2760e-12,  7.2760e-12,  0.0000e+00, -1.2733e-11,\n",
      "         1.0914e-11, -2.9104e-11, -9.0949e-12,  7.2760e-12,  0.0000e+00,\n",
      "        -2.7285e-12,  9.0949e-13,  1.8190e-12,  1.4552e-11,  7.2760e-12,\n",
      "         2.9559e-12, -2.1600e-12, -3.6380e-12, -9.0949e-13, -1.0914e-11,\n",
      "        -3.6380e-12, -1.0914e-11,  3.6380e-12, -1.0914e-11, -2.1828e-11,\n",
      "         0.0000e+00,  2.5466e-11,  4.5475e-12,  0.0000e+00,  1.4552e-11,\n",
      "        -1.3642e-12,  3.6380e-12, -1.2733e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 0.0001, -0.0009,  0.0004,  0.0001,  0.0002], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = loss_landscape_join.landscape(maml_system.model.classifier, arbiter_system.model.classifier, args_arbiter)\n",
    "ls.show_2djoin(x_support_set_task, y_support_set_task, title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
