{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda16bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from prompters import padding\n",
    "from utils.parser_utils import get_args\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "869f1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DATASET_DIR'] = os.path.join(os.getcwd(), \"datasets\")\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"alfa+maml\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "    \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"attenuate\": False,\n",
    "  \"alfa\": True,\n",
    "  \"random_init\": False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "   \"loss_function\": \"Softmax\",\n",
    "  \"ole\": True,\n",
    "  \"arbiter\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "def get_inner_loop_parameter_dict(params):\n",
    "\n",
    "    param_dict = dict()\n",
    "    for name, param in params:\n",
    "        if param.requires_grad:\n",
    "            param_dict[name] = param.to(device=device)\n",
    "\n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbba0c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([25, 3, 84, 84])\n",
      "tensor([99, 95,  8, 59, 49, 37,  4, 51, 42, 14, 33, 26, 23, 88,  0, 72, 86, 13,\n",
      "        60, 54, 37, 85, 13, 18, 26])\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(84),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR100(\"./data\", transform=preprocess,\n",
    "                          download=True, train=True)\n",
    "\n",
    "val_dataset = CIFAR100(\"./data\", transform=preprocess,\n",
    "                        download=True, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=25, pin_memory=True,\n",
    "                          num_workers=16, shuffle=True)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "print(images.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1be2fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 무작위 이미지 데이터 생성 (배치 크기, 채널, 높이, 너비)\n",
    "# batch_size = 25\n",
    "# channels = 3  # RGB 이미지이므로 3개의 채널\n",
    "# height, width = 84, 84  # 높이와 너비\n",
    "\n",
    "# # 무작위 이미지 데이터 생성 (0과 1 사이의 무작위 값)\n",
    "# images = torch.rand(batch_size, channels, height, width)\n",
    "\n",
    "# # 무작위 레이블 데이터 생성 (예시를 위해 10개의 클래스)\n",
    "# num_classes = 25\n",
    "# targets = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a60b8e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 3, 84, 84])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
      "        4], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "targets = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4])\n",
    "targets = torch.Tensor(targets)\n",
    "targets = targets.type(torch.LongTensor)\n",
    "targets = targets.to(device)\n",
    "\n",
    "print(images.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0123c723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "class MAMLFewShotClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, im_shape, device, args):\n",
    "        \n",
    "        super(MAMLFewShotClassifier, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.batch_size = args.batch_size\n",
    "        self.use_cuda = args.use_cuda\n",
    "        self.im_shape = im_shape\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.classifier = VGGReLUNormNetwork(im_shape=self.im_shape, num_output_classes=self.args.\n",
    "                                                 num_classes_per_set,\n",
    "                                                 args=args, device=device, meta_classifier=True).to(device=self.device)        \n",
    "        \n",
    "        self.optimizer = optim.Adam(self.trainable_parameters(), lr=args.meta_learning_rate, amsgrad=False)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=self.optimizer, T_max=self.args.total_epochs,\n",
    "                                                              eta_min=self.args.min_learning_rate)\n",
    "        \n",
    "        names_weights_copy = self.get_inner_loop_parameter_dict(self.classifier.named_parameters())\n",
    "        \n",
    "        # Gradient Arbiter\n",
    "        num_layers = len(names_weights_copy)\n",
    "        input_dim = num_layers * 2\n",
    "        output_dim = 5 #num_layers / 2\n",
    "        self.arbiter = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Softplus(beta=2)\n",
    "        ).to(device=self.device)\n",
    "        \n",
    "    def trainable_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                yield param\n",
    "                \n",
    "    def get_inner_loop_parameter_dict(self, params):\n",
    " \n",
    "        param_dict = dict()\n",
    "        for name, param in params:\n",
    "            if param.requires_grad:\n",
    "                if self.args.enable_inner_loop_optimizable_bn_params:\n",
    "                    param_dict[name] = param.to(device=self.device)\n",
    "                else:\n",
    "                    if \"norm_layer\" not in name:\n",
    "                        param_dict[name] = param.to(device=self.device)\n",
    "\n",
    "        return param_dict\n",
    "    \n",
    "    def weight_scaling(self, task_embeddings, names_weights_copy):\n",
    "\n",
    "        generated_alpha_params = {}\n",
    "\n",
    "        gamma = self.arbiter(task_embeddings)\n",
    "\n",
    "        g = 0\n",
    "        # for key in names_weights_copy.keys():\n",
    "        #     generated_alpha_params[key] = gamma[g]\n",
    "        #     g += 1\n",
    "        for key in names_weights_copy.keys():\n",
    "            if 'weight' in key:  # weight에 대해서만 SVD를 수행\n",
    "                generated_alpha_params[key] = gamma[g]\n",
    "                g += 1\n",
    "\n",
    "        for name, param in names_weights_copy.items():\n",
    "            if 'weight' in name:  # weight에 대해서만 SVD를 수행\n",
    "                if \"norm_layer\" not in name:\n",
    "                    param_matrix = param.view(param.data.size(0), -1)  # 텐서를 2D로 변환하여 특이값 분해 수행\n",
    "                    u, s, v = torch.svd(param_matrix)\n",
    "                    s = s * generated_alpha_params[name]\n",
    "                    s_diag = torch.diag(s)\n",
    "\n",
    "                    rescale_weight = u @ s_diag @ v.T\n",
    "                    rescale_weight = rescale_weight.view(param.size())\n",
    "                    names_weights_copy[name] = rescale_weight\n",
    "                    \n",
    "                    if \"linear\" in name:\n",
    "                        print(\"original param : \")\n",
    "                        print(param)\n",
    "                        print(\"param_matrix : \")\n",
    "                        print(param_matrix)\n",
    "                        print(\"singular value : \")\n",
    "                        print(s)\n",
    "                        print(\"singular diag : \")\n",
    "                        print(s_diag)\n",
    "                        print(\"rescale_weight : \")\n",
    "                        print(rescale_weight)\n",
    "                        \n",
    "                    \n",
    "\n",
    "        return names_weights_copy\n",
    "\n",
    "    def get_task_embeddings(self, x_support_set_task, y_support_set_task, names_weights_copy):\n",
    "\n",
    "        support_loss, support_preds = self.net_forward(x=x_support_set_task,\n",
    "                                                       y=y_support_set_task,\n",
    "                                                       weights=names_weights_copy,\n",
    "                                                       backup_running_statistics=True,\n",
    "                                                       training=True, num_step=0)\n",
    "\n",
    "        self.classifier.zero_grad(names_weights_copy)\n",
    "\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(), create_graph=False)\n",
    "        # support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(), retain_graph=True)\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "        for k, v in names_weights_copy.items():\n",
    "            # per_step_task_embedding.append(v.mean())\n",
    "            # per_step_task_embedding.append(v.norm())\n",
    "            per_step_task_embedding.append(v.clone().detach().norm())\n",
    "\n",
    "        for i in range(len(support_loss_grad)):\n",
    "            # per_step_task_embedding.append(support_loss_grad[i].mean())\n",
    "            # per_step_task_embedding.append(support_loss_grad[i].norm())\n",
    "            per_step_task_embedding.append(support_loss_grad[i].clone().detach().norm())\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "        # per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "        #             per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        return per_step_task_embedding\n",
    "    \n",
    "    def apply_inner_loop_update(self, loss, names_weights_copy, use_second_order, current_step_idx):\n",
    "        \n",
    "        grads = torch.autograd.grad(loss, names_weights_copy.values(),\n",
    "                                    create_graph=use_second_order, allow_unused=True)\n",
    "                \n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        names_weights_copy = self.get_inner_loop_parameter_dict(self.classifier.named_parameters())\n",
    "        \n",
    "        names_weights_copy = {\n",
    "                        name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                            [1] + [1 for i in range(len(value.shape))]) for\n",
    "                        name, value in names_weights_copy.items()}\n",
    "        \n",
    "        loss, preds = self.net_forward(x, y, names_weights_copy)\n",
    "        \n",
    "        \n",
    "        support_loss_grad = torch.autograd.grad(loss, names_weights_copy.values(),\n",
    "                                                            retain_graph=True)\n",
    "        \n",
    "        per_step_task_embedding = []\n",
    "        for k, v in names_weights_copy.items():\n",
    "            # per_step_task_embedding.append(v.mean())\n",
    "            per_step_task_embedding.append(v.norm())\n",
    "\n",
    "        for i in range(len(support_loss_grad)):\n",
    "            # per_step_task_embedding.append(support_loss_grad[i].mean())\n",
    "            per_step_task_embedding.append(support_loss_grad[i].norm())\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "        \n",
    "        names_weights_copy = self.weight_scaling(task_embeddings=per_step_task_embedding, names_weights_copy=names_weights_copy)\n",
    "        \n",
    "        return loss, preds\n",
    "    \n",
    "    \n",
    "    def net_forward(self, x, y, names_weights_copy):\n",
    "        \n",
    "    \n",
    "        preds = self.classifier.forward(x, params=names_weights_copy,num_step=4)\n",
    "        \n",
    "        loss = F.cross_entropy(input=preds, target=y)        \n",
    "        \n",
    "        return loss, preds\n",
    "    \n",
    "    \n",
    "\n",
    "model = MAMLFewShotClassifier(args=args, device=device, im_shape=(2, 3, args.image_height, args.image_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26061063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original param : \n",
      "tensor([[[ 0.0128, -0.0304,  0.0372,  ..., -0.0228,  0.0657, -0.0432],\n",
      "         [-0.0159,  0.0273, -0.0468,  ...,  0.0087, -0.0554,  0.0273],\n",
      "         [-0.0540,  0.0122,  0.0213,  ..., -0.0619, -0.0476,  0.0097],\n",
      "         [-0.0158, -0.0154, -0.0632,  ...,  0.0328, -0.0568, -0.0117],\n",
      "         [ 0.0366,  0.0704,  0.0008,  ..., -0.0619,  0.0412, -0.0640]]],\n",
      "       device='cuda:0', grad_fn=<RepeatBackward>)\n",
      "param_matrix : \n",
      "tensor([[ 0.0128, -0.0304,  0.0372,  ..., -0.0619,  0.0412, -0.0640]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "singular value : \n",
      "tensor([0.4691], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "singular diag : \n",
      "tensor([[0.4691]], device='cuda:0', grad_fn=<DiagBackward>)\n",
      "rescale_weight : \n",
      "tensor([[[ 0.0019, -0.0045,  0.0055,  ..., -0.0034,  0.0097, -0.0064],\n",
      "         [-0.0024,  0.0040, -0.0069,  ...,  0.0013, -0.0082,  0.0040],\n",
      "         [-0.0080,  0.0018,  0.0032,  ..., -0.0092, -0.0071,  0.0014],\n",
      "         [-0.0023, -0.0023, -0.0094,  ...,  0.0049, -0.0084, -0.0017],\n",
      "         [ 0.0054,  0.0104,  0.0001,  ..., -0.0092,  0.0061, -0.0095]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss, preds = model.forward(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dec087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: classifier.layer_dict.conv0.conv.weight\n",
      "original shape: torch.Size([48, 3, 3, 3])\n",
      "U matrix shape: torch.Size([48, 3, 3, 3])\n",
      "Singular values shape: torch.Size([48, 3, 3])\n",
      "V transpose matrix shape: torch.Size([48, 3, 3, 3])\n",
      "restored matrix shape: torch.Size([48, 3, 3, 3])\n",
      "원본 텐서와 복원된 텐서 간의 차이:\n",
      "tensor(0.2580, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "------------------------------------\n",
      "Layer: classifier.layer_dict.conv1.conv.weight\n",
      "original shape: torch.Size([48, 48, 3, 3])\n",
      "U matrix shape: torch.Size([48, 48, 3, 3])\n",
      "Singular values shape: torch.Size([48, 48, 3])\n",
      "V transpose matrix shape: torch.Size([48, 48, 3, 3])\n",
      "restored matrix shape: torch.Size([48, 48, 3, 3])\n",
      "원본 텐서와 복원된 텐서 간의 차이:\n",
      "tensor(0.2174, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "------------------------------------\n",
      "Layer: classifier.layer_dict.conv2.conv.weight\n",
      "original shape: torch.Size([48, 48, 3, 3])\n",
      "U matrix shape: torch.Size([48, 48, 3, 3])\n",
      "Singular values shape: torch.Size([48, 48, 3])\n",
      "V transpose matrix shape: torch.Size([48, 48, 3, 3])\n",
      "restored matrix shape: torch.Size([48, 48, 3, 3])\n",
      "원본 텐서와 복원된 텐서 간의 차이:\n",
      "tensor(0.1985, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "------------------------------------\n",
      "Layer: classifier.layer_dict.conv3.conv.weight\n",
      "original shape: torch.Size([48, 48, 3, 3])\n",
      "U matrix shape: torch.Size([48, 48, 3, 3])\n",
      "Singular values shape: torch.Size([48, 48, 3])\n",
      "V transpose matrix shape: torch.Size([48, 48, 3, 3])\n",
      "restored matrix shape: torch.Size([48, 48, 3, 3])\n",
      "원본 텐서와 복원된 텐서 간의 차이:\n",
      "tensor(0.2012, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "------------------------------------\n",
      "Layer: classifier.layer_dict.linear.weights\n",
      "original shape: torch.Size([5, 1200])\n",
      "U matrix shape: torch.Size([5, 5])\n",
      "Singular values shape: torch.Size([5])\n",
      "V transpose matrix shape: torch.Size([1200, 1200])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9872\\2747771688.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;31m# 복원된 가중치 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mrestored_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'restored matrix shape: {restored_weight.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "# 각 레이어의 가중치에 대한 SVD를 수행합니다\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:  # weight에 대해서만 SVD를 수행합니다\n",
    "        if \"norm_layer\" not in name:\n",
    "            #if \"linear\" not in name:\n",
    "            print(f'Layer: {name}')\n",
    "            original_shape = param.shape\n",
    "            u, s, v = torch.svd(param, some=False)  # SVD 수행\n",
    "            print(f'original shape: {original_shape}')\n",
    "            print(f'U matrix shape: {u.shape}')\n",
    "            print(f'Singular values shape: {s.shape}')\n",
    "            print(f'V transpose matrix shape: {v.shape}')\n",
    "\n",
    "            # 복원된 가중치 계산\n",
    "            restored_weight = torch.matmul(torch.matmul(u, torch.diag_embed(s)), v)\n",
    "            print(f'restored matrix shape: {restored_weight.shape}')\n",
    "\n",
    "            # 복원된 텐서와 원본 텐서 간의 차이 계산\n",
    "            difference = torch.abs(param - restored_weight)\n",
    "            print(\"원본 텐서와 복원된 텐서 간의 차이:\")\n",
    "            print(difference.max())  # 차이의 최댓값 출력\n",
    "\n",
    "            print('------------------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
