{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb3f6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51c0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b159aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import basic_utils\n",
    "\n",
    "# datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6799c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_filter128\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 101,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428adcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\anaconda3\\envs\\metal\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_filter128\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 2953, 'train': 5885, 'val': 2950}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f45f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6349777774016062,\n",
       " 'best_val_iter': 49000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 98,\n",
       " 'train_loss_mean': 0.6467063835263253,\n",
       " 'train_loss_std': 0.12827674334670136,\n",
       " 'train_accuracy_mean': 0.7585199998617173,\n",
       " 'train_accuracy_std': 0.06066912721379857,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.9470661379893621,\n",
       " 'val_loss_std': 0.13992390640594285,\n",
       " 'val_accuracy_mean': 0.6296222216884295,\n",
       " 'val_accuracy_std': 0.06292857691682094,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-1.1780e-02, -9.7069e-02,  4.9382e-02],\n",
       "                         [-7.8473e-02, -6.2654e-02, -3.4362e-03],\n",
       "                         [-4.5643e-02,  9.4547e-02, -3.3218e-02]],\n",
       "               \n",
       "                        [[ 6.7605e-02, -5.2963e-02,  9.2958e-02],\n",
       "                         [-2.1604e-02,  2.7979e-02,  9.0784e-02],\n",
       "                         [-3.6842e-02,  5.5913e-02, -6.1223e-03]],\n",
       "               \n",
       "                        [[ 6.2964e-02, -2.1813e-02, -3.5406e-02],\n",
       "                         [ 2.6693e-02,  5.2481e-02, -6.6993e-02],\n",
       "                         [-5.9290e-02,  1.8957e-02, -6.4061e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.8600e-02,  1.2523e-01,  4.3109e-02],\n",
       "                         [ 4.9678e-02, -5.9569e-02,  2.0727e-02],\n",
       "                         [-8.5274e-02, -8.5274e-02, -5.4068e-02]],\n",
       "               \n",
       "                        [[-2.2248e-02,  5.4598e-02, -6.4576e-03],\n",
       "                         [ 5.7441e-02, -7.0255e-02, -4.9313e-02],\n",
       "                         [ 6.2666e-02, -7.1975e-02,  7.3105e-02]],\n",
       "               \n",
       "                        [[-5.8888e-02,  9.2294e-02, -7.7028e-02],\n",
       "                         [-2.2854e-02,  5.1992e-02,  8.4847e-02],\n",
       "                         [-3.0098e-02, -4.0611e-02, -2.0583e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.1895e-02, -3.6297e-02,  5.6826e-02],\n",
       "                         [-1.3018e-02, -4.2398e-02, -3.1096e-02],\n",
       "                         [ 6.2629e-02,  5.3818e-02, -6.7808e-02]],\n",
       "               \n",
       "                        [[-6.4547e-02, -3.6998e-02,  4.5179e-02],\n",
       "                         [ 8.6612e-02, -8.4809e-03,  1.7349e-02],\n",
       "                         [-7.3469e-02, -3.9852e-02,  3.7788e-02]],\n",
       "               \n",
       "                        [[ 3.9884e-02, -7.6097e-02,  3.9826e-02],\n",
       "                         [ 7.6747e-02,  4.1424e-03, -6.1259e-02],\n",
       "                         [-7.5593e-02,  2.9137e-02,  1.2676e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.5761e-02, -7.9410e-02, -6.7951e-02],\n",
       "                         [-1.7234e-02, -9.2701e-02,  5.2250e-02],\n",
       "                         [-4.2844e-02,  4.6021e-02, -5.4536e-02]],\n",
       "               \n",
       "                        [[ 7.2973e-02,  2.1289e-02,  7.0794e-02],\n",
       "                         [ 5.0688e-02, -1.7859e-02, -1.8455e-02],\n",
       "                         [ 2.0132e-02,  7.9344e-02,  1.4915e-02]],\n",
       "               \n",
       "                        [[ 1.8205e-02, -8.1724e-02, -4.8699e-02],\n",
       "                         [-5.7691e-02,  8.6202e-03,  3.4885e-02],\n",
       "                         [-4.9060e-02,  3.6032e-02,  5.4931e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.6163e-02, -7.8952e-03, -5.1283e-02],\n",
       "                         [ 1.5249e-02,  7.9766e-02, -5.8870e-02],\n",
       "                         [ 4.3539e-02,  8.4609e-02, -5.4723e-02]],\n",
       "               \n",
       "                        [[ 6.3208e-02,  3.5481e-02,  7.6848e-02],\n",
       "                         [ 3.3199e-02, -2.2691e-02, -2.6409e-02],\n",
       "                         [-2.8338e-02, -8.1967e-02,  2.5193e-02]],\n",
       "               \n",
       "                        [[-4.0346e-02, -3.2833e-02, -8.4267e-03],\n",
       "                         [-5.1841e-02, -6.2962e-02,  8.9521e-02],\n",
       "                         [-1.1134e-01, -6.9775e-02,  1.1176e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.8414e-02,  7.2352e-02, -3.4504e-02],\n",
       "                         [-5.3590e-02,  6.5156e-02,  6.2475e-02],\n",
       "                         [ 7.1550e-02,  9.3662e-02,  1.0721e-01]],\n",
       "               \n",
       "                        [[-3.7690e-02, -5.1493e-02,  1.7465e-02],\n",
       "                         [ 4.0045e-02, -7.2737e-02, -5.9441e-02],\n",
       "                         [-2.7723e-02, -1.1387e-04, -2.0172e-02]],\n",
       "               \n",
       "                        [[ 3.8503e-02, -3.5160e-02,  2.9476e-02],\n",
       "                         [-8.3745e-03, -4.6594e-02, -4.8900e-02],\n",
       "                         [-3.9258e-02, -6.3435e-02, -4.8839e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.0010, -0.0045, -0.0149, -0.0194, -0.0124, -0.0337, -0.0157,  0.0088,\n",
       "                        0.0023,  0.0149, -0.0228,  0.0076, -0.0070,  0.0217, -0.0170, -0.0018,\n",
       "                        0.0261,  0.0006, -0.0069, -0.0004,  0.0076, -0.0015,  0.0003, -0.0078,\n",
       "                        0.0005, -0.0012,  0.0052, -0.0015, -0.0049, -0.0061,  0.0190, -0.0066,\n",
       "                       -0.0246, -0.0163,  0.0120,  0.0044, -0.0102,  0.0037,  0.0088, -0.0100,\n",
       "                        0.0191, -0.0028,  0.0102, -0.0163,  0.0150, -0.0016,  0.0156, -0.0018,\n",
       "                        0.0154, -0.0030, -0.0360,  0.0067,  0.0140,  0.0077, -0.0020, -0.0051,\n",
       "                        0.0022, -0.0054, -0.0057, -0.0024, -0.0023,  0.0086,  0.0071,  0.0153,\n",
       "                       -0.0045, -0.0191, -0.0140, -0.0132, -0.0118,  0.0288,  0.0090,  0.0137,\n",
       "                       -0.0078,  0.0031, -0.0099, -0.0028,  0.0094,  0.0012, -0.0187,  0.0030,\n",
       "                        0.0174, -0.0078, -0.0020,  0.0066, -0.0074,  0.0088, -0.0324,  0.0038,\n",
       "                        0.0022, -0.0010, -0.0013, -0.0260,  0.0139,  0.0043, -0.0243, -0.0297,\n",
       "                       -0.0010, -0.0067, -0.0001, -0.0042,  0.0227, -0.0124,  0.0154, -0.0169,\n",
       "                        0.0142, -0.0083,  0.0144,  0.0027,  0.0047,  0.0096, -0.0242, -0.0072,\n",
       "                        0.0250, -0.0168, -0.0065, -0.0109,  0.0063, -0.0024,  0.0084,  0.0014,\n",
       "                       -0.0158,  0.0139,  0.0103,  0.0036, -0.0156, -0.0240, -0.0114, -0.0093],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1029,  0.0966, -0.0964, -0.0803,  0.1460,  0.0008, -0.0635, -0.0572,\n",
       "                       -0.1421, -0.0833,  0.0590, -0.0391,  0.3083, -0.1149,  0.3027,  0.0043,\n",
       "                        0.0676,  0.2664,  0.1918, -0.0404,  0.1349, -0.1102, -0.0608, -0.0449,\n",
       "                       -0.0080, -0.1275, -0.1025,  0.1584,  0.2386,  0.0653,  0.0844, -0.0591,\n",
       "                        0.0860, -0.1762, -0.0248, -0.1125, -0.0598,  0.2251, -0.1432, -0.0642,\n",
       "                        0.0368,  0.1199, -0.0937, -0.0765,  0.2856, -0.1433,  0.0171, -0.0884,\n",
       "                        0.0173,  0.0321,  0.3177,  0.1388,  0.3505,  0.1901,  0.1716, -0.0849,\n",
       "                       -0.0953,  0.2753, -0.0391,  0.2527, -0.0382,  0.4093,  0.0423, -0.1601,\n",
       "                        0.2038, -0.0247,  0.2719, -0.0774,  0.3470,  0.0451,  0.1638,  0.4857,\n",
       "                        0.0742, -0.1409,  0.2289, -0.0970,  0.1194,  0.3660,  0.0711,  0.2941,\n",
       "                        0.2609,  0.0260, -0.1778, -0.1598,  0.3800,  0.3970,  0.2725,  0.2955,\n",
       "                       -0.1356,  0.0901, -0.1180, -0.0717,  0.0382,  0.1884,  0.2273, -0.0178,\n",
       "                        0.0515,  0.1721,  0.0400, -0.1486,  0.0648,  0.3028, -0.1156,  0.0497,\n",
       "                        0.2586, -0.1407,  0.1132, -0.1074, -0.0108,  0.2252,  0.3168,  0.5070,\n",
       "                        0.0466,  0.0727, -0.1384,  0.0446, -0.0438,  0.3752,  0.0130,  0.2514,\n",
       "                       -0.0948, -0.0623, -0.0686, -0.0231,  0.4249,  0.0247, -0.1914, -0.0748],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.9634, 1.0239, 0.8599, 1.0473, 0.9841, 0.9468, 0.9625, 0.9892, 0.8796,\n",
       "                       0.9593, 1.0480, 0.9863, 1.0946, 0.9737, 1.1061, 1.1753, 0.8717, 1.0362,\n",
       "                       0.9670, 0.9726, 0.9351, 0.9379, 0.9640, 0.9941, 0.9523, 0.9568, 0.9372,\n",
       "                       0.9427, 0.9718, 0.9895, 1.0252, 0.9544, 1.1865, 0.8950, 1.0029, 0.9668,\n",
       "                       0.9607, 1.1119, 0.9355, 0.9708, 1.1335, 1.0613, 0.9736, 1.0179, 1.0237,\n",
       "                       0.9967, 0.9075, 0.9731, 0.9602, 1.0216, 0.9838, 1.0646, 1.1137, 1.0075,\n",
       "                       1.0618, 0.9569, 0.9342, 1.1021, 1.0120, 1.0469, 1.0391, 1.0254, 0.9730,\n",
       "                       0.9516, 0.9942, 0.9469, 1.0086, 0.9734, 1.1890, 0.9382, 1.0549, 1.1704,\n",
       "                       1.0084, 0.9477, 0.9703, 0.9176, 1.0419, 1.1996, 1.1365, 1.0157, 0.9838,\n",
       "                       1.0717, 0.9015, 0.9199, 1.0656, 1.1151, 0.9881, 1.0717, 0.9213, 0.9780,\n",
       "                       0.8685, 0.9294, 0.9460, 1.0647, 0.9758, 0.9765, 0.9643, 0.9640, 0.9434,\n",
       "                       0.9497, 0.9677, 1.0728, 0.9378, 1.0580, 1.0615, 0.9730, 0.9993, 0.9577,\n",
       "                       0.9536, 1.0009, 1.0222, 1.1238, 0.9931, 0.9847, 0.9763, 1.1780, 0.9359,\n",
       "                       1.0033, 1.0317, 1.1043, 0.9365, 0.9790, 0.9645, 0.9539, 0.9531, 0.9966,\n",
       "                       0.9583, 0.9256], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-0.0347,  0.0251, -0.0059],\n",
       "                         [-0.0489,  0.0008, -0.0440],\n",
       "                         [-0.0027,  0.0441, -0.0504]],\n",
       "               \n",
       "                        [[ 0.0423,  0.0564, -0.0449],\n",
       "                         [-0.0488, -0.0076, -0.0275],\n",
       "                         [-0.0341,  0.0894,  0.0477]],\n",
       "               \n",
       "                        [[ 0.0271, -0.0065, -0.0010],\n",
       "                         [-0.0412,  0.0567,  0.0912],\n",
       "                         [ 0.0101,  0.0389, -0.0060]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0187, -0.0350, -0.0225],\n",
       "                         [-0.0561, -0.0520, -0.0046],\n",
       "                         [ 0.0230,  0.0090,  0.0016]],\n",
       "               \n",
       "                        [[ 0.0048,  0.0333, -0.0310],\n",
       "                         [ 0.0173,  0.0662,  0.0080],\n",
       "                         [ 0.0691, -0.0080,  0.0132]],\n",
       "               \n",
       "                        [[-0.0309,  0.0375, -0.0511],\n",
       "                         [ 0.0147,  0.0456, -0.0043],\n",
       "                         [-0.0216, -0.0146,  0.0205]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0310, -0.0418,  0.0292],\n",
       "                         [ 0.0320,  0.0135, -0.0226],\n",
       "                         [ 0.0309,  0.0102,  0.0057]],\n",
       "               \n",
       "                        [[ 0.0560,  0.0547,  0.0372],\n",
       "                         [-0.0192,  0.0236,  0.0458],\n",
       "                         [-0.0397, -0.0096,  0.0519]],\n",
       "               \n",
       "                        [[ 0.0178,  0.0168, -0.0550],\n",
       "                         [-0.0218, -0.0188, -0.0027],\n",
       "                         [-0.0615, -0.0240, -0.0455]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0149, -0.0027,  0.0065],\n",
       "                         [-0.0342, -0.0567, -0.0352],\n",
       "                         [ 0.0016,  0.0063, -0.0268]],\n",
       "               \n",
       "                        [[ 0.0041, -0.0347, -0.0313],\n",
       "                         [ 0.0426,  0.0463,  0.0215],\n",
       "                         [ 0.0589,  0.0103,  0.0532]],\n",
       "               \n",
       "                        [[-0.0467, -0.0729, -0.0016],\n",
       "                         [-0.0314, -0.0996, -0.0217],\n",
       "                         [-0.0995, -0.0993, -0.0002]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0483, -0.0756,  0.0035],\n",
       "                         [-0.0176, -0.0152, -0.0515],\n",
       "                         [ 0.0018,  0.0136, -0.0020]],\n",
       "               \n",
       "                        [[ 0.0463,  0.0113, -0.0455],\n",
       "                         [ 0.0488,  0.0267, -0.0542],\n",
       "                         [ 0.0233,  0.0402, -0.0137]],\n",
       "               \n",
       "                        [[-0.0406,  0.0177, -0.0414],\n",
       "                         [-0.0360,  0.0115,  0.0164],\n",
       "                         [ 0.0052,  0.0153,  0.0508]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0334,  0.0043, -0.0006],\n",
       "                         [-0.0413, -0.0544, -0.0235],\n",
       "                         [-0.0284, -0.0488, -0.0621]],\n",
       "               \n",
       "                        [[ 0.0481,  0.0348,  0.0502],\n",
       "                         [-0.0334, -0.0074, -0.0333],\n",
       "                         [-0.0190,  0.0290, -0.0178]],\n",
       "               \n",
       "                        [[-0.0292, -0.0477,  0.0029],\n",
       "                         [-0.0248,  0.0199,  0.0299],\n",
       "                         [ 0.0197,  0.0072, -0.0158]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0480, -0.0068,  0.0286],\n",
       "                         [ 0.0603,  0.0295,  0.0210],\n",
       "                         [ 0.0711, -0.0130, -0.0328]],\n",
       "               \n",
       "                        [[ 0.0223,  0.0061,  0.0421],\n",
       "                         [-0.0303, -0.0200, -0.0043],\n",
       "                         [-0.0363, -0.0105, -0.0171]],\n",
       "               \n",
       "                        [[ 0.0173, -0.0089,  0.0043],\n",
       "                         [ 0.0222, -0.0574, -0.0297],\n",
       "                         [-0.0309, -0.0476, -0.0274]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0169, -0.0266, -0.0023],\n",
       "                         [ 0.0255, -0.0444,  0.0158],\n",
       "                         [ 0.0346,  0.0343, -0.0035]],\n",
       "               \n",
       "                        [[ 0.0006,  0.0081,  0.0177],\n",
       "                         [ 0.0342, -0.0040, -0.0359],\n",
       "                         [-0.0153,  0.0568,  0.0447]],\n",
       "               \n",
       "                        [[ 0.0030, -0.0215,  0.0005],\n",
       "                         [-0.0138,  0.0037,  0.0198],\n",
       "                         [ 0.0162,  0.0281, -0.0176]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0259, -0.0444,  0.0085],\n",
       "                         [-0.0211, -0.0559,  0.0187],\n",
       "                         [-0.0232,  0.0275, -0.0026]],\n",
       "               \n",
       "                        [[-0.0676,  0.0422,  0.0139],\n",
       "                         [-0.0091,  0.0115, -0.0253],\n",
       "                         [-0.0105, -0.0283, -0.0476]],\n",
       "               \n",
       "                        [[-0.0283,  0.0285,  0.0265],\n",
       "                         [ 0.0026,  0.0031, -0.0451],\n",
       "                         [-0.0338,  0.0228,  0.0246]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0284, -0.0118,  0.0133],\n",
       "                         [-0.0035,  0.0336, -0.0507],\n",
       "                         [-0.0020, -0.0136, -0.0629]],\n",
       "               \n",
       "                        [[ 0.0089,  0.0384,  0.0680],\n",
       "                         [ 0.0120, -0.0243,  0.0088],\n",
       "                         [ 0.0107, -0.0269,  0.0135]],\n",
       "               \n",
       "                        [[-0.0330, -0.0100,  0.0112],\n",
       "                         [-0.0103,  0.0418,  0.0121],\n",
       "                         [ 0.0298, -0.0036,  0.0211]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0252, -0.0638, -0.0390],\n",
       "                         [-0.0040, -0.0076, -0.0783],\n",
       "                         [-0.0369,  0.0337, -0.0299]],\n",
       "               \n",
       "                        [[-0.0364,  0.0125,  0.0387],\n",
       "                         [ 0.0425, -0.0616, -0.0510],\n",
       "                         [-0.0186, -0.0443, -0.0011]],\n",
       "               \n",
       "                        [[-0.0391, -0.0658,  0.0025],\n",
       "                         [-0.0270,  0.0051,  0.0114],\n",
       "                         [-0.0236, -0.0422,  0.0300]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0359, -0.0145, -0.0686],\n",
       "                         [ 0.0429,  0.0545, -0.0574],\n",
       "                         [ 0.0256,  0.0182, -0.0637]],\n",
       "               \n",
       "                        [[-0.0303, -0.0429,  0.0154],\n",
       "                         [ 0.0285,  0.0513,  0.0445],\n",
       "                         [-0.0162,  0.0132, -0.0194]],\n",
       "               \n",
       "                        [[-0.0280,  0.0199, -0.0135],\n",
       "                         [-0.0352,  0.0049,  0.0488],\n",
       "                         [-0.0226, -0.0304, -0.0049]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-5.5792e-04, -7.9113e-04,  5.8506e-03,  3.2236e-03, -5.2895e-05,\n",
       "                        2.4525e-03,  1.1385e-03,  3.2890e-03,  2.1566e-03, -4.4674e-03,\n",
       "                       -4.4331e-04, -5.2731e-04,  2.1078e-03, -4.9023e-04, -1.3092e-03,\n",
       "                       -1.4069e-03, -6.0741e-04, -4.1923e-03, -3.4886e-03,  5.4846e-03,\n",
       "                       -4.3377e-03, -6.4095e-03, -3.6762e-03, -3.5850e-03,  2.8613e-04,\n",
       "                        2.1329e-03, -5.2668e-03,  2.0436e-03,  9.3361e-03, -7.8858e-04,\n",
       "                        4.9879e-03,  5.7962e-03, -3.0368e-03, -6.9034e-03,  8.6001e-04,\n",
       "                        4.3462e-03, -3.2043e-03,  7.5440e-03,  5.2666e-03, -2.0861e-03,\n",
       "                        2.5734e-03,  4.2173e-03, -3.6059e-03, -6.7514e-03, -2.6683e-03,\n",
       "                        2.1980e-03, -4.5047e-03, -1.6393e-03, -2.0635e-03,  5.1352e-03,\n",
       "                        8.4229e-05,  6.2826e-03,  7.9123e-04, -1.9315e-03, -2.5483e-03,\n",
       "                       -7.2603e-03,  3.1633e-03,  3.7547e-03, -1.4993e-03,  2.9674e-04,\n",
       "                        7.5202e-05, -1.4950e-03, -1.0496e-03,  4.4114e-03,  2.0476e-03,\n",
       "                       -3.5670e-03, -2.7809e-03,  2.1432e-03, -7.8467e-04,  2.8549e-03,\n",
       "                       -2.3947e-03,  3.9240e-03,  8.0568e-05,  7.5405e-03, -1.0619e-02,\n",
       "                        5.2392e-03, -3.3721e-03, -8.5462e-03, -3.0968e-03, -2.6044e-03,\n",
       "                        1.3781e-03, -1.2559e-03,  2.2476e-03,  2.2112e-03, -6.2271e-03,\n",
       "                        8.7330e-03, -6.2751e-04, -8.3543e-03, -4.3314e-03, -4.6436e-03,\n",
       "                        2.3579e-04, -5.9767e-03,  3.2189e-03,  1.9951e-03, -2.0301e-03,\n",
       "                       -5.4900e-03,  2.0585e-03,  5.1867e-03,  1.5442e-03,  2.7891e-03,\n",
       "                        5.6408e-03,  5.5771e-03, -1.2628e-03,  4.2821e-03,  2.5217e-03,\n",
       "                       -2.3154e-03,  2.9257e-03, -9.0225e-04,  2.7326e-03,  2.8792e-03,\n",
       "                       -1.8633e-03,  1.3265e-03,  5.1772e-03, -2.0485e-04,  1.2734e-03,\n",
       "                        2.5520e-03,  2.9322e-03,  3.2171e-04,  5.8223e-05,  9.9714e-04,\n",
       "                       -2.1446e-03,  9.6302e-04, -4.6296e-03,  4.8222e-03, -1.2124e-03,\n",
       "                        4.7727e-03, -5.0751e-03,  5.1568e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.1429, -0.0496, -0.1436, -0.0732, -0.0292, -0.2046, -0.0064, -0.2082,\n",
       "                       -0.0769, -0.0424, -0.0602, -0.1655, -0.0567, -0.0850, -0.1310, -0.0864,\n",
       "                       -0.1121,  0.0377, -0.1097, -0.1241, -0.1153, -0.1180, -0.1770, -0.1879,\n",
       "                       -0.0740, -0.0532, -0.1527, -0.0476, -0.0532, -0.0993, -0.0664, -0.0954,\n",
       "                       -0.0869, -0.0983, -0.1258, -0.1488, -0.0308, -0.0279, -0.2106, -0.0336,\n",
       "                       -0.1650, -0.0569, -0.0972, -0.2325, -0.1286, -0.0827, -0.0753, -0.1852,\n",
       "                       -0.0530, -0.1454, -0.0975,  0.0415, -0.0388, -0.1522, -0.0547, -0.1536,\n",
       "                       -0.1188, -0.0783, -0.1884, -0.0681,  0.0063, -0.1422, -0.0944, -0.0852,\n",
       "                       -0.0441, -0.0308, -0.0500, -0.0563, -0.1915, -0.0746, -0.0951, -0.1460,\n",
       "                       -0.1157, -0.0387, -0.1201, -0.0942, -0.1219,  0.0755, -0.0434, -0.1220,\n",
       "                       -0.2786, -0.0729,  0.0313, -0.1002, -0.1045, -0.0814, -0.0310, -0.0547,\n",
       "                       -0.0063, -0.1588, -0.0526, -0.1195,  0.0017, -0.1784, -0.1219, -0.0307,\n",
       "                       -0.1209, -0.1106, -0.1079, -0.2058, -0.1349, -0.0044, -0.1542, -0.1129,\n",
       "                       -0.1338, -0.0860, -0.1576, -0.1786, -0.0051, -0.0820, -0.0670, -0.0445,\n",
       "                       -0.2022, -0.0493, -0.1398,  0.0359, -0.0788, -0.1657, -0.0828, -0.0630,\n",
       "                       -0.0771, -0.0915, -0.0102, -0.1967, -0.0508, -0.0919, -0.0956, -0.1072],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9698, 1.0109, 1.1265, 0.9856, 0.9902, 1.0483, 0.9617, 1.1151, 1.0579,\n",
       "                       0.9476, 0.9782, 1.1198, 0.9851, 1.1242, 1.0475, 1.0133, 0.9602, 0.9657,\n",
       "                       1.0513, 0.9880, 0.9724, 1.0062, 0.9952, 1.0506, 1.0367, 0.9674, 0.9811,\n",
       "                       0.9443, 0.9962, 1.0167, 1.0308, 0.9534, 1.0692, 1.0065, 1.0054, 0.9931,\n",
       "                       1.0099, 1.0049, 0.9671, 1.0442, 1.0852, 0.9753, 1.0893, 0.9968, 1.0108,\n",
       "                       0.9629, 0.9710, 1.1061, 1.0898, 0.9157, 0.9043, 1.0010, 0.9590, 1.0357,\n",
       "                       0.9562, 1.0235, 0.9584, 0.9346, 1.0211, 0.9312, 1.0132, 0.9670, 1.0352,\n",
       "                       1.0168, 0.9353, 0.9917, 1.0386, 1.0379, 0.8983, 1.0347, 0.9529, 0.9743,\n",
       "                       0.9704, 1.0028, 1.0774, 0.9714, 1.0144, 0.9686, 0.9532, 0.9652, 1.1262,\n",
       "                       0.9970, 0.9855, 0.9532, 1.0537, 0.9325, 1.0047, 0.9282, 1.0632, 0.9409,\n",
       "                       0.9606, 1.0761, 0.9709, 1.0417, 0.9615, 0.9635, 0.9539, 0.9418, 1.0209,\n",
       "                       1.0887, 1.0605, 1.0113, 0.9916, 0.9200, 1.0174, 1.0101, 0.9527, 1.0405,\n",
       "                       0.9554, 0.9998, 0.9859, 0.9911, 1.1357, 0.9664, 0.9612, 1.0647, 1.0444,\n",
       "                       1.0444, 0.9618, 0.9837, 0.9814, 1.0089, 0.9519, 0.9316, 0.9638, 0.9875,\n",
       "                       1.0312, 1.0080], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-0.0041,  0.0464, -0.0032],\n",
       "                         [-0.0424, -0.0043,  0.0048],\n",
       "                         [ 0.0100, -0.0051,  0.0314]],\n",
       "               \n",
       "                        [[ 0.0132, -0.0181,  0.0148],\n",
       "                         [-0.0249,  0.0010, -0.0206],\n",
       "                         [ 0.0492,  0.0398, -0.0294]],\n",
       "               \n",
       "                        [[-0.0128, -0.0399,  0.0002],\n",
       "                         [ 0.0122, -0.0294, -0.0561],\n",
       "                         [-0.0636, -0.0639, -0.0782]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0377, -0.0382, -0.0503],\n",
       "                         [ 0.0265,  0.0189,  0.0467],\n",
       "                         [-0.0059, -0.0418, -0.0206]],\n",
       "               \n",
       "                        [[-0.0149, -0.0613, -0.0233],\n",
       "                         [ 0.0361, -0.0186, -0.0376],\n",
       "                         [ 0.0676,  0.0150, -0.0174]],\n",
       "               \n",
       "                        [[-0.0112,  0.0123,  0.0228],\n",
       "                         [ 0.0112, -0.0002, -0.0178],\n",
       "                         [ 0.0243,  0.0187,  0.0285]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0011,  0.0009, -0.0166],\n",
       "                         [-0.0142, -0.0071, -0.0478],\n",
       "                         [ 0.0081, -0.0388, -0.0251]],\n",
       "               \n",
       "                        [[-0.0710, -0.0438, -0.0168],\n",
       "                         [-0.0384, -0.0315, -0.0120],\n",
       "                         [-0.0479, -0.0183,  0.0118]],\n",
       "               \n",
       "                        [[-0.0016,  0.0636,  0.0453],\n",
       "                         [ 0.0184,  0.0768,  0.0357],\n",
       "                         [ 0.0136,  0.0325,  0.0596]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0168,  0.0309, -0.0028],\n",
       "                         [-0.0099, -0.0448, -0.0311],\n",
       "                         [ 0.0195,  0.0280,  0.0056]],\n",
       "               \n",
       "                        [[-0.0166, -0.0425, -0.0207],\n",
       "                         [-0.0548, -0.0369, -0.0604],\n",
       "                         [-0.0473, -0.0121, -0.0266]],\n",
       "               \n",
       "                        [[ 0.0107, -0.0601, -0.0417],\n",
       "                         [-0.0345,  0.0076,  0.0121],\n",
       "                         [-0.0281,  0.0265, -0.0093]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0295, -0.0228, -0.0372],\n",
       "                         [-0.0794, -0.0650,  0.0144],\n",
       "                         [-0.0362, -0.0430, -0.0257]],\n",
       "               \n",
       "                        [[ 0.0120,  0.0421, -0.0080],\n",
       "                         [-0.0459,  0.0306, -0.0127],\n",
       "                         [-0.0663,  0.0371, -0.0183]],\n",
       "               \n",
       "                        [[ 0.0310, -0.0286,  0.0322],\n",
       "                         [-0.0406,  0.0304, -0.0145],\n",
       "                         [-0.0492,  0.0252,  0.0382]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0060,  0.0110, -0.0361],\n",
       "                         [-0.0065, -0.0474, -0.0375],\n",
       "                         [-0.0376, -0.0210,  0.0328]],\n",
       "               \n",
       "                        [[-0.0261, -0.0292,  0.0184],\n",
       "                         [ 0.0078, -0.0488, -0.0432],\n",
       "                         [-0.0573, -0.0471,  0.0375]],\n",
       "               \n",
       "                        [[-0.0400, -0.0107,  0.0365],\n",
       "                         [ 0.0010,  0.0374,  0.0262],\n",
       "                         [-0.0589, -0.0114,  0.0135]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0560, -0.0388, -0.0655],\n",
       "                         [-0.0012, -0.0577, -0.0371],\n",
       "                         [-0.0291,  0.0007, -0.0254]],\n",
       "               \n",
       "                        [[-0.0207, -0.0252,  0.0243],\n",
       "                         [ 0.0107,  0.0223, -0.0271],\n",
       "                         [ 0.0299,  0.0139,  0.0147]],\n",
       "               \n",
       "                        [[ 0.0632,  0.0398,  0.0166],\n",
       "                         [ 0.0410,  0.0132,  0.0760],\n",
       "                         [ 0.0320, -0.0117,  0.0011]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0059,  0.0064,  0.0227],\n",
       "                         [ 0.0405,  0.0412,  0.0488],\n",
       "                         [ 0.0122,  0.0112,  0.0333]],\n",
       "               \n",
       "                        [[ 0.0619, -0.0259,  0.0515],\n",
       "                         [ 0.0194,  0.0323,  0.0043],\n",
       "                         [-0.0070,  0.0484, -0.0731]],\n",
       "               \n",
       "                        [[ 0.0765, -0.0118, -0.0210],\n",
       "                         [ 0.0284,  0.0235, -0.0069],\n",
       "                         [ 0.0197,  0.0736, -0.0300]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0372,  0.0562, -0.0220],\n",
       "                         [-0.0314, -0.0309, -0.0552],\n",
       "                         [-0.0053,  0.0228, -0.0748]],\n",
       "               \n",
       "                        [[-0.0405, -0.0706, -0.0123],\n",
       "                         [-0.0362, -0.0373,  0.0222],\n",
       "                         [-0.0391,  0.0271,  0.0214]],\n",
       "               \n",
       "                        [[ 0.0229,  0.0290,  0.0064],\n",
       "                         [ 0.0479,  0.0470,  0.0276],\n",
       "                         [-0.0151, -0.0382, -0.0028]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0473, -0.0533, -0.0130],\n",
       "                         [-0.0381,  0.0474, -0.0242],\n",
       "                         [-0.0350,  0.0047, -0.0202]],\n",
       "               \n",
       "                        [[-0.0805, -0.0208, -0.0532],\n",
       "                         [-0.0255, -0.0425,  0.0174],\n",
       "                         [ 0.0332,  0.0239, -0.0495]],\n",
       "               \n",
       "                        [[ 0.0072,  0.0615, -0.0374],\n",
       "                         [-0.0377,  0.0580, -0.0349],\n",
       "                         [ 0.0077,  0.0610,  0.0555]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0238, -0.0288, -0.0151],\n",
       "                         [-0.0048, -0.0152,  0.0489],\n",
       "                         [-0.0014,  0.0028,  0.0615]],\n",
       "               \n",
       "                        [[ 0.0446,  0.0245,  0.0180],\n",
       "                         [-0.0220, -0.0566, -0.0778],\n",
       "                         [-0.0056, -0.0120,  0.0013]],\n",
       "               \n",
       "                        [[ 0.0193, -0.0170, -0.0209],\n",
       "                         [ 0.0017, -0.0140, -0.0319],\n",
       "                         [ 0.0104,  0.0243, -0.0320]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0501,  0.0361, -0.0478],\n",
       "                         [ 0.0639,  0.0492, -0.0373],\n",
       "                         [ 0.0036,  0.0074, -0.0241]],\n",
       "               \n",
       "                        [[ 0.0186,  0.0188, -0.0020],\n",
       "                         [ 0.0013,  0.0263, -0.0145],\n",
       "                         [ 0.0346, -0.0020, -0.0376]],\n",
       "               \n",
       "                        [[-0.0546,  0.0166,  0.0015],\n",
       "                         [-0.0100,  0.0100, -0.0214],\n",
       "                         [ 0.0656, -0.0059, -0.0353]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 0.0016, -0.0038, -0.0011,  0.0016,  0.0076, -0.0016,  0.0075,  0.0054,\n",
       "                        0.0079,  0.0010, -0.0017,  0.0032, -0.0004, -0.0067, -0.0003,  0.0002,\n",
       "                        0.0028, -0.0044, -0.0016, -0.0003,  0.0012,  0.0001, -0.0021,  0.0020,\n",
       "                        0.0034, -0.0128,  0.0064,  0.0049, -0.0062,  0.0036,  0.0028,  0.0051,\n",
       "                        0.0030, -0.0001, -0.0039, -0.0007, -0.0024, -0.0040,  0.0003,  0.0043,\n",
       "                        0.0077, -0.0070, -0.0027, -0.0016, -0.0044, -0.0033, -0.0003,  0.0033,\n",
       "                        0.0072,  0.0002, -0.0004,  0.0005, -0.0110, -0.0017,  0.0008,  0.0034,\n",
       "                       -0.0022, -0.0090,  0.0103, -0.0002, -0.0004,  0.0038, -0.0004,  0.0076,\n",
       "                       -0.0065, -0.0044, -0.0046,  0.0077, -0.0012,  0.0047, -0.0007, -0.0044,\n",
       "                        0.0043, -0.0027, -0.0030, -0.0006, -0.0006,  0.0022, -0.0085,  0.0027,\n",
       "                       -0.0054, -0.0032,  0.0070, -0.0003, -0.0004,  0.0041,  0.0008, -0.0008,\n",
       "                       -0.0012, -0.0020, -0.0027,  0.0050,  0.0008, -0.0024,  0.0011, -0.0010,\n",
       "                        0.0046, -0.0068,  0.0064, -0.0168, -0.0041, -0.0039,  0.0012,  0.0014,\n",
       "                        0.0039, -0.0020,  0.0025,  0.0012, -0.0079, -0.0009,  0.0006, -0.0046,\n",
       "                        0.0014, -0.0027, -0.0094,  0.0006, -0.0007, -0.0035,  0.0007, -0.0055,\n",
       "                       -0.0034, -0.0046,  0.0040, -0.0059,  0.0090,  0.0006,  0.0076,  0.0031],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.0421, -0.1116, -0.1055, -0.0928, -0.0095, -0.1364, -0.1398, -0.1019,\n",
       "                       -0.1074, -0.1131, -0.1300, -0.0447, -0.1526, -0.0871, -0.0757, -0.2259,\n",
       "                       -0.1209, -0.0844, -0.1135, -0.1146, -0.1229, -0.0476, -0.1129, -0.1467,\n",
       "                        0.0539, -0.1339, -0.1526, -0.0978, -0.3210, -0.2010, -0.1782, -0.0861,\n",
       "                       -0.1257, -0.1625, -0.1607, -0.1491, -0.0748, -0.1737, -0.1923, -0.0705,\n",
       "                       -0.0842, -0.2337, -0.2269, -0.1465, -0.2113, -0.1181, -0.1329, -0.1023,\n",
       "                       -0.0075, -0.2606, -0.1638, -0.1513, -0.1211, -0.1118, -0.1264, -0.2641,\n",
       "                       -0.1832, -0.1711, -0.2516, -0.1133, -0.2346, -0.1305, -0.1572, -0.1064,\n",
       "                       -0.1350, -0.1728, -0.1169, -0.0812, -0.0626, -0.1919, -0.1534, -0.1062,\n",
       "                       -0.1489, -0.1472, -0.1228, -0.1373, -0.1461, -0.0940, -0.1121, -0.0729,\n",
       "                       -0.1168, -0.1166, -0.1321, -0.1034, -0.0461, -0.1542, -0.1561, -0.0461,\n",
       "                       -0.1715, -0.1117, -0.1616, -0.0798, -0.1613, -0.1429, -0.1995, -0.1100,\n",
       "                       -0.0853, -0.1605, -0.0440, -0.0555, -0.1381, -0.0693, -0.0791, -0.2111,\n",
       "                       -0.1204, -0.2483, -0.1759, -0.0950, -0.0868, -0.1298, -0.1025, -0.1668,\n",
       "                       -0.0857, -0.1547, -0.0610, -0.1091, -0.2388, -0.0210, -0.1152, -0.1244,\n",
       "                       -0.0864, -0.0877, -0.0648, -0.0774, -0.1602, -0.1419, -0.1275, -0.0906],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.9922, 0.9754, 0.8999, 1.0176, 0.9902, 0.9183, 1.0014, 1.0994, 1.0165,\n",
       "                       1.0297, 0.9774, 1.0767, 0.9982, 0.9782, 0.9947, 0.9802, 0.9759, 1.0174,\n",
       "                       1.0347, 1.0063, 1.0189, 0.9529, 1.0372, 0.9780, 1.0294, 0.9646, 0.9739,\n",
       "                       0.9818, 1.1532, 1.0436, 0.9813, 1.0218, 0.9697, 1.0840, 1.0247, 1.0813,\n",
       "                       0.9215, 1.0491, 0.9741, 0.9364, 0.9011, 1.0681, 0.9903, 1.0119, 1.0662,\n",
       "                       1.0049, 1.0219, 1.0104, 0.9993, 0.9599, 0.9403, 1.0864, 0.9962, 1.0052,\n",
       "                       0.9442, 1.0319, 1.0130, 0.9770, 0.9796, 1.0515, 1.0083, 1.0219, 1.0180,\n",
       "                       0.9983, 0.9353, 0.9891, 0.9353, 0.9666, 0.8969, 0.9892, 1.0544, 0.9787,\n",
       "                       0.9501, 0.9682, 0.9476, 0.9637, 1.0193, 0.9853, 0.9277, 0.9618, 1.0788,\n",
       "                       0.9349, 0.9583, 1.1168, 0.9422, 1.0290, 0.9501, 0.9911, 1.0266, 0.9934,\n",
       "                       0.9972, 0.9857, 0.9518, 0.9144, 0.9282, 0.9882, 0.9610, 0.9967, 1.0139,\n",
       "                       1.0453, 0.9466, 1.0218, 0.9197, 1.0204, 1.0175, 1.0494, 0.9914, 0.9428,\n",
       "                       0.9705, 0.9307, 1.0384, 1.0437, 0.9407, 1.0344, 0.9367, 0.9942, 0.9264,\n",
       "                       1.0072, 1.0353, 0.9423, 0.9944, 1.0356, 0.9787, 0.9966, 0.9665, 1.0274,\n",
       "                       0.9621, 0.9388], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-3.2616e-02, -2.4932e-02, -2.0655e-02],\n",
       "                         [ 8.9913e-03,  2.3146e-02, -7.6671e-03],\n",
       "                         [-8.4909e-03,  6.0398e-03,  3.1839e-02]],\n",
       "               \n",
       "                        [[ 2.4757e-03, -5.1543e-02, -1.9752e-02],\n",
       "                         [ 3.5121e-03, -1.7556e-02, -1.2990e-02],\n",
       "                         [ 4.8698e-03,  3.0376e-02, -2.1583e-02]],\n",
       "               \n",
       "                        [[ 5.4075e-02, -1.7164e-02,  5.9379e-02],\n",
       "                         [-4.5453e-02, -5.0168e-02,  2.5293e-02],\n",
       "                         [-3.9245e-02,  1.8367e-03, -5.6766e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.5344e-02, -1.5439e-02, -3.9790e-02],\n",
       "                         [-1.9808e-03, -2.4804e-02, -1.1805e-02],\n",
       "                         [ 2.9644e-02,  5.2937e-02,  2.8858e-02]],\n",
       "               \n",
       "                        [[-4.8010e-02, -3.3648e-02, -4.0663e-02],\n",
       "                         [ 2.7305e-02,  1.1206e-02, -3.4574e-04],\n",
       "                         [ 8.3257e-03, -2.6022e-03, -1.8228e-02]],\n",
       "               \n",
       "                        [[-4.0210e-02, -5.1254e-02, -1.5467e-02],\n",
       "                         [-6.5395e-02, -3.3563e-02, -6.4346e-02],\n",
       "                         [-3.3204e-02, -5.8817e-02, -3.4641e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1014e-02, -2.1644e-02, -2.5366e-02],\n",
       "                         [ 1.9658e-02, -6.8745e-02,  3.4953e-04],\n",
       "                         [-3.9772e-03, -3.8414e-02, -4.6109e-02]],\n",
       "               \n",
       "                        [[-3.0052e-02, -3.8806e-03,  1.1035e-02],\n",
       "                         [-4.3393e-02, -1.3985e-02, -4.0940e-02],\n",
       "                         [-4.8262e-02,  2.2771e-02, -1.0985e-02]],\n",
       "               \n",
       "                        [[ 3.9794e-03,  6.3887e-03,  3.9383e-02],\n",
       "                         [ 3.6960e-02, -1.3598e-02, -1.9116e-03],\n",
       "                         [ 7.7164e-03,  3.1468e-02,  4.6221e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.8603e-02, -9.0717e-03,  8.0257e-02],\n",
       "                         [-1.3675e-02, -4.7192e-02,  3.7982e-02],\n",
       "                         [ 6.0364e-02, -4.8749e-03, -2.8348e-02]],\n",
       "               \n",
       "                        [[-1.2567e-02,  2.6043e-02, -2.5764e-02],\n",
       "                         [ 1.4639e-02,  2.7775e-02, -2.3279e-03],\n",
       "                         [ 5.3442e-02,  1.6549e-03,  4.2369e-02]],\n",
       "               \n",
       "                        [[-1.2943e-02,  1.3481e-03, -7.2848e-02],\n",
       "                         [-5.8800e-02, -3.2945e-02, -5.4671e-02],\n",
       "                         [-2.6958e-02, -6.9885e-03, -2.6796e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.8730e-03,  9.6808e-03, -8.5861e-03],\n",
       "                         [-5.3937e-03,  2.0185e-04,  8.4631e-04],\n",
       "                         [ 4.3993e-02, -1.5065e-02, -3.1825e-02]],\n",
       "               \n",
       "                        [[ 4.7858e-02,  6.0402e-02,  8.1491e-02],\n",
       "                         [ 2.4329e-02,  5.8101e-02,  3.3962e-02],\n",
       "                         [ 1.8964e-02, -3.5551e-04,  1.8016e-02]],\n",
       "               \n",
       "                        [[-1.4426e-02, -3.3960e-02, -2.6048e-02],\n",
       "                         [-6.2025e-02, -1.7535e-02, -3.7161e-02],\n",
       "                         [-1.4047e-02,  2.8182e-02, -2.7651e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.6949e-02, -4.6111e-02, -5.3481e-02],\n",
       "                         [ 1.7327e-02, -6.6139e-02, -4.1320e-02],\n",
       "                         [-1.6330e-02,  3.3298e-03,  2.8208e-02]],\n",
       "               \n",
       "                        [[ 2.6015e-02,  2.6877e-02,  3.7420e-02],\n",
       "                         [ 3.3668e-02,  4.2677e-02,  6.5444e-02],\n",
       "                         [-3.1017e-02, -3.0965e-03,  3.9945e-03]],\n",
       "               \n",
       "                        [[ 6.6623e-05,  4.0553e-02,  2.6456e-02],\n",
       "                         [ 3.2330e-02,  1.2408e-02,  2.2046e-02],\n",
       "                         [ 3.4478e-02,  1.7792e-02,  4.9181e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4783e-02, -4.7150e-03, -1.8101e-03],\n",
       "                         [ 1.5606e-02, -9.7497e-03, -1.6395e-02],\n",
       "                         [ 8.3795e-03, -2.5722e-02, -4.3247e-02]],\n",
       "               \n",
       "                        [[-2.6882e-02,  2.7049e-03, -1.5982e-02],\n",
       "                         [ 2.2673e-02, -9.9841e-03, -2.1198e-02],\n",
       "                         [ 1.7227e-03, -2.7470e-02,  1.1726e-02]],\n",
       "               \n",
       "                        [[ 1.9837e-02,  3.2764e-02,  6.2590e-02],\n",
       "                         [ 7.6517e-03,  1.3291e-02, -1.6781e-02],\n",
       "                         [-2.7680e-02,  9.9408e-03,  1.5392e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3952e-03,  2.6139e-02, -3.0925e-02],\n",
       "                         [ 2.0876e-02,  5.1119e-03,  7.3498e-03],\n",
       "                         [ 4.3291e-02,  1.9333e-02,  2.8315e-02]],\n",
       "               \n",
       "                        [[-5.2263e-02,  5.5803e-03, -1.5884e-02],\n",
       "                         [-6.3904e-03,  2.3327e-02, -2.0179e-02],\n",
       "                         [-4.8943e-02, -5.4209e-02, -7.2376e-02]],\n",
       "               \n",
       "                        [[-6.1664e-02,  1.6211e-02, -1.0296e-02],\n",
       "                         [-4.6212e-02,  2.2559e-02,  2.4096e-02],\n",
       "                         [-5.8087e-02, -2.9418e-02, -4.1723e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.5604e-02,  3.7355e-03, -2.4063e-02],\n",
       "                         [ 2.3695e-03,  9.6156e-03,  1.5818e-02],\n",
       "                         [ 1.7364e-02, -3.1665e-02, -2.4331e-02]],\n",
       "               \n",
       "                        [[ 2.9400e-02, -3.2532e-02,  1.3463e-02],\n",
       "                         [ 4.4850e-02,  3.1855e-02,  3.7362e-02],\n",
       "                         [ 5.7858e-02,  9.6127e-03,  3.9063e-03]],\n",
       "               \n",
       "                        [[ 1.8970e-02, -2.4511e-02, -1.7551e-02],\n",
       "                         [-4.5911e-02, -5.1581e-02, -4.5014e-02],\n",
       "                         [-2.5128e-02,  1.6611e-03, -3.2152e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.9892e-02, -7.9130e-02, -1.6907e-02],\n",
       "                         [-4.4282e-02, -9.4538e-02, -4.2968e-02],\n",
       "                         [-4.9086e-02, -4.1722e-03, -6.0847e-02]],\n",
       "               \n",
       "                        [[ 3.2284e-04, -3.6971e-02,  3.6122e-02],\n",
       "                         [-1.1710e-02,  3.8149e-02,  1.1104e-02],\n",
       "                         [ 1.7850e-02, -2.6091e-02,  5.3877e-02]],\n",
       "               \n",
       "                        [[ 2.8154e-02,  1.7670e-02, -1.5391e-02],\n",
       "                         [-2.1366e-02,  2.3606e-02, -2.4063e-02],\n",
       "                         [ 4.3109e-02,  1.4696e-02,  2.2945e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.9470e-02, -5.1701e-02, -9.8365e-03],\n",
       "                         [ 3.8208e-02,  1.7543e-02, -2.3029e-02],\n",
       "                         [-2.4315e-02, -1.9914e-02,  9.8562e-03]],\n",
       "               \n",
       "                        [[ 2.1215e-02,  5.4988e-02,  5.0239e-02],\n",
       "                         [ 3.4587e-02, -5.0368e-03,  5.5182e-02],\n",
       "                         [ 3.0228e-02,  3.2669e-02,  3.5409e-02]],\n",
       "               \n",
       "                        [[ 2.4233e-02,  3.3993e-02,  4.5775e-03],\n",
       "                         [-1.8958e-02,  1.8024e-02,  2.3348e-02],\n",
       "                         [ 5.3974e-02, -1.2263e-02,  2.0287e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.5606e-03, -5.3625e-02,  1.4732e-03],\n",
       "                         [ 3.5650e-03, -7.3834e-02, -9.7786e-03],\n",
       "                         [-6.4864e-02, -6.6571e-02, -5.7274e-02]],\n",
       "               \n",
       "                        [[-1.0687e-03,  2.9164e-02,  5.3362e-02],\n",
       "                         [ 1.2333e-02,  3.0188e-02, -2.0492e-03],\n",
       "                         [-4.3987e-02,  3.4548e-02,  7.0645e-04]],\n",
       "               \n",
       "                        [[ 2.5849e-02,  1.9746e-02,  1.0487e-02],\n",
       "                         [ 2.6397e-02,  1.9059e-02, -2.5707e-02],\n",
       "                         [ 2.5952e-02, -1.7236e-02, -1.6408e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-1.2836e-02, -3.5970e-02,  2.3509e-03, -2.4860e-02,  7.6290e-02,\n",
       "                       -1.4588e-02, -9.9419e-03,  1.2774e-02, -1.9242e-02, -1.8327e-02,\n",
       "                        8.9031e-02,  1.5505e-02,  8.3930e-03, -8.2259e-03, -2.3932e-03,\n",
       "                       -1.1221e-01,  7.1580e-02, -1.2972e-02, -5.3087e-02,  9.2938e-04,\n",
       "                        4.2967e-03, -6.4166e-03, -1.7645e-02,  2.4251e-02,  4.8568e-02,\n",
       "                       -1.3764e-02,  3.0519e-02,  5.7802e-03,  1.5275e-02,  1.0852e-02,\n",
       "                        1.3195e-02,  8.5356e-03, -3.7144e-03, -1.1546e-02,  7.3099e-03,\n",
       "                       -1.7464e-02, -7.2114e-04,  3.6533e-03,  6.1359e-03, -8.6594e-02,\n",
       "                        9.2045e-03, -2.6454e-02, -2.6271e-03, -7.9267e-03, -2.0432e-02,\n",
       "                       -4.2015e-03, -1.8169e-02,  3.2869e-03, -1.3827e-02, -1.6898e-02,\n",
       "                        1.8528e-03, -1.9193e-02,  4.8083e-03,  4.8877e-03,  2.0913e-02,\n",
       "                       -7.8361e-02, -1.7456e-01, -1.1555e-02,  6.2293e-03,  9.3753e-03,\n",
       "                       -2.4261e-02,  9.0392e-02, -5.2868e-03, -2.5372e-03, -8.7470e-03,\n",
       "                        8.5045e-03,  5.8099e-03, -2.8439e-03, -1.3180e-03,  3.7708e-02,\n",
       "                       -3.7829e-03, -5.3542e-03, -1.1604e-02, -5.0707e-03, -8.9997e-03,\n",
       "                       -2.1839e-02, -1.6735e-02,  1.2865e-02,  2.5948e-03,  1.1602e-02,\n",
       "                       -3.3620e-02, -1.0965e-02, -6.5164e-04,  5.3839e-04,  3.5176e-03,\n",
       "                       -3.0328e-02,  3.2388e-02, -2.5660e-03,  5.8180e-03,  1.7375e-03,\n",
       "                        4.5361e-03,  6.8815e-04,  1.1260e-02, -1.2046e-02,  1.0185e-01,\n",
       "                       -2.9011e-03, -1.6405e-02, -2.2311e-03, -1.8100e-02,  7.4445e-03,\n",
       "                       -1.3307e-04, -1.9340e-03, -1.3466e-02,  9.4725e-03, -1.0266e-02,\n",
       "                       -9.6220e-03, -1.3725e-03,  3.6779e-04, -1.2281e-02,  7.2958e-03,\n",
       "                        4.1408e-03,  6.2526e-03,  2.2820e-02,  9.0402e-03, -5.2334e-02,\n",
       "                       -6.0659e-02, -7.7524e-03,  3.2637e-03,  1.7055e-02,  2.1500e-02,\n",
       "                       -3.2636e-02, -5.3517e-02, -1.8367e-02, -2.7861e-02, -1.2279e-02,\n",
       "                        1.7566e-04, -1.1118e-03,  1.6942e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.0216, -0.0816, -0.1118, -0.2294, -0.1433, -0.2316, -0.1975, -0.1314,\n",
       "                       -0.2521, -0.1323, -0.1592, -0.1602, -0.2391, -0.2632, -0.1186, -0.1614,\n",
       "                       -0.1964, -0.2262, -0.0417, -0.1123, -0.1179, -0.1042,  0.0357, -0.1571,\n",
       "                       -0.1601, -0.1838, -0.1578, -0.2397, -0.2621, -0.0441, -0.2675, -0.2502,\n",
       "                        0.0075, -0.0831, -0.0806, -0.2391, -0.1396, -0.1580, -0.1737, -0.0788,\n",
       "                       -0.0260, -0.0271, -0.2057, -0.1902, -0.1695, -0.1710, -0.1845, -0.0317,\n",
       "                       -0.0648,  0.0142, -0.0240, -0.1625, -0.0539,  0.0514, -0.1875, -0.0333,\n",
       "                       -0.3101, -0.1199, -0.1565, -0.0773, -0.0397, -0.2279, -0.0721, -0.1536,\n",
       "                       -0.0276, -0.1498, -0.1901, -0.1749, -0.1626, -0.2518, -0.2773, -0.1814,\n",
       "                       -0.0899, -0.0862,  0.0288, -0.1338, -0.0863, -0.2674, -0.1651, -0.2073,\n",
       "                       -0.2717, -0.0966, -0.0088, -0.0814, -0.0925, -0.2008, -0.1523, -0.0919,\n",
       "                       -0.0660, -0.0735, -0.1561, -0.0917, -0.1910, -0.2280, -0.3475, -0.3082,\n",
       "                       -0.0601, -0.1246, -0.2019, -0.0845, -0.0481, -0.0787, -0.2317, -0.0700,\n",
       "                       -0.1467, -0.2580, -0.2312, -0.1519, -0.3757, -0.1571, -0.1049, -0.1055,\n",
       "                       -0.1346,  0.0020, -0.1305, -0.1655, -0.0984, -0.0566, -0.0577, -0.2156,\n",
       "                       -0.1525, -0.2066, -0.0852, -0.1477, -0.1520, -0.1927, -0.0525, -0.0349],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([1.2571, 1.1051, 1.1719, 1.0949, 0.9862, 0.9338, 1.1018, 1.0506, 0.8400,\n",
       "                       1.1926, 0.8334, 1.0804, 1.3354, 1.0674, 1.0099, 1.0667, 0.9758, 1.0148,\n",
       "                       1.1930, 1.1072, 1.3623, 1.2371, 1.1795, 0.8250, 0.9098, 0.9614, 0.9536,\n",
       "                       1.0282, 0.9824, 1.0793, 1.0920, 1.3309, 1.3259, 1.0767, 1.1717, 1.0339,\n",
       "                       1.1137, 0.9305, 1.1927, 1.2165, 1.2797, 1.0719, 1.1332, 0.9391, 0.9980,\n",
       "                       1.1336, 0.9289, 1.2609, 1.2987, 1.2598, 1.2159, 1.0432, 1.2382, 1.2895,\n",
       "                       0.8967, 1.2576, 0.9887, 1.1643, 1.0784, 1.3266, 1.2983, 0.8829, 1.1079,\n",
       "                       1.3064, 1.2816, 0.9526, 0.8499, 1.1524, 1.2207, 0.8576, 0.8474, 0.9693,\n",
       "                       1.0616, 1.1683, 1.3155, 1.1131, 1.0600, 1.1460, 1.0195, 0.7634, 1.0407,\n",
       "                       1.1432, 1.1596, 1.1931, 1.1236, 1.1252, 1.0016, 1.1539, 1.2041, 1.1657,\n",
       "                       1.2230, 1.2866, 1.1097, 0.9602, 0.9412, 0.9845, 1.2572, 1.0955, 1.0284,\n",
       "                       1.3036, 1.1939, 1.2709, 0.8236, 1.2092, 1.1324, 1.3368, 1.0795, 1.2006,\n",
       "                       0.9766, 1.0319, 1.2499, 1.1554, 0.8969, 1.1355, 1.1060, 0.8293, 1.1235,\n",
       "                       1.1743, 1.1542, 0.9514, 0.9998, 1.0608, 1.1136, 1.2189, 1.4515, 1.1467,\n",
       "                       1.1914, 1.3611], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0117, -0.0072, -0.0029,  ..., -0.0139, -0.0032, -0.0198],\n",
       "                       [ 0.0004, -0.0015,  0.0084,  ..., -0.0056,  0.0124,  0.0058],\n",
       "                       [ 0.0007, -0.0063,  0.0103,  ..., -0.0072,  0.0157, -0.0184],\n",
       "                       [-0.0141, -0.0183,  0.0148,  ..., -0.0052, -0.0075, -0.0101],\n",
       "                       [-0.0079,  0.0040,  0.0061,  ..., -0.0026,  0.0113, -0.0048]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.1478,  0.0516, -0.0065, -0.0220,  0.0790], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.4767158913612366,\n",
       "   1.3634480488300325,\n",
       "   1.3071567190885545,\n",
       "   1.2579104353189468,\n",
       "   1.2171199986934662,\n",
       "   1.173917641043663,\n",
       "   1.1412354547977448,\n",
       "   1.1206057991981506,\n",
       "   1.1068594620227814,\n",
       "   1.098137009382248,\n",
       "   1.0645314890146256,\n",
       "   1.0544401775598526,\n",
       "   1.0516756229400634,\n",
       "   1.0237542641162873,\n",
       "   1.0251861793994903,\n",
       "   1.0149526587724687,\n",
       "   1.000187307715416,\n",
       "   0.9930191729068756,\n",
       "   0.9857469847202301,\n",
       "   0.97888145840168,\n",
       "   0.9816915237903595,\n",
       "   0.9523157875537872,\n",
       "   0.9452668331861496,\n",
       "   0.9477606774568558,\n",
       "   0.9446416703462601,\n",
       "   0.9375249929428101,\n",
       "   0.926528405547142,\n",
       "   0.9199783676862716,\n",
       "   0.9149922953844071,\n",
       "   0.9206487500667572,\n",
       "   0.9009133068919182,\n",
       "   0.9013869527578354,\n",
       "   0.892678109407425,\n",
       "   0.8734482300281524,\n",
       "   0.8892216812372208,\n",
       "   0.8612553506493569,\n",
       "   0.868316612958908,\n",
       "   0.8687641552686691,\n",
       "   0.8637579326629639,\n",
       "   0.8440353071689606,\n",
       "   0.8590596607327461,\n",
       "   0.8482160774469376,\n",
       "   0.8369134679436684,\n",
       "   0.8475043286085129,\n",
       "   0.8311423906087876,\n",
       "   0.8321626817584038,\n",
       "   0.8311296834945678,\n",
       "   0.8195699316859245,\n",
       "   0.8129925481081008,\n",
       "   0.818500467658043,\n",
       "   0.8169194771647453,\n",
       "   0.7981972977519035,\n",
       "   0.8081083199381829,\n",
       "   0.7970852879881859,\n",
       "   0.8058020141720772,\n",
       "   0.797419324696064,\n",
       "   0.790470686852932,\n",
       "   0.7883597086668015,\n",
       "   0.7819636951088905,\n",
       "   0.788220066010952,\n",
       "   0.7838750763535499,\n",
       "   0.7755201852917671,\n",
       "   0.7602797461152077,\n",
       "   0.759028637111187,\n",
       "   0.7724482110738754,\n",
       "   0.7648694373369217,\n",
       "   0.7616731026172638,\n",
       "   0.7416369780302048,\n",
       "   0.7462354970574379,\n",
       "   0.7512644548416137,\n",
       "   0.7400297101140022,\n",
       "   0.7446809396743774,\n",
       "   0.7435496129393577,\n",
       "   0.7367992547750473,\n",
       "   0.7295039440989495,\n",
       "   0.7241930704712868,\n",
       "   0.7361687917113304,\n",
       "   0.7226850116252899,\n",
       "   0.7129439452290535,\n",
       "   0.7273463283777237,\n",
       "   0.7083207130432129,\n",
       "   0.7122180984020233,\n",
       "   0.7097160971164703,\n",
       "   0.7079002514481545,\n",
       "   0.6958721086978913,\n",
       "   0.7014135954976082,\n",
       "   0.6957080595493317,\n",
       "   0.6935309938192368,\n",
       "   0.6845272625088692,\n",
       "   0.6794929493665696,\n",
       "   0.6790566522479057,\n",
       "   0.6917429084181785,\n",
       "   0.6626861661076545,\n",
       "   0.676537765622139,\n",
       "   0.6783934021592141,\n",
       "   0.6826036170721054,\n",
       "   0.6723332619667053,\n",
       "   0.6706071465015412,\n",
       "   0.6548732869029045],\n",
       "  'train_loss_std': [0.17689869570606706,\n",
       "   0.1352598287025092,\n",
       "   0.1402718352415171,\n",
       "   0.12589288336623228,\n",
       "   0.12743361064392014,\n",
       "   0.12653900803185938,\n",
       "   0.13695869766030852,\n",
       "   0.1279429461229426,\n",
       "   0.13241911997481837,\n",
       "   0.129164871752536,\n",
       "   0.13028109339728397,\n",
       "   0.13962508927272124,\n",
       "   0.12443879021028362,\n",
       "   0.13489186732330807,\n",
       "   0.13065722566108395,\n",
       "   0.14001642828509483,\n",
       "   0.1255756429448517,\n",
       "   0.13064612735631276,\n",
       "   0.13565883376182772,\n",
       "   0.14491696541554733,\n",
       "   0.1401324864026962,\n",
       "   0.12921438635842897,\n",
       "   0.13516714985126937,\n",
       "   0.14054323466242138,\n",
       "   0.1316063673110581,\n",
       "   0.14460529546779322,\n",
       "   0.13170280040826113,\n",
       "   0.1374121798220071,\n",
       "   0.13352969128221234,\n",
       "   0.14712204805684753,\n",
       "   0.1338416687373143,\n",
       "   0.1401658684332562,\n",
       "   0.1350383977507164,\n",
       "   0.13860401789236776,\n",
       "   0.14070488309891707,\n",
       "   0.14680906679560532,\n",
       "   0.13471583883667407,\n",
       "   0.14066478346316288,\n",
       "   0.14015173261218436,\n",
       "   0.14116981859155872,\n",
       "   0.1379769976349596,\n",
       "   0.13813283970561482,\n",
       "   0.13982973239620655,\n",
       "   0.14280001799343328,\n",
       "   0.1374314160286445,\n",
       "   0.1419353664887383,\n",
       "   0.14500682724332872,\n",
       "   0.13878236656181298,\n",
       "   0.1375143629837021,\n",
       "   0.14081425341317164,\n",
       "   0.14098792862039106,\n",
       "   0.13687021924732626,\n",
       "   0.1375878903198021,\n",
       "   0.14228470901298912,\n",
       "   0.1467323818112154,\n",
       "   0.13844840427482985,\n",
       "   0.13540140748386084,\n",
       "   0.14336061255393076,\n",
       "   0.13568132335342445,\n",
       "   0.1344603872763495,\n",
       "   0.14427170381842588,\n",
       "   0.1390511317773623,\n",
       "   0.1379599147951751,\n",
       "   0.1355961693713135,\n",
       "   0.13990828414643708,\n",
       "   0.13632645303447527,\n",
       "   0.13531807939660623,\n",
       "   0.13792951241946935,\n",
       "   0.1289075020836924,\n",
       "   0.1385780753614346,\n",
       "   0.12634596326955091,\n",
       "   0.137157145942509,\n",
       "   0.13427220186760544,\n",
       "   0.13925920746517967,\n",
       "   0.13253181562297753,\n",
       "   0.13629290623456894,\n",
       "   0.14115670406457906,\n",
       "   0.13553057099686636,\n",
       "   0.13421703272382052,\n",
       "   0.1372666131079493,\n",
       "   0.13093064260114395,\n",
       "   0.13944052684284544,\n",
       "   0.13639528578801074,\n",
       "   0.1288356022087902,\n",
       "   0.13839768371734984,\n",
       "   0.1268363783604946,\n",
       "   0.1338305106451409,\n",
       "   0.13547001245851428,\n",
       "   0.12955838199573078,\n",
       "   0.1284751677369886,\n",
       "   0.13531452558800008,\n",
       "   0.1353320099747665,\n",
       "   0.13055500166112857,\n",
       "   0.13792777154253402,\n",
       "   0.14305275530025072,\n",
       "   0.14585163349476607,\n",
       "   0.12778105214291216,\n",
       "   0.1306075286124997,\n",
       "   0.12904162314860465],\n",
       "  'train_accuracy_mean': [0.42624000072479246,\n",
       "   0.44893333303928373,\n",
       "   0.47028000020980837,\n",
       "   0.4879600003361702,\n",
       "   0.5122799999117851,\n",
       "   0.5338799992799759,\n",
       "   0.5497466670274734,\n",
       "   0.5590666652917862,\n",
       "   0.5661866648197174,\n",
       "   0.5699733330011367,\n",
       "   0.5884400001168251,\n",
       "   0.5930933327674865,\n",
       "   0.5919466652870178,\n",
       "   0.6038266664147377,\n",
       "   0.6041466662287712,\n",
       "   0.6089066665768623,\n",
       "   0.6143599977493286,\n",
       "   0.6183866647481918,\n",
       "   0.6224133322834968,\n",
       "   0.6259200001955032,\n",
       "   0.6216799988746643,\n",
       "   0.6347599993944169,\n",
       "   0.635306665956974,\n",
       "   0.6381599992513657,\n",
       "   0.6368799999952316,\n",
       "   0.6400666655898094,\n",
       "   0.6459733318090439,\n",
       "   0.6484133321642875,\n",
       "   0.6502266671061516,\n",
       "   0.6472533336877823,\n",
       "   0.6546799997091294,\n",
       "   0.6536799983382225,\n",
       "   0.6603066659569741,\n",
       "   0.666306665301323,\n",
       "   0.6598533327579499,\n",
       "   0.673093332529068,\n",
       "   0.6693199978470802,\n",
       "   0.6682399987578392,\n",
       "   0.6710933324098587,\n",
       "   0.6799600003361702,\n",
       "   0.6735733338594436,\n",
       "   0.6774533334374427,\n",
       "   0.6830800006985664,\n",
       "   0.6782666675448418,\n",
       "   0.6836800006628037,\n",
       "   0.6838799984455108,\n",
       "   0.6831466662883758,\n",
       "   0.6885466662049293,\n",
       "   0.6937333330512047,\n",
       "   0.6905333328247071,\n",
       "   0.68916000020504,\n",
       "   0.6994800003767013,\n",
       "   0.6929466663599014,\n",
       "   0.7012666657567024,\n",
       "   0.6956666675806046,\n",
       "   0.6989066653847694,\n",
       "   0.7021733337640762,\n",
       "   0.6994933343529701,\n",
       "   0.7046800002455711,\n",
       "   0.7041333317756653,\n",
       "   0.7028133336305619,\n",
       "   0.7067599996328354,\n",
       "   0.7123999997973443,\n",
       "   0.7147999982833863,\n",
       "   0.7098533335924149,\n",
       "   0.7116933334469795,\n",
       "   0.7135733338594437,\n",
       "   0.7228666672706604,\n",
       "   0.7216266669034958,\n",
       "   0.7172266674041748,\n",
       "   0.7242533316612244,\n",
       "   0.7199333332777024,\n",
       "   0.7206399997472763,\n",
       "   0.7244266666769982,\n",
       "   0.7274533331394195,\n",
       "   0.7284399999380111,\n",
       "   0.7241600005626678,\n",
       "   0.7299733337163925,\n",
       "   0.7333466655015946,\n",
       "   0.7276799998283386,\n",
       "   0.7327599999904633,\n",
       "   0.7346133334636689,\n",
       "   0.7346266676783562,\n",
       "   0.7359066668748856,\n",
       "   0.7392533333301544,\n",
       "   0.7387066663503646,\n",
       "   0.7397999992370605,\n",
       "   0.743439998626709,\n",
       "   0.7459200006723404,\n",
       "   0.7469333341121673,\n",
       "   0.7477599991559982,\n",
       "   0.7433066674470902,\n",
       "   0.7532800006866455,\n",
       "   0.7505200006961823,\n",
       "   0.7490399998426437,\n",
       "   0.7458533319234848,\n",
       "   0.7495466667413712,\n",
       "   0.7507466659545898,\n",
       "   0.7570799990892411],\n",
       "  'train_accuracy_std': [0.06335785683253592,\n",
       "   0.06411531177101042,\n",
       "   0.07041850633702317,\n",
       "   0.0661353362895313,\n",
       "   0.06695704470958411,\n",
       "   0.067171182044868,\n",
       "   0.06918334930348208,\n",
       "   0.06635942515956623,\n",
       "   0.06848026884656477,\n",
       "   0.06636129539450848,\n",
       "   0.0664313328470571,\n",
       "   0.07025879314151429,\n",
       "   0.06505407487703867,\n",
       "   0.0671861501885416,\n",
       "   0.06817465070565085,\n",
       "   0.0715340183810681,\n",
       "   0.06650055464929086,\n",
       "   0.06782573632926546,\n",
       "   0.06909669748336145,\n",
       "   0.07111538595937673,\n",
       "   0.06777757696367606,\n",
       "   0.06462342499553302,\n",
       "   0.06544221805242341,\n",
       "   0.06917283593953348,\n",
       "   0.06476932700445752,\n",
       "   0.06986014560636114,\n",
       "   0.06665106141763555,\n",
       "   0.0687169252454645,\n",
       "   0.06612877746976915,\n",
       "   0.06999309720818367,\n",
       "   0.06681090973600466,\n",
       "   0.06889518531451835,\n",
       "   0.06543797217198165,\n",
       "   0.06986688459972916,\n",
       "   0.06824840935183389,\n",
       "   0.07083461322747152,\n",
       "   0.0644924097654616,\n",
       "   0.06990543051578835,\n",
       "   0.07017283516242254,\n",
       "   0.0691439612780549,\n",
       "   0.06664756530614828,\n",
       "   0.0658240343205486,\n",
       "   0.06836261358159264,\n",
       "   0.06806284678253202,\n",
       "   0.06746300966883763,\n",
       "   0.06956221048930056,\n",
       "   0.07037335923398355,\n",
       "   0.06599208014550724,\n",
       "   0.06635272797330252,\n",
       "   0.06812524437316153,\n",
       "   0.06715492041553364,\n",
       "   0.06429097628248225,\n",
       "   0.06546742654608223,\n",
       "   0.06788680001841359,\n",
       "   0.06891008565398465,\n",
       "   0.06772660955732257,\n",
       "   0.06566572841124316,\n",
       "   0.06962318455251487,\n",
       "   0.06635835287918632,\n",
       "   0.06434200395366552,\n",
       "   0.07038305164008465,\n",
       "   0.0690886071848284,\n",
       "   0.06663345752662257,\n",
       "   0.06569004635617602,\n",
       "   0.06690009825029306,\n",
       "   0.06511851894253681,\n",
       "   0.06546609046280179,\n",
       "   0.06542208276180214,\n",
       "   0.06311909859968705,\n",
       "   0.06532344953072376,\n",
       "   0.06119802250733679,\n",
       "   0.06609417545703132,\n",
       "   0.06333080148847023,\n",
       "   0.06709433037808447,\n",
       "   0.06273474159243822,\n",
       "   0.06431839060191367,\n",
       "   0.06835028118316099,\n",
       "   0.06505176863713832,\n",
       "   0.06404650936458149,\n",
       "   0.06485981326303152,\n",
       "   0.06480555627724946,\n",
       "   0.06761036544648215,\n",
       "   0.06378587864038786,\n",
       "   0.06180291315934207,\n",
       "   0.06777067806049072,\n",
       "   0.06098173331087307,\n",
       "   0.06338212161688704,\n",
       "   0.06251532906869894,\n",
       "   0.05974146334827796,\n",
       "   0.061351590279286876,\n",
       "   0.06333161556623249,\n",
       "   0.06497657143043357,\n",
       "   0.06516438491315163,\n",
       "   0.06576318299174357,\n",
       "   0.06647882412539771,\n",
       "   0.06802976985884175,\n",
       "   0.06036679656565102,\n",
       "   0.06123232746175497,\n",
       "   0.06074286540796301],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.4921785310904185,\n",
       "   1.4349011488755543,\n",
       "   1.392868107954661,\n",
       "   1.3589893639087678,\n",
       "   1.328627698024114,\n",
       "   1.3024658135573068,\n",
       "   1.271812090476354,\n",
       "   1.2584273489316304,\n",
       "   1.2391207496325174,\n",
       "   1.2277060691515604,\n",
       "   1.2133793389797212,\n",
       "   1.208962291876475,\n",
       "   1.1959229749441147,\n",
       "   1.1993104217449824,\n",
       "   1.171453205148379,\n",
       "   1.172314426501592,\n",
       "   1.1707304189602534,\n",
       "   1.1606350376208623,\n",
       "   1.1502177876234054,\n",
       "   1.156397728919983,\n",
       "   1.1540645796060562,\n",
       "   1.1320610352357228,\n",
       "   1.1273574829101562,\n",
       "   1.1345601433515549,\n",
       "   1.1245780428250631,\n",
       "   1.116837926506996,\n",
       "   1.12111396809419,\n",
       "   1.1114483022689818,\n",
       "   1.0995471062262854,\n",
       "   1.0912707628806433,\n",
       "   1.086010674238205,\n",
       "   1.0953877993424734,\n",
       "   1.079481848080953,\n",
       "   1.0849100877841313,\n",
       "   1.0798055267333984,\n",
       "   1.0767035655180612,\n",
       "   1.0832824140787125,\n",
       "   1.0603507779041925,\n",
       "   1.0652726151545842,\n",
       "   1.0606907238562902,\n",
       "   1.0700321374336879,\n",
       "   1.0542350057760874,\n",
       "   1.057567328810692,\n",
       "   1.0506270160277684,\n",
       "   1.0483927522102992,\n",
       "   1.0343332370122273,\n",
       "   1.0510062378644944,\n",
       "   1.0367580616474152,\n",
       "   1.02929842710495,\n",
       "   1.0348185245196024,\n",
       "   1.0305743078390757,\n",
       "   1.0309193015098572,\n",
       "   1.0196438692013423,\n",
       "   1.0301621369520824,\n",
       "   1.0239421419302623,\n",
       "   1.0189169259866078,\n",
       "   1.0217961829900741,\n",
       "   1.0205125226577123,\n",
       "   1.0072122931480407,\n",
       "   1.0073687398433686,\n",
       "   1.003256685535113,\n",
       "   1.0026022956768672,\n",
       "   1.0217383182048798,\n",
       "   1.0009512865543366,\n",
       "   0.9996233741442363,\n",
       "   0.9884869609276453,\n",
       "   1.0051524855693181,\n",
       "   0.9970224869251251,\n",
       "   1.0037873468796412,\n",
       "   1.00113931953907,\n",
       "   0.9906762452920278,\n",
       "   0.9903932599226634,\n",
       "   0.9866717565059662,\n",
       "   0.9815253869692484,\n",
       "   0.9954759675264359,\n",
       "   0.9891566904385884,\n",
       "   0.9821875421206157,\n",
       "   0.9764660207430521,\n",
       "   0.9780807489156723,\n",
       "   0.9838510831197103,\n",
       "   0.9736264783143997,\n",
       "   0.9593421590328216,\n",
       "   0.9674913106362025,\n",
       "   0.964883951942126,\n",
       "   0.967737796107928,\n",
       "   0.9672007817029953,\n",
       "   0.957558278242747,\n",
       "   0.9573809723059337,\n",
       "   0.9459697665770849,\n",
       "   0.9600377760330836,\n",
       "   0.9573052529493967,\n",
       "   0.9631415802240372,\n",
       "   0.9627398401498795,\n",
       "   0.9558938606580099,\n",
       "   0.9470425182580948,\n",
       "   0.9628892914454142,\n",
       "   0.9466452024380366,\n",
       "   0.9376032749811808,\n",
       "   0.94147561053435],\n",
       "  'val_loss_std': [0.11929786656224975,\n",
       "   0.11146555234101863,\n",
       "   0.11086744711440859,\n",
       "   0.11052912726610216,\n",
       "   0.11444834340748697,\n",
       "   0.11436475501567293,\n",
       "   0.11786439849059259,\n",
       "   0.11741146109701944,\n",
       "   0.12215122102012117,\n",
       "   0.12327949691402151,\n",
       "   0.1235761315671149,\n",
       "   0.12795582859593835,\n",
       "   0.12419028732659704,\n",
       "   0.12985909211388524,\n",
       "   0.12643318478485954,\n",
       "   0.12927582639675017,\n",
       "   0.13099254229144056,\n",
       "   0.13362351708408662,\n",
       "   0.13051026043961303,\n",
       "   0.13620371191713312,\n",
       "   0.1334187120279226,\n",
       "   0.1334287375671124,\n",
       "   0.13393095419502893,\n",
       "   0.13453374848317518,\n",
       "   0.1318913938526434,\n",
       "   0.13377956761959406,\n",
       "   0.13768182279907895,\n",
       "   0.13250010598926146,\n",
       "   0.1354563845831876,\n",
       "   0.13225341209345104,\n",
       "   0.13481565702166173,\n",
       "   0.13799973625347678,\n",
       "   0.13669916927882786,\n",
       "   0.1380053162815218,\n",
       "   0.13844933174672616,\n",
       "   0.13784439986167055,\n",
       "   0.1358499761072414,\n",
       "   0.136518861654771,\n",
       "   0.13863321305729678,\n",
       "   0.13996780681266338,\n",
       "   0.1402954539635983,\n",
       "   0.13592912494765869,\n",
       "   0.137576437435553,\n",
       "   0.13479446292067243,\n",
       "   0.13564883803103603,\n",
       "   0.13323803944496818,\n",
       "   0.13739304184433693,\n",
       "   0.138128465542972,\n",
       "   0.1418721534403241,\n",
       "   0.1372901388829498,\n",
       "   0.13730886740285306,\n",
       "   0.1355905547932385,\n",
       "   0.14051916877817827,\n",
       "   0.14192051531585217,\n",
       "   0.1360594606654521,\n",
       "   0.1370589385564305,\n",
       "   0.13708133076654405,\n",
       "   0.1407754691296458,\n",
       "   0.1378753972233289,\n",
       "   0.1364285384042301,\n",
       "   0.14218233417150897,\n",
       "   0.13937260469266427,\n",
       "   0.14230035889376969,\n",
       "   0.13775325500677998,\n",
       "   0.1373685962855831,\n",
       "   0.13359392526683253,\n",
       "   0.13553911126011942,\n",
       "   0.13669462204661936,\n",
       "   0.13624126678916212,\n",
       "   0.14248101503336688,\n",
       "   0.13841907538424641,\n",
       "   0.13605225331279472,\n",
       "   0.1445674469048421,\n",
       "   0.13650323421038044,\n",
       "   0.1394539902618096,\n",
       "   0.13708784340155294,\n",
       "   0.1367327038945441,\n",
       "   0.1388196441932379,\n",
       "   0.1436936403378568,\n",
       "   0.13402682890843895,\n",
       "   0.1378594659223292,\n",
       "   0.13530694699854454,\n",
       "   0.13827652335721194,\n",
       "   0.1385294375512518,\n",
       "   0.13838314104734847,\n",
       "   0.14180207472333858,\n",
       "   0.13831437060286933,\n",
       "   0.13735279681833973,\n",
       "   0.1293957510920837,\n",
       "   0.1396553462547449,\n",
       "   0.13985184905734624,\n",
       "   0.13929082146481025,\n",
       "   0.142671113682312,\n",
       "   0.13736121922849995,\n",
       "   0.141417942698821,\n",
       "   0.14553463319794768,\n",
       "   0.1367370973337495,\n",
       "   0.13539989500058044,\n",
       "   0.13806926128204494],\n",
       "  'val_accuracy_mean': [0.3982444446782271,\n",
       "   0.4116000004609426,\n",
       "   0.42940000027418135,\n",
       "   0.44373333305120466,\n",
       "   0.4563333335518837,\n",
       "   0.4699777766068776,\n",
       "   0.4842888890703519,\n",
       "   0.4931555551290512,\n",
       "   0.5026000003019969,\n",
       "   0.5071333341797193,\n",
       "   0.5141555562615394,\n",
       "   0.5156222208340963,\n",
       "   0.5222222203016281,\n",
       "   0.5197333331902821,\n",
       "   0.531355556746324,\n",
       "   0.533866666952769,\n",
       "   0.5327111096183459,\n",
       "   0.5353999989231427,\n",
       "   0.5449333324035008,\n",
       "   0.5406888877352078,\n",
       "   0.541511110663414,\n",
       "   0.5513111113508542,\n",
       "   0.5549555546045304,\n",
       "   0.5517555542786916,\n",
       "   0.5552666674057642,\n",
       "   0.5559555550416311,\n",
       "   0.5550888875126838,\n",
       "   0.5597333326935768,\n",
       "   0.5643999992807707,\n",
       "   0.5681555552283922,\n",
       "   0.5707555528481801,\n",
       "   0.5652444444100062,\n",
       "   0.572711109717687,\n",
       "   0.5702666649222374,\n",
       "   0.5736444427569707,\n",
       "   0.5730666654308637,\n",
       "   0.5702444418271383,\n",
       "   0.5820888882875442,\n",
       "   0.5766888882716497,\n",
       "   0.5804666651288668,\n",
       "   0.5772888877987862,\n",
       "   0.5831777776281039,\n",
       "   0.5829999995231628,\n",
       "   0.5836444435516993,\n",
       "   0.5861999974648158,\n",
       "   0.5923111093044281,\n",
       "   0.5835111104448636,\n",
       "   0.5914444443583489,\n",
       "   0.592911109427611,\n",
       "   0.5933555547396342,\n",
       "   0.5925111092130343,\n",
       "   0.5928444439172744,\n",
       "   0.5982666668295861,\n",
       "   0.5954444429278374,\n",
       "   0.597533330321312,\n",
       "   0.5992444434762001,\n",
       "   0.5989999970793725,\n",
       "   0.5988222222526868,\n",
       "   0.6045999997854232,\n",
       "   0.6031111112236976,\n",
       "   0.604533332089583,\n",
       "   0.6039777771631877,\n",
       "   0.597955553929011,\n",
       "   0.6048666653037071,\n",
       "   0.6058666658401489,\n",
       "   0.6108444448312124,\n",
       "   0.6067555545767148,\n",
       "   0.6084888860583305,\n",
       "   0.6050888885060947,\n",
       "   0.6051555541157723,\n",
       "   0.6101111131906509,\n",
       "   0.6104222202301025,\n",
       "   0.6126444453001022,\n",
       "   0.6151777769128481,\n",
       "   0.6091333302855492,\n",
       "   0.6102666668097179,\n",
       "   0.6139111112554868,\n",
       "   0.6161111112435659,\n",
       "   0.6154888887206713,\n",
       "   0.6145111120740573,\n",
       "   0.6180666654308637,\n",
       "   0.6244888875881831,\n",
       "   0.6237999978661537,\n",
       "   0.6213555544614792,\n",
       "   0.6185333325465521,\n",
       "   0.62191110988458,\n",
       "   0.6240888882676761,\n",
       "   0.6257999990383784,\n",
       "   0.6318666656812032,\n",
       "   0.6217999989787738,\n",
       "   0.6260222209493319,\n",
       "   0.6232444429397583,\n",
       "   0.6238444446523984,\n",
       "   0.6260666649540265,\n",
       "   0.6308444428443909,\n",
       "   0.6245111113786698,\n",
       "   0.6323111103971799,\n",
       "   0.6349777774016062,\n",
       "   0.631622221271197],\n",
       "  'val_accuracy_std': [0.05357196089101228,\n",
       "   0.05547334299751085,\n",
       "   0.05760474552428505,\n",
       "   0.0555083167905495,\n",
       "   0.05881830018012891,\n",
       "   0.057269986341242744,\n",
       "   0.06103894930496433,\n",
       "   0.06438976915800927,\n",
       "   0.06172984190931424,\n",
       "   0.06123848957176068,\n",
       "   0.06201127200063257,\n",
       "   0.06484127060404057,\n",
       "   0.0646937689071434,\n",
       "   0.06376753908356589,\n",
       "   0.06426403417051545,\n",
       "   0.0634794561604474,\n",
       "   0.06413802887736449,\n",
       "   0.06692413408092443,\n",
       "   0.06499426745967721,\n",
       "   0.06085537752714573,\n",
       "   0.06209739898875849,\n",
       "   0.06527139475528428,\n",
       "   0.06428321082760093,\n",
       "   0.06484876712309179,\n",
       "   0.06312101297273637,\n",
       "   0.062065181700038614,\n",
       "   0.06394464821812818,\n",
       "   0.06141723742391005,\n",
       "   0.06506671196220667,\n",
       "   0.06450728982410568,\n",
       "   0.06218865759256398,\n",
       "   0.0644433099269903,\n",
       "   0.06450654373301666,\n",
       "   0.06747127689554476,\n",
       "   0.06774390316810898,\n",
       "   0.06617319471013898,\n",
       "   0.06405159432371996,\n",
       "   0.06319698487779594,\n",
       "   0.0630426031421928,\n",
       "   0.0662671710169029,\n",
       "   0.06586674083537652,\n",
       "   0.06299774959994459,\n",
       "   0.06302997381166525,\n",
       "   0.06395871970594534,\n",
       "   0.06426280443141245,\n",
       "   0.06604930506454663,\n",
       "   0.06680739130363432,\n",
       "   0.06645847104069703,\n",
       "   0.06561819602220538,\n",
       "   0.06723644985321785,\n",
       "   0.0680701571215505,\n",
       "   0.06337003311215106,\n",
       "   0.06578271747478277,\n",
       "   0.06767998488279747,\n",
       "   0.06795197546404651,\n",
       "   0.06525181590736026,\n",
       "   0.0659699690775924,\n",
       "   0.06713572387745659,\n",
       "   0.0660484341469822,\n",
       "   0.0669866408718385,\n",
       "   0.06607210505174846,\n",
       "   0.07096185405494657,\n",
       "   0.0682628760057837,\n",
       "   0.06650550640746568,\n",
       "   0.06493492992554185,\n",
       "   0.06536301710970549,\n",
       "   0.06649082029672311,\n",
       "   0.0671577313470273,\n",
       "   0.06475503288084325,\n",
       "   0.06647370616575683,\n",
       "   0.06455880620756267,\n",
       "   0.06343476885650055,\n",
       "   0.06903761363587323,\n",
       "   0.06564103946044912,\n",
       "   0.06409937991632723,\n",
       "   0.06618103025072332,\n",
       "   0.0653149029283859,\n",
       "   0.06675873235121595,\n",
       "   0.06598724440229141,\n",
       "   0.06615765444087114,\n",
       "   0.0663434049342906,\n",
       "   0.06278002229911803,\n",
       "   0.06627387641059826,\n",
       "   0.06389412253662376,\n",
       "   0.06709361203470075,\n",
       "   0.06317916706353086,\n",
       "   0.06734229524013823,\n",
       "   0.06506513132611491,\n",
       "   0.0648319559064256,\n",
       "   0.06505055810415787,\n",
       "   0.06298994323184291,\n",
       "   0.0644876656311861,\n",
       "   0.06768471240672773,\n",
       "   0.06641125471619012,\n",
       "   0.06592497126588712,\n",
       "   0.0639115554347532,\n",
       "   0.06425521692545537,\n",
       "   0.06292117097393285,\n",
       "   0.061571083127110904],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2ada4",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54da463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAMLFewShotClassifier(\n",
       "  (classifier): VGGReLUNormNetwork(\n",
       "    (layer_dict): ModuleDict(\n",
       "      (conv0): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (linear): MetaLinearLayer()\n",
       "    )\n",
       "  )\n",
       "  (inner_loop_optimizer): GradientDescentLearningRule()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "maml_system.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbfddb9",
   "metadata": {},
   "source": [
    "# Grad-CAM 구현\n",
    "## 추후 jupyter notebook이 아닌 python 파일로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b81648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\MAML\\meta_neural_network_architectures.py:993: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  if param.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "figure_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        num_steps=5\n",
    "        for num_step in range(num_steps):            \n",
    "            support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "                    x=x_support_set_task,\n",
    "                    y=y_support_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    backup_running_statistics=num_step == 0,\n",
    "                    training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase='test',\n",
    "                    epoch=0\n",
    "                )\n",
    "        \n",
    "            generated_alpha_params = {}\n",
    "\n",
    "            if maml_system.model.args.arbiter:\n",
    "                support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                        retain_graph=True)\n",
    "\n",
    "                names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "                per_step_task_embedding = []\n",
    "\n",
    "                for key, weight in names_weights_copy.items():\n",
    "                    weight_norm = torch.norm(weight, p=2)\n",
    "                    per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "                for key, grad in names_grads_copy.items():\n",
    "                    gradient_l2norm = torch.norm(grad, p=2)\n",
    "                    per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "                per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "                per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                            per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "                generated_gradient_rate = maml_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "                g = 0\n",
    "                for key in names_weights_copy.keys():\n",
    "                    generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "                    g += 1\n",
    "\n",
    "            names_weights_copy, names_grads_copy = maml_system.model.apply_inner_loop_update(\n",
    "                loss=support_loss,\n",
    "                support_loss_seperate=support_loss_seperate,\n",
    "                names_weights_copy=names_weights_copy,\n",
    "                alpha=generated_alpha_params,\n",
    "                use_second_order=args.second_order,\n",
    "                current_step_idx=num_step,\n",
    "                current_iter=maml_system.state['current_iter'],\n",
    "                training_phase='test')\n",
    "                      \n",
    "            if num_step==4:\n",
    "                individual_images = torch.split(x_support_set_task, 1, dim=0)  # (25, 3, 84, 84) -> (1, 3, 84, 84) 텐서로 분리\n",
    "                individual_labels = torch.split(y_support_set_task, 1, dim=0)\n",
    "                \n",
    "                original_save_path = \"Grad_CAM/\" + datasets + \"/MAML/origin/\" + \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "                grad_cam_save_path = \"Grad_CAM/\" + datasets + \"/MAML/gradcam/\"+ \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "                basic_utils.make_folder(original_save_path)\n",
    "                basic_utils.make_folder(grad_cam_save_path)\n",
    "\n",
    "                # 각 이미지 텐서 확인\n",
    "                for i in range(y_support_set_task.shape[0]):\n",
    "                    \n",
    "                    input_image = individual_images[i]\n",
    "                    y_label = individual_labels[i]\n",
    "\n",
    "                    _, support_preds, _, feature_map = maml_system.model.net_forward(x=input_image,\n",
    "                                                                     y=y_label, weights=names_weights_copy,\n",
    "                                                                     backup_running_statistics=False, training=True,\n",
    "                                                                     num_step=num_step, training_phase='test',\n",
    "                                                                     epoch=0)\n",
    "\n",
    "                    # 선택한 클래스에 대한 스칼라 출력값을 기준으로 gradient 계산\n",
    "                    target_class = y_label\n",
    "                    output = support_preds\n",
    "\n",
    "                    class_score = output[:, target_class].sum()\n",
    "                    gradients = torch.autograd.grad(outputs=class_score, inputs=feature_map, grad_outputs=torch.ones_like(class_score))\n",
    "\n",
    "                    # gradients를 사용하여 feature map의 가중치를 계산\n",
    "                    weights = torch.mean(gradients[0], dim=(2, 3), keepdim=True)\n",
    "                    grad_cam = F.relu(torch.sum(weights * feature_map, dim=1, keepdim=True))\n",
    "\n",
    "                    # grad_cam을 이미지 크기로 리사이즈\n",
    "                    grad_cam = F.interpolate(grad_cam, size=(input_image.shape[2], input_image.shape[3]), mode='bilinear', align_corners=False)\n",
    "                    grad_cam = grad_cam.squeeze().cpu().detach().numpy()\n",
    "\n",
    "                    basic_utils.visualize_and_save_grad_cam(\n",
    "                        input_image=input_image,\n",
    "                        grad_cam=grad_cam,\n",
    "                        grad_cam_save_path=grad_cam_save_path + str(i) + \".png\",\n",
    "                        original_save_path=original_save_path + str(i) + \".png\",\n",
    "                        datasets=datasets)\n",
    "\n",
    "        ##########Inner loop 종료 #############\n",
    "        # Query set으로 Grad-CAM을 구한다?\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
