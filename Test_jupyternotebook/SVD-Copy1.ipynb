{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda16bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from prompters import padding\n",
    "from utils.parser_utils import get_args\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from inner_loop_optimizers import LSLRGradientDescentLearningRule\n",
    "\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869f1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DATASET_DIR'] = os.path.join(os.getcwd(), \"datasets\")\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"alfa+maml\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "    \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"attenuate\": False,\n",
    "  \"alfa\": True,\n",
    "  \"random_init\": False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "   \"loss_function\": \"Softmax\",\n",
    "  \"ole\": True,\n",
    "  \"arbiter\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "def get_inner_loop_parameter_dict(params):\n",
    "\n",
    "    param_dict = dict()\n",
    "    for name, param in params:\n",
    "        if param.requires_grad:\n",
    "            param_dict[name] = param.to(device=device)\n",
    "\n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbba0c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([25, 3, 84, 84])\n",
      "tensor([69,  2, 19, 77, 24, 34,  3, 60, 67, 21, 79, 21, 47, 89, 46, 99, 56, 74,\n",
      "        52, 95, 20, 54,  3, 44, 23])\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(84),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR100(\"./data\", transform=preprocess,\n",
    "                          download=True, train=True)\n",
    "\n",
    "val_dataset = CIFAR100(\"./data\", transform=preprocess,\n",
    "                        download=True, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=25, pin_memory=True,\n",
    "                          num_workers=16, shuffle=True)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "print(images.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be2fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 무작위 이미지 데이터 생성 (배치 크기, 채널, 높이, 너비)\n",
    "# batch_size = 25\n",
    "# channels = 3  # RGB 이미지이므로 3개의 채널\n",
    "# height, width = 84, 84  # 높이와 너비\n",
    "\n",
    "# # 무작위 이미지 데이터 생성 (0과 1 사이의 무작위 값)\n",
    "# images = torch.rand(batch_size, channels, height, width)\n",
    "\n",
    "# # 무작위 레이블 데이터 생성 (예시를 위해 10개의 클래스)\n",
    "# num_classes = 25\n",
    "# targets = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60b8e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 3, 84, 84])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
      "        4], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "targets = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4])\n",
    "targets = torch.Tensor(targets)\n",
    "targets = targets.type(torch.LongTensor)\n",
    "targets = targets.to(device)\n",
    "\n",
    "print(images.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0123c723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "class MAMLFewShotClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, im_shape, device, args):\n",
    "        \n",
    "        super(MAMLFewShotClassifier, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.batch_size = args.batch_size\n",
    "        self.use_cuda = args.use_cuda\n",
    "        self.im_shape = im_shape\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.classifier = VGGReLUNormNetwork(im_shape=self.im_shape, num_output_classes=self.args.\n",
    "                                                 num_classes_per_set,\n",
    "                                                 args=args, device=device, meta_classifier=True).to(device=self.device)                \n",
    "        \n",
    "        self.task_learning_rate = args.init_inner_loop_learning_rate\n",
    "        \n",
    "        self.inner_loop_optimizer = LSLRGradientDescentLearningRule(device=device,\n",
    "                                                                    init_learning_rate=self.task_learning_rate,\n",
    "                                                                    total_num_inner_loop_steps=self.args.number_of_training_steps_per_iter,\n",
    "                                                                    use_learnable_learning_rates=self.args.learnable_per_layer_per_step_inner_loop_learning_rate)\n",
    "        \n",
    "        names_weights_copy = self.get_inner_loop_parameter_dict(self.classifier.named_parameters())\n",
    "        \n",
    "        # Weighted Arbiter\n",
    "        num_layers = len(names_weights_copy)\n",
    "        input_dim = num_layers * 2\n",
    "        output_dim = num_layers\n",
    "        self.arbiter = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(input_dim, output_dim)\n",
    "        ).to(device=self.device)\n",
    "        \n",
    "        self.inner_loop_optimizer.initialise(\n",
    "            names_weights_dict=names_weights_copy)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.trainable_parameters(), lr=args.meta_learning_rate, amsgrad=False)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=self.optimizer, T_max=self.args.total_epochs,\n",
    "                                                              eta_min=self.args.min_learning_rate)\n",
    "        \n",
    "    def trainable_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                yield param\n",
    "                \n",
    "    def get_inner_loop_parameter_dict(self, params):\n",
    "        \n",
    "        param_dict = dict()\n",
    "        for name, param in params:\n",
    "            if param.requires_grad:\n",
    "                param_dict[name] = param.to(device=device)\n",
    "\n",
    "        return param_dict\n",
    "    \n",
    "    def weight_scaling(self, task_embeddings, names_weights_copy):\n",
    "\n",
    "        generated_alpha_params = {}\n",
    "\n",
    "        gamma = self.arbiter(task_embeddings)\n",
    "\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = gamma[g]\n",
    "            g += 1\n",
    "\n",
    "        for name, param in names_weights_copy.items():\n",
    "            if 'weight' in name:  # weight에 대해서만 SVD를 수행\n",
    "                if \"norm_layer\" not in name:\n",
    "                    if \"linear\" not in name:\n",
    "                        print(name)\n",
    "                        u, s, v = torch.svd(param, some=False)  # SVD 수행\n",
    "                        s = s * generated_alpha_params[name]\n",
    "                        rescale_weight = torch.matmul(torch.matmul(u, torch.diag_embed(s)), v)\n",
    "                        names_weights_copy[name] = rescale_weight\n",
    "\n",
    "        return names_weights_copy\n",
    "    \n",
    "    def apply_inner_loop_update(self, loss, names_weights_copy, use_second_order, current_step_idx):\n",
    "        \n",
    "        grads = torch.autograd.grad(loss, names_weights_copy.values(),\n",
    "                                    create_graph=use_second_order, allow_unused=True)\n",
    "        \n",
    "        print(\"grad == \", grad)\n",
    "                \n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        names_weights_copy = self.get_inner_loop_parameter_dict(self.classifier.named_parameters())\n",
    "        \n",
    "        names_weights_copy = {\n",
    "                        name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                            [1] + [1 for i in range(len(value.shape))]) for\n",
    "                        name, value in names_weights_copy.items()}\n",
    "        \n",
    "        loss, preds = self.net_forward(x, y, names_weights_copy)\n",
    "        \n",
    "        \n",
    "        support_loss_grad = torch.autograd.grad(loss, names_weights_copy.values(),\n",
    "                                                            retain_graph=True)\n",
    "        \n",
    "        per_step_task_embedding = []\n",
    "        for k, v in names_weights_copy.items():\n",
    "            # per_step_task_embedding.append(v.mean())\n",
    "            per_step_task_embedding.append(v.norm())\n",
    "\n",
    "        for i in range(len(support_loss_grad)):\n",
    "            # per_step_task_embedding.append(support_loss_grad[i].mean())\n",
    "            per_step_task_embedding.append(support_loss_grad[i].norm())\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "        \n",
    "        print(\"per_step_task_embedding == \", per_step_task_embedding.shape)\n",
    "        \n",
    "        names_weights_copy = self.weight_scaling(task_embeddings=per_step_task_embedding, names_weights_copy=names_weights_copy)\n",
    "        \n",
    "        return loss, preds\n",
    "    \n",
    "    \n",
    "    def net_forward(self, x, y, names_weights_copy):\n",
    "        \n",
    "    \n",
    "        preds = self.classifier.forward(x, params=names_weights_copy,num_step=4)\n",
    "        \n",
    "        loss = F.cross_entropy(input=preds, target=y)        \n",
    "        \n",
    "        return loss, preds\n",
    "    \n",
    "    \n",
    "\n",
    "model = MAMLFewShotClassifier(args=args, device=device, im_shape=(2, 3, args.image_height, args.image_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26061063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_step_task_embedding ==  torch.Size([36])\n",
      "layer_dict.conv0.conv.weight\n",
      "layer_dict.conv1.conv.weight\n",
      "layer_dict.conv2.conv.weight\n",
      "layer_dict.conv3.conv.weight\n"
     ]
    }
   ],
   "source": [
    "loss, preds = model.forward(images, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa477ea",
   "metadata": {},
   "source": [
    "## weight의 shape과 grad shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7427b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: classifier.layer_dict.conv0.conv.weight, Weight shape: torch.Size([48, 3, 3, 3]), Gradient shape: torch.Size([48, 3, 3, 3])\n",
      "Layer: classifier.layer_dict.conv1.conv.weight, Weight shape: torch.Size([48, 48, 3, 3]), Gradient shape: torch.Size([48, 48, 3, 3])\n",
      "Layer: classifier.layer_dict.conv2.conv.weight, Weight shape: torch.Size([48, 48, 3, 3]), Gradient shape: torch.Size([48, 48, 3, 3])\n",
      "Layer: classifier.layer_dict.conv3.conv.weight, Weight shape: torch.Size([48, 48, 3, 3]), Gradient shape: torch.Size([48, 48, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name: ## outter-loop에 대해서만 SVD를 수행\n",
    "        if 'weight' in name:  # weight에 대해서만 SVD를 수행\n",
    "            if \"norm_layer\" not in name:\n",
    "                if \"linear\" not in name:\n",
    "                    print(f'Layer: {name}, Weight shape: {param.shape}, Gradient shape: {param.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5dec087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: classifier.layer_dict.conv0.conv.weight\n",
      "tensor([[[1.5995e-01, 9.6029e-02, 2.6594e-02],\n",
      "         [9.8462e-02, 8.7972e-02, 5.7105e-02],\n",
      "         [1.7982e-01, 1.3723e-01, 3.2917e-02]],\n",
      "\n",
      "        [[1.7864e-01, 1.1662e-01, 1.7792e-02],\n",
      "         [1.8018e-01, 9.0658e-02, 5.7948e-02],\n",
      "         [1.1764e-01, 7.7829e-02, 8.5581e-03]],\n",
      "\n",
      "        [[1.3449e-01, 1.0131e-01, 4.4687e-02],\n",
      "         [2.0310e-01, 9.0821e-02, 7.5179e-02],\n",
      "         [1.7952e-01, 1.2019e-01, 9.0341e-02]],\n",
      "\n",
      "        [[1.7022e-01, 1.0981e-01, 5.5604e-02],\n",
      "         [1.6771e-01, 1.2204e-01, 3.8385e-03],\n",
      "         [1.4200e-01, 8.6509e-02, 2.2647e-02]],\n",
      "\n",
      "        [[1.7174e-01, 1.2751e-01, 1.4020e-02],\n",
      "         [1.7328e-01, 1.2873e-01, 1.0251e-02],\n",
      "         [1.0007e-01, 7.6592e-02, 3.9226e-03]],\n",
      "\n",
      "        [[1.2413e-01, 6.3647e-02, 2.1011e-03],\n",
      "         [1.4912e-01, 5.7726e-02, 4.3019e-02],\n",
      "         [1.6461e-01, 9.4490e-02, 8.7046e-03]],\n",
      "\n",
      "        [[1.1286e-01, 7.6989e-02, 6.0134e-02],\n",
      "         [1.1984e-01, 9.6592e-02, 4.1853e-02],\n",
      "         [1.2279e-01, 6.6411e-02, 8.7866e-03]],\n",
      "\n",
      "        [[1.5423e-01, 5.6521e-02, 7.1426e-03],\n",
      "         [1.6580e-01, 4.2195e-02, 1.0314e-02],\n",
      "         [1.5301e-01, 1.4099e-01, 3.4058e-02]],\n",
      "\n",
      "        [[8.8389e-02, 2.9816e-02, 7.6772e-04],\n",
      "         [1.3787e-01, 9.8524e-02, 8.6649e-02],\n",
      "         [2.0929e-01, 8.3413e-02, 2.3952e-02]],\n",
      "\n",
      "        [[1.5889e-01, 1.0986e-01, 2.3393e-03],\n",
      "         [1.7842e-01, 1.3186e-01, 2.9137e-02],\n",
      "         [1.3943e-01, 1.0828e-01, 1.4717e-03]],\n",
      "\n",
      "        [[1.8163e-01, 6.4395e-02, 1.2593e-02],\n",
      "         [2.0761e-01, 1.0956e-01, 4.4913e-02],\n",
      "         [1.7274e-01, 1.1248e-01, 3.2147e-02]],\n",
      "\n",
      "        [[1.5342e-01, 1.0859e-01, 3.9387e-02],\n",
      "         [1.4962e-01, 1.0650e-01, 6.6317e-02],\n",
      "         [1.3668e-01, 1.0629e-01, 1.5958e-02]],\n",
      "\n",
      "        [[1.3065e-01, 1.2601e-01, 6.1566e-03],\n",
      "         [9.9336e-02, 8.1738e-02, 4.3196e-02],\n",
      "         [1.1916e-01, 7.4023e-02, 5.2288e-03]],\n",
      "\n",
      "        [[2.0198e-01, 9.9028e-02, 2.9974e-02],\n",
      "         [2.0693e-01, 9.8767e-02, 3.8321e-02],\n",
      "         [1.8200e-01, 1.0256e-01, 1.4127e-02]],\n",
      "\n",
      "        [[2.0227e-01, 1.1591e-01, 6.6053e-02],\n",
      "         [1.6952e-01, 9.3135e-02, 8.3443e-02],\n",
      "         [1.1801e-01, 1.0508e-01, 4.0264e-02]],\n",
      "\n",
      "        [[1.7421e-01, 1.2144e-01, 7.9071e-03],\n",
      "         [1.5701e-01, 6.7539e-02, 2.4718e-02],\n",
      "         [1.6501e-01, 8.6306e-02, 5.8477e-02]],\n",
      "\n",
      "        [[1.7011e-01, 1.1447e-01, 2.9930e-03],\n",
      "         [1.9347e-01, 1.2406e-01, 5.4383e-03],\n",
      "         [1.7393e-01, 1.0228e-01, 8.7754e-03]],\n",
      "\n",
      "        [[1.2561e-01, 9.1206e-02, 2.2078e-02],\n",
      "         [1.6401e-01, 8.0886e-02, 3.4032e-02],\n",
      "         [1.8778e-01, 1.1184e-01, 7.8750e-03]],\n",
      "\n",
      "        [[1.7676e-01, 8.0960e-02, 9.7941e-04],\n",
      "         [1.6752e-01, 1.0263e-01, 1.1944e-02],\n",
      "         [1.2403e-01, 6.3020e-02, 4.3453e-03]],\n",
      "\n",
      "        [[1.4898e-01, 1.0006e-01, 4.3256e-02],\n",
      "         [1.4255e-01, 5.7067e-02, 2.5651e-02],\n",
      "         [1.4860e-01, 1.0140e-01, 5.5712e-02]],\n",
      "\n",
      "        [[1.5315e-01, 5.5486e-02, 1.5839e-02],\n",
      "         [1.9130e-01, 1.4150e-01, 2.3397e-02],\n",
      "         [1.5862e-01, 8.4973e-02, 1.9724e-02]],\n",
      "\n",
      "        [[1.3245e-01, 1.1130e-01, 6.3666e-02],\n",
      "         [1.7312e-01, 6.3253e-02, 3.6600e-03],\n",
      "         [2.0795e-01, 1.0726e-01, 1.3207e-02]],\n",
      "\n",
      "        [[1.7463e-01, 9.3978e-02, 7.7068e-03],\n",
      "         [1.7773e-01, 1.3096e-01, 4.5333e-02],\n",
      "         [1.2863e-01, 8.0830e-02, 2.1089e-02]],\n",
      "\n",
      "        [[1.2915e-01, 1.0151e-01, 3.9124e-02],\n",
      "         [1.2833e-01, 8.8391e-02, 2.1249e-03],\n",
      "         [1.1545e-01, 9.0398e-02, 2.1890e-02]],\n",
      "\n",
      "        [[2.1219e-01, 9.5971e-02, 7.7497e-03],\n",
      "         [1.6908e-01, 9.7060e-02, 2.1006e-02],\n",
      "         [1.9805e-01, 1.4376e-01, 2.6517e-02]],\n",
      "\n",
      "        [[1.5129e-01, 1.1899e-01, 5.3823e-02],\n",
      "         [1.8387e-01, 1.3633e-01, 2.0189e-03],\n",
      "         [1.4210e-01, 9.1149e-02, 3.5727e-02]],\n",
      "\n",
      "        [[1.8862e-01, 7.7159e-02, 1.7930e-02],\n",
      "         [1.5377e-01, 1.3328e-01, 3.2634e-02],\n",
      "         [2.1323e-01, 8.5061e-02, 3.7912e-02]],\n",
      "\n",
      "        [[1.6438e-01, 6.3683e-02, 4.6110e-02],\n",
      "         [1.5119e-01, 1.1692e-01, 4.3833e-02],\n",
      "         [1.7266e-01, 1.2845e-01, 1.0020e-01]],\n",
      "\n",
      "        [[1.5724e-01, 1.2396e-01, 6.2808e-02],\n",
      "         [1.8917e-01, 8.7134e-02, 4.2033e-02],\n",
      "         [1.5229e-01, 8.5533e-02, 1.5664e-02]],\n",
      "\n",
      "        [[2.1212e-01, 5.2488e-02, 2.5765e-02],\n",
      "         [1.6963e-01, 1.1479e-01, 5.0491e-02],\n",
      "         [1.7005e-01, 1.2457e-01, 6.3376e-02]],\n",
      "\n",
      "        [[1.6098e-01, 6.7671e-02, 1.6571e-02],\n",
      "         [1.9217e-01, 1.1850e-01, 2.4182e-02],\n",
      "         [1.1570e-01, 6.6964e-02, 4.2370e-02]],\n",
      "\n",
      "        [[1.9604e-01, 6.6154e-02, 4.8971e-02],\n",
      "         [1.2062e-01, 3.2129e-02, 1.3564e-04],\n",
      "         [1.9606e-01, 1.0542e-01, 2.3874e-02]],\n",
      "\n",
      "        [[1.3791e-01, 1.0336e-01, 3.5351e-02],\n",
      "         [1.7166e-01, 6.7499e-02, 5.9450e-02],\n",
      "         [1.4703e-01, 9.1002e-02, 2.7681e-02]],\n",
      "\n",
      "        [[1.6584e-01, 1.0692e-01, 2.2772e-02],\n",
      "         [1.5536e-01, 1.1810e-01, 2.3183e-02],\n",
      "         [1.5849e-01, 7.7800e-02, 1.6566e-02]],\n",
      "\n",
      "        [[1.5928e-01, 8.7378e-02, 1.4323e-02],\n",
      "         [1.5669e-01, 9.0469e-02, 6.9575e-02],\n",
      "         [1.7089e-01, 1.3589e-01, 6.9220e-02]],\n",
      "\n",
      "        [[1.4979e-01, 1.0767e-01, 1.5659e-02],\n",
      "         [1.5934e-01, 8.4107e-02, 1.1323e-03],\n",
      "         [1.8011e-01, 9.5094e-02, 2.4333e-02]],\n",
      "\n",
      "        [[1.8531e-01, 1.1699e-01, 1.1605e-02],\n",
      "         [2.0697e-01, 1.2479e-01, 3.3016e-03],\n",
      "         [2.0389e-01, 7.4200e-02, 1.2287e-02]],\n",
      "\n",
      "        [[1.8776e-01, 1.0571e-01, 5.6494e-02],\n",
      "         [9.5333e-02, 8.6450e-02, 5.3247e-02],\n",
      "         [1.8620e-01, 2.4006e-02, 2.0907e-02]],\n",
      "\n",
      "        [[1.2971e-01, 6.7338e-02, 1.3966e-03],\n",
      "         [1.9278e-01, 1.2834e-01, 5.4683e-02],\n",
      "         [1.5222e-01, 1.2981e-01, 3.1517e-02]],\n",
      "\n",
      "        [[1.9551e-01, 8.5820e-02, 6.3126e-02],\n",
      "         [1.9042e-01, 1.3038e-01, 9.5293e-02],\n",
      "         [1.5021e-01, 1.2441e-01, 4.6813e-02]],\n",
      "\n",
      "        [[1.7770e-01, 1.2423e-01, 3.8174e-02],\n",
      "         [1.0802e-01, 8.5356e-02, 3.3302e-02],\n",
      "         [1.8686e-01, 7.2221e-02, 3.0713e-03]],\n",
      "\n",
      "        [[1.8667e-01, 1.4778e-01, 6.3516e-02],\n",
      "         [1.5994e-01, 5.9784e-02, 5.6142e-03],\n",
      "         [1.7721e-01, 8.2610e-02, 1.4947e-02]],\n",
      "\n",
      "        [[2.1329e-01, 6.1529e-02, 1.8391e-03],\n",
      "         [1.3122e-01, 1.0855e-01, 4.6588e-02],\n",
      "         [2.0167e-01, 1.3598e-01, 1.2285e-02]],\n",
      "\n",
      "        [[1.3466e-01, 9.4107e-02, 1.8178e-02],\n",
      "         [1.8403e-01, 7.2103e-02, 4.3759e-03],\n",
      "         [2.1491e-01, 7.9027e-02, 3.1206e-02]],\n",
      "\n",
      "        [[1.6656e-01, 1.2600e-01, 3.9569e-02],\n",
      "         [1.6879e-01, 1.2567e-01, 2.3393e-02],\n",
      "         [1.8600e-01, 1.4245e-01, 7.9323e-02]],\n",
      "\n",
      "        [[1.4091e-01, 9.0222e-02, 2.4381e-05],\n",
      "         [1.5356e-01, 1.0926e-01, 5.6929e-02],\n",
      "         [1.5680e-01, 1.0564e-01, 4.7629e-03]],\n",
      "\n",
      "        [[1.7023e-01, 7.4936e-02, 4.9400e-02],\n",
      "         [1.4650e-01, 1.0259e-01, 3.4441e-02],\n",
      "         [1.6495e-01, 1.2678e-01, 2.3493e-02]],\n",
      "\n",
      "        [[1.9867e-01, 9.8639e-02, 4.1525e-02],\n",
      "         [1.2912e-01, 4.6070e-02, 5.9351e-03],\n",
      "         [1.8786e-01, 6.9466e-02, 2.9694e-02]]], device='cuda:0',\n",
      "       grad_fn=<LinalgSvdBackward0>)\n",
      "원본 텐서와 복원된 텐서 간의 차이:\n",
      "tensor(0.2725, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "------------------------------------\n",
      "Layer: classifier.layer_dict.conv1.conv.weight\n",
      "tensor([[[0.0613, 0.0489, 0.0247],\n",
      "         [0.1748, 0.0465, 0.0243],\n",
      "         [0.0949, 0.0440, 0.0116],\n",
      "         ...,\n",
      "         [0.1233, 0.1002, 0.0067],\n",
      "         [0.1319, 0.0705, 0.0078],\n",
      "         [0.1009, 0.0762, 0.0362]],\n",
      "\n",
      "        [[0.1045, 0.0965, 0.0027],\n",
      "         [0.0992, 0.0501, 0.0216],\n",
      "         [0.1184, 0.0754, 0.0628],\n",
      "         ...,\n",
      "         [0.0986, 0.0700, 0.0108],\n",
      "         [0.0892, 0.0638, 0.0129],\n",
      "         [0.1053, 0.0590, 0.0040]],\n",
      "\n",
      "        [[0.1414, 0.0515, 0.0289],\n",
      "         [0.1265, 0.0600, 0.0230],\n",
      "         [0.1196, 0.0791, 0.0087],\n",
      "         ...,\n",
      "         [0.1334, 0.1086, 0.0451],\n",
      "         [0.0950, 0.0744, 0.0036],\n",
      "         [0.1157, 0.0733, 0.0272]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1269, 0.0470, 0.0138],\n",
      "         [0.1268, 0.0408, 0.0296],\n",
      "         [0.1121, 0.0870, 0.0574],\n",
      "         ...,\n",
      "         [0.1293, 0.0739, 0.0134],\n",
      "         [0.1210, 0.0994, 0.0493],\n",
      "         [0.1073, 0.0310, 0.0129]],\n",
      "\n",
      "        [[0.1206, 0.0568, 0.0143],\n",
      "         [0.1179, 0.0681, 0.0494],\n",
      "         [0.1462, 0.0425, 0.0072],\n",
      "         ...,\n",
      "         [0.1447, 0.0654, 0.0305],\n",
      "         [0.1849, 0.0424, 0.0095],\n",
      "         [0.0986, 0.0452, 0.0092]],\n",
      "\n",
      "        [[0.1053, 0.0743, 0.0163],\n",
      "         [0.0957, 0.0769, 0.0541],\n",
      "         [0.0933, 0.0747, 0.0284],\n",
      "         ...,\n",
      "         [0.1461, 0.0394, 0.0118],\n",
      "         [0.1260, 0.0678, 0.0541],\n",
      "         [0.1308, 0.0760, 0.0208]]], device='cuda:0',\n",
      "       grad_fn=<LinalgSvdBackward0>)\n",
      "원본 텐서와 복원된 텐서 간의 차이:\n",
      "tensor(0.2051, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "------------------------------------\n",
      "Layer: classifier.layer_dict.conv2.conv.weight\n",
      "tensor([[[0.1151, 0.0763, 0.0449],\n",
      "         [0.1081, 0.0766, 0.0229],\n",
      "         [0.1105, 0.0411, 0.0130],\n",
      "         ...,\n",
      "         [0.1080, 0.0471, 0.0195],\n",
      "         [0.0824, 0.0606, 0.0476],\n",
      "         [0.1060, 0.0852, 0.0235]],\n",
      "\n",
      "        [[0.1127, 0.0678, 0.0331],\n",
      "         [0.1133, 0.0586, 0.0303],\n",
      "         [0.1176, 0.0603, 0.0334],\n",
      "         ...,\n",
      "         [0.1317, 0.0398, 0.0244],\n",
      "         [0.1364, 0.0357, 0.0035],\n",
      "         [0.1518, 0.0477, 0.0233]],\n",
      "\n",
      "        [[0.1114, 0.0899, 0.0266],\n",
      "         [0.1461, 0.0962, 0.0011],\n",
      "         [0.1178, 0.0491, 0.0116],\n",
      "         ...,\n",
      "         [0.1084, 0.0542, 0.0082],\n",
      "         [0.0992, 0.0645, 0.0160],\n",
      "         [0.1188, 0.0658, 0.0210]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1401, 0.0381, 0.0141],\n",
      "         [0.1191, 0.0592, 0.0055],\n",
      "         [0.1430, 0.0695, 0.0256],\n",
      "         ...,\n",
      "         [0.1099, 0.0526, 0.0332],\n",
      "         [0.1128, 0.0468, 0.0219],\n",
      "         [0.1383, 0.0647, 0.0113]],\n",
      "\n",
      "        [[0.1043, 0.0623, 0.0254],\n",
      "         [0.1330, 0.0399, 0.0090],\n",
      "         [0.1626, 0.0733, 0.0075],\n",
      "         ...,\n",
      "         [0.1421, 0.0714, 0.0221],\n",
      "         [0.1240, 0.0928, 0.0324],\n",
      "         [0.0997, 0.0635, 0.0299]],\n",
      "\n",
      "        [[0.1058, 0.0847, 0.0339],\n",
      "         [0.1218, 0.0400, 0.0189],\n",
      "         [0.1323, 0.1180, 0.0347],\n",
      "         ...,\n",
      "         [0.1423, 0.0950, 0.0120],\n",
      "         [0.1204, 0.0647, 0.0169],\n",
      "         [0.1422, 0.0695, 0.0367]]], device='cuda:0',\n",
      "       grad_fn=<LinalgSvdBackward0>)\n",
      "원본 텐서와 복원된 텐서 간의 차이:\n",
      "tensor(0.2054, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "------------------------------------\n",
      "Layer: classifier.layer_dict.conv3.conv.weight\n",
      "tensor([[[0.0982, 0.0460, 0.0423],\n",
      "         [0.0797, 0.0683, 0.0392],\n",
      "         [0.1051, 0.0972, 0.0159],\n",
      "         ...,\n",
      "         [0.1174, 0.0766, 0.0332],\n",
      "         [0.1212, 0.0464, 0.0038],\n",
      "         [0.1239, 0.1118, 0.0288]],\n",
      "\n",
      "        [[0.1339, 0.0381, 0.0104],\n",
      "         [0.1243, 0.1129, 0.0587],\n",
      "         [0.1101, 0.0743, 0.0379],\n",
      "         ...,\n",
      "         [0.1309, 0.0718, 0.0660],\n",
      "         [0.1619, 0.0882, 0.0318],\n",
      "         [0.1145, 0.0753, 0.0645]],\n",
      "\n",
      "        [[0.1161, 0.0642, 0.0071],\n",
      "         [0.1362, 0.0459, 0.0151],\n",
      "         [0.0908, 0.0778, 0.0163],\n",
      "         ...,\n",
      "         [0.1165, 0.0713, 0.0212],\n",
      "         [0.1095, 0.0551, 0.0380],\n",
      "         [0.1228, 0.0535, 0.0233]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1179, 0.0964, 0.0447],\n",
      "         [0.1274, 0.0810, 0.0305],\n",
      "         [0.0962, 0.0580, 0.0213],\n",
      "         ...,\n",
      "         [0.1203, 0.0591, 0.0009],\n",
      "         [0.1030, 0.0180, 0.0124],\n",
      "         [0.1144, 0.0690, 0.0338]],\n",
      "\n",
      "        [[0.0934, 0.0527, 0.0315],\n",
      "         [0.1440, 0.0982, 0.0168],\n",
      "         [0.1169, 0.0460, 0.0288],\n",
      "         ...,\n",
      "         [0.1316, 0.1166, 0.0276],\n",
      "         [0.1058, 0.0713, 0.0212],\n",
      "         [0.1099, 0.0297, 0.0199]],\n",
      "\n",
      "        [[0.1054, 0.0502, 0.0198],\n",
      "         [0.1363, 0.0878, 0.0538],\n",
      "         [0.1071, 0.0557, 0.0234],\n",
      "         ...,\n",
      "         [0.1264, 0.0764, 0.0330],\n",
      "         [0.1004, 0.0931, 0.0028],\n",
      "         [0.1210, 0.0840, 0.0129]]], device='cuda:0',\n",
      "       grad_fn=<LinalgSvdBackward0>)\n",
      "원본 텐서와 복원된 텐서 간의 차이:\n",
      "tensor(0.2083, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 각 레이어의 가중치에 대한 SVD를 수행합니다\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name: ## outter-loop에 대해서만 SVD를 수행\n",
    "        if 'weight' in name:  # weight에 대해서만 SVD를 수행\n",
    "            if \"norm_layer\" not in name:\n",
    "                if \"linear\" not in name:\n",
    "                    print(f'Layer: {name}')\n",
    "                    original_shape = param.shape\n",
    "                    u, s, v = torch.svd(param)  # SVD 수행\n",
    "#                     print(f'original shape: {original_shape}')\n",
    "#                     print(f'U matrix shape: {u.shape}')\n",
    "#                     print(f'Singular values shape: {s.shape}')\n",
    "#                     print(f'V transpose matrix shape: {v.shape}')\n",
    "\n",
    "                    print(s)\n",
    "\n",
    "                    # 복원된 가중치 계산\n",
    "                    restored_weight = torch.matmul(torch.matmul(u, torch.diag_embed(s)), v)\n",
    "                    # print(f'restored matrix shape: {restored_weight.shape}')\n",
    "\n",
    "                    # 복원된 텐서와 원본 텐서 간의 차이 계산\n",
    "                    difference = torch.abs(param - restored_weight)\n",
    "                    print(\"원본 텐서와 복원된 텐서 간의 차이:\")\n",
    "                    print(difference.max())  # 차이의 최댓값 출력\n",
    "\n",
    "                    print('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "feab71ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: classifier.layer_dict.conv0.conv.weight\n",
      "-----------------------\n",
      "Layer: classifier.layer_dict.conv1.conv.weight\n",
      "-----------------------\n",
      "Layer: classifier.layer_dict.conv2.conv.weight\n",
      "-----------------------\n",
      "Layer: classifier.layer_dict.conv3.conv.weight\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "k=20\n",
    "\n",
    "# 각 레이어의 가중치에 대한 SVD를 수행합니다\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name: ## outter-loop에 대해서만 SVD를 수행\n",
    "        if 'weight' in name:  # weight에 대해서만 SVD를 수행\n",
    "            if \"norm_layer\" not in name:\n",
    "                if \"linear\" not in name:\n",
    "                    print(f'Layer: {name}')\n",
    "                    \n",
    "                    original_shape = param.shape \n",
    "                    param_matrix = param.data.view(param.data.size(0), -1) # 텐서를 2D로 변환하여 특이값 분해 수행\n",
    "                    u, s, v = torch.svd(param_matrix)\n",
    "                    \n",
    "                    s_diag = torch.diag(s)\n",
    "                    \n",
    "                    restored_weight = u @ s_diag @ v.T\n",
    "                    restored_weight = restored_weight.view(param.size())\n",
    "                    \n",
    "                    print('-----------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
