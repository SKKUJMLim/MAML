{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_JM\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f85286c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_JM\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6043999990820885,\n",
       " 'best_val_iter': 50000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 100,\n",
       " 'train_loss_mean': 0.6226911242604256,\n",
       " 'train_loss_std': 0.13918239569613705,\n",
       " 'train_accuracy_mean': 0.7648533337116241,\n",
       " 'train_accuracy_std': 0.061526512771772165,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 1.0186213918526967,\n",
       " 'val_loss_std': 0.15208621574346637,\n",
       " 'val_accuracy_mean': 0.6043999990820885,\n",
       " 'val_accuracy_std': 0.0653144201321873,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.1415, -0.4236,  0.1998],\n",
       "                         [-0.1713, -0.0461,  0.0164],\n",
       "                         [-0.1761,  0.4311, -0.1085]],\n",
       "               \n",
       "                        [[ 0.2328, -0.3953,  0.2600],\n",
       "                         [-0.0981,  0.0654,  0.1166],\n",
       "                         [-0.2208,  0.2962, -0.1574]],\n",
       "               \n",
       "                        [[ 0.3255, -0.2839,  0.0395],\n",
       "                         [ 0.0638,  0.1251, -0.1740],\n",
       "                         [-0.2084,  0.2396, -0.2464]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1875,  0.5014,  0.2474],\n",
       "                         [ 0.1544, -0.2668,  0.0860],\n",
       "                         [-0.3616, -0.3614, -0.1655]],\n",
       "               \n",
       "                        [[-0.2363, -0.0171, -0.1561],\n",
       "                         [ 0.1492, -0.1978, -0.0796],\n",
       "                         [ 0.1933,  0.0403,  0.2115]],\n",
       "               \n",
       "                        [[-0.2765,  0.0396, -0.2101],\n",
       "                         [ 0.0150,  0.1352,  0.1952],\n",
       "                         [ 0.0525,  0.1417,  0.0232]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.1492, -0.1725, -0.1051],\n",
       "                         [-0.1926, -0.3951, -0.3913],\n",
       "                         [-0.0894, -0.3333, -0.3379]],\n",
       "               \n",
       "                        [[-0.0514, -0.1003, -0.1233],\n",
       "                         [ 0.1032, -0.2429, -0.3301],\n",
       "                         [ 0.0405, -0.2802, -0.2640]],\n",
       "               \n",
       "                        [[ 0.2617, -0.1097, -0.0576],\n",
       "                         [ 0.2572, -0.1426, -0.3215],\n",
       "                         [ 0.1547, -0.0846, -0.1630]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.2174, -0.0688, -0.3065],\n",
       "                         [ 0.4035, -0.1600, -0.4319],\n",
       "                         [ 0.5228,  0.1330, -0.3054]],\n",
       "               \n",
       "                        [[-0.0751,  0.0596,  0.0545],\n",
       "                         [-0.0889,  0.0189,  0.0429],\n",
       "                         [-0.0173, -0.0141, -0.0623]],\n",
       "               \n",
       "                        [[-0.0601, -0.0021,  0.1741],\n",
       "                         [-0.1412,  0.0579,  0.2906],\n",
       "                         [-0.3163, -0.1346,  0.2244]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.1237, -0.0430,  0.1307],\n",
       "                         [ 0.0034,  0.0810, -0.1365],\n",
       "                         [ 0.0910,  0.0330,  0.0978]],\n",
       "               \n",
       "                        [[ 0.6457,  0.3912,  0.0302],\n",
       "                         [ 0.0725, -0.4890, -0.3415],\n",
       "                         [ 0.0058, -0.4160, -0.2012]],\n",
       "               \n",
       "                        [[ 0.3093,  0.1060, -0.0702],\n",
       "                         [ 0.1247, -0.3161, -0.2786],\n",
       "                         [-0.0499, -0.0532,  0.2008]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1332,  0.1290,  0.0628],\n",
       "                         [ 0.1058,  0.1795,  0.0401],\n",
       "                         [ 0.0878,  0.0309,  0.0737]],\n",
       "               \n",
       "                        [[-0.2363, -0.4717, -0.4133],\n",
       "                         [-0.4697, -0.6111, -0.5232],\n",
       "                         [-0.2127, -0.2342, -0.0393]],\n",
       "               \n",
       "                        [[ 0.1869,  0.2756,  0.2614],\n",
       "                         [ 0.0666,  0.0254,  0.1473],\n",
       "                         [ 0.0872,  0.1207,  0.1052]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.1079,  0.1533,  0.0298,  0.0424,  0.0120,  0.0172, -0.0352, -0.0849,\n",
       "                       -0.0027, -0.0029, -0.1696, -0.0507, -0.0978, -0.0096, -0.0139, -0.0571,\n",
       "                       -0.0277,  0.0133, -0.1832,  0.0207,  0.0475,  0.0382, -0.0093,  0.1206,\n",
       "                       -0.0363,  0.0441, -0.0256,  0.0009, -0.0575, -0.1635,  0.0191,  0.0316,\n",
       "                        0.1330, -0.0871, -0.1776, -0.0055, -0.0965, -0.0069, -0.0748, -0.1066,\n",
       "                       -0.1182, -0.1133, -0.0178,  0.0383, -0.1249,  0.0433, -0.1961,  0.0108],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.0492, -0.8958, -0.4051,  0.8105, -0.6262, -0.6682, -0.4872,  0.0857,\n",
       "                       -0.6040, -0.4665, -0.7822,  0.0090,  0.1068, -0.2369, -0.1499,  0.4213,\n",
       "                       -0.3831,  1.7442,  0.1110, -0.5386,  0.0054, -0.5725, -0.5747,  0.4129,\n",
       "                        0.0176, -0.4716, -0.5823,  1.5346, -0.6748,  0.4765, -0.1141, -0.3090,\n",
       "                        1.5082, -0.6039,  0.0158, -0.6858, -0.3406, -0.2699,  0.0542, -0.0115,\n",
       "                       -0.1427,  0.2829, -0.6469, -0.1414,  0.3097, -0.1944, -0.8449, -0.4598],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.9440, 1.0726, 0.7143, 1.1748, 0.5556, 0.7899, 0.9555, 1.5488, 0.7604,\n",
       "                       0.6334, 0.9766, 0.6885, 0.7163, 0.9471, 1.4916, 0.9023, 0.6071, 0.7075,\n",
       "                       1.0795, 0.7558, 0.8734, 0.7586, 0.9389, 0.8637, 0.6076, 0.7619, 0.7687,\n",
       "                       0.8312, 0.8849, 0.8520, 1.2105, 0.5702, 1.1903, 0.8739, 0.8051, 0.7634,\n",
       "                       0.9878, 0.9696, 1.0832, 1.0103, 1.3579, 1.2162, 0.8702, 1.6696, 0.8657,\n",
       "                       1.0446, 1.0933, 0.7857], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 2.2371e-01,  2.4832e-01,  2.2291e-01],\n",
       "                         [ 2.9426e-01,  3.2950e-01,  2.4456e-01],\n",
       "                         [ 2.1469e-01,  3.5508e-01, -2.8439e-01]],\n",
       "               \n",
       "                        [[ 1.8908e-01, -2.4296e-01,  6.1377e-02],\n",
       "                         [-1.8135e-01, -7.0336e-02, -6.9888e-02],\n",
       "                         [-5.2125e-01,  3.9737e-02, -2.4104e-01]],\n",
       "               \n",
       "                        [[ 1.8964e-01,  1.6258e-01,  6.3449e-02],\n",
       "                         [ 1.6314e-02, -3.9382e-02,  2.1019e-02],\n",
       "                         [-5.7485e-02,  1.7632e-01,  7.7633e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.7026e-02, -1.1407e-02,  3.4443e-01],\n",
       "                         [-4.0760e-01,  2.6972e-01,  3.7007e-01],\n",
       "                         [-1.0451e-02,  2.7140e-01,  1.6235e-01]],\n",
       "               \n",
       "                        [[-4.9493e-02, -1.7939e-01,  4.9788e-02],\n",
       "                         [-1.4360e-01, -3.7325e-03, -3.7699e-01],\n",
       "                         [-2.0780e-01, -1.0592e-01, -5.5949e-01]],\n",
       "               \n",
       "                        [[-2.2768e-01, -2.9414e-01, -1.2014e-01],\n",
       "                         [-2.8917e-01, -4.6860e-01, -2.5696e-01],\n",
       "                         [-2.9989e-01, -3.3699e-01, -3.1916e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4891e-01, -1.2961e-01,  3.1222e-02],\n",
       "                         [-2.4995e-01,  1.9436e-01, -3.9277e-01],\n",
       "                         [ 1.8867e-02,  1.5840e-01,  1.6580e-01]],\n",
       "               \n",
       "                        [[-3.5734e-01, -1.1989e-01, -3.3167e-02],\n",
       "                         [-5.3605e-02, -1.0488e-01, -5.3263e-02],\n",
       "                         [ 1.4715e-01,  3.1856e-01,  3.2120e-01]],\n",
       "               \n",
       "                        [[-1.3000e-02, -7.2356e-02, -5.0258e-01],\n",
       "                         [ 2.1462e-01,  1.4832e-01, -1.7191e-01],\n",
       "                         [ 1.8312e-01,  3.7357e-01,  2.6056e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.6875e-01, -6.3180e-01,  3.0741e-02],\n",
       "                         [-1.9742e-01, -1.4530e-01, -3.3071e-01],\n",
       "                         [ 3.1513e-01,  4.8809e-01,  3.2683e-01]],\n",
       "               \n",
       "                        [[ 2.8206e-01, -2.6242e-01, -8.9526e-01],\n",
       "                         [ 2.3911e-01, -3.5794e-01, -9.0906e-01],\n",
       "                         [ 1.3953e-01,  2.8722e-01,  1.0540e-01]],\n",
       "               \n",
       "                        [[-8.0115e-02, -2.8758e-01, -4.6846e-01],\n",
       "                         [ 6.0729e-02, -5.1874e-02, -2.2539e-01],\n",
       "                         [ 2.0468e-01,  6.8486e-02,  4.5198e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4316e-01, -1.8622e-01, -8.8409e-02],\n",
       "                         [-2.1033e-01,  1.6694e-01,  5.4731e-02],\n",
       "                         [-2.3095e-01,  1.1636e-01, -1.6257e-01]],\n",
       "               \n",
       "                        [[-3.7680e-01, -1.7902e-01,  5.1298e-02],\n",
       "                         [ 1.0737e-01,  1.6496e-01, -1.3361e-01],\n",
       "                         [ 5.3158e-02, -6.3857e-02, -9.5982e-02]],\n",
       "               \n",
       "                        [[-7.9973e-01, -7.2402e-01, -6.2713e-01],\n",
       "                         [-5.4906e-01, -5.4334e-01, -4.4202e-01],\n",
       "                         [-4.6225e-01, -8.1638e-01, -5.8272e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.5159e-01, -5.6963e-01, -3.1188e-01],\n",
       "                         [-5.1183e-01, -5.0804e-01, -1.2936e-02],\n",
       "                         [-3.4478e-02, -3.8518e-02, -7.2975e-03]],\n",
       "               \n",
       "                        [[ 2.3873e-01, -2.9560e-01, -2.3332e-02],\n",
       "                         [ 1.7747e-01, -4.1003e-01, -1.5689e-01],\n",
       "                         [ 2.1548e-02, -1.6752e-01,  2.5952e-01]],\n",
       "               \n",
       "                        [[ 2.0894e-01,  1.4160e-01,  1.0893e-01],\n",
       "                         [ 6.8129e-02,  1.0290e-01,  2.1371e-01],\n",
       "                         [-2.3420e-01, -1.5461e-01,  9.8721e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.7783e-01, -1.8476e-02,  2.3260e-02],\n",
       "                         [-3.5045e-01, -3.4136e-01, -2.8794e-01],\n",
       "                         [-1.0425e-02, -5.5994e-02, -3.6534e-02]],\n",
       "               \n",
       "                        [[ 1.3926e-01,  8.9128e-03, -1.1763e-01],\n",
       "                         [ 2.6761e-01, -1.7982e-01, -4.0487e-01],\n",
       "                         [-6.8539e-02, -4.6417e-01, -2.9071e-01]],\n",
       "               \n",
       "                        [[ 1.8388e-01,  1.0331e-01,  6.5166e-02],\n",
       "                         [ 7.0851e-02,  1.5660e-01,  2.2236e-02],\n",
       "                         [ 2.3607e-01,  1.8036e-01,  2.0287e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.6276e-01, -4.1850e-01,  1.6148e-02],\n",
       "                         [ 1.0800e-01, -1.0887e-01,  4.3845e-02],\n",
       "                         [-6.6280e-02, -4.2268e-01,  1.7854e-01]],\n",
       "               \n",
       "                        [[ 1.2858e-01,  3.3416e-02,  1.1374e-01],\n",
       "                         [ 7.7871e-02, -3.4656e-01, -1.7840e-02],\n",
       "                         [-1.3453e-01, -3.2538e-01, -2.8439e-01]],\n",
       "               \n",
       "                        [[-7.7268e-02,  5.3884e-02,  8.0440e-02],\n",
       "                         [-2.2366e-02, -7.9371e-02, -1.5611e-04],\n",
       "                         [ 4.8693e-02, -2.7928e-02,  7.1094e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.4182e-02,  9.7602e-02, -1.8383e-01],\n",
       "                         [ 4.6292e-02, -5.0426e-02,  1.5989e-01],\n",
       "                         [-6.1726e-02,  1.0558e-01,  2.0821e-01]],\n",
       "               \n",
       "                        [[-1.4688e-01,  2.7918e-01,  1.6604e-02],\n",
       "                         [-3.9471e-01, -1.1908e-01, -3.4683e-01],\n",
       "                         [ 1.0190e-01,  1.5937e-01,  3.4325e-01]],\n",
       "               \n",
       "                        [[ 8.2493e-02, -1.8364e-01, -4.3994e-02],\n",
       "                         [ 2.9920e-02,  1.6899e-01,  1.3069e-01],\n",
       "                         [ 1.4345e-01,  3.3920e-01,  1.8741e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.8595e-02, -3.7865e-02,  2.0759e-02],\n",
       "                         [ 9.1452e-03,  3.2831e-02,  1.2359e-01],\n",
       "                         [ 6.4195e-02,  7.9965e-02, -2.1055e-02]],\n",
       "               \n",
       "                        [[-4.4761e-01, -1.7904e-01,  4.3025e-01],\n",
       "                         [-1.9264e-02, -1.5593e-01, -1.3127e-01],\n",
       "                         [-2.1960e-03, -1.5292e-01, -6.5129e-01]],\n",
       "               \n",
       "                        [[ 2.9250e-02, -3.0086e-01, -1.9450e-01],\n",
       "                         [-4.8975e-02, -1.9281e-01,  3.9139e-02],\n",
       "                         [ 1.4408e-01,  1.7218e-01,  1.0990e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.5452e-03, -7.7688e-02, -3.1644e-01],\n",
       "                         [ 2.5588e-01, -1.7493e-01, -1.2147e-01],\n",
       "                         [ 3.6686e-02, -1.2620e-01, -2.3486e-01]],\n",
       "               \n",
       "                        [[-4.0436e-01, -1.9524e-01, -1.3163e-02],\n",
       "                         [-2.3131e-03, -2.0292e-01,  5.7021e-02],\n",
       "                         [ 2.4939e-01, -1.9504e-01, -1.5615e-01]],\n",
       "               \n",
       "                        [[-6.0391e-01, -6.0431e-01, -4.7786e-01],\n",
       "                         [-6.3172e-01, -6.6100e-01, -5.1120e-01],\n",
       "                         [-5.1513e-01, -3.2708e-01, -2.7110e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.9243e-02, -7.2449e-02,  2.3878e-01],\n",
       "                         [ 1.3123e-01, -3.7885e-02, -9.1095e-02],\n",
       "                         [-6.7964e-02, -9.2570e-02, -2.5017e-01]],\n",
       "               \n",
       "                        [[ 2.2054e-01,  1.7599e-01,  1.3353e-01],\n",
       "                         [ 2.2634e-01,  2.1161e-01,  9.3703e-02],\n",
       "                         [-1.7341e-01, -7.0477e-02,  1.2029e-01]],\n",
       "               \n",
       "                        [[-2.2885e-01, -2.6899e-01, -3.1754e-01],\n",
       "                         [-2.2839e-01, -2.9206e-01, -5.4042e-01],\n",
       "                         [-7.2817e-02, -2.5679e-01, -2.8763e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-0.0692,  0.0225,  0.0146,  0.0062, -0.0003, -0.0353, -0.0207, -0.0075,\n",
       "                       -0.0361, -0.0140, -0.0116, -0.0363,  0.0138,  0.0071,  0.0018,  0.0140,\n",
       "                       -0.0228,  0.0212,  0.0163,  0.0620,  0.0307,  0.0104, -0.0111,  0.0154,\n",
       "                        0.0466,  0.0570,  0.0253,  0.0456, -0.0116, -0.0136,  0.0368,  0.0043,\n",
       "                        0.0467, -0.0006, -0.0226,  0.0234,  0.0070,  0.0026, -0.0140,  0.0060,\n",
       "                       -0.0246, -0.0086,  0.0180, -0.0355,  0.0091,  0.0087,  0.0217,  0.0231],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.5255, -0.4440, -0.7502, -0.4592, -0.5407, -0.3258, -0.4470, -0.2782,\n",
       "                       -0.5453, -0.6598, -0.4410, -0.5283, -0.4360, -0.4407, -0.5334, -0.7102,\n",
       "                       -0.6524, -0.4788, -0.4769, -0.9036, -0.4927, -0.3680, -0.5875, -0.5920,\n",
       "                       -0.2959, -0.6984, -0.5142, -0.5197, -0.3080, -0.4096, -0.3908, -0.4162,\n",
       "                       -0.3157, -0.4861, -0.7821, -0.5637, -0.5118, -0.5033, -0.4667, -0.3168,\n",
       "                       -0.3553, -0.9253, -0.5934, -0.4293, -0.4843, -0.5167, -0.5467, -0.4649],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.8997, 1.1266, 0.7953, 0.8191, 0.7902, 0.6564, 0.9023, 0.8947, 0.8237,\n",
       "                       1.0059, 1.0619, 0.9432, 1.0210, 0.7675, 0.9553, 0.8466, 0.8415, 0.9622,\n",
       "                       0.8966, 1.2893, 0.7947, 0.9116, 1.0173, 1.0942, 0.7765, 0.9688, 0.9011,\n",
       "                       0.9799, 0.8172, 1.0734, 1.0630, 0.7877, 0.8588, 1.0285, 1.2855, 1.1365,\n",
       "                       1.1018, 1.0648, 1.1588, 0.8368, 1.0537, 1.3252, 1.0389, 0.7222, 0.9511,\n",
       "                       0.8036, 0.9329, 0.8107], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[ 0.0395, -0.1449,  0.1605],\n",
       "                         [ 0.0544,  0.4058,  0.1935],\n",
       "                         [ 0.2905,  0.2596,  0.2494]],\n",
       "               \n",
       "                        [[ 0.0326,  0.1080,  0.3976],\n",
       "                         [ 0.0244, -0.1985,  0.2211],\n",
       "                         [-0.1545, -0.3506,  0.0830]],\n",
       "               \n",
       "                        [[-0.2255, -0.0501,  0.1122],\n",
       "                         [ 0.0418, -0.2074,  0.2122],\n",
       "                         [ 0.0199,  0.1283, -0.2099]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1298,  0.0286, -0.2199],\n",
       "                         [ 0.0669, -0.1701, -0.3475],\n",
       "                         [ 0.0481,  0.0074, -0.2604]],\n",
       "               \n",
       "                        [[ 0.3875,  0.0814, -0.0179],\n",
       "                         [ 0.2203,  0.0760, -0.3791],\n",
       "                         [ 0.0551, -0.0020, -0.0286]],\n",
       "               \n",
       "                        [[-0.2270, -0.2263, -0.4227],\n",
       "                         [-0.3156, -0.2103, -0.1378],\n",
       "                         [-0.1348,  0.1853, -0.0482]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0149, -0.2903, -0.2751],\n",
       "                         [ 0.1026, -0.0308,  0.1329],\n",
       "                         [ 0.0863, -0.1017,  0.2450]],\n",
       "               \n",
       "                        [[ 0.2057, -0.1479,  0.1485],\n",
       "                         [-0.0948, -0.1160, -0.2755],\n",
       "                         [ 0.1557,  0.0630,  0.0786]],\n",
       "               \n",
       "                        [[-0.0367, -0.1468, -0.1525],\n",
       "                         [-0.1829,  0.2110, -0.1127],\n",
       "                         [ 0.0079,  0.3520,  0.0888]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0217, -0.0177, -0.2181],\n",
       "                         [-0.1610,  0.0261, -0.0094],\n",
       "                         [-0.0056,  0.2374, -0.0435]],\n",
       "               \n",
       "                        [[ 0.2800, -0.1006,  0.0262],\n",
       "                         [ 0.1396,  0.0736, -0.1275],\n",
       "                         [ 0.0855, -0.1649, -0.1997]],\n",
       "               \n",
       "                        [[-0.1366, -0.3664, -0.2847],\n",
       "                         [-0.0186,  0.0403,  0.1084],\n",
       "                         [ 0.0477, -0.0829,  0.0453]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.3657, -0.1533, -0.0611],\n",
       "                         [-0.0521, -0.1040, -0.0143],\n",
       "                         [-0.0221, -0.1611,  0.1034]],\n",
       "               \n",
       "                        [[-0.3145,  0.1614,  0.4258],\n",
       "                         [-0.4726, -0.2714, -0.1111],\n",
       "                         [-0.3096, -0.1618, -0.0023]],\n",
       "               \n",
       "                        [[-0.0757, -0.0498, -0.1108],\n",
       "                         [ 0.3376,  0.1433,  0.1957],\n",
       "                         [ 0.0878, -0.0588, -0.2911]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0206, -0.0938,  0.1063],\n",
       "                         [ 0.0989,  0.0386,  0.1279],\n",
       "                         [ 0.1610,  0.3243,  0.1295]],\n",
       "               \n",
       "                        [[-0.4959, -0.0519, -0.0039],\n",
       "                         [ 0.1069,  0.0479, -0.5173],\n",
       "                         [-0.1383, -0.2400, -0.2087]],\n",
       "               \n",
       "                        [[-0.1665, -0.0872, -0.1649],\n",
       "                         [-0.1219,  0.0771, -0.0911],\n",
       "                         [-0.2397, -0.1058, -0.1802]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.1898,  0.2641,  0.0080],\n",
       "                         [-0.1909,  0.4374,  0.1361],\n",
       "                         [-0.1808,  0.3121,  0.1420]],\n",
       "               \n",
       "                        [[ 0.2982,  0.5320,  0.0684],\n",
       "                         [ 0.0248,  0.2461,  0.1590],\n",
       "                         [-0.0131,  0.3119,  0.1335]],\n",
       "               \n",
       "                        [[ 0.2229,  0.0330, -0.4745],\n",
       "                         [ 0.0349,  0.0355,  0.2760],\n",
       "                         [-0.0131,  0.0196, -0.1564]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.2131, -0.1052, -0.2246],\n",
       "                         [-0.1275,  0.0159, -0.4143],\n",
       "                         [ 0.0019,  0.0447, -0.3801]],\n",
       "               \n",
       "                        [[-0.3385, -0.1984, -0.4100],\n",
       "                         [ 0.2179, -0.3673,  0.1627],\n",
       "                         [ 0.1760,  0.1028,  0.0999]],\n",
       "               \n",
       "                        [[ 0.4098,  0.1083, -0.1471],\n",
       "                         [ 0.3239,  0.0373, -0.1915],\n",
       "                         [ 0.0111,  0.1138, -0.0135]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1976,  0.0841,  0.4326],\n",
       "                         [-0.0667, -0.1346, -0.3809],\n",
       "                         [-0.0222, -0.1374, -0.3774]],\n",
       "               \n",
       "                        [[ 0.1116, -0.0423, -0.1678],\n",
       "                         [ 0.0731,  0.0665, -0.2632],\n",
       "                         [-0.1153, -0.3031, -0.5431]],\n",
       "               \n",
       "                        [[-0.1675, -0.2013,  0.1226],\n",
       "                         [-0.1923, -0.1430, -0.0612],\n",
       "                         [ 0.0705,  0.1593, -0.0981]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.1262,  0.3089,  0.1381],\n",
       "                         [ 0.0743,  0.0764,  0.1250],\n",
       "                         [ 0.0546, -0.1493, -0.1395]],\n",
       "               \n",
       "                        [[ 0.2165,  0.0106, -0.1086],\n",
       "                         [ 0.1022, -0.1329,  0.1673],\n",
       "                         [ 0.0596,  0.0604,  0.1921]],\n",
       "               \n",
       "                        [[-0.0064,  0.0141, -0.0454],\n",
       "                         [-0.1104,  0.2018,  0.0120],\n",
       "                         [-0.1227,  0.0576, -0.0826]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.1750, -0.2484, -0.3828],\n",
       "                         [ 0.1789, -0.0133,  0.2406],\n",
       "                         [ 0.2862, -0.0726,  0.0356]],\n",
       "               \n",
       "                        [[-0.0442,  0.2122,  0.7398],\n",
       "                         [ 0.1078, -0.0694,  0.3660],\n",
       "                         [ 0.1978,  0.1943,  0.0611]],\n",
       "               \n",
       "                        [[-0.0820, -0.4296, -0.1729],\n",
       "                         [-0.1297, -0.3550, -0.1275],\n",
       "                         [-0.2188, -0.0305,  0.1986]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0711, -0.1316,  0.1204],\n",
       "                         [ 0.0265, -0.0455,  0.0436],\n",
       "                         [ 0.2689, -0.0896,  0.1050]],\n",
       "               \n",
       "                        [[-0.0364,  0.1952, -0.0613],\n",
       "                         [-0.1191,  0.1246, -0.1646],\n",
       "                         [ 0.0689,  0.0155,  0.1674]],\n",
       "               \n",
       "                        [[-0.0530,  0.1517, -0.0738],\n",
       "                         [-0.0648,  0.0303,  0.1503],\n",
       "                         [ 0.3143,  0.2827,  0.3454]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-0.0221, -0.0169,  0.0499,  0.0771,  0.0070, -0.0751,  0.0656, -0.0207,\n",
       "                       -0.0109,  0.0138, -0.0136,  0.0172,  0.0138, -0.0492, -0.0466,  0.0242,\n",
       "                       -0.0100, -0.0020,  0.0193,  0.0007,  0.0796,  0.0216, -0.0530, -0.0039,\n",
       "                       -0.0207,  0.0114,  0.0065,  0.0056,  0.0320, -0.0479,  0.0349,  0.0489,\n",
       "                        0.0114,  0.0520,  0.0175, -0.0356,  0.0162,  0.0075, -0.0118,  0.0423,\n",
       "                        0.0413,  0.0637, -0.0693,  0.0379,  0.0385,  0.0120, -0.0103, -0.0030],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.5705, -0.5475, -0.4630, -0.7548, -0.5557, -0.9379, -0.7739, -0.7426,\n",
       "                       -0.4699, -0.8756, -0.7905, -0.8504, -0.9051, -0.5405, -0.8511, -0.2814,\n",
       "                       -1.4971, -0.9032, -0.8464, -0.3955, -0.7784, -2.0626, -0.7094, -0.6113,\n",
       "                       -1.0906, -0.5182, -0.6837, -1.0183, -0.4826, -0.6841, -0.4772, -0.5764,\n",
       "                       -0.5228, -0.5118, -0.5365, -0.7824, -0.6109, -0.3926, -0.6651, -0.5065,\n",
       "                       -0.6903, -0.6874, -0.3175, -0.5309, -1.0580, -0.5574, -0.6119, -0.6868],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.8490, 0.8716, 0.8321, 0.9883, 0.8497, 0.8543, 0.8872, 0.7809, 0.7360,\n",
       "                       0.9331, 0.7648, 0.9920, 0.9915, 0.7314, 1.1002, 0.6987, 0.8375, 0.8971,\n",
       "                       0.9303, 0.7209, 0.8626, 0.8365, 0.8688, 0.8090, 1.0327, 0.7210, 0.7682,\n",
       "                       1.1171, 0.7246, 0.7619, 0.7036, 0.8985, 0.8829, 0.7218, 0.8274, 0.8633,\n",
       "                       0.8396, 0.8299, 0.9351, 0.8501, 0.7610, 1.0180, 0.6978, 0.7580, 0.9623,\n",
       "                       0.7724, 0.8249, 0.7504], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-2.5893e-02, -1.9872e-01,  1.7401e-01],\n",
       "                         [-1.2388e-01,  7.6305e-02,  4.3584e-02],\n",
       "                         [-1.5219e-01, -2.9584e-02,  1.3664e-01]],\n",
       "               \n",
       "                        [[ 1.0942e-01,  1.4078e-01,  5.9457e-02],\n",
       "                         [ 3.1695e-02,  5.3186e-01,  1.7648e-01],\n",
       "                         [-1.3128e-01, -1.5093e-01, -4.4432e-03]],\n",
       "               \n",
       "                        [[-6.8923e-02, -9.1175e-02, -1.8597e-01],\n",
       "                         [ 1.6730e-01, -8.8805e-02, -1.0182e-01],\n",
       "                         [-1.1819e-01,  9.1011e-02,  1.3153e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.7604e-02,  6.8509e-02,  2.3041e-01],\n",
       "                         [ 9.2632e-02,  4.5426e-02,  9.9065e-02],\n",
       "                         [ 1.8936e-01,  1.4728e-01,  1.6417e-01]],\n",
       "               \n",
       "                        [[-8.7920e-02, -1.3510e-01, -3.5542e-02],\n",
       "                         [-4.1920e-02,  1.9008e-01, -1.9611e-01],\n",
       "                         [ 1.5886e-01,  1.6061e-01,  5.6400e-02]],\n",
       "               \n",
       "                        [[-5.2234e-01, -1.0618e-01, -6.2289e-02],\n",
       "                         [-3.8046e-01, -3.9147e-01, -2.6192e-01],\n",
       "                         [-3.3372e-01, -3.5018e-01, -2.4115e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.7839e-01,  1.1002e-01,  2.4277e-02],\n",
       "                         [ 1.8788e-01,  6.8132e-02,  3.0452e-02],\n",
       "                         [-1.0729e-01, -3.3932e-01, -1.7916e-01]],\n",
       "               \n",
       "                        [[-3.0612e-01, -2.2680e-01, -1.2279e-01],\n",
       "                         [-5.6316e-02, -1.6619e-01, -1.7122e-01],\n",
       "                         [-9.5438e-02, -1.3948e-01,  3.9039e-02]],\n",
       "               \n",
       "                        [[ 2.9026e-01,  1.5291e-01, -1.0455e-01],\n",
       "                         [ 3.0438e-02, -3.7721e-03,  6.6752e-02],\n",
       "                         [ 2.5083e-01,  1.4897e-01,  1.5322e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.8886e-01,  1.4256e-01,  1.8783e-01],\n",
       "                         [ 1.0923e-01,  2.0619e-02,  6.2494e-02],\n",
       "                         [ 1.4979e-01,  6.6209e-02,  6.9644e-02]],\n",
       "               \n",
       "                        [[-4.4614e-03,  9.1116e-02, -1.7864e-01],\n",
       "                         [-7.7936e-02, -8.6367e-02, -2.3165e-01],\n",
       "                         [-4.7769e-02, -8.3296e-02, -2.3039e-01]],\n",
       "               \n",
       "                        [[ 1.9761e-01,  2.4210e-01, -3.1403e-02],\n",
       "                         [ 2.2760e-01,  7.3006e-02,  5.2480e-02],\n",
       "                         [ 2.4318e-01,  1.6187e-01,  4.1467e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.5213e-01,  1.7540e-01,  8.8847e-02],\n",
       "                         [ 7.3308e-02, -4.3422e-02,  1.9891e-01],\n",
       "                         [-1.5981e-01, -2.0798e-01, -5.5314e-02]],\n",
       "               \n",
       "                        [[-2.4366e-01, -3.4911e-01, -2.9937e-01],\n",
       "                         [-1.8104e-01, -2.9740e-01, -1.8361e-01],\n",
       "                         [-2.4908e-01, -4.1977e-01, -2.4674e-01]],\n",
       "               \n",
       "                        [[-2.5447e-01, -1.9331e-01, -1.2451e-01],\n",
       "                         [-8.2556e-02, -5.9263e-02, -8.1090e-02],\n",
       "                         [-1.8305e-01, -2.6282e-02, -1.6807e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.7045e-02, -6.2196e-02, -2.0237e-01],\n",
       "                         [ 7.5362e-02,  6.9880e-02,  5.2474e-02],\n",
       "                         [-2.1995e-02,  8.7597e-02, -3.9357e-02]],\n",
       "               \n",
       "                        [[-3.0693e-01, -6.1595e-03, -3.4861e-01],\n",
       "                         [-8.4696e-02, -1.3468e-02, -2.5379e-01],\n",
       "                         [ 1.0580e-01,  4.4998e-02,  3.8945e-02]],\n",
       "               \n",
       "                        [[ 9.5354e-02,  1.7677e-01,  1.3106e-01],\n",
       "                         [ 4.8504e-02,  3.2273e-02,  3.5599e-02],\n",
       "                         [ 9.8448e-02,  1.4774e-02,  2.2673e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-6.7402e-02, -5.0538e-02,  1.0031e-01],\n",
       "                         [ 3.1717e-02, -2.5765e-01,  4.3627e-02],\n",
       "                         [-1.3087e-01,  1.3679e-01, -1.3877e-01]],\n",
       "               \n",
       "                        [[-1.0225e-01, -1.7620e-01, -2.7829e-02],\n",
       "                         [ 4.1821e-02, -2.5510e-01, -1.4902e-01],\n",
       "                         [ 2.9439e-01,  2.0130e-01,  2.7650e-02]],\n",
       "               \n",
       "                        [[ 8.1816e-02,  1.2647e-01,  1.1551e-01],\n",
       "                         [-1.7144e-01,  1.5370e-01,  1.9529e-01],\n",
       "                         [-3.2217e-01, -1.2119e-01, -3.2641e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.1373e-02,  2.2392e-01,  1.2397e-01],\n",
       "                         [ 1.0325e-01,  2.4217e-01, -4.4031e-02],\n",
       "                         [-1.0391e-01,  5.7919e-02,  7.3877e-03]],\n",
       "               \n",
       "                        [[ 1.8301e-01,  2.8668e-01, -8.9577e-02],\n",
       "                         [ 9.7177e-02, -5.5842e-02,  1.3722e-02],\n",
       "                         [ 1.3907e-01,  2.1665e-02, -2.0730e-02]],\n",
       "               \n",
       "                        [[-2.0773e-01, -2.5168e-01, -7.0841e-02],\n",
       "                         [-1.7900e-01, -3.4501e-01, -1.0403e-02],\n",
       "                         [-3.7830e-01, -4.5212e-02, -4.5194e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0880e-01,  5.2731e-02,  3.0243e-02],\n",
       "                         [ 4.4676e-01,  2.5921e-01,  2.7609e-01],\n",
       "                         [ 5.3789e-02,  7.7635e-02,  4.1659e-02]],\n",
       "               \n",
       "                        [[ 1.2929e-01,  1.5191e-01,  7.6129e-02],\n",
       "                         [ 1.6673e-01,  3.1565e-01,  3.6239e-01],\n",
       "                         [-1.6713e-01,  2.5938e-02, -1.5355e-01]],\n",
       "               \n",
       "                        [[-2.2635e-01, -2.4739e-01, -5.7770e-01],\n",
       "                         [-1.2982e-01, -2.8080e-01, -3.9924e-01],\n",
       "                         [-1.4881e-01,  7.3266e-05, -1.9699e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0577e-02, -8.8375e-02,  5.2550e-02],\n",
       "                         [-1.7384e-01, -2.9489e-02, -3.8631e-02],\n",
       "                         [ 6.4170e-03,  3.3754e-02,  1.0251e-01]],\n",
       "               \n",
       "                        [[-1.5468e-01,  2.2198e-01, -2.0018e-01],\n",
       "                         [-2.3020e-01, -5.7735e-02, -5.7236e-02],\n",
       "                         [-1.8337e-01,  1.0237e-01, -2.9356e-02]],\n",
       "               \n",
       "                        [[ 8.4268e-02, -6.6122e-02,  6.2562e-02],\n",
       "                         [-2.0638e-02,  1.6101e-02,  1.3887e-01],\n",
       "                         [-2.2086e-02, -7.2461e-02,  1.3505e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.9091e-01, -5.2906e-01, -4.2391e-01],\n",
       "                         [-3.1801e-01, -3.3421e-01, -1.8258e-01],\n",
       "                         [-4.4773e-01, -3.0447e-01, -1.7767e-01]],\n",
       "               \n",
       "                        [[-2.1980e-02, -6.3720e-02, -2.4317e-01],\n",
       "                         [-5.8408e-02, -1.4782e-01, -1.7004e-01],\n",
       "                         [-1.0184e-01, -1.0752e-01, -1.2917e-01]],\n",
       "               \n",
       "                        [[-2.3339e-01, -2.4494e-01, -1.2352e-01],\n",
       "                         [-7.0202e-03, -2.0109e-02, -1.2122e-01],\n",
       "                         [-3.4366e-03, -3.0595e-02, -5.3962e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.5338e-01,  1.9042e-01,  1.3424e-01],\n",
       "                         [-4.9031e-02,  8.8519e-02, -2.3286e-02],\n",
       "                         [-2.5956e-01, -7.0516e-02, -2.0332e-01]],\n",
       "               \n",
       "                        [[-1.2031e-02, -7.5485e-02,  2.4874e-03],\n",
       "                         [ 4.6480e-02, -1.3936e-01,  1.1231e-02],\n",
       "                         [ 6.9002e-02,  9.3447e-02,  1.0166e-01]],\n",
       "               \n",
       "                        [[ 8.2753e-02,  3.8096e-02, -5.1775e-02],\n",
       "                         [ 9.4423e-02,  1.2247e-01,  6.1907e-02],\n",
       "                         [ 1.9450e-01,  5.0312e-02,  7.0485e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-3.4973e-02, -6.3321e-02,  3.7624e-02,  2.0549e-02,  9.3764e-03,\n",
       "                        1.9099e-02, -4.1331e-02, -3.8106e-01,  2.7837e-02, -5.2256e-02,\n",
       "                        3.6548e-02, -1.4445e-01,  1.2837e-02,  7.1669e-02,  9.9550e-02,\n",
       "                       -2.6171e-02, -4.5096e-05, -6.5230e-02,  2.9843e-02, -8.5645e-02,\n",
       "                        6.0826e-03,  1.7000e-03, -4.2373e-03, -4.7114e-02,  5.5219e-04,\n",
       "                        1.1759e-02, -2.9474e-02, -7.4576e-02,  4.5647e-02,  3.8886e-03,\n",
       "                        1.2397e-01,  4.4577e-02,  1.9942e-02,  1.1318e-03,  4.4901e-02,\n",
       "                        2.6835e-02, -2.0051e-01,  2.2206e-02,  1.6423e-02,  1.4340e-02,\n",
       "                       -3.0084e-01, -4.4376e-02, -1.2915e-01, -1.1483e-02, -1.6185e-01,\n",
       "                        9.8336e-03, -7.9029e-03, -4.2992e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-1.0147, -0.9782, -0.6467, -1.5398, -0.6410, -1.4147, -0.9696, -1.9602,\n",
       "                       -1.4608, -1.9787, -1.9355, -1.1464, -1.0105, -1.5715, -1.3460, -0.9903,\n",
       "                       -1.3574, -1.0115, -0.9691, -1.6517, -0.8416, -1.2877, -1.0187, -2.0871,\n",
       "                       -1.6473, -0.8950, -0.9149, -1.6465, -1.7982, -1.2020, -1.4334, -1.3372,\n",
       "                       -1.4258, -0.7707, -1.7243, -0.9803, -1.2790, -0.9144, -1.5699, -1.3509,\n",
       "                       -0.9126, -1.3738, -1.0743, -1.0572, -1.8387, -0.7729, -0.9240, -1.3372],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.3955, 3.3604, 2.9153, 0.5645, 4.2218, 0.4013, 3.0752, 4.7397, 0.4329,\n",
       "                       2.6791, 0.7139, 3.9940, 0.2250, 0.6909, 3.7555, 3.4898, 0.4976, 2.8304,\n",
       "                       0.3728, 3.1240, 3.5686, 0.4233, 0.4568, 3.5470, 0.8014, 0.3533, 3.0185,\n",
       "                       3.9224, 0.7691, 0.3819, 1.4561, 2.8606, 0.5002, 0.2373, 2.2364, 0.4097,\n",
       "                       4.0688, 0.2358, 0.5814, 0.4921, 3.5743, 3.4437, 3.6762, 0.4048, 3.7582,\n",
       "                       0.3098, 0.2211, 3.2147], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0457, -0.1510, -0.1908,  ..., -0.0305, -0.0184,  0.0053],\n",
       "                       [-0.2940, -0.0436, -0.1644,  ..., -0.0230, -0.0080, -0.0080],\n",
       "                       [-0.2617, -0.1989, -0.2172,  ..., -0.0280, -0.0130, -0.0105],\n",
       "                       [-0.1487, -0.0473, -0.3341,  ..., -0.0439, -0.0092, -0.0077],\n",
       "                       [ 0.5786,  0.4263,  0.6883,  ..., -0.0044,  0.0142, -0.0087]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.2075,  0.3487, -0.2448, -0.2514,  0.5072], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3894465959072113,\n",
       "   1.3188828508853911,\n",
       "   1.2568164163827895,\n",
       "   1.2061334789991378,\n",
       "   1.1838849287033082,\n",
       "   1.138939864039421,\n",
       "   1.1213825660943986,\n",
       "   1.1053147413730622,\n",
       "   1.0837177706956864,\n",
       "   1.072655676841736,\n",
       "   1.045293864250183,\n",
       "   1.049022758245468,\n",
       "   1.0495843685865403,\n",
       "   1.0023881969451904,\n",
       "   1.0032941131591797,\n",
       "   0.9885607409477234,\n",
       "   0.965736586689949,\n",
       "   0.9576659375429153,\n",
       "   0.9499159274101258,\n",
       "   0.9330177795886994,\n",
       "   0.9286260566711426,\n",
       "   0.8967779486179351,\n",
       "   0.8930581948757171,\n",
       "   0.892057703614235,\n",
       "   0.8871328309774399,\n",
       "   0.8800080753564835,\n",
       "   0.8607329999208451,\n",
       "   0.8595154427289963,\n",
       "   0.8490343161821365,\n",
       "   0.8579992569684982,\n",
       "   0.8358696038126946,\n",
       "   0.8362479876279831,\n",
       "   0.8232812049388886,\n",
       "   0.8024942023158074,\n",
       "   0.8216189768910408,\n",
       "   0.7907844183444976,\n",
       "   0.7995114775300026,\n",
       "   0.7901956652402877,\n",
       "   0.7925056771039963,\n",
       "   0.768370218217373,\n",
       "   0.7878785945773125,\n",
       "   0.7732486748099328,\n",
       "   0.763284193277359,\n",
       "   0.7670016507506371,\n",
       "   0.7557082507014274,\n",
       "   0.7548003917932511,\n",
       "   0.7590099629163742,\n",
       "   0.7411054357886314,\n",
       "   0.7432691705226898,\n",
       "   0.7467964823246003,\n",
       "   0.743869562804699,\n",
       "   0.721649453163147,\n",
       "   0.7314055954217911,\n",
       "   0.7221497538089752,\n",
       "   0.7348781760334968,\n",
       "   0.7270159770846367,\n",
       "   0.7161220132112504,\n",
       "   0.7178113833069801,\n",
       "   0.7096423832178116,\n",
       "   0.7100899875164032,\n",
       "   0.7102364325523376,\n",
       "   0.7079005706906318,\n",
       "   0.6927461556792259,\n",
       "   0.6883299989104271,\n",
       "   0.7042433821558952,\n",
       "   0.7007082766294479,\n",
       "   0.7037208490371704,\n",
       "   0.6782381798028946,\n",
       "   0.6826338396072388,\n",
       "   0.6940277944803238,\n",
       "   0.6783129224777221,\n",
       "   0.6817049302458763,\n",
       "   0.6900172064900398,\n",
       "   0.6850895961523056,\n",
       "   0.6719325332045555,\n",
       "   0.6674961098432541,\n",
       "   0.6819405376315116,\n",
       "   0.6781457500457764,\n",
       "   0.663913799583912,\n",
       "   0.6758210509419441,\n",
       "   0.6586329529285431,\n",
       "   0.6640590678453445,\n",
       "   0.66498611921072,\n",
       "   0.6591051014661788,\n",
       "   0.6534209434986115,\n",
       "   0.6603045228719712,\n",
       "   0.6510416877269745,\n",
       "   0.658975203037262,\n",
       "   0.6431071430444717,\n",
       "   0.6401615181565284,\n",
       "   0.6429493899941444,\n",
       "   0.6584385730624199,\n",
       "   0.6260878340601921,\n",
       "   0.6397856315374374,\n",
       "   0.6435390568971634,\n",
       "   0.6510772843956947,\n",
       "   0.6443251947760582,\n",
       "   0.6429542524814605,\n",
       "   0.6296522496938706],\n",
       "  'train_loss_std': [0.12701539838672235,\n",
       "   0.11540202852164343,\n",
       "   0.12584150276328984,\n",
       "   0.12122904353482102,\n",
       "   0.12439287499327963,\n",
       "   0.12972842975410653,\n",
       "   0.13651291341207708,\n",
       "   0.12375098663618045,\n",
       "   0.13091144632227297,\n",
       "   0.12823525403144934,\n",
       "   0.13087294053126758,\n",
       "   0.14054026237926387,\n",
       "   0.1357628327500694,\n",
       "   0.14091692741751524,\n",
       "   0.1370637814986152,\n",
       "   0.14902398130312797,\n",
       "   0.13361284025949063,\n",
       "   0.14049558327672243,\n",
       "   0.14439499641525308,\n",
       "   0.15193124435881478,\n",
       "   0.14538835117771368,\n",
       "   0.13291965903322855,\n",
       "   0.14407724739074,\n",
       "   0.1454671572739593,\n",
       "   0.14322073820942313,\n",
       "   0.1545851549978417,\n",
       "   0.13973282856084188,\n",
       "   0.14341218000119438,\n",
       "   0.14002094403431045,\n",
       "   0.14852114226316257,\n",
       "   0.14171801752592333,\n",
       "   0.14411694696915286,\n",
       "   0.14686143024282378,\n",
       "   0.14210967163293156,\n",
       "   0.15116684901344857,\n",
       "   0.15291056218218965,\n",
       "   0.1366683289060348,\n",
       "   0.14955675404584096,\n",
       "   0.1523749859563398,\n",
       "   0.15175435512452928,\n",
       "   0.14826300846396603,\n",
       "   0.14430706048588468,\n",
       "   0.14729852561473605,\n",
       "   0.1510507373875203,\n",
       "   0.15046990077983854,\n",
       "   0.148961352028513,\n",
       "   0.14972175033073515,\n",
       "   0.14479977320684895,\n",
       "   0.15083259490511108,\n",
       "   0.14496607694681798,\n",
       "   0.15153556935805623,\n",
       "   0.14145530608676785,\n",
       "   0.14560823886351987,\n",
       "   0.1477013482033879,\n",
       "   0.1603360971723319,\n",
       "   0.15075703669654597,\n",
       "   0.1350534627320653,\n",
       "   0.15399497134848955,\n",
       "   0.14554601207206738,\n",
       "   0.14528399077209112,\n",
       "   0.14752868789120163,\n",
       "   0.14787018440429714,\n",
       "   0.14640166769071558,\n",
       "   0.1414578317540379,\n",
       "   0.14585840744270476,\n",
       "   0.14607437971459786,\n",
       "   0.14371700542426105,\n",
       "   0.14997683726714828,\n",
       "   0.13799398287204734,\n",
       "   0.14809754919784252,\n",
       "   0.13424942998352984,\n",
       "   0.14017815715431106,\n",
       "   0.1416078332367257,\n",
       "   0.14628157824192053,\n",
       "   0.1429637340553634,\n",
       "   0.1439187929681398,\n",
       "   0.1484051690597267,\n",
       "   0.14684220401422937,\n",
       "   0.14184983678134627,\n",
       "   0.14492728041615904,\n",
       "   0.14120342890331916,\n",
       "   0.14515267442621219,\n",
       "   0.14753720175418059,\n",
       "   0.13846663892261224,\n",
       "   0.14863270751678664,\n",
       "   0.13092250487054458,\n",
       "   0.13929899483377123,\n",
       "   0.14463495337777788,\n",
       "   0.13994241631762247,\n",
       "   0.14364170434468287,\n",
       "   0.14280043955918362,\n",
       "   0.14657449507308692,\n",
       "   0.1357793580698006,\n",
       "   0.14821126144397356,\n",
       "   0.14875189960132423,\n",
       "   0.1580483626941345,\n",
       "   0.13998733924988388,\n",
       "   0.14366836034575817,\n",
       "   0.13912867806233228],\n",
       "  'train_accuracy_mean': [0.42566666668653486,\n",
       "   0.45950666633248327,\n",
       "   0.49434666711091996,\n",
       "   0.5174800001978874,\n",
       "   0.5299066652059555,\n",
       "   0.5484133325219155,\n",
       "   0.5583199992775917,\n",
       "   0.5682399998307228,\n",
       "   0.5760933331251145,\n",
       "   0.5830666657090187,\n",
       "   0.5959733303785324,\n",
       "   0.5895866663455963,\n",
       "   0.5871866667866706,\n",
       "   0.6124533326029777,\n",
       "   0.6089999985694885,\n",
       "   0.615066665828228,\n",
       "   0.6258399998545646,\n",
       "   0.627986665725708,\n",
       "   0.6307599986195565,\n",
       "   0.6409866655468941,\n",
       "   0.6410533331632614,\n",
       "   0.653439999461174,\n",
       "   0.656253332734108,\n",
       "   0.6550133336186409,\n",
       "   0.6575733328461647,\n",
       "   0.6616933327317238,\n",
       "   0.6710666664838791,\n",
       "   0.670066666662693,\n",
       "   0.6740666654706001,\n",
       "   0.6728266659379005,\n",
       "   0.6816399990916252,\n",
       "   0.6811066666841507,\n",
       "   0.686239999294281,\n",
       "   0.6945733331441879,\n",
       "   0.6860533329248428,\n",
       "   0.6999733328223229,\n",
       "   0.6954133347272873,\n",
       "   0.6998266653418541,\n",
       "   0.696399999320507,\n",
       "   0.7084133331775665,\n",
       "   0.7013599997758866,\n",
       "   0.7059066651463508,\n",
       "   0.7114266664385795,\n",
       "   0.7078933343887329,\n",
       "   0.7123466650247574,\n",
       "   0.7137733348608017,\n",
       "   0.7124799993634224,\n",
       "   0.7180533338785171,\n",
       "   0.7196399991512299,\n",
       "   0.7185333338975907,\n",
       "   0.717826666712761,\n",
       "   0.7278533320426941,\n",
       "   0.7219733340144158,\n",
       "   0.7271199989914894,\n",
       "   0.7235066670179368,\n",
       "   0.7243333342075348,\n",
       "   0.729666666328907,\n",
       "   0.728559997677803,\n",
       "   0.7319199994206429,\n",
       "   0.7324133335351944,\n",
       "   0.7298399990797043,\n",
       "   0.7314933323860169,\n",
       "   0.7389333349466324,\n",
       "   0.7412533332109451,\n",
       "   0.732359999537468,\n",
       "   0.735773333311081,\n",
       "   0.7330666664242744,\n",
       "   0.745973332643509,\n",
       "   0.7449599987268448,\n",
       "   0.7392533318996429,\n",
       "   0.7442800011634827,\n",
       "   0.7421199997663498,\n",
       "   0.7390533332824707,\n",
       "   0.7399333326816558,\n",
       "   0.7477866661548614,\n",
       "   0.7487333331108094,\n",
       "   0.7412000008821488,\n",
       "   0.7442933323383332,\n",
       "   0.7488800005912781,\n",
       "   0.7451600005626678,\n",
       "   0.7505733337402344,\n",
       "   0.7477600009441375,\n",
       "   0.7496533335447312,\n",
       "   0.7497333331108094,\n",
       "   0.7526800007820129,\n",
       "   0.752226666688919,\n",
       "   0.7558933318853378,\n",
       "   0.7527733342647552,\n",
       "   0.7585999987125397,\n",
       "   0.7578533334732056,\n",
       "   0.7571733338832856,\n",
       "   0.7513599990606308,\n",
       "   0.7644000000953675,\n",
       "   0.7586533334255219,\n",
       "   0.7569066665172577,\n",
       "   0.7540399993062019,\n",
       "   0.7573200001716613,\n",
       "   0.756666667342186,\n",
       "   0.7634266662597656],\n",
       "  'train_accuracy_std': [0.0666424954376357,\n",
       "   0.06498427901991534,\n",
       "   0.07071598768953695,\n",
       "   0.06762613285573901,\n",
       "   0.06780160622343785,\n",
       "   0.0695577629239441,\n",
       "   0.07322188367479672,\n",
       "   0.06660407108460376,\n",
       "   0.07010677681314374,\n",
       "   0.06757264472832027,\n",
       "   0.06995337465810189,\n",
       "   0.07219669279018942,\n",
       "   0.06806791233072053,\n",
       "   0.06887495161123851,\n",
       "   0.06779691917564928,\n",
       "   0.07307603472291471,\n",
       "   0.06755693005369309,\n",
       "   0.06816541850078518,\n",
       "   0.0729057252269946,\n",
       "   0.0724559936908898,\n",
       "   0.07130810733005985,\n",
       "   0.066690576554243,\n",
       "   0.06889933300972764,\n",
       "   0.07165116018025651,\n",
       "   0.07055447391215743,\n",
       "   0.07375198180791641,\n",
       "   0.066902383789398,\n",
       "   0.06860706202822776,\n",
       "   0.06546310450809771,\n",
       "   0.0691028948577757,\n",
       "   0.06610344936562708,\n",
       "   0.06954916529752597,\n",
       "   0.06771571477755768,\n",
       "   0.06731795385155238,\n",
       "   0.07277469948901179,\n",
       "   0.07157031940350309,\n",
       "   0.06409390906585503,\n",
       "   0.06692925870353357,\n",
       "   0.07208155775238523,\n",
       "   0.07051094707200697,\n",
       "   0.06830597163872208,\n",
       "   0.06686138425545429,\n",
       "   0.06816457318920477,\n",
       "   0.0692442200864396,\n",
       "   0.06698659896640431,\n",
       "   0.07037349317565947,\n",
       "   0.069589147782412,\n",
       "   0.06717497371154285,\n",
       "   0.06713538029351493,\n",
       "   0.06764863527196173,\n",
       "   0.07094276984471662,\n",
       "   0.06516383340446995,\n",
       "   0.0672535450834177,\n",
       "   0.0677667172235225,\n",
       "   0.07336281881605133,\n",
       "   0.06834926659506212,\n",
       "   0.06460117108093195,\n",
       "   0.0720183137233499,\n",
       "   0.06775742781177198,\n",
       "   0.06466836993882077,\n",
       "   0.06686418240054555,\n",
       "   0.06851174246849269,\n",
       "   0.0646090463810962,\n",
       "   0.06525051045422364,\n",
       "   0.06690779097558866,\n",
       "   0.0646756157429189,\n",
       "   0.06433606177297808,\n",
       "   0.06830297975875022,\n",
       "   0.06476022810871217,\n",
       "   0.06622636432369632,\n",
       "   0.06156400012295792,\n",
       "   0.0651728887361474,\n",
       "   0.06266731894294701,\n",
       "   0.06634787310042489,\n",
       "   0.06450901086542231,\n",
       "   0.0653040396307577,\n",
       "   0.06801816715225162,\n",
       "   0.06717448702978422,\n",
       "   0.06555668396673933,\n",
       "   0.06534249045083225,\n",
       "   0.0626888629442405,\n",
       "   0.06852902198520107,\n",
       "   0.06361526574000236,\n",
       "   0.0651044970957132,\n",
       "   0.06536747259457004,\n",
       "   0.06113534843666355,\n",
       "   0.061214575774707056,\n",
       "   0.06546277764661945,\n",
       "   0.06293908951014375,\n",
       "   0.06433741550743187,\n",
       "   0.06380394443680379,\n",
       "   0.06422750631679307,\n",
       "   0.06430220203574527,\n",
       "   0.06759394974342588,\n",
       "   0.0667588381167771,\n",
       "   0.0709700291826374,\n",
       "   0.06094091660525087,\n",
       "   0.06422391326113468,\n",
       "   0.060985172860133684],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.4319868238766988,\n",
       "   1.3870793974399567,\n",
       "   1.3548857653141022,\n",
       "   1.2833127645651499,\n",
       "   1.2684562190373738,\n",
       "   1.263748548825582,\n",
       "   1.2293064320087432,\n",
       "   1.2193325018882752,\n",
       "   1.1999534509579342,\n",
       "   1.1860659807920455,\n",
       "   1.1902659557263056,\n",
       "   1.197732854882876,\n",
       "   1.1745983284711838,\n",
       "   1.1627070418993632,\n",
       "   1.1467224965492884,\n",
       "   1.1618999628225963,\n",
       "   1.1787308249870936,\n",
       "   1.152689579129219,\n",
       "   1.1578577868143718,\n",
       "   1.123151752948761,\n",
       "   1.134512508312861,\n",
       "   1.1191212479273478,\n",
       "   1.0946123701334,\n",
       "   1.1043832163016,\n",
       "   1.1266709597905478,\n",
       "   1.1162122323115666,\n",
       "   1.1012370103597642,\n",
       "   1.1113551688194274,\n",
       "   1.0840968851248423,\n",
       "   1.0796949166059493,\n",
       "   1.0922645970185598,\n",
       "   1.0856957759459813,\n",
       "   1.0557025516033172,\n",
       "   1.074795064330101,\n",
       "   1.0754908088843027,\n",
       "   1.0776037947336832,\n",
       "   1.0749225781361262,\n",
       "   1.0785073947906494,\n",
       "   1.0526387776931128,\n",
       "   1.063338538010915,\n",
       "   1.054018347064654,\n",
       "   1.0450335260232289,\n",
       "   1.0750093386570612,\n",
       "   1.0546939925352732,\n",
       "   1.0531557726860046,\n",
       "   1.0432714535792669,\n",
       "   1.0260687462488811,\n",
       "   1.061207211613655,\n",
       "   1.04544835348924,\n",
       "   1.0490387829144796,\n",
       "   1.03739111383756,\n",
       "   1.0336476741234462,\n",
       "   1.0397945954402288,\n",
       "   1.0332008216778437,\n",
       "   1.0516832325855892,\n",
       "   1.0233430755138397,\n",
       "   1.0329658943414688,\n",
       "   1.0187088986237844,\n",
       "   1.024901805917422,\n",
       "   1.0307466326157253,\n",
       "   1.0372356237967808,\n",
       "   1.0362680021921793,\n",
       "   1.0312752864758175,\n",
       "   1.0399649796883266,\n",
       "   1.0314087555805842,\n",
       "   1.0293280255794526,\n",
       "   1.0449736712376276,\n",
       "   1.0624102093776067,\n",
       "   1.0249902719259263,\n",
       "   1.0473262023925782,\n",
       "   1.0387612468004226,\n",
       "   1.0145396763086318,\n",
       "   1.0296018226941426,\n",
       "   1.018415226340294,\n",
       "   1.0226824198166529,\n",
       "   1.058827022711436,\n",
       "   1.0244754751523335,\n",
       "   1.0288131082057952,\n",
       "   1.0183161811033885,\n",
       "   1.0478675693273545,\n",
       "   1.0301450574398041,\n",
       "   1.0292582178115846,\n",
       "   1.013197161157926,\n",
       "   1.0220163635412851,\n",
       "   1.0260990206400553,\n",
       "   1.0468194021781285,\n",
       "   1.0271870001157124,\n",
       "   1.0400924561421077,\n",
       "   1.030657256046931,\n",
       "   1.0323947280645371,\n",
       "   1.0341174207131067,\n",
       "   1.018318499326706,\n",
       "   1.0314710704485575,\n",
       "   1.025820834239324,\n",
       "   1.0573597554365795,\n",
       "   1.0236763622363407,\n",
       "   1.018751184741656,\n",
       "   1.0363039207458495,\n",
       "   1.0242360379298527],\n",
       "  'val_loss_std': [0.09436342238354416,\n",
       "   0.09689655437381231,\n",
       "   0.10968607539543987,\n",
       "   0.11671549056508639,\n",
       "   0.11931557047668946,\n",
       "   0.12109152262472538,\n",
       "   0.11916165989163716,\n",
       "   0.12335969855774871,\n",
       "   0.12030677487130058,\n",
       "   0.12026477669780782,\n",
       "   0.12633936411312716,\n",
       "   0.1319357127814,\n",
       "   0.13788343593736205,\n",
       "   0.13928335054853844,\n",
       "   0.1405716410059145,\n",
       "   0.13449692946158862,\n",
       "   0.13799928875694503,\n",
       "   0.14017074196321316,\n",
       "   0.1377304349351507,\n",
       "   0.14039416126619128,\n",
       "   0.14001894054189878,\n",
       "   0.14507960902905026,\n",
       "   0.14358713250266747,\n",
       "   0.1400337606058403,\n",
       "   0.1441873846378625,\n",
       "   0.1505758943625085,\n",
       "   0.1488970502231805,\n",
       "   0.15053363415272278,\n",
       "   0.1500638176004992,\n",
       "   0.14794726902126812,\n",
       "   0.15095952420926967,\n",
       "   0.14379657439927093,\n",
       "   0.14324017138742948,\n",
       "   0.14287170589891265,\n",
       "   0.1485758946247989,\n",
       "   0.14252846762079205,\n",
       "   0.14393637830424752,\n",
       "   0.15759022654077576,\n",
       "   0.14988908410763704,\n",
       "   0.14645966991541853,\n",
       "   0.1440249294626754,\n",
       "   0.15155962388162408,\n",
       "   0.1584157506368571,\n",
       "   0.15106243299882918,\n",
       "   0.1480295556714593,\n",
       "   0.14205619217069537,\n",
       "   0.14949213997052188,\n",
       "   0.14661365637435422,\n",
       "   0.1510624442168992,\n",
       "   0.14217433221781955,\n",
       "   0.15148596698094033,\n",
       "   0.14507975199827866,\n",
       "   0.14863526774571176,\n",
       "   0.14251733733014274,\n",
       "   0.15736385844018191,\n",
       "   0.1422945139933652,\n",
       "   0.14262840574230462,\n",
       "   0.14890066220976483,\n",
       "   0.1535461068978775,\n",
       "   0.15517500002058962,\n",
       "   0.14989481779522726,\n",
       "   0.1503794817115947,\n",
       "   0.1487851224726787,\n",
       "   0.14969316999330715,\n",
       "   0.1556449611001786,\n",
       "   0.14773858941265758,\n",
       "   0.15169464100041447,\n",
       "   0.15573234120736218,\n",
       "   0.153089567609245,\n",
       "   0.1483188405239798,\n",
       "   0.14787268310567853,\n",
       "   0.15089539857494477,\n",
       "   0.14877354300020165,\n",
       "   0.15091015857014625,\n",
       "   0.1490240952824955,\n",
       "   0.14573787060765594,\n",
       "   0.14486285581515776,\n",
       "   0.14462791230026062,\n",
       "   0.15148369932950645,\n",
       "   0.14145766370628512,\n",
       "   0.15523849810112766,\n",
       "   0.14593114504558125,\n",
       "   0.14732337193123476,\n",
       "   0.16057746910187903,\n",
       "   0.14442730802814027,\n",
       "   0.14999315804089838,\n",
       "   0.15137523671308808,\n",
       "   0.15062010544028884,\n",
       "   0.15200270624883483,\n",
       "   0.15359854234020587,\n",
       "   0.15609704916669723,\n",
       "   0.1467944420913411,\n",
       "   0.1523358504997996,\n",
       "   0.15109653782513668,\n",
       "   0.1564377697552661,\n",
       "   0.14809686688978765,\n",
       "   0.15251031658812803,\n",
       "   0.1454410248451062,\n",
       "   0.15208025816013515],\n",
       "  'val_accuracy_mean': [0.3982888896266619,\n",
       "   0.42240000029404956,\n",
       "   0.43835555454095204,\n",
       "   0.47542222201824186,\n",
       "   0.48235555479923886,\n",
       "   0.4891777777671814,\n",
       "   0.4981333320339521,\n",
       "   0.5041555553674698,\n",
       "   0.5179555544257164,\n",
       "   0.5208666655421257,\n",
       "   0.5221111106872559,\n",
       "   0.5150000006953875,\n",
       "   0.5253777788082759,\n",
       "   0.5326888862252236,\n",
       "   0.539533331990242,\n",
       "   0.5332888878385226,\n",
       "   0.5276222218076388,\n",
       "   0.5342444428801536,\n",
       "   0.535288888613383,\n",
       "   0.5501999987165133,\n",
       "   0.5484666660428047,\n",
       "   0.5514222227533658,\n",
       "   0.5634888882438341,\n",
       "   0.5586888884504636,\n",
       "   0.548844445546468,\n",
       "   0.5557333330313364,\n",
       "   0.5639111119508743,\n",
       "   0.5557999985416731,\n",
       "   0.5689777768651645,\n",
       "   0.5658222207427025,\n",
       "   0.5690888894597689,\n",
       "   0.5663999980688095,\n",
       "   0.5799333321054777,\n",
       "   0.571822223563989,\n",
       "   0.5745111081997554,\n",
       "   0.56868888626496,\n",
       "   0.573577777047952,\n",
       "   0.574244444668293,\n",
       "   0.5837333306670189,\n",
       "   0.5825555558999379,\n",
       "   0.5817333338658015,\n",
       "   0.587311111887296,\n",
       "   0.5787111100554466,\n",
       "   0.5820888864000638,\n",
       "   0.5859111100435257,\n",
       "   0.5890888902544975,\n",
       "   0.5951999987165133,\n",
       "   0.5819111092885335,\n",
       "   0.586422221561273,\n",
       "   0.5875555543104808,\n",
       "   0.591711110273997,\n",
       "   0.5961999997496605,\n",
       "   0.5925555542111397,\n",
       "   0.5953777785102526,\n",
       "   0.5896666659911474,\n",
       "   0.600199999610583,\n",
       "   0.593222221036752,\n",
       "   0.5990444424748421,\n",
       "   0.6006888896226883,\n",
       "   0.5966888867815335,\n",
       "   0.5944222218791644,\n",
       "   0.5913333311676979,\n",
       "   0.5969333322842916,\n",
       "   0.5926222228010496,\n",
       "   0.598644443054994,\n",
       "   0.5971555559833844,\n",
       "   0.5870000011722247,\n",
       "   0.587466666897138,\n",
       "   0.6001777787009875,\n",
       "   0.5920666662851969,\n",
       "   0.5919111106793086,\n",
       "   0.600933333436648,\n",
       "   0.5987555560469627,\n",
       "   0.6036222208539644,\n",
       "   0.5965777778625488,\n",
       "   0.5856666666269302,\n",
       "   0.5991999999682108,\n",
       "   0.5954666658242543,\n",
       "   0.601577778160572,\n",
       "   0.5881555538376172,\n",
       "   0.6015111099680265,\n",
       "   0.5951777770121892,\n",
       "   0.6029333326220513,\n",
       "   0.6020888886849085,\n",
       "   0.599311110774676,\n",
       "   0.5928888884186745,\n",
       "   0.5988222218553225,\n",
       "   0.5970444417993228,\n",
       "   0.5971777775883674,\n",
       "   0.5974666647116343,\n",
       "   0.598911108970642,\n",
       "   0.6007999980449676,\n",
       "   0.5970444443821907,\n",
       "   0.6003999989231428,\n",
       "   0.5904222212235133,\n",
       "   0.6017999980847041,\n",
       "   0.602111110985279,\n",
       "   0.5961777759591739,\n",
       "   0.6001777770121892],\n",
       "  'val_accuracy_std': [0.05690178200710071,\n",
       "   0.056820862395598984,\n",
       "   0.06176868006767115,\n",
       "   0.06501344029219483,\n",
       "   0.06141847574627391,\n",
       "   0.0614810245125792,\n",
       "   0.06204476223679052,\n",
       "   0.06808974061834586,\n",
       "   0.0647225252194037,\n",
       "   0.0660773176282639,\n",
       "   0.0665498052467343,\n",
       "   0.06658467186799416,\n",
       "   0.0684092119255373,\n",
       "   0.0668687994537689,\n",
       "   0.0697609985886111,\n",
       "   0.06536052215964243,\n",
       "   0.06458853873403915,\n",
       "   0.0678772261091167,\n",
       "   0.06829368630947164,\n",
       "   0.06470300092713815,\n",
       "   0.06531420265738179,\n",
       "   0.06721700170430431,\n",
       "   0.06625074336924056,\n",
       "   0.06789250477004599,\n",
       "   0.06639498698065095,\n",
       "   0.06674564234175286,\n",
       "   0.06443917311927762,\n",
       "   0.06904848392975686,\n",
       "   0.06811966252384669,\n",
       "   0.06589013026327954,\n",
       "   0.0647884263833644,\n",
       "   0.06430427602871198,\n",
       "   0.06557434940074176,\n",
       "   0.06731527465780372,\n",
       "   0.0658906346820877,\n",
       "   0.06481814482642682,\n",
       "   0.06464329061815176,\n",
       "   0.06763233481889101,\n",
       "   0.06764824015062593,\n",
       "   0.06182042771705637,\n",
       "   0.06420305728397396,\n",
       "   0.06672241838758929,\n",
       "   0.0663266735987276,\n",
       "   0.06585342550393854,\n",
       "   0.0659339144746261,\n",
       "   0.06393676960022592,\n",
       "   0.06721604657794729,\n",
       "   0.06320729682708494,\n",
       "   0.06907279632586674,\n",
       "   0.06278554591815853,\n",
       "   0.06453622250424912,\n",
       "   0.06365826565658164,\n",
       "   0.06466427883059744,\n",
       "   0.06219656727248662,\n",
       "   0.06719815806418611,\n",
       "   0.0628327938398282,\n",
       "   0.06454044668458218,\n",
       "   0.06515262506907141,\n",
       "   0.06242482394636958,\n",
       "   0.06522098934310487,\n",
       "   0.06733553019910442,\n",
       "   0.06731352861465419,\n",
       "   0.06446649405178079,\n",
       "   0.06382683444138104,\n",
       "   0.06454696566473857,\n",
       "   0.06728753930336805,\n",
       "   0.0679980935525747,\n",
       "   0.06603442471272912,\n",
       "   0.06355841617720845,\n",
       "   0.06101143896001191,\n",
       "   0.06426144315457188,\n",
       "   0.06448441674615878,\n",
       "   0.06344381735635843,\n",
       "   0.06537010058685072,\n",
       "   0.06587411932684557,\n",
       "   0.0646841325338309,\n",
       "   0.06373984147549301,\n",
       "   0.06517131645275728,\n",
       "   0.06486018836433881,\n",
       "   0.06517899833660395,\n",
       "   0.06552645798874387,\n",
       "   0.06306497170333031,\n",
       "   0.06611989186732851,\n",
       "   0.065318079393119,\n",
       "   0.06299417669250312,\n",
       "   0.06828987021332714,\n",
       "   0.06885694887829981,\n",
       "   0.06541722010049332,\n",
       "   0.06554301536631414,\n",
       "   0.06785025019131102,\n",
       "   0.06399652396996235,\n",
       "   0.06545530507511378,\n",
       "   0.0668242989195334,\n",
       "   0.06614341408032892,\n",
       "   0.06434144635880516,\n",
       "   0.06574375695522229,\n",
       "   0.06668101697629811,\n",
       "   0.06576882779799026,\n",
       "   0.0654531634228836],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.layer_dict.conv0.conv.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1415, -0.4236,  0.1998],\n",
      "          [-0.1713, -0.0461,  0.0164],\n",
      "          [-0.1761,  0.4311, -0.1085]],\n",
      "\n",
      "         [[ 0.2328, -0.3953,  0.2600],\n",
      "          [-0.0981,  0.0654,  0.1166],\n",
      "          [-0.2208,  0.2962, -0.1574]],\n",
      "\n",
      "         [[ 0.3255, -0.2839,  0.0395],\n",
      "          [ 0.0638,  0.1251, -0.1740],\n",
      "          [-0.2084,  0.2396, -0.2464]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1875,  0.5014,  0.2474],\n",
      "          [ 0.1544, -0.2668,  0.0860],\n",
      "          [-0.3616, -0.3614, -0.1655]],\n",
      "\n",
      "         [[-0.2363, -0.0171, -0.1561],\n",
      "          [ 0.1492, -0.1978, -0.0796],\n",
      "          [ 0.1933,  0.0403,  0.2115]],\n",
      "\n",
      "         [[-0.2765,  0.0396, -0.2101],\n",
      "          [ 0.0150,  0.1352,  0.1952],\n",
      "          [ 0.0525,  0.1417,  0.0232]]],\n",
      "\n",
      "\n",
      "        [[[-0.1492, -0.1725, -0.1051],\n",
      "          [-0.1926, -0.3951, -0.3913],\n",
      "          [-0.0894, -0.3333, -0.3379]],\n",
      "\n",
      "         [[-0.0514, -0.1003, -0.1233],\n",
      "          [ 0.1032, -0.2429, -0.3301],\n",
      "          [ 0.0405, -0.2802, -0.2640]],\n",
      "\n",
      "         [[ 0.2617, -0.1097, -0.0576],\n",
      "          [ 0.2572, -0.1426, -0.3215],\n",
      "          [ 0.1547, -0.0846, -0.1630]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.2174, -0.0688, -0.3065],\n",
      "          [ 0.4035, -0.1600, -0.4319],\n",
      "          [ 0.5228,  0.1330, -0.3054]],\n",
      "\n",
      "         [[-0.0751,  0.0596,  0.0545],\n",
      "          [-0.0889,  0.0189,  0.0429],\n",
      "          [-0.0173, -0.0141, -0.0623]],\n",
      "\n",
      "         [[-0.0601, -0.0021,  0.1741],\n",
      "          [-0.1412,  0.0579,  0.2906],\n",
      "          [-0.3163, -0.1346,  0.2244]]],\n",
      "\n",
      "\n",
      "        [[[-0.1237, -0.0430,  0.1307],\n",
      "          [ 0.0034,  0.0810, -0.1365],\n",
      "          [ 0.0910,  0.0330,  0.0978]],\n",
      "\n",
      "         [[ 0.6457,  0.3912,  0.0302],\n",
      "          [ 0.0725, -0.4890, -0.3415],\n",
      "          [ 0.0058, -0.4160, -0.2012]],\n",
      "\n",
      "         [[ 0.3093,  0.1060, -0.0702],\n",
      "          [ 0.1247, -0.3161, -0.2786],\n",
      "          [-0.0499, -0.0532,  0.2008]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1332,  0.1290,  0.0628],\n",
      "          [ 0.1058,  0.1795,  0.0401],\n",
      "          [ 0.0878,  0.0309,  0.0737]],\n",
      "\n",
      "         [[-0.2363, -0.4717, -0.4133],\n",
      "          [-0.4697, -0.6111, -0.5232],\n",
      "          [-0.2127, -0.2342, -0.0393]],\n",
      "\n",
      "         [[ 0.1869,  0.2756,  0.2614],\n",
      "          [ 0.0666,  0.0254,  0.1473],\n",
      "          [ 0.0872,  0.1207,  0.1052]]]], device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv0.conv.bias\n",
      "Parameter containing:\n",
      "tensor([-0.1079,  0.1533,  0.0298,  0.0424,  0.0120,  0.0172, -0.0352, -0.0849,\n",
      "        -0.0027, -0.0029, -0.1696, -0.0507, -0.0978, -0.0096, -0.0139, -0.0571,\n",
      "        -0.0277,  0.0133, -0.1832,  0.0207,  0.0475,  0.0382, -0.0093,  0.1206,\n",
      "        -0.0363,  0.0441, -0.0256,  0.0009, -0.0575, -0.1635,  0.0191,  0.0316,\n",
      "         0.1330, -0.0871, -0.1776, -0.0055, -0.0965, -0.0069, -0.0748, -0.1066,\n",
      "        -0.1182, -0.1133, -0.0178,  0.0383, -0.1249,  0.0433, -0.1961,  0.0108],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv0.norm_layer.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0492, -0.8958, -0.4051,  0.8105, -0.6262, -0.6682, -0.4872,  0.0857,\n",
      "        -0.6040, -0.4665, -0.7822,  0.0090,  0.1068, -0.2369, -0.1499,  0.4213,\n",
      "        -0.3831,  1.7442,  0.1110, -0.5386,  0.0054, -0.5725, -0.5747,  0.4129,\n",
      "         0.0176, -0.4716, -0.5823,  1.5346, -0.6748,  0.4765, -0.1141, -0.3090,\n",
      "         1.5082, -0.6039,  0.0158, -0.6858, -0.3406, -0.2699,  0.0542, -0.0115,\n",
      "        -0.1427,  0.2829, -0.6469, -0.1414,  0.3097, -0.1944, -0.8449, -0.4598],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv0.norm_layer.weight\n",
      "Parameter containing:\n",
      "tensor([0.9440, 1.0726, 0.7143, 1.1748, 0.5556, 0.7899, 0.9555, 1.5488, 0.7604,\n",
      "        0.6334, 0.9766, 0.6885, 0.7163, 0.9471, 1.4916, 0.9023, 0.6071, 0.7075,\n",
      "        1.0795, 0.7558, 0.8734, 0.7586, 0.9389, 0.8637, 0.6076, 0.7619, 0.7687,\n",
      "        0.8312, 0.8849, 0.8520, 1.2105, 0.5702, 1.1903, 0.8739, 0.8051, 0.7634,\n",
      "        0.9878, 0.9696, 1.0832, 1.0103, 1.3579, 1.2162, 0.8702, 1.6696, 0.8657,\n",
      "        1.0446, 1.0933, 0.7857], device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv1.conv.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 2.2371e-01,  2.4832e-01,  2.2291e-01],\n",
      "          [ 2.9426e-01,  3.2950e-01,  2.4456e-01],\n",
      "          [ 2.1469e-01,  3.5508e-01, -2.8439e-01]],\n",
      "\n",
      "         [[ 1.8908e-01, -2.4296e-01,  6.1377e-02],\n",
      "          [-1.8135e-01, -7.0336e-02, -6.9888e-02],\n",
      "          [-5.2125e-01,  3.9737e-02, -2.4104e-01]],\n",
      "\n",
      "         [[ 1.8964e-01,  1.6258e-01,  6.3449e-02],\n",
      "          [ 1.6314e-02, -3.9382e-02,  2.1019e-02],\n",
      "          [-5.7485e-02,  1.7632e-01,  7.7633e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.7026e-02, -1.1407e-02,  3.4443e-01],\n",
      "          [-4.0760e-01,  2.6972e-01,  3.7007e-01],\n",
      "          [-1.0451e-02,  2.7140e-01,  1.6235e-01]],\n",
      "\n",
      "         [[-4.9493e-02, -1.7939e-01,  4.9788e-02],\n",
      "          [-1.4360e-01, -3.7325e-03, -3.7699e-01],\n",
      "          [-2.0780e-01, -1.0592e-01, -5.5949e-01]],\n",
      "\n",
      "         [[-2.2768e-01, -2.9414e-01, -1.2014e-01],\n",
      "          [-2.8917e-01, -4.6860e-01, -2.5696e-01],\n",
      "          [-2.9989e-01, -3.3699e-01, -3.1916e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4891e-01, -1.2961e-01,  3.1222e-02],\n",
      "          [-2.4995e-01,  1.9436e-01, -3.9277e-01],\n",
      "          [ 1.8867e-02,  1.5840e-01,  1.6580e-01]],\n",
      "\n",
      "         [[-3.5734e-01, -1.1989e-01, -3.3167e-02],\n",
      "          [-5.3605e-02, -1.0488e-01, -5.3263e-02],\n",
      "          [ 1.4715e-01,  3.1856e-01,  3.2120e-01]],\n",
      "\n",
      "         [[-1.3000e-02, -7.2356e-02, -5.0258e-01],\n",
      "          [ 2.1462e-01,  1.4832e-01, -1.7191e-01],\n",
      "          [ 1.8312e-01,  3.7357e-01,  2.6056e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.6875e-01, -6.3180e-01,  3.0741e-02],\n",
      "          [-1.9742e-01, -1.4530e-01, -3.3071e-01],\n",
      "          [ 3.1513e-01,  4.8809e-01,  3.2683e-01]],\n",
      "\n",
      "         [[ 2.8206e-01, -2.6242e-01, -8.9526e-01],\n",
      "          [ 2.3911e-01, -3.5794e-01, -9.0906e-01],\n",
      "          [ 1.3953e-01,  2.8722e-01,  1.0540e-01]],\n",
      "\n",
      "         [[-8.0115e-02, -2.8758e-01, -4.6846e-01],\n",
      "          [ 6.0729e-02, -5.1874e-02, -2.2539e-01],\n",
      "          [ 2.0468e-01,  6.8486e-02,  4.5198e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4316e-01, -1.8622e-01, -8.8409e-02],\n",
      "          [-2.1033e-01,  1.6694e-01,  5.4731e-02],\n",
      "          [-2.3095e-01,  1.1636e-01, -1.6257e-01]],\n",
      "\n",
      "         [[-3.7680e-01, -1.7902e-01,  5.1298e-02],\n",
      "          [ 1.0737e-01,  1.6496e-01, -1.3361e-01],\n",
      "          [ 5.3158e-02, -6.3857e-02, -9.5982e-02]],\n",
      "\n",
      "         [[-7.9973e-01, -7.2402e-01, -6.2713e-01],\n",
      "          [-5.4906e-01, -5.4334e-01, -4.4202e-01],\n",
      "          [-4.6225e-01, -8.1638e-01, -5.8272e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5159e-01, -5.6963e-01, -3.1188e-01],\n",
      "          [-5.1183e-01, -5.0804e-01, -1.2936e-02],\n",
      "          [-3.4478e-02, -3.8518e-02, -7.2975e-03]],\n",
      "\n",
      "         [[ 2.3873e-01, -2.9560e-01, -2.3332e-02],\n",
      "          [ 1.7747e-01, -4.1003e-01, -1.5689e-01],\n",
      "          [ 2.1548e-02, -1.6752e-01,  2.5952e-01]],\n",
      "\n",
      "         [[ 2.0894e-01,  1.4160e-01,  1.0893e-01],\n",
      "          [ 6.8129e-02,  1.0290e-01,  2.1371e-01],\n",
      "          [-2.3420e-01, -1.5461e-01,  9.8721e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.7783e-01, -1.8476e-02,  2.3260e-02],\n",
      "          [-3.5045e-01, -3.4136e-01, -2.8794e-01],\n",
      "          [-1.0425e-02, -5.5994e-02, -3.6534e-02]],\n",
      "\n",
      "         [[ 1.3926e-01,  8.9128e-03, -1.1763e-01],\n",
      "          [ 2.6761e-01, -1.7982e-01, -4.0487e-01],\n",
      "          [-6.8539e-02, -4.6417e-01, -2.9071e-01]],\n",
      "\n",
      "         [[ 1.8388e-01,  1.0331e-01,  6.5166e-02],\n",
      "          [ 7.0851e-02,  1.5660e-01,  2.2236e-02],\n",
      "          [ 2.3607e-01,  1.8036e-01,  2.0287e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6276e-01, -4.1850e-01,  1.6148e-02],\n",
      "          [ 1.0800e-01, -1.0887e-01,  4.3845e-02],\n",
      "          [-6.6280e-02, -4.2268e-01,  1.7854e-01]],\n",
      "\n",
      "         [[ 1.2858e-01,  3.3416e-02,  1.1374e-01],\n",
      "          [ 7.7871e-02, -3.4656e-01, -1.7840e-02],\n",
      "          [-1.3453e-01, -3.2538e-01, -2.8439e-01]],\n",
      "\n",
      "         [[-7.7268e-02,  5.3884e-02,  8.0440e-02],\n",
      "          [-2.2366e-02, -7.9371e-02, -1.5611e-04],\n",
      "          [ 4.8693e-02, -2.7928e-02,  7.1094e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.4182e-02,  9.7602e-02, -1.8383e-01],\n",
      "          [ 4.6292e-02, -5.0426e-02,  1.5989e-01],\n",
      "          [-6.1726e-02,  1.0558e-01,  2.0821e-01]],\n",
      "\n",
      "         [[-1.4688e-01,  2.7918e-01,  1.6604e-02],\n",
      "          [-3.9471e-01, -1.1908e-01, -3.4683e-01],\n",
      "          [ 1.0190e-01,  1.5937e-01,  3.4325e-01]],\n",
      "\n",
      "         [[ 8.2493e-02, -1.8364e-01, -4.3994e-02],\n",
      "          [ 2.9920e-02,  1.6899e-01,  1.3069e-01],\n",
      "          [ 1.4345e-01,  3.3920e-01,  1.8741e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8595e-02, -3.7865e-02,  2.0759e-02],\n",
      "          [ 9.1452e-03,  3.2831e-02,  1.2359e-01],\n",
      "          [ 6.4195e-02,  7.9965e-02, -2.1055e-02]],\n",
      "\n",
      "         [[-4.4761e-01, -1.7904e-01,  4.3025e-01],\n",
      "          [-1.9264e-02, -1.5593e-01, -1.3127e-01],\n",
      "          [-2.1960e-03, -1.5292e-01, -6.5129e-01]],\n",
      "\n",
      "         [[ 2.9250e-02, -3.0086e-01, -1.9450e-01],\n",
      "          [-4.8975e-02, -1.9281e-01,  3.9139e-02],\n",
      "          [ 1.4408e-01,  1.7218e-01,  1.0990e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.5452e-03, -7.7688e-02, -3.1644e-01],\n",
      "          [ 2.5588e-01, -1.7493e-01, -1.2147e-01],\n",
      "          [ 3.6686e-02, -1.2620e-01, -2.3486e-01]],\n",
      "\n",
      "         [[-4.0436e-01, -1.9524e-01, -1.3163e-02],\n",
      "          [-2.3131e-03, -2.0292e-01,  5.7021e-02],\n",
      "          [ 2.4939e-01, -1.9504e-01, -1.5615e-01]],\n",
      "\n",
      "         [[-6.0391e-01, -6.0431e-01, -4.7786e-01],\n",
      "          [-6.3172e-01, -6.6100e-01, -5.1120e-01],\n",
      "          [-5.1513e-01, -3.2708e-01, -2.7110e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9243e-02, -7.2449e-02,  2.3878e-01],\n",
      "          [ 1.3123e-01, -3.7885e-02, -9.1095e-02],\n",
      "          [-6.7964e-02, -9.2570e-02, -2.5017e-01]],\n",
      "\n",
      "         [[ 2.2054e-01,  1.7599e-01,  1.3353e-01],\n",
      "          [ 2.2634e-01,  2.1161e-01,  9.3703e-02],\n",
      "          [-1.7341e-01, -7.0477e-02,  1.2029e-01]],\n",
      "\n",
      "         [[-2.2885e-01, -2.6899e-01, -3.1754e-01],\n",
      "          [-2.2839e-01, -2.9206e-01, -5.4042e-01],\n",
      "          [-7.2817e-02, -2.5679e-01, -2.8763e-01]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "classifier.layer_dict.conv1.conv.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0692,  0.0225,  0.0146,  0.0062, -0.0003, -0.0353, -0.0207, -0.0075,\n",
      "        -0.0361, -0.0140, -0.0116, -0.0363,  0.0138,  0.0071,  0.0018,  0.0140,\n",
      "        -0.0228,  0.0212,  0.0163,  0.0620,  0.0307,  0.0104, -0.0111,  0.0154,\n",
      "         0.0466,  0.0570,  0.0253,  0.0456, -0.0116, -0.0136,  0.0368,  0.0043,\n",
      "         0.0467, -0.0006, -0.0226,  0.0234,  0.0070,  0.0026, -0.0140,  0.0060,\n",
      "        -0.0246, -0.0086,  0.0180, -0.0355,  0.0091,  0.0087,  0.0217,  0.0231],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv1.norm_layer.bias\n",
      "Parameter containing:\n",
      "tensor([-0.5255, -0.4440, -0.7502, -0.4592, -0.5407, -0.3258, -0.4470, -0.2782,\n",
      "        -0.5453, -0.6598, -0.4410, -0.5283, -0.4360, -0.4407, -0.5334, -0.7102,\n",
      "        -0.6524, -0.4788, -0.4769, -0.9036, -0.4927, -0.3680, -0.5875, -0.5920,\n",
      "        -0.2959, -0.6984, -0.5142, -0.5197, -0.3080, -0.4096, -0.3908, -0.4162,\n",
      "        -0.3157, -0.4861, -0.7821, -0.5637, -0.5118, -0.5033, -0.4667, -0.3168,\n",
      "        -0.3553, -0.9253, -0.5934, -0.4293, -0.4843, -0.5167, -0.5467, -0.4649],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv1.norm_layer.weight\n",
      "Parameter containing:\n",
      "tensor([0.8997, 1.1266, 0.7953, 0.8191, 0.7902, 0.6564, 0.9023, 0.8947, 0.8237,\n",
      "        1.0059, 1.0619, 0.9432, 1.0210, 0.7675, 0.9553, 0.8466, 0.8415, 0.9622,\n",
      "        0.8966, 1.2893, 0.7947, 0.9116, 1.0173, 1.0942, 0.7765, 0.9688, 0.9011,\n",
      "        0.9799, 0.8172, 1.0734, 1.0630, 0.7877, 0.8588, 1.0285, 1.2855, 1.1365,\n",
      "        1.1018, 1.0648, 1.1588, 0.8368, 1.0537, 1.3252, 1.0389, 0.7222, 0.9511,\n",
      "        0.8036, 0.9329, 0.8107], device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv2.conv.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0395, -0.1449,  0.1605],\n",
      "          [ 0.0544,  0.4058,  0.1935],\n",
      "          [ 0.2905,  0.2596,  0.2494]],\n",
      "\n",
      "         [[ 0.0326,  0.1080,  0.3976],\n",
      "          [ 0.0244, -0.1985,  0.2211],\n",
      "          [-0.1545, -0.3506,  0.0830]],\n",
      "\n",
      "         [[-0.2255, -0.0501,  0.1122],\n",
      "          [ 0.0418, -0.2074,  0.2122],\n",
      "          [ 0.0199,  0.1283, -0.2099]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1298,  0.0286, -0.2199],\n",
      "          [ 0.0669, -0.1701, -0.3475],\n",
      "          [ 0.0481,  0.0074, -0.2604]],\n",
      "\n",
      "         [[ 0.3875,  0.0814, -0.0179],\n",
      "          [ 0.2203,  0.0760, -0.3791],\n",
      "          [ 0.0551, -0.0020, -0.0286]],\n",
      "\n",
      "         [[-0.2270, -0.2263, -0.4227],\n",
      "          [-0.3156, -0.2103, -0.1378],\n",
      "          [-0.1348,  0.1853, -0.0482]]],\n",
      "\n",
      "\n",
      "        [[[-0.0149, -0.2903, -0.2751],\n",
      "          [ 0.1026, -0.0308,  0.1329],\n",
      "          [ 0.0863, -0.1017,  0.2450]],\n",
      "\n",
      "         [[ 0.2057, -0.1479,  0.1485],\n",
      "          [-0.0948, -0.1160, -0.2755],\n",
      "          [ 0.1557,  0.0630,  0.0786]],\n",
      "\n",
      "         [[-0.0367, -0.1468, -0.1525],\n",
      "          [-0.1829,  0.2110, -0.1127],\n",
      "          [ 0.0079,  0.3520,  0.0888]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0217, -0.0177, -0.2181],\n",
      "          [-0.1610,  0.0261, -0.0094],\n",
      "          [-0.0056,  0.2374, -0.0435]],\n",
      "\n",
      "         [[ 0.2800, -0.1006,  0.0262],\n",
      "          [ 0.1396,  0.0736, -0.1275],\n",
      "          [ 0.0855, -0.1649, -0.1997]],\n",
      "\n",
      "         [[-0.1366, -0.3664, -0.2847],\n",
      "          [-0.0186,  0.0403,  0.1084],\n",
      "          [ 0.0477, -0.0829,  0.0453]]],\n",
      "\n",
      "\n",
      "        [[[-0.3657, -0.1533, -0.0611],\n",
      "          [-0.0521, -0.1040, -0.0143],\n",
      "          [-0.0221, -0.1611,  0.1034]],\n",
      "\n",
      "         [[-0.3145,  0.1614,  0.4258],\n",
      "          [-0.4726, -0.2714, -0.1111],\n",
      "          [-0.3096, -0.1618, -0.0023]],\n",
      "\n",
      "         [[-0.0757, -0.0498, -0.1108],\n",
      "          [ 0.3376,  0.1433,  0.1957],\n",
      "          [ 0.0878, -0.0588, -0.2911]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0206, -0.0938,  0.1063],\n",
      "          [ 0.0989,  0.0386,  0.1279],\n",
      "          [ 0.1610,  0.3243,  0.1295]],\n",
      "\n",
      "         [[-0.4959, -0.0519, -0.0039],\n",
      "          [ 0.1069,  0.0479, -0.5173],\n",
      "          [-0.1383, -0.2400, -0.2087]],\n",
      "\n",
      "         [[-0.1665, -0.0872, -0.1649],\n",
      "          [-0.1219,  0.0771, -0.0911],\n",
      "          [-0.2397, -0.1058, -0.1802]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1898,  0.2641,  0.0080],\n",
      "          [-0.1909,  0.4374,  0.1361],\n",
      "          [-0.1808,  0.3121,  0.1420]],\n",
      "\n",
      "         [[ 0.2982,  0.5320,  0.0684],\n",
      "          [ 0.0248,  0.2461,  0.1590],\n",
      "          [-0.0131,  0.3119,  0.1335]],\n",
      "\n",
      "         [[ 0.2229,  0.0330, -0.4745],\n",
      "          [ 0.0349,  0.0355,  0.2760],\n",
      "          [-0.0131,  0.0196, -0.1564]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2131, -0.1052, -0.2246],\n",
      "          [-0.1275,  0.0159, -0.4143],\n",
      "          [ 0.0019,  0.0447, -0.3801]],\n",
      "\n",
      "         [[-0.3385, -0.1984, -0.4100],\n",
      "          [ 0.2179, -0.3673,  0.1627],\n",
      "          [ 0.1760,  0.1028,  0.0999]],\n",
      "\n",
      "         [[ 0.4098,  0.1083, -0.1471],\n",
      "          [ 0.3239,  0.0373, -0.1915],\n",
      "          [ 0.0111,  0.1138, -0.0135]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1976,  0.0841,  0.4326],\n",
      "          [-0.0667, -0.1346, -0.3809],\n",
      "          [-0.0222, -0.1374, -0.3774]],\n",
      "\n",
      "         [[ 0.1116, -0.0423, -0.1678],\n",
      "          [ 0.0731,  0.0665, -0.2632],\n",
      "          [-0.1153, -0.3031, -0.5431]],\n",
      "\n",
      "         [[-0.1675, -0.2013,  0.1226],\n",
      "          [-0.1923, -0.1430, -0.0612],\n",
      "          [ 0.0705,  0.1593, -0.0981]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1262,  0.3089,  0.1381],\n",
      "          [ 0.0743,  0.0764,  0.1250],\n",
      "          [ 0.0546, -0.1493, -0.1395]],\n",
      "\n",
      "         [[ 0.2165,  0.0106, -0.1086],\n",
      "          [ 0.1022, -0.1329,  0.1673],\n",
      "          [ 0.0596,  0.0604,  0.1921]],\n",
      "\n",
      "         [[-0.0064,  0.0141, -0.0454],\n",
      "          [-0.1104,  0.2018,  0.0120],\n",
      "          [-0.1227,  0.0576, -0.0826]]],\n",
      "\n",
      "\n",
      "        [[[-0.1750, -0.2484, -0.3828],\n",
      "          [ 0.1789, -0.0133,  0.2406],\n",
      "          [ 0.2862, -0.0726,  0.0356]],\n",
      "\n",
      "         [[-0.0442,  0.2122,  0.7398],\n",
      "          [ 0.1078, -0.0694,  0.3660],\n",
      "          [ 0.1978,  0.1943,  0.0611]],\n",
      "\n",
      "         [[-0.0820, -0.4296, -0.1729],\n",
      "          [-0.1297, -0.3550, -0.1275],\n",
      "          [-0.2188, -0.0305,  0.1986]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0711, -0.1316,  0.1204],\n",
      "          [ 0.0265, -0.0455,  0.0436],\n",
      "          [ 0.2689, -0.0896,  0.1050]],\n",
      "\n",
      "         [[-0.0364,  0.1952, -0.0613],\n",
      "          [-0.1191,  0.1246, -0.1646],\n",
      "          [ 0.0689,  0.0155,  0.1674]],\n",
      "\n",
      "         [[-0.0530,  0.1517, -0.0738],\n",
      "          [-0.0648,  0.0303,  0.1503],\n",
      "          [ 0.3143,  0.2827,  0.3454]]]], device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv2.conv.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0221, -0.0169,  0.0499,  0.0771,  0.0070, -0.0751,  0.0656, -0.0207,\n",
      "        -0.0109,  0.0138, -0.0136,  0.0172,  0.0138, -0.0492, -0.0466,  0.0242,\n",
      "        -0.0100, -0.0020,  0.0193,  0.0007,  0.0796,  0.0216, -0.0530, -0.0039,\n",
      "        -0.0207,  0.0114,  0.0065,  0.0056,  0.0320, -0.0479,  0.0349,  0.0489,\n",
      "         0.0114,  0.0520,  0.0175, -0.0356,  0.0162,  0.0075, -0.0118,  0.0423,\n",
      "         0.0413,  0.0637, -0.0693,  0.0379,  0.0385,  0.0120, -0.0103, -0.0030],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv2.norm_layer.bias\n",
      "Parameter containing:\n",
      "tensor([-0.5705, -0.5475, -0.4630, -0.7548, -0.5557, -0.9379, -0.7739, -0.7426,\n",
      "        -0.4699, -0.8756, -0.7905, -0.8504, -0.9051, -0.5405, -0.8511, -0.2814,\n",
      "        -1.4971, -0.9032, -0.8464, -0.3955, -0.7784, -2.0626, -0.7094, -0.6113,\n",
      "        -1.0906, -0.5182, -0.6837, -1.0183, -0.4826, -0.6841, -0.4772, -0.5764,\n",
      "        -0.5228, -0.5118, -0.5365, -0.7824, -0.6109, -0.3926, -0.6651, -0.5065,\n",
      "        -0.6903, -0.6874, -0.3175, -0.5309, -1.0580, -0.5574, -0.6119, -0.6868],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv2.norm_layer.weight\n",
      "Parameter containing:\n",
      "tensor([0.8490, 0.8716, 0.8321, 0.9883, 0.8497, 0.8543, 0.8872, 0.7809, 0.7360,\n",
      "        0.9331, 0.7648, 0.9920, 0.9915, 0.7314, 1.1002, 0.6987, 0.8375, 0.8971,\n",
      "        0.9303, 0.7209, 0.8626, 0.8365, 0.8688, 0.8090, 1.0327, 0.7210, 0.7682,\n",
      "        1.1171, 0.7246, 0.7619, 0.7036, 0.8985, 0.8829, 0.7218, 0.8274, 0.8633,\n",
      "        0.8396, 0.8299, 0.9351, 0.8501, 0.7610, 1.0180, 0.6978, 0.7580, 0.9623,\n",
      "        0.7724, 0.8249, 0.7504], device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv3.conv.weight\n",
      "Parameter containing:\n",
      "tensor([[[[-2.5893e-02, -1.9872e-01,  1.7401e-01],\n",
      "          [-1.2388e-01,  7.6305e-02,  4.3584e-02],\n",
      "          [-1.5219e-01, -2.9584e-02,  1.3664e-01]],\n",
      "\n",
      "         [[ 1.0942e-01,  1.4078e-01,  5.9457e-02],\n",
      "          [ 3.1695e-02,  5.3186e-01,  1.7648e-01],\n",
      "          [-1.3128e-01, -1.5093e-01, -4.4432e-03]],\n",
      "\n",
      "         [[-6.8923e-02, -9.1175e-02, -1.8597e-01],\n",
      "          [ 1.6730e-01, -8.8805e-02, -1.0182e-01],\n",
      "          [-1.1819e-01,  9.1011e-02,  1.3153e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.7604e-02,  6.8509e-02,  2.3041e-01],\n",
      "          [ 9.2632e-02,  4.5426e-02,  9.9065e-02],\n",
      "          [ 1.8936e-01,  1.4728e-01,  1.6417e-01]],\n",
      "\n",
      "         [[-8.7920e-02, -1.3510e-01, -3.5542e-02],\n",
      "          [-4.1920e-02,  1.9008e-01, -1.9611e-01],\n",
      "          [ 1.5886e-01,  1.6061e-01,  5.6400e-02]],\n",
      "\n",
      "         [[-5.2234e-01, -1.0618e-01, -6.2289e-02],\n",
      "          [-3.8046e-01, -3.9147e-01, -2.6192e-01],\n",
      "          [-3.3372e-01, -3.5018e-01, -2.4115e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7839e-01,  1.1002e-01,  2.4277e-02],\n",
      "          [ 1.8788e-01,  6.8132e-02,  3.0452e-02],\n",
      "          [-1.0729e-01, -3.3932e-01, -1.7916e-01]],\n",
      "\n",
      "         [[-3.0612e-01, -2.2680e-01, -1.2279e-01],\n",
      "          [-5.6316e-02, -1.6619e-01, -1.7122e-01],\n",
      "          [-9.5438e-02, -1.3948e-01,  3.9039e-02]],\n",
      "\n",
      "         [[ 2.9026e-01,  1.5291e-01, -1.0455e-01],\n",
      "          [ 3.0438e-02, -3.7721e-03,  6.6752e-02],\n",
      "          [ 2.5083e-01,  1.4897e-01,  1.5322e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.8886e-01,  1.4256e-01,  1.8783e-01],\n",
      "          [ 1.0923e-01,  2.0619e-02,  6.2494e-02],\n",
      "          [ 1.4979e-01,  6.6209e-02,  6.9644e-02]],\n",
      "\n",
      "         [[-4.4614e-03,  9.1116e-02, -1.7864e-01],\n",
      "          [-7.7936e-02, -8.6367e-02, -2.3165e-01],\n",
      "          [-4.7769e-02, -8.3296e-02, -2.3039e-01]],\n",
      "\n",
      "         [[ 1.9761e-01,  2.4210e-01, -3.1403e-02],\n",
      "          [ 2.2760e-01,  7.3006e-02,  5.2480e-02],\n",
      "          [ 2.4318e-01,  1.6187e-01,  4.1467e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5213e-01,  1.7540e-01,  8.8847e-02],\n",
      "          [ 7.3308e-02, -4.3422e-02,  1.9891e-01],\n",
      "          [-1.5981e-01, -2.0798e-01, -5.5314e-02]],\n",
      "\n",
      "         [[-2.4366e-01, -3.4911e-01, -2.9937e-01],\n",
      "          [-1.8104e-01, -2.9740e-01, -1.8361e-01],\n",
      "          [-2.4908e-01, -4.1977e-01, -2.4674e-01]],\n",
      "\n",
      "         [[-2.5447e-01, -1.9331e-01, -1.2451e-01],\n",
      "          [-8.2556e-02, -5.9263e-02, -8.1090e-02],\n",
      "          [-1.8305e-01, -2.6282e-02, -1.6807e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7045e-02, -6.2196e-02, -2.0237e-01],\n",
      "          [ 7.5362e-02,  6.9880e-02,  5.2474e-02],\n",
      "          [-2.1995e-02,  8.7597e-02, -3.9357e-02]],\n",
      "\n",
      "         [[-3.0693e-01, -6.1595e-03, -3.4861e-01],\n",
      "          [-8.4696e-02, -1.3468e-02, -2.5379e-01],\n",
      "          [ 1.0580e-01,  4.4998e-02,  3.8945e-02]],\n",
      "\n",
      "         [[ 9.5354e-02,  1.7677e-01,  1.3106e-01],\n",
      "          [ 4.8504e-02,  3.2273e-02,  3.5599e-02],\n",
      "          [ 9.8448e-02,  1.4774e-02,  2.2673e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.7402e-02, -5.0538e-02,  1.0031e-01],\n",
      "          [ 3.1717e-02, -2.5765e-01,  4.3627e-02],\n",
      "          [-1.3087e-01,  1.3679e-01, -1.3877e-01]],\n",
      "\n",
      "         [[-1.0225e-01, -1.7620e-01, -2.7829e-02],\n",
      "          [ 4.1821e-02, -2.5510e-01, -1.4902e-01],\n",
      "          [ 2.9439e-01,  2.0130e-01,  2.7650e-02]],\n",
      "\n",
      "         [[ 8.1816e-02,  1.2647e-01,  1.1551e-01],\n",
      "          [-1.7144e-01,  1.5370e-01,  1.9529e-01],\n",
      "          [-3.2217e-01, -1.2119e-01, -3.2641e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.1373e-02,  2.2392e-01,  1.2397e-01],\n",
      "          [ 1.0325e-01,  2.4217e-01, -4.4031e-02],\n",
      "          [-1.0391e-01,  5.7919e-02,  7.3877e-03]],\n",
      "\n",
      "         [[ 1.8301e-01,  2.8668e-01, -8.9577e-02],\n",
      "          [ 9.7177e-02, -5.5842e-02,  1.3722e-02],\n",
      "          [ 1.3907e-01,  2.1665e-02, -2.0730e-02]],\n",
      "\n",
      "         [[-2.0773e-01, -2.5168e-01, -7.0841e-02],\n",
      "          [-1.7900e-01, -3.4501e-01, -1.0403e-02],\n",
      "          [-3.7830e-01, -4.5212e-02, -4.5194e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0880e-01,  5.2731e-02,  3.0243e-02],\n",
      "          [ 4.4676e-01,  2.5921e-01,  2.7609e-01],\n",
      "          [ 5.3789e-02,  7.7635e-02,  4.1659e-02]],\n",
      "\n",
      "         [[ 1.2929e-01,  1.5191e-01,  7.6129e-02],\n",
      "          [ 1.6673e-01,  3.1565e-01,  3.6239e-01],\n",
      "          [-1.6713e-01,  2.5938e-02, -1.5355e-01]],\n",
      "\n",
      "         [[-2.2635e-01, -2.4739e-01, -5.7770e-01],\n",
      "          [-1.2982e-01, -2.8080e-01, -3.9924e-01],\n",
      "          [-1.4881e-01,  7.3266e-05, -1.9699e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0577e-02, -8.8375e-02,  5.2550e-02],\n",
      "          [-1.7384e-01, -2.9489e-02, -3.8631e-02],\n",
      "          [ 6.4170e-03,  3.3754e-02,  1.0251e-01]],\n",
      "\n",
      "         [[-1.5468e-01,  2.2198e-01, -2.0018e-01],\n",
      "          [-2.3020e-01, -5.7735e-02, -5.7236e-02],\n",
      "          [-1.8337e-01,  1.0237e-01, -2.9356e-02]],\n",
      "\n",
      "         [[ 8.4268e-02, -6.6122e-02,  6.2562e-02],\n",
      "          [-2.0638e-02,  1.6101e-02,  1.3887e-01],\n",
      "          [-2.2086e-02, -7.2461e-02,  1.3505e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.9091e-01, -5.2906e-01, -4.2391e-01],\n",
      "          [-3.1801e-01, -3.3421e-01, -1.8258e-01],\n",
      "          [-4.4773e-01, -3.0447e-01, -1.7767e-01]],\n",
      "\n",
      "         [[-2.1980e-02, -6.3720e-02, -2.4317e-01],\n",
      "          [-5.8408e-02, -1.4782e-01, -1.7004e-01],\n",
      "          [-1.0184e-01, -1.0752e-01, -1.2917e-01]],\n",
      "\n",
      "         [[-2.3339e-01, -2.4494e-01, -1.2352e-01],\n",
      "          [-7.0202e-03, -2.0109e-02, -1.2122e-01],\n",
      "          [-3.4366e-03, -3.0595e-02, -5.3962e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5338e-01,  1.9042e-01,  1.3424e-01],\n",
      "          [-4.9031e-02,  8.8519e-02, -2.3286e-02],\n",
      "          [-2.5956e-01, -7.0516e-02, -2.0332e-01]],\n",
      "\n",
      "         [[-1.2031e-02, -7.5485e-02,  2.4874e-03],\n",
      "          [ 4.6480e-02, -1.3936e-01,  1.1231e-02],\n",
      "          [ 6.9002e-02,  9.3447e-02,  1.0166e-01]],\n",
      "\n",
      "         [[ 8.2753e-02,  3.8096e-02, -5.1775e-02],\n",
      "          [ 9.4423e-02,  1.2247e-01,  6.1907e-02],\n",
      "          [ 1.9450e-01,  5.0312e-02,  7.0485e-03]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "classifier.layer_dict.conv3.conv.bias\n",
      "Parameter containing:\n",
      "tensor([-3.4973e-02, -6.3321e-02,  3.7624e-02,  2.0549e-02,  9.3764e-03,\n",
      "         1.9099e-02, -4.1331e-02, -3.8106e-01,  2.7837e-02, -5.2256e-02,\n",
      "         3.6548e-02, -1.4445e-01,  1.2837e-02,  7.1669e-02,  9.9550e-02,\n",
      "        -2.6171e-02, -4.5096e-05, -6.5230e-02,  2.9843e-02, -8.5645e-02,\n",
      "         6.0826e-03,  1.7000e-03, -4.2373e-03, -4.7114e-02,  5.5219e-04,\n",
      "         1.1759e-02, -2.9474e-02, -7.4576e-02,  4.5647e-02,  3.8886e-03,\n",
      "         1.2397e-01,  4.4577e-02,  1.9942e-02,  1.1318e-03,  4.4901e-02,\n",
      "         2.6835e-02, -2.0051e-01,  2.2206e-02,  1.6423e-02,  1.4340e-02,\n",
      "        -3.0084e-01, -4.4376e-02, -1.2915e-01, -1.1483e-02, -1.6185e-01,\n",
      "         9.8336e-03, -7.9029e-03, -4.2992e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "classifier.layer_dict.conv3.norm_layer.bias\n",
      "Parameter containing:\n",
      "tensor([-1.0147, -0.9782, -0.6467, -1.5398, -0.6410, -1.4147, -0.9696, -1.9602,\n",
      "        -1.4608, -1.9787, -1.9355, -1.1464, -1.0105, -1.5715, -1.3460, -0.9903,\n",
      "        -1.3574, -1.0115, -0.9691, -1.6517, -0.8416, -1.2877, -1.0187, -2.0871,\n",
      "        -1.6473, -0.8950, -0.9149, -1.6465, -1.7982, -1.2020, -1.4334, -1.3372,\n",
      "        -1.4258, -0.7707, -1.7243, -0.9803, -1.2790, -0.9144, -1.5699, -1.3509,\n",
      "        -0.9126, -1.3738, -1.0743, -1.0572, -1.8387, -0.7729, -0.9240, -1.3372],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.conv3.norm_layer.weight\n",
      "Parameter containing:\n",
      "tensor([0.3955, 3.3604, 2.9153, 0.5645, 4.2218, 0.4013, 3.0752, 4.7397, 0.4329,\n",
      "        2.6791, 0.7139, 3.9940, 0.2250, 0.6909, 3.7555, 3.4898, 0.4976, 2.8304,\n",
      "        0.3728, 3.1240, 3.5686, 0.4233, 0.4568, 3.5470, 0.8014, 0.3533, 3.0185,\n",
      "        3.9224, 0.7691, 0.3819, 1.4561, 2.8606, 0.5002, 0.2373, 2.2364, 0.4097,\n",
      "        4.0688, 0.2358, 0.5814, 0.4921, 3.5743, 3.4437, 3.6762, 0.4048, 3.7582,\n",
      "        0.3098, 0.2211, 3.2147], device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.linear.weights\n",
      "Parameter containing:\n",
      "tensor([[-0.0457, -0.1510, -0.1908,  ..., -0.0305, -0.0184,  0.0053],\n",
      "        [-0.2940, -0.0436, -0.1644,  ..., -0.0230, -0.0080, -0.0080],\n",
      "        [-0.2617, -0.1989, -0.2172,  ..., -0.0280, -0.0130, -0.0105],\n",
      "        [-0.1487, -0.0473, -0.3341,  ..., -0.0439, -0.0092, -0.0077],\n",
      "        [ 0.5786,  0.4263,  0.6883,  ..., -0.0044,  0.0142, -0.0087]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "classifier.layer_dict.linear.bias\n",
      "Parameter containing:\n",
      "tensor([-0.2075,  0.3487, -0.2448, -0.2514,  0.5072], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for key, value in maml_system.model.named_parameters():\n",
    "    #print(key)\n",
    "    if value.requires_grad:\n",
    "        print(key)\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "170a7604",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40000\\2188585274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfigure_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_sample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mx_support_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_target_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_support_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_target_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MAML\\data.py\u001b[0m in \u001b[0;36mget_train_batches\u001b[1;34m(self, total_batches, augment_images)\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugment_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maugment_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_train_iters_produced\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_of_gpus\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_per_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msample_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_batched\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "figure_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        # Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "        num_steps=5\n",
    "        for num_step in range(num_steps):            \n",
    "            support_loss, support_preds = maml_system.model.net_forward(\n",
    "                    x=x_support_set_task,\n",
    "                    y=y_support_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    backup_running_statistics=num_step == 0,\n",
    "                    training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase='test',\n",
    "                    epoch=0,\n",
    "                    soft_target=None\n",
    "                )\n",
    "            \n",
    "            generated_alpha_params = {}\n",
    "                        \n",
    "            names_weights_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                              names_weights_copy=names_weights_copy,\n",
    "                                                              out_feature_dict=out_feature_dict,\n",
    "                                                              alpha=generated_alpha_params,\n",
    "                                                              use_second_order=args.second_order,\n",
    "                                                              current_step_idx=num_step,\n",
    "                                                              current_iter=maml_system.state['current_iter'],\n",
    "                                                              training_phase='test')\n",
    "            \n",
    "            \n",
    "        for name, param in maml_system.model.classifier.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if \"norm_layer\" not in name:\n",
    "                    param.data = names_weights_copy[name].squeeze().to(device=device)\n",
    "\n",
    "        ls = loss_landscape.landscape(maml_system.model.classifier, args)\n",
    "        \n",
    "        ls.show(x_support_set_task, y_support_set_task, title=str(figure_idx))\n",
    "        figure_idx = figure_idx + 1\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
