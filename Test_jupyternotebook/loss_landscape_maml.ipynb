{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"SWA\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f85286c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "arbiter.0.weight torch.Size([20, 20]) cuda:0 True\n",
      "arbiter.0.bias torch.Size([20]) cuda:0 True\n",
      "arbiter.2.weight torch.Size([10, 20]) cuda:0 True\n",
      "arbiter.2.bias torch.Size([10]) cuda:0 True\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6542444431781769,\n",
       " 'best_val_iter': 16000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 32,\n",
       " 'train_loss_mean': 0.4496441180706024,\n",
       " 'train_loss_std': 0.12430861797929893,\n",
       " 'train_accuracy_mean': 0.8344133331775665,\n",
       " 'train_accuracy_std': 0.05293675929356405,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.9212178971370061,\n",
       " 'val_loss_std': 0.1378287118752748,\n",
       " 'val_accuracy_mean': 0.6422444424033165,\n",
       " 'val_accuracy_std': 0.05979594587457209,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-9.2420e-03, -4.2841e-01,  3.0116e-01],\n",
       "                         [-1.7578e-01, -8.3019e-02,  8.1573e-02],\n",
       "                         [-9.1437e-02,  3.7958e-01, -2.7273e-03]],\n",
       "               \n",
       "                        [[ 2.2214e-01, -3.4631e-01,  2.9974e-01],\n",
       "                         [-9.1582e-02,  5.1024e-02,  9.6501e-02],\n",
       "                         [-2.5836e-01,  1.4183e-01, -1.4209e-01]],\n",
       "               \n",
       "                        [[ 3.1487e-01, -1.9859e-01, -1.0493e-01],\n",
       "                         [ 1.5040e-01,  2.1750e-01, -2.3930e-01],\n",
       "                         [-1.0072e-01,  2.1121e-01, -2.3503e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.8902e-02,  5.4940e-01,  7.9530e-03],\n",
       "                         [ 1.8810e-01, -1.4940e-01,  6.4084e-02],\n",
       "                         [-2.0655e-01, -3.8260e-01, -9.7125e-02]],\n",
       "               \n",
       "                        [[-2.5672e-01,  2.5995e-01, -2.0516e-01],\n",
       "                         [ 1.7166e-01, -2.2283e-01, -3.7939e-02],\n",
       "                         [ 2.4555e-01, -1.7969e-01,  2.6681e-01]],\n",
       "               \n",
       "                        [[-2.6352e-01,  2.4822e-01, -2.7551e-01],\n",
       "                         [ 1.7703e-02,  1.6849e-02,  1.9993e-01],\n",
       "                         [ 4.8506e-02, -1.0896e-01,  9.6475e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-7.0600e-04, -3.8632e-02,  3.0753e-01],\n",
       "                         [ 1.2012e-01, -3.8486e-02,  6.2404e-03],\n",
       "                         [ 7.6611e-02, -2.5255e-01, -4.4790e-01]],\n",
       "               \n",
       "                        [[-1.1252e-01, -9.4092e-02,  3.0871e-01],\n",
       "                         [ 3.7256e-01,  4.8387e-02,  1.0759e-01],\n",
       "                         [-1.8726e-01, -2.9239e-01, -1.1086e-01]],\n",
       "               \n",
       "                        [[ 6.4373e-02, -4.8488e-01,  4.1492e-02],\n",
       "                         [ 2.6376e-01, -1.6067e-01, -2.3686e-01],\n",
       "                         [-1.6881e-01, -9.5936e-02, -1.1421e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.2307e-01, -8.5983e-02, -9.3396e-02],\n",
       "                         [-2.8775e-01, -8.0900e-01, -4.5745e-01],\n",
       "                         [ 2.1824e-01, -1.2336e-01, -4.3534e-01]],\n",
       "               \n",
       "                        [[-1.9882e-01, -7.5247e-02,  9.7369e-02],\n",
       "                         [-2.3072e-01, -3.1824e-01, -6.6113e-02],\n",
       "                         [ 2.1019e-02, -2.2340e-01, -1.8445e-01]],\n",
       "               \n",
       "                        [[ 1.7712e-01,  5.1526e-02,  3.3039e-01],\n",
       "                         [-5.5094e-02, -1.7805e-01,  1.4264e-01],\n",
       "                         [-9.6980e-02, -3.0537e-01,  4.9012e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.5232e-02, -1.7595e-01, -2.7444e-01],\n",
       "                         [ 1.9038e-01,  4.4885e-01, -1.0017e-01],\n",
       "                         [ 5.8903e-02,  4.6424e-01,  4.5375e-01]],\n",
       "               \n",
       "                        [[ 7.7671e-02, -1.1619e-02, -3.1148e-01],\n",
       "                         [-1.5840e-01, -4.6056e-03, -1.0808e-01],\n",
       "                         [-2.8773e-01, -1.3547e-02, -2.9761e-02]],\n",
       "               \n",
       "                        [[ 2.1034e-01, -5.3589e-04, -3.4048e-01],\n",
       "                         [ 6.7773e-02,  1.5574e-01, -1.5827e-01],\n",
       "                         [-2.4198e-01,  3.3417e-01,  2.3253e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.1863e-03, -8.2525e-03, -8.0905e-02],\n",
       "                         [ 1.5910e-01,  2.3668e-01,  9.4835e-02],\n",
       "                         [-3.0047e-02,  2.8073e-02,  1.0984e-01]],\n",
       "               \n",
       "                        [[-3.7171e-01, -5.7232e-01,  1.5735e-02],\n",
       "                         [-3.7619e-01, -6.7938e-01, -1.7892e-01],\n",
       "                         [-6.5298e-03,  7.2885e-02,  3.8755e-01]],\n",
       "               \n",
       "                        [[ 2.3895e-01,  1.8046e-01,  3.0757e-01],\n",
       "                         [ 1.0552e-01, -1.4696e-01,  4.7012e-02],\n",
       "                         [ 3.2209e-02, -2.3370e-04, -4.3250e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.2507,  0.1374,  0.0695,  0.0283, -0.0307,  0.1600,  0.0303, -0.0098,\n",
       "                        0.0735, -0.2094,  0.0175, -0.0736,  0.1567,  0.0019,  0.1251, -0.0624,\n",
       "                       -0.3373,  0.0223,  0.1915, -0.0383, -0.2383,  0.0709, -0.0694,  0.1056,\n",
       "                        0.0442,  0.0433,  0.0092, -0.0286,  0.1730,  0.1829,  0.0188,  0.0606,\n",
       "                       -0.0855, -0.0420, -0.0013,  0.0216,  0.0127, -0.3873, -0.1163, -0.1251,\n",
       "                        0.1177,  0.4540, -0.0203, -0.1445, -0.0362, -0.0095, -0.0290, -0.0847],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.2829, -0.0338, -0.7348, -0.1689,  0.0022,  0.3701, -0.7843,  0.5674,\n",
       "                       -0.6233, -0.4914, -0.0046, -0.8196, -0.0761, -0.5953,  0.6437,  0.5376,\n",
       "                        0.3812, -0.4640,  0.0473, -0.4245, -0.2213, -0.0577, -0.7362, -0.0937,\n",
       "                        0.1132, -0.4043, -0.6306,  1.9772, -0.2074,  0.3804,  0.1493, -1.0453,\n",
       "                       -0.0466, -0.7914, -0.5520, -0.0699, -0.6277, -0.2622, -0.3442, -0.0447,\n",
       "                        1.3243, -0.0037, -0.0049,  0.0188, -0.6432, -0.3625, -0.4323, -0.4544],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.1480, 1.0570, 0.6975, 0.9909, 0.5764, 0.8663, 0.9620, 1.2472, 0.7542,\n",
       "                       0.9784, 0.9442, 0.8364, 0.8942, 0.6853, 1.0219, 0.6726, 0.6724, 0.7618,\n",
       "                       1.3292, 0.7181, 0.8642, 0.5643, 0.6341, 0.9987, 0.8297, 0.8218, 0.7115,\n",
       "                       0.8301, 0.7223, 0.6974, 1.1947, 0.8512, 1.3384, 0.8482, 0.8702, 1.0977,\n",
       "                       0.8214, 1.4203, 0.8242, 0.7655, 0.9957, 1.3179, 0.7956, 1.3415, 0.7822,\n",
       "                       0.7100, 0.6686, 0.9323], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 2.1677e-01, -1.4459e-01, -9.2818e-05],\n",
       "                         [ 2.1092e-01,  6.5329e-02,  3.5458e-01],\n",
       "                         [-1.8996e-01,  3.3358e-01, -3.8276e-01]],\n",
       "               \n",
       "                        [[-1.6349e-01, -2.2558e-01,  3.3899e-02],\n",
       "                         [-3.0898e-01,  2.7362e-01,  2.8541e-02],\n",
       "                         [ 8.0974e-02,  8.9332e-02, -4.8683e-01]],\n",
       "               \n",
       "                        [[-4.6083e-01,  1.6621e-01,  3.5297e-01],\n",
       "                         [ 2.0278e-01,  2.2274e-01,  2.9703e-01],\n",
       "                         [ 7.4974e-02,  7.6536e-02, -3.3798e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.4031e-01, -1.6943e-01,  1.1019e-01],\n",
       "                         [-1.9915e-01,  1.0008e-01,  1.4754e-01],\n",
       "                         [ 2.5032e-01,  1.4321e-01, -1.9474e-01]],\n",
       "               \n",
       "                        [[ 1.5385e-01,  1.7343e-01, -2.4566e-01],\n",
       "                         [ 4.2820e-02, -1.4889e-01, -3.1402e-01],\n",
       "                         [-1.1345e-01, -1.4720e-01,  5.2237e-02]],\n",
       "               \n",
       "                        [[ 6.9268e-02, -2.0699e-01, -2.0551e-01],\n",
       "                         [-2.3838e-01, -1.3198e-01,  1.8776e-01],\n",
       "                         [-1.5885e-01,  7.1989e-02,  4.9657e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.4132e-01, -1.7715e-01, -2.2230e-01],\n",
       "                         [ 3.8240e-01,  2.1546e-01, -2.7562e-01],\n",
       "                         [ 5.7365e-02,  3.0601e-01,  6.4926e-02]],\n",
       "               \n",
       "                        [[ 1.1830e-01, -3.8392e-01,  1.8111e-03],\n",
       "                         [ 2.0859e-01, -3.2235e-01, -3.1126e-01],\n",
       "                         [-6.2049e-02,  4.7597e-02, -2.1432e-01]],\n",
       "               \n",
       "                        [[ 2.3831e-03, -5.8735e-02,  2.8086e-01],\n",
       "                         [-6.1121e-02,  4.7561e-01,  3.6996e-01],\n",
       "                         [ 3.7668e-01,  4.5231e-01,  3.9194e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.2943e-02, -4.6555e-01,  4.1514e-02],\n",
       "                         [-3.1925e-02, -3.3106e-01, -2.8237e-01],\n",
       "                         [-1.4268e-03, -1.8111e-01, -1.9724e-01]],\n",
       "               \n",
       "                        [[ 1.7832e-01, -4.1938e-02, -5.8581e-02],\n",
       "                         [ 4.8968e-01, -6.7898e-03, -2.0483e-01],\n",
       "                         [ 3.1400e-01,  4.5429e-01, -1.6762e-01]],\n",
       "               \n",
       "                        [[ 3.1722e-01,  4.1150e-02, -1.7313e-01],\n",
       "                         [ 2.2682e-01, -1.4492e-01, -6.1364e-01],\n",
       "                         [ 1.6961e-01,  5.6562e-02, -1.5405e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.0585e-01,  3.6552e-01, -2.6263e-01],\n",
       "                         [ 1.5414e-01,  5.1289e-01,  1.3036e-01],\n",
       "                         [ 1.3017e-01,  6.3586e-02,  3.7991e-01]],\n",
       "               \n",
       "                        [[ 5.0689e-02,  3.4917e-01,  2.3254e-01],\n",
       "                         [ 1.1185e-01,  3.5573e-01,  3.6269e-01],\n",
       "                         [ 1.9367e-01,  2.6071e-01,  1.7051e-01]],\n",
       "               \n",
       "                        [[-4.9240e-01, -1.3882e-01, -1.3019e-01],\n",
       "                         [-3.0796e-02, -2.2895e-01, -1.5431e-01],\n",
       "                         [-1.6306e-01, -7.3120e-01, -2.3248e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.1854e-01, -2.4350e-01,  1.8162e-01],\n",
       "                         [-2.9123e-01, -4.2484e-01,  2.4024e-01],\n",
       "                         [-3.0722e-01, -4.9224e-01, -7.2411e-04]],\n",
       "               \n",
       "                        [[-4.1266e-01, -4.5840e-01,  4.9604e-02],\n",
       "                         [-2.0925e-01, -3.9451e-01, -2.3259e-01],\n",
       "                         [-4.9139e-02, -3.2523e-01, -1.4385e-01]],\n",
       "               \n",
       "                        [[ 9.1693e-02, -3.3221e-01,  1.7044e-01],\n",
       "                         [-1.7955e-01, -1.7821e-01,  5.1324e-02],\n",
       "                         [-2.5713e-01,  6.5665e-03,  1.6393e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.2099e-01, -1.2562e-01,  4.0438e-01],\n",
       "                         [ 1.0820e-01, -1.3490e-01, -1.7520e-02],\n",
       "                         [-1.3801e-01, -1.9744e-02, -5.3576e-03]],\n",
       "               \n",
       "                        [[ 3.5093e-01,  1.0029e-01, -1.3084e-01],\n",
       "                         [ 3.6878e-02, -9.7811e-02,  2.6315e-02],\n",
       "                         [-2.1996e-01, -1.3221e-01,  3.3208e-02]],\n",
       "               \n",
       "                        [[-2.4909e-01, -2.6592e-01, -2.0645e-01],\n",
       "                         [-1.0762e-01, -6.6946e-02,  8.7811e-03],\n",
       "                         [-6.0665e-01, -3.9572e-01,  2.6259e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.5713e-01,  2.0598e-02, -2.7367e-01],\n",
       "                         [ 6.5564e-01,  7.0835e-01,  3.2843e-01],\n",
       "                         [ 5.3082e-01,  4.2170e-01,  4.4941e-01]],\n",
       "               \n",
       "                        [[-2.3635e-01,  1.3916e-01, -6.9457e-02],\n",
       "                         [-2.0772e-01, -2.0619e-01,  1.1433e-01],\n",
       "                         [-9.7579e-02,  1.3461e-01, -1.9435e-01]],\n",
       "               \n",
       "                        [[ 1.1243e-01,  7.3248e-02, -5.8005e-01],\n",
       "                         [ 4.1625e-01,  1.5802e-01, -1.5220e-01],\n",
       "                         [ 1.0954e-01,  2.1287e-01,  6.4479e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.8514e-01, -6.7767e-01, -6.2687e-03],\n",
       "                         [ 7.9792e-02, -1.5496e-01,  7.1891e-02],\n",
       "                         [ 4.1729e-01,  8.8093e-02,  1.1415e-02]],\n",
       "               \n",
       "                        [[ 1.0472e-01,  3.1409e-01, -8.2013e-02],\n",
       "                         [-1.7413e-01,  4.8220e-02,  2.3044e-01],\n",
       "                         [-2.2661e-01, -3.1395e-01,  3.5782e-03]],\n",
       "               \n",
       "                        [[ 1.0326e-02,  9.5574e-02,  2.7251e-01],\n",
       "                         [ 1.0675e-01,  6.2986e-01,  6.7351e-02],\n",
       "                         [ 2.0625e-01, -1.3773e-01, -2.4176e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0384e-01,  1.9216e-01,  4.9292e-01],\n",
       "                         [-6.2377e-02, -4.8770e-02,  2.5647e-01],\n",
       "                         [ 8.8091e-03, -2.2785e-01, -2.9479e-01]],\n",
       "               \n",
       "                        [[-1.9465e-01,  2.0875e-01,  1.3799e-01],\n",
       "                         [-7.9501e-02, -1.2001e-01,  8.5799e-02],\n",
       "                         [ 4.1575e-02, -1.1280e-01, -1.3597e-01]],\n",
       "               \n",
       "                        [[-8.6483e-02,  2.1124e-01, -4.1575e-02],\n",
       "                         [ 4.4607e-02, -3.8374e-01, -8.8831e-02],\n",
       "                         [-6.3992e-03, -3.0904e-01,  1.4238e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-9.5150e-02, -1.2026e-01, -4.7868e-01],\n",
       "                         [ 1.1366e-01,  3.2316e-01, -4.8886e-02],\n",
       "                         [-2.2111e-01,  4.1699e-01, -1.3402e-01]],\n",
       "               \n",
       "                        [[-4.6084e-02, -9.5220e-02,  1.3270e-01],\n",
       "                         [ 7.3283e-02,  4.6227e-01,  6.0677e-02],\n",
       "                         [ 8.0982e-02,  5.3135e-02, -3.8011e-01]],\n",
       "               \n",
       "                        [[-3.9134e-02, -4.4792e-02, -1.6132e-01],\n",
       "                         [-1.9085e-02,  2.2989e-01,  2.8438e-01],\n",
       "                         [-1.2687e-01, -1.6844e-01, -1.1649e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.2950e-02,  1.6138e-01, -1.3951e-01],\n",
       "                         [ 1.9325e-01,  2.2958e-01,  1.3838e-04],\n",
       "                         [ 1.5654e-01,  1.9797e-01,  2.3748e-02]],\n",
       "               \n",
       "                        [[-4.3690e-02,  2.0704e-01,  1.5514e-01],\n",
       "                         [-1.4521e-02,  1.1969e-01,  1.7547e-01],\n",
       "                         [-1.4499e-01, -1.1951e-01, -8.5195e-02]],\n",
       "               \n",
       "                        [[ 1.1795e-01,  2.9087e-01, -9.9679e-03],\n",
       "                         [ 1.4717e-03,  3.8095e-02,  1.6303e-02],\n",
       "                         [-1.9939e-01, -2.4597e-02,  5.6245e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-0.0329,  0.0076, -0.0186, -0.0108, -0.0339, -0.0197, -0.0016, -0.0328,\n",
       "                       -0.0221,  0.0538,  0.1613, -0.0712,  0.0406, -0.0192,  0.0004,  0.0020,\n",
       "                       -0.0230,  0.0113, -0.0054,  0.0304, -0.0420, -0.0046, -0.0610,  0.0347,\n",
       "                       -0.0150,  0.0948, -0.0050,  0.0200, -0.0487, -0.0260,  0.0248,  0.0243,\n",
       "                        0.0470, -0.0028, -0.0245,  0.0253,  0.0163,  0.0624, -0.0293, -0.0285,\n",
       "                        0.0053,  0.0136,  0.0167,  0.0048, -0.0406,  0.0204,  0.0439, -0.0006],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.4304, -0.4345, -0.5333, -0.5142, -0.5146, -0.4574, -0.2607, -0.6476,\n",
       "                       -0.5226, -0.7339, -0.3557, -0.6526, -0.3827, -0.5060, -0.6920, -0.5347,\n",
       "                       -0.5618, -0.5161, -0.3714, -0.5171, -0.8430, -0.3703, -0.4904, -0.5754,\n",
       "                       -0.4580, -0.5587, -0.2553, -0.3038, -0.5594, -0.5349, -0.5377, -0.2308,\n",
       "                       -0.3876, -0.6925, -0.6291, -0.4366, -0.4763, -0.2601, -0.2796, -0.3697,\n",
       "                       -0.4186, -0.6934, -0.5950, -0.3696, -0.5702, -0.4626, -0.4917, -0.3401],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9212, 0.8697, 0.8523, 1.0124, 1.0174, 0.9678, 1.1767, 0.7103, 0.8055,\n",
       "                       0.8389, 1.1517, 1.0097, 0.6234, 0.8290, 1.1433, 1.0160, 1.0019, 1.1706,\n",
       "                       0.8001, 1.0345, 1.1402, 0.8585, 0.9387, 0.9455, 0.8976, 0.9676, 1.0437,\n",
       "                       0.7782, 1.1580, 0.9720, 1.0089, 0.8770, 0.7915, 1.0672, 1.0063, 0.9985,\n",
       "                       1.3480, 0.9780, 0.9090, 0.6284, 0.7256, 1.1761, 1.0711, 0.7730, 1.2416,\n",
       "                       0.6414, 0.6992, 1.0323], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[ 2.4549e-01,  2.5847e-01,  1.0454e-01],\n",
       "                         [ 7.7286e-02,  3.3138e-01,  3.2097e-01],\n",
       "                         [ 4.8473e-01,  3.3912e-01,  7.7782e-02]],\n",
       "               \n",
       "                        [[-2.0340e-01,  2.1817e-01,  7.4440e-02],\n",
       "                         [ 9.8932e-02,  8.8508e-02,  1.4834e-01],\n",
       "                         [ 2.4463e-01, -2.8272e-02, -3.2642e-01]],\n",
       "               \n",
       "                        [[-1.4590e-03, -4.1734e-01, -3.7396e-02],\n",
       "                         [-2.9451e-01, -1.6112e-01, -6.6461e-02],\n",
       "                         [-3.1281e-01,  1.0927e-02, -1.9482e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0687e-01, -1.4027e-01, -9.8042e-02],\n",
       "                         [ 1.0702e-01, -4.5900e-01, -2.3066e-01],\n",
       "                         [-2.6252e-01, -2.0689e-01, -1.4543e-01]],\n",
       "               \n",
       "                        [[-5.5574e-02,  1.9709e-01, -4.1687e-01],\n",
       "                         [-6.4237e-02, -5.1425e-02, -1.8742e-01],\n",
       "                         [-6.8694e-02, -6.1672e-02, -2.0805e-01]],\n",
       "               \n",
       "                        [[-2.2694e-02, -1.5993e-01,  6.5287e-02],\n",
       "                         [-1.6701e-01, -4.1218e-01, -1.7733e-01],\n",
       "                         [-7.9389e-02,  3.0221e-01,  2.6804e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.1293e-02,  8.5440e-02,  6.2510e-02],\n",
       "                         [-5.2402e-01,  4.0840e-01,  3.4895e-01],\n",
       "                         [-1.8414e-01,  4.1813e-02, -1.8922e-02]],\n",
       "               \n",
       "                        [[-1.8183e-01,  2.8712e-01,  3.6843e-02],\n",
       "                         [-1.4976e-01,  3.9988e-02,  3.1769e-02],\n",
       "                         [-2.2052e-01, -1.3178e-01, -8.2666e-03]],\n",
       "               \n",
       "                        [[-1.0987e-02,  1.1520e-01,  6.5045e-02],\n",
       "                         [-2.8627e-01,  2.8054e-01,  1.9490e-01],\n",
       "                         [-3.3616e-01, -1.7016e-01,  1.0980e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7812e-01,  2.2633e-01, -1.5931e-01],\n",
       "                         [ 1.6444e-01, -1.2592e-01,  1.0437e-01],\n",
       "                         [-7.8552e-02, -1.3336e-01, -2.2701e-01]],\n",
       "               \n",
       "                        [[-4.9333e-02, -2.6297e-01, -2.3004e-01],\n",
       "                         [ 6.8446e-02,  1.2761e-01, -1.6410e-01],\n",
       "                         [ 1.6785e-01,  5.0741e-02, -1.6915e-01]],\n",
       "               \n",
       "                        [[-3.7354e-02,  1.4744e-01, -2.8789e-01],\n",
       "                         [-2.9584e-01,  7.3733e-02,  2.9048e-01],\n",
       "                         [-2.5140e-01,  9.0306e-02,  1.0614e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.9974e-02,  4.4222e-02,  1.1808e-01],\n",
       "                         [-4.7956e-02,  2.7788e-01, -2.1017e-01],\n",
       "                         [ 1.3672e-01,  2.9520e-01,  7.1339e-02]],\n",
       "               \n",
       "                        [[ 2.7446e-01,  8.9280e-02, -2.7824e-01],\n",
       "                         [ 2.0821e-01, -1.7941e-01, -4.1485e-01],\n",
       "                         [-4.8224e-01, -3.6463e-01, -1.9661e-01]],\n",
       "               \n",
       "                        [[ 2.7581e-01, -7.6286e-02, -2.8250e-01],\n",
       "                         [ 1.3885e-02, -7.9687e-03,  1.3981e-01],\n",
       "                         [ 2.5039e-01, -1.2190e-01,  2.3697e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.3907e-01, -8.1180e-02,  3.2926e-01],\n",
       "                         [-1.7035e-03,  8.2414e-02,  2.0983e-01],\n",
       "                         [-3.1340e-02, -1.2375e-01,  1.3808e-01]],\n",
       "               \n",
       "                        [[-2.2722e-01, -6.5160e-02,  1.3883e-01],\n",
       "                         [-8.3940e-02,  1.3297e-04,  3.5131e-02],\n",
       "                         [-1.7716e-02, -1.7864e-01, -3.5251e-01]],\n",
       "               \n",
       "                        [[ 1.6763e-01,  4.8312e-02,  9.2901e-02],\n",
       "                         [-2.9436e-02, -6.5738e-02, -7.9424e-02],\n",
       "                         [-4.0691e-02, -1.9156e-01,  1.3348e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.5346e-01, -1.2185e-01, -3.5528e-02],\n",
       "                         [ 1.1681e-01, -3.4166e-01,  3.0948e-01],\n",
       "                         [ 3.7600e-01, -1.4377e-01,  8.0332e-02]],\n",
       "               \n",
       "                        [[ 1.9662e-01, -2.8229e-01, -9.5155e-02],\n",
       "                         [ 2.1560e-01, -4.9343e-02,  2.4265e-01],\n",
       "                         [-6.3908e-02,  9.7519e-02, -2.6488e-01]],\n",
       "               \n",
       "                        [[ 1.9296e-01,  4.1335e-01,  1.8412e-02],\n",
       "                         [ 1.5878e-01, -1.0945e-01,  2.0183e-02],\n",
       "                         [ 3.6425e-02,  1.3105e-01,  4.5901e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 5.0332e-02,  1.3929e-01,  1.1186e-02],\n",
       "                         [-1.6881e-01,  2.0558e-01,  3.0479e-01],\n",
       "                         [-1.9465e-01, -1.5111e-01, -1.7442e-01]],\n",
       "               \n",
       "                        [[ 1.7245e-01, -6.3128e-02,  1.0966e-01],\n",
       "                         [ 1.4785e-01, -1.1358e-02,  4.4072e-02],\n",
       "                         [-2.3047e-01, -1.9480e-01, -3.6843e-01]],\n",
       "               \n",
       "                        [[ 6.5583e-02,  4.3287e-02,  9.5875e-02],\n",
       "                         [ 1.5306e-02, -1.9196e-01,  9.7573e-02],\n",
       "                         [ 3.3543e-01,  1.3394e-01,  5.5534e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.8403e-01, -6.7915e-02, -8.3255e-02],\n",
       "                         [ 2.8147e-01, -2.2715e-01,  1.4711e-01],\n",
       "                         [-4.9215e-01,  3.3187e-02, -1.0947e-01]],\n",
       "               \n",
       "                        [[-4.1156e-01, -4.6810e-01, -1.4943e-01],\n",
       "                         [-2.4137e-01, -8.1182e-02,  7.9947e-03],\n",
       "                         [ 1.2040e-01,  9.5524e-03,  1.7108e-01]],\n",
       "               \n",
       "                        [[ 1.1197e-01,  1.6591e-01,  3.9222e-01],\n",
       "                         [-1.0676e-01, -7.7532e-02,  3.5692e-02],\n",
       "                         [ 1.0571e-01, -1.7920e-01, -4.9146e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0443e-01,  1.6609e-01,  1.7897e-01],\n",
       "                         [-4.4188e-01, -2.1222e-01,  3.4465e-01],\n",
       "                         [-1.2874e-01,  1.3213e-01,  4.2678e-01]],\n",
       "               \n",
       "                        [[-1.8101e-01, -3.1850e-01, -1.4375e-01],\n",
       "                         [-1.7774e-01, -2.4946e-01, -4.4879e-01],\n",
       "                         [-3.3877e-01, -3.1757e-01, -1.2390e-01]],\n",
       "               \n",
       "                        [[ 1.2054e-01,  2.3060e-02,  3.2912e-01],\n",
       "                         [-1.6222e-01, -2.1900e-02,  4.5365e-02],\n",
       "                         [-2.6235e-01,  3.4127e-02, -1.1586e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4132e-01,  1.0333e-01,  1.6047e-01],\n",
       "                         [-4.6080e-02,  6.5285e-02, -2.7473e-02],\n",
       "                         [-7.9620e-02, -1.6466e-01, -4.5081e-01]],\n",
       "               \n",
       "                        [[-1.2032e-01, -1.4584e-01,  2.9436e-02],\n",
       "                         [ 2.4179e-01,  3.5906e-02, -1.7551e-01],\n",
       "                         [ 3.5838e-01,  2.0579e-01, -2.2640e-01]],\n",
       "               \n",
       "                        [[ 2.8517e-01,  1.4383e-01,  1.6230e-01],\n",
       "                         [ 5.4796e-03, -1.5349e-01, -9.0931e-02],\n",
       "                         [ 1.3547e-02, -1.6966e-01, -3.7585e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.0525e-01,  1.3899e-01, -2.2681e-01],\n",
       "                         [ 2.7978e-01, -1.2026e-01, -3.0919e-01],\n",
       "                         [ 1.7165e-01, -3.6785e-01, -1.6713e-01]],\n",
       "               \n",
       "                        [[-1.8233e-01, -2.3156e-01,  1.5796e-02],\n",
       "                         [ 1.0380e-01, -2.1537e-01, -2.3803e-02],\n",
       "                         [-1.6771e-01, -3.0854e-01, -3.4553e-01]],\n",
       "               \n",
       "                        [[ 2.7610e-01, -4.7905e-02,  1.8626e-01],\n",
       "                         [ 2.1234e-01,  1.4442e-01,  2.5251e-01],\n",
       "                         [ 3.8522e-02,  3.0156e-01,  2.4454e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-0.0877,  0.0023, -0.0376, -0.0640, -0.0558, -0.0492, -0.0712,  0.0384,\n",
       "                        0.0128, -0.0420, -0.0930, -0.0566, -0.0319, -0.1364,  0.0349,  0.0320,\n",
       "                       -0.0986,  0.2366, -0.0639, -0.0719, -0.1615,  0.0126, -0.0164,  0.2751,\n",
       "                       -0.0989,  0.1213, -0.0041, -0.1039, -0.0417, -0.0902, -0.0538, -0.0886,\n",
       "                        0.0083, -0.0346,  0.1271, -0.0450, -0.1059,  0.0044, -0.1248, -0.0260,\n",
       "                       -0.0834, -0.0914, -0.0490, -0.0264, -0.0739,  0.0183, -0.1078, -0.1302],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.5949, -0.7788, -0.7880, -0.7466, -0.6135, -1.0065, -0.5644, -0.7884,\n",
       "                       -0.6787, -0.5656, -0.7406, -0.5386, -0.8404, -0.8894, -0.4902, -0.5821,\n",
       "                       -0.7360, -0.8093, -0.5860, -0.8698, -0.5723, -0.6899, -0.7785, -0.8635,\n",
       "                       -0.7334, -0.7971, -0.6033, -1.0569, -0.9189, -1.1420, -0.9178, -0.6445,\n",
       "                       -0.5795, -0.4786, -0.3683, -0.6311, -1.5928, -0.4780, -0.6913, -0.4625,\n",
       "                       -0.6796, -0.7789, -0.7732, -0.8903, -0.7531, -0.6598, -0.6171, -0.7321],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.7071, 0.8695, 0.7773, 0.8650, 0.8455, 0.9980, 0.6671, 0.8089, 0.9074,\n",
       "                       0.8966, 0.8511, 0.7180, 0.9046, 0.9215, 0.7185, 0.6047, 0.8742, 1.2465,\n",
       "                       0.6919, 0.8145, 0.6539, 0.7987, 0.6534, 1.0370, 0.8965, 0.8471, 0.8757,\n",
       "                       1.2521, 0.9317, 1.1705, 0.9870, 0.9418, 0.7088, 0.7082, 0.6715, 0.8272,\n",
       "                       1.2326, 0.5916, 0.9352, 0.7657, 0.6323, 0.8456, 1.0063, 0.9364, 0.6543,\n",
       "                       0.8201, 0.7544, 0.8710], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-2.0459e-01, -8.4365e-02, -1.0680e-01],\n",
       "                         [-2.4442e-01, -5.0273e-02, -7.8724e-02],\n",
       "                         [-9.6904e-02, -1.3160e-01, -3.2295e-02]],\n",
       "               \n",
       "                        [[-2.2729e-01, -2.7357e-01, -3.1115e-01],\n",
       "                         [-1.1110e-01,  2.6167e-02, -2.3969e-01],\n",
       "                         [-3.0304e-01, -2.1493e-01, -8.8380e-02]],\n",
       "               \n",
       "                        [[-1.6748e-01, -1.0030e-01, -2.4359e-01],\n",
       "                         [-1.5814e-01, -2.0185e-01, -9.2237e-02],\n",
       "                         [-1.9136e-01, -1.2386e-01, -4.0967e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.6445e-02, -4.1455e-02,  2.2409e-01],\n",
       "                         [ 2.2754e-01,  3.9414e-01,  2.7529e-01],\n",
       "                         [-4.0295e-02,  2.0674e-02,  6.1777e-02]],\n",
       "               \n",
       "                        [[ 1.1443e-01, -3.6887e-02,  1.0964e-01],\n",
       "                         [ 4.6583e-02,  6.8231e-02, -2.7080e-03],\n",
       "                         [ 1.0169e-01,  3.5468e-02, -1.5149e-01]],\n",
       "               \n",
       "                        [[-1.2192e-01,  1.9149e-01,  9.8602e-02],\n",
       "                         [ 6.8092e-02,  2.6787e-01,  1.9773e-01],\n",
       "                         [ 1.3956e-01,  2.7847e-01,  2.5147e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.1813e-02,  7.9722e-02, -4.6994e-03],\n",
       "                         [-2.1382e-02,  9.2675e-03,  2.7499e-02],\n",
       "                         [ 1.5760e-02,  8.7869e-03,  5.6520e-02]],\n",
       "               \n",
       "                        [[-9.9726e-03, -2.6515e-02, -3.0455e-02],\n",
       "                         [ 4.5747e-02,  8.0527e-02,  9.8227e-02],\n",
       "                         [ 6.4579e-02,  1.1608e-01,  1.7990e-01]],\n",
       "               \n",
       "                        [[ 1.3058e-01,  7.1330e-02, -7.5656e-02],\n",
       "                         [ 7.9242e-02,  3.2647e-02,  8.4565e-02],\n",
       "                         [ 1.1707e-01, -8.4182e-04,  1.7320e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.1825e-02, -3.9066e-03, -7.3329e-02],\n",
       "                         [-5.4164e-02, -2.7161e-02, -1.2240e-01],\n",
       "                         [ 6.6366e-02,  6.7999e-02, -2.1571e-02]],\n",
       "               \n",
       "                        [[ 7.6220e-02,  7.6708e-02,  3.7360e-02],\n",
       "                         [ 7.4579e-03,  2.6419e-02,  3.0145e-02],\n",
       "                         [-4.9123e-02, -1.2312e-01, -2.8876e-02]],\n",
       "               \n",
       "                        [[-1.0599e-01, -2.1741e-01, -1.4791e-02],\n",
       "                         [-1.6360e-01, -1.3339e-01, -6.0747e-02],\n",
       "                         [-5.3684e-02, -1.0150e-01,  9.3847e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.2344e-01,  7.5209e-02, -3.2955e-01],\n",
       "                         [-2.4891e-01, -4.1140e-01, -3.4098e-01],\n",
       "                         [ 1.9807e-01, -1.0983e-01, -1.4272e-01]],\n",
       "               \n",
       "                        [[-1.0951e-01,  5.4531e-02, -2.4592e-01],\n",
       "                         [-2.6688e-02,  4.5938e-02, -3.0180e-01],\n",
       "                         [ 2.1480e-01,  2.3019e-01,  2.1506e-01]],\n",
       "               \n",
       "                        [[-4.4562e-01, -3.7466e-01, -2.1436e-01],\n",
       "                         [-2.0995e-01, -1.7431e-01, -4.7698e-01],\n",
       "                         [-2.5191e-02,  1.6022e-02, -6.4040e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3041e-01,  6.7363e-02, -1.2642e-01],\n",
       "                         [ 5.1967e-01,  1.3314e-01,  2.0036e-01],\n",
       "                         [ 4.0107e-03,  6.3314e-02, -4.8053e-03]],\n",
       "               \n",
       "                        [[ 2.7523e-01,  3.9019e-01, -1.0661e-01],\n",
       "                         [ 4.1568e-01,  2.4950e-01,  1.8807e-01],\n",
       "                         [-6.0319e-01, -2.8670e-01, -2.0880e-01]],\n",
       "               \n",
       "                        [[-1.4294e-02, -4.0188e-01,  3.9233e-01],\n",
       "                         [ 3.5388e-02,  2.4834e-02,  3.0181e-01],\n",
       "                         [-1.5131e-03,  1.7115e-01, -1.5098e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.2414e-01, -4.7446e-02, -1.2911e-01],\n",
       "                         [ 1.2241e-01,  1.3555e-01,  2.1511e-01],\n",
       "                         [ 3.4014e-01,  2.4265e-01,  3.3169e-01]],\n",
       "               \n",
       "                        [[-4.0369e-02,  1.5118e-01,  8.1829e-02],\n",
       "                         [-5.3496e-02,  2.5203e-02, -1.4704e-01],\n",
       "                         [-3.0604e-01, -1.1004e-01, -1.2336e-01]],\n",
       "               \n",
       "                        [[-6.4162e-02, -1.8585e-02, -2.1969e-01],\n",
       "                         [-1.8833e-01, -9.2162e-02, -2.8038e-01],\n",
       "                         [-3.3636e-02, -1.5890e-01, -9.7827e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.6249e-02, -4.9382e-01, -1.5533e-01],\n",
       "                         [-1.3802e-01, -2.0947e-02,  3.1026e-02],\n",
       "                         [-7.3374e-02, -2.0758e-01, -2.3607e-01]],\n",
       "               \n",
       "                        [[-7.8566e-02, -6.7067e-02,  6.4306e-02],\n",
       "                         [-6.0312e-02, -2.9894e-01,  5.9674e-02],\n",
       "                         [-1.3209e-01, -4.7107e-01, -9.8594e-02]],\n",
       "               \n",
       "                        [[-3.6677e-02,  7.1678e-02,  7.5430e-02],\n",
       "                         [-5.8716e-02, -1.5807e-01,  1.7849e-01],\n",
       "                         [-2.7926e-01, -2.0088e-01,  7.8701e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.1853e-01, -6.0608e-02,  3.9033e-01],\n",
       "                         [ 1.6900e-01,  1.4960e-01, -2.0439e-02],\n",
       "                         [-6.9034e-02,  1.4096e-01,  3.0732e-01]],\n",
       "               \n",
       "                        [[-2.2637e-01, -3.3940e-01, -1.0124e-01],\n",
       "                         [ 2.6204e-04, -1.0727e-01,  1.5555e-02],\n",
       "                         [ 1.0444e-01, -3.0837e-01,  7.3571e-04]],\n",
       "               \n",
       "                        [[ 1.0017e-01, -2.9949e-01, -8.3789e-02],\n",
       "                         [-1.3365e-02,  1.4483e-02, -1.5821e-01],\n",
       "                         [-1.2575e-01, -2.7508e-01, -3.0131e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.3556e-01,  1.8656e-01,  4.4582e-01],\n",
       "                         [ 3.5240e-01,  1.4942e-01,  5.4494e-01],\n",
       "                         [ 5.9869e-01,  2.4622e-01,  1.8589e-01]],\n",
       "               \n",
       "                        [[-3.5133e-01, -1.0746e-01, -1.6267e-01],\n",
       "                         [-1.0638e-01, -2.8611e-01,  1.2333e-01],\n",
       "                         [-2.6930e-01, -1.6662e-01,  7.8572e-02]],\n",
       "               \n",
       "                        [[-2.2341e-01, -3.5009e-01, -3.2329e-01],\n",
       "                         [-3.8637e-01, -3.3331e-01, -3.3028e-03],\n",
       "                         [-2.6943e-01,  6.2298e-02,  2.3679e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.6665e-02,  6.4546e-02,  1.2089e-02],\n",
       "                         [ 9.6224e-03, -6.2301e-02,  1.6918e-02],\n",
       "                         [ 3.4178e-02,  1.5356e-02,  5.9921e-02]],\n",
       "               \n",
       "                        [[ 5.7247e-02,  5.3222e-03,  5.9031e-02],\n",
       "                         [ 1.7299e-02,  1.1680e-01,  1.2865e-01],\n",
       "                         [ 4.9161e-02,  7.1454e-02,  6.7179e-02]],\n",
       "               \n",
       "                        [[ 7.7367e-02,  3.4430e-02, -6.0610e-02],\n",
       "                         [ 4.6197e-02, -6.6605e-03,  2.7399e-02],\n",
       "                         [ 1.1000e-01, -6.0356e-03, -2.6387e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.9626e-02, -5.6386e-02, -4.8871e-03],\n",
       "                         [ 2.3261e-02, -1.0355e-01, -3.5961e-02],\n",
       "                         [ 2.2009e-02,  6.8089e-02, -4.3653e-02]],\n",
       "               \n",
       "                        [[ 8.1906e-02,  7.1204e-02,  2.1098e-02],\n",
       "                         [ 9.6310e-02,  1.2330e-03,  4.0216e-02],\n",
       "                         [-1.5326e-02,  2.5112e-02,  2.4048e-02]],\n",
       "               \n",
       "                        [[-1.4966e-01, -8.3312e-02,  3.3522e-02],\n",
       "                         [-1.1627e-01, -2.3901e-01, -3.5298e-02],\n",
       "                         [-1.1663e-01, -1.3862e-02,  7.0965e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-7.7400e-02, -1.6487e+00, -1.5310e-01, -2.5010e-01, -1.1542e+00,\n",
       "                       -2.0747e-01, -3.1312e-01, -2.5296e-01, -6.6167e-02, -1.0743e-01,\n",
       "                       -1.6641e+00, -1.0022e-01, -3.2794e-01, -4.5677e-01, -1.9883e-01,\n",
       "                       -1.7019e-01, -5.7371e-01, -3.9004e-01, -7.8508e-03, -3.1320e-01,\n",
       "                       -6.0012e-01, -2.4004e-01, -3.9910e-03, -1.4160e-01, -3.3410e-01,\n",
       "                       -4.1296e-01, -2.4801e-01, -3.2825e-01, -4.1676e-01, -2.6253e-01,\n",
       "                       -1.6345e-02, -1.7900e-02,  1.3540e-03, -5.4109e-02,  1.3195e-03,\n",
       "                       -6.7602e-02,  3.4870e-03, -7.5968e-03, -1.2228e-01, -4.2025e-01,\n",
       "                       -3.9132e-01, -2.6037e-01, -2.8956e-01, -3.4559e-01, -2.3766e-01,\n",
       "                       -1.5480e-01, -2.7266e-01, -1.1641e+00], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.8522, -0.3410, -1.4733, -0.7980, -0.2966, -0.5938, -0.7963, -0.4635,\n",
       "                       -0.8682, -0.8026, -0.2516, -0.1343, -0.4746, -0.3484, -0.9831, -0.6640,\n",
       "                       -1.3425, -1.4309, -0.9185, -0.5757, -1.0268, -0.5835, -0.8744, -0.4689,\n",
       "                       -0.7214, -0.5428, -0.7791, -0.7104, -0.6917, -0.3308, -0.8832, -0.8812,\n",
       "                       -0.9696, -0.8198, -0.7112, -0.9641, -0.8182, -0.7399, -0.7880, -0.8582,\n",
       "                       -0.3953, -0.2718, -0.8091, -0.3548, -0.4451, -0.9478, -0.7033, -0.2702],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.3098, 0.3202, 0.7028, 0.5855, 0.2599, 0.6091, 0.5385, 0.7165, 0.2697,\n",
       "                       0.3252, 0.2638, 0.4211, 0.7873, 0.2843, 0.4916, 0.5110, 0.9202, 0.8916,\n",
       "                       0.2327, 0.5921, 0.8864, 0.6685, 0.1929, 0.5606, 0.4550, 0.5141, 0.5519,\n",
       "                       0.6143, 0.8405, 0.5100, 0.1843, 0.2259, 0.2590, 0.3242, 0.1427, 0.3078,\n",
       "                       0.2185, 0.1363, 0.6754, 0.8303, 0.6430, 0.4632, 0.5864, 0.5430, 0.7119,\n",
       "                       0.7958, 0.8594, 0.2817], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[ 0.0775,  0.0360, -0.0757,  ..., -0.1118, -0.1221, -0.0472],\n",
       "                       [-0.1407,  0.1495,  0.2562,  ..., -0.1349, -0.0663, -0.1425],\n",
       "                       [-0.2792, -0.1754, -0.2051,  ..., -0.1163, -0.0203, -0.0997],\n",
       "                       [ 0.3169,  0.1465, -0.0330,  ..., -0.1304, -0.0937, -0.1754],\n",
       "                       [ 0.0515,  0.1285,  0.2233,  ...,  0.3056,  0.2616,  0.2186]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([ 0.0935, -0.3501,  0.2727,  0.0914, -0.2260], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('arbiter.0.weight',\n",
       "               tensor([[-5.9029e-01, -5.3795e-01, -1.5661e-01, -2.5986e-01, -3.1954e-01,\n",
       "                        -4.9768e-01, -3.2275e-01, -1.2587e-01, -4.6661e-01, -4.2619e-01,\n",
       "                         3.9449e-01, -2.9062e-01,  3.1589e-01, -3.6946e-01,  8.7661e-01,\n",
       "                        -3.3710e-01,  4.6817e-01, -2.3055e-01,  1.6675e-01, -2.6510e+00],\n",
       "                       [-8.7019e-01, -1.0105e+00, -1.1579e+00, -9.3530e-01, -8.2814e-01,\n",
       "                        -9.3626e-01, -7.4222e-01, -8.1796e-01, -1.1467e+00, -9.3128e-01,\n",
       "                         1.1218e+00, -9.5300e-01,  1.1893e+00, -8.4276e-01,  1.6632e+00,\n",
       "                        -1.0992e+00,  1.1598e+00, -7.6434e-01,  1.4629e-01, -5.6038e+00],\n",
       "                       [-1.7650e-01, -7.4263e-02, -1.9603e-01,  1.7395e-01, -1.2017e-01,\n",
       "                         1.5592e-01, -2.8765e-03,  7.1014e-02,  7.4614e-02,  1.7035e-01,\n",
       "                        -1.8075e-01, -3.1389e-03, -6.3424e-02,  4.2344e-02, -8.9193e-02,\n",
       "                         1.8377e-01, -5.3476e-02,  1.4562e-01, -1.4323e-01, -1.9829e-01],\n",
       "                       [-5.7256e-02,  3.0856e-02, -5.4239e-02, -8.6291e-02,  1.0822e-01,\n",
       "                        -6.2852e-02,  1.8812e-01,  1.9100e-01, -1.6266e-01,  6.1604e-02,\n",
       "                        -1.5123e-01, -2.5679e-02,  1.4868e-01, -1.9586e-01, -1.3195e-02,\n",
       "                         1.2474e-01, -8.8378e-02, -1.6145e-01, -2.0578e-01,  1.1818e-01],\n",
       "                       [-7.3623e-01, -6.0644e-01, -3.1303e-01, -4.3669e-01, -6.6999e-01,\n",
       "                        -4.7027e-01, -3.3417e-01, -5.6876e-01, -3.0895e-01, -6.1169e-01,\n",
       "                         1.0986e+00, -5.8277e-01,  1.2235e+00, -5.4190e-01,  1.3720e+00,\n",
       "                        -3.1581e-01,  2.7115e-01, -5.5397e-01, -8.2026e-01, -6.4324e+00],\n",
       "                       [-6.1735e-01, -3.8340e-01, -5.5814e-01, -4.9269e-01, -4.0294e-01,\n",
       "                        -3.3116e-01, -4.0873e-01, -5.3923e-01, -5.8814e-01, -3.2241e-01,\n",
       "                         1.2334e+00, -6.4057e-01,  1.1687e+00, -5.7191e-01,  1.2466e+00,\n",
       "                        -2.8693e-01,  1.4586e-01, -6.1125e-01, -1.1817e+00, -7.0028e+00],\n",
       "                       [ 1.0714e-02,  1.8095e-01, -1.1328e-02, -4.4111e-02, -1.8175e-01,\n",
       "                        -1.7345e-01, -9.5249e-02, -1.9606e-01, -1.2823e-01,  4.4785e-02,\n",
       "                        -1.6487e-01,  1.9681e-01,  1.6048e-01, -6.9412e-02, -1.7510e-02,\n",
       "                         2.1979e-01,  1.7670e-02, -5.7673e-03, -1.7022e-01, -2.3977e-02],\n",
       "                       [-5.2119e-02,  5.8366e-02, -9.5376e-02,  4.1151e-02, -1.4242e-02,\n",
       "                        -1.3989e-01, -2.9618e-02, -4.6825e-02,  1.5251e-02, -5.1655e-03,\n",
       "                        -1.6485e-01,  1.5387e-01,  1.4536e-01,  3.0741e-02, -1.5371e-01,\n",
       "                         2.2002e-02, -5.1757e-02, -5.1059e-02, -1.4909e-02,  2.1917e-01],\n",
       "                       [ 1.1688e-01,  2.1151e-01,  1.4778e-01,  1.1768e-01,  2.2312e-01,\n",
       "                        -8.8966e-02,  1.5089e-01, -1.1936e-01,  1.0614e-01,  7.8510e-02,\n",
       "                        -7.6256e-03, -1.3070e-01,  1.3243e-01,  1.8272e-01, -2.0383e-01,\n",
       "                        -9.6117e-02, -1.3217e-01,  1.5964e-01, -1.7313e-01,  1.2106e-01],\n",
       "                       [ 8.7365e-02,  9.4045e-02, -5.1892e-02,  1.6762e-01,  1.1671e-01,\n",
       "                         7.5187e-02, -2.6933e-02, -8.9412e-03,  7.3725e-02,  6.3618e-03,\n",
       "                         6.2247e-03, -2.5323e-02, -2.1953e-01, -1.3890e-01, -4.5655e-02,\n",
       "                        -1.6598e-01, -1.5479e-01, -1.0123e-02, -4.2731e-02,  1.1125e-01],\n",
       "                       [-4.1115e-01, -1.6570e-01, -3.3532e-01, -4.6333e-01, -2.4726e-01,\n",
       "                        -2.9219e-01, -8.4968e-02, -2.7568e-01, -1.3568e-01, -1.6816e-01,\n",
       "                         1.0562e+00, -1.1782e-01,  9.7169e-01, -5.2290e-02,  1.2544e+00,\n",
       "                        -1.7507e-01, -2.2543e-01, -1.0114e-01, -1.5591e+00, -7.6597e+00],\n",
       "                       [-1.1983e-01,  1.5614e-01, -1.2172e-01,  1.8576e-01,  7.3353e-02,\n",
       "                        -1.6203e-02,  1.3759e-01, -2.0994e-01,  2.1366e-01, -1.8662e-01,\n",
       "                        -1.1727e-01, -1.1385e-01, -3.9996e-03,  1.4979e-01, -1.4717e-01,\n",
       "                         6.0627e-02,  1.8551e-02, -5.7218e-02, -5.6653e-02,  2.0733e-01],\n",
       "                       [-8.6713e-01, -7.1265e-01, -9.3696e-01, -7.8758e-01, -8.2849e-01,\n",
       "                        -6.5304e-01, -9.0559e-01, -7.5495e-01, -6.0328e-01, -9.6782e-01,\n",
       "                         7.5313e-01, -9.2936e-01,  8.5076e-01, -6.4514e-01,  1.2284e+00,\n",
       "                        -9.5162e-01,  1.1903e+00, -7.8464e-01, -3.1248e-02, -5.3264e+00],\n",
       "                       [ 9.6642e-02,  3.9319e-02, -6.7998e-02, -9.8098e-03,  1.5438e-01,\n",
       "                         5.7538e-02, -1.2209e-01,  2.5769e-01, -1.5013e-01,  1.6490e-02,\n",
       "                        -1.7372e-01,  1.1015e-01,  8.8218e-02, -1.3506e-01, -2.6332e-01,\n",
       "                         1.4997e-02,  1.3280e-02,  1.9106e-02,  6.1276e-02, -1.5640e-01],\n",
       "                       [ 2.1353e-01,  2.0006e-01, -1.8942e-01,  7.9496e-02,  4.1478e-02,\n",
       "                        -1.1179e-01,  5.3119e-02,  1.8549e-01, -3.3323e-03, -1.2598e-01,\n",
       "                         2.5711e-02, -2.9604e-03, -2.0496e-02,  1.7962e-01, -1.9416e-02,\n",
       "                        -4.8471e-02, -7.4401e-02,  2.2016e-01, -8.8840e-03, -1.8109e-01],\n",
       "                       [-8.5593e-01, -8.0253e-01, -5.3770e-01, -7.1620e-01, -5.4736e-01,\n",
       "                        -4.2729e-01, -7.3555e-01, -4.9435e-01, -7.6810e-01, -8.0610e-01,\n",
       "                         1.1936e+00, -5.7552e-01,  1.1435e+00, -7.9414e-01,  1.3673e+00,\n",
       "                        -4.4523e-01,  5.3758e-01, -8.0454e-01, -9.2493e-01, -6.7744e+00],\n",
       "                       [-2.7364e-01, -4.3885e-01, -4.4961e-01, -4.8198e-01, -3.5917e-01,\n",
       "                        -5.8706e-01, -5.4683e-01, -6.3895e-01, -2.5407e-01, -3.0597e-01,\n",
       "                         1.1502e+00, -5.0423e-01,  1.1282e+00, -6.2438e-01,  1.3824e+00,\n",
       "                        -4.8713e-01, -2.2746e-02, -4.4713e-01, -1.0334e+00, -6.2128e+00],\n",
       "                       [-5.9038e-01, -5.0077e-01, -7.8565e-01, -8.5617e-01, -6.1241e-01,\n",
       "                        -5.4356e-01, -8.6500e-01, -8.2575e-01, -4.9568e-01, -8.4503e-01,\n",
       "                         1.0357e+00, -6.6297e-01,  1.2667e+00, -5.3725e-01,  1.6418e+00,\n",
       "                        -7.3191e-01,  5.3606e-01, -8.7648e-01, -4.6736e-01, -6.0921e+00],\n",
       "                       [-2.2760e-01, -3.4731e-01, -4.2479e-01, -3.2570e-01, -1.5208e-01,\n",
       "                        -4.1844e-01, -2.9004e-01, -3.8252e-01, -2.1515e-01, -4.0105e-01,\n",
       "                         8.9929e-01, -1.0297e-01,  1.1806e+00, -1.9034e-01,  1.1976e+00,\n",
       "                        -1.5566e-01, -8.1065e-02, -4.1297e-01, -1.0365e+00, -6.1294e+00],\n",
       "                       [ 1.2256e-01, -1.2452e-01,  2.0354e-01,  2.0327e-01, -1.5081e-01,\n",
       "                        -1.3786e-01,  1.1737e-01, -1.7185e-01, -1.0510e-01,  1.4711e-01,\n",
       "                        -1.4424e-01, -8.3434e-02, -7.8773e-02,  1.0417e-01,  1.4677e-01,\n",
       "                         4.7335e-02,  1.0653e-01,  1.9805e-01,  3.6867e-02,  2.4315e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.0.bias',\n",
       "               tensor([ 0.5779,  1.4630,  0.0644, -0.0222,  1.0881,  1.0257,  0.0573, -0.2082,\n",
       "                        0.0964, -0.0406,  1.0319, -0.1191,  1.2168, -0.0490, -0.0097,  0.9679,\n",
       "                        1.1930,  1.0564,  0.9237, -0.0218], device='cuda:0')),\n",
       "              ('arbiter.2.weight',\n",
       "               tensor([[-2.7696e-01, -2.0690e-01, -1.9416e-01,  8.0315e-02,  9.9638e-02,\n",
       "                         1.1186e-01,  4.5160e-02,  2.3965e-02,  1.4061e-02,  2.2705e-02,\n",
       "                         1.2049e-01, -1.1446e-02, -2.8662e-01, -1.1019e-01, -1.1449e-01,\n",
       "                         1.0774e-02,  1.3232e-01, -4.3487e-02,  1.7941e-01, -1.8308e-01],\n",
       "                       [ 8.9045e-02,  1.0461e-01,  2.0148e-02, -1.9925e-01,  9.4846e-02,\n",
       "                         1.3727e-01,  1.4924e-01,  1.6985e-02, -3.1922e-02, -1.8997e-01,\n",
       "                         1.2385e-01, -9.4876e-02,  1.8950e-01, -1.7378e-01,  1.4378e-01,\n",
       "                         1.1648e-01,  2.1102e-01,  1.1773e-01,  1.3853e-02,  1.5089e-01],\n",
       "                       [-1.3032e-01, -1.1176e-01, -2.0589e-01,  5.7043e-02, -1.6127e-01,\n",
       "                         1.3371e-01,  1.7142e-01, -1.1048e-01,  1.9873e-01,  1.9033e-01,\n",
       "                        -2.0928e-01, -8.5642e-03, -3.7499e-01, -1.7186e-01,  3.9366e-03,\n",
       "                        -1.1968e-01,  1.2172e-02, -5.6698e-02,  8.3250e-02, -1.0491e-01],\n",
       "                       [ 9.8387e-02, -1.3402e-01,  1.6478e-01,  9.6023e-02, -1.0133e-01,\n",
       "                        -2.2337e-01,  6.4815e-02, -1.5311e-01,  1.4839e-01,  1.4678e-01,\n",
       "                         2.7220e-02, -1.6420e-02,  1.9317e-01, -5.1189e-02, -1.8708e-01,\n",
       "                         8.1884e-02, -1.8736e-01,  6.4915e-02,  2.0665e-02,  1.1247e-01],\n",
       "                       [-1.3734e-01, -4.3429e-02,  2.1410e-01,  6.7579e-02, -1.2165e-01,\n",
       "                        -1.5032e-02,  7.8945e-02, -5.4656e-02,  8.8937e-02, -1.9746e-02,\n",
       "                        -7.2334e-01,  1.3736e-01, -1.1583e-01,  6.0569e-02,  9.8967e-02,\n",
       "                        -2.0337e-01, -7.0027e-03, -2.6471e-02, -5.6458e-02,  3.6071e-02],\n",
       "                       [ 2.0000e-03,  1.2369e-02,  9.4219e-02,  1.6797e-01, -1.1251e-02,\n",
       "                         1.9300e-01,  1.8008e-01, -1.9471e-01,  1.4875e-01, -8.0597e-02,\n",
       "                         2.1163e-01, -1.4089e-01,  1.6965e-01,  1.1741e-01,  2.2125e-01,\n",
       "                         1.9232e-01, -1.4408e-01, -4.3063e-02, -2.0308e-01, -1.8770e-01],\n",
       "                       [ 1.6617e-01,  8.0233e-01, -2.1800e-01,  3.4234e-03,  1.0386e+00,\n",
       "                         1.1546e+00,  9.3585e-02, -2.1838e-01,  1.5790e-01,  1.6616e-01,\n",
       "                         1.5319e+00, -2.0903e-01,  1.1677e+00,  1.4261e-03, -8.1823e-02,\n",
       "                         1.1348e+00,  1.1217e+00,  7.9287e-01,  1.3367e+00, -7.9305e-02],\n",
       "                       [-1.1812e-01, -9.0398e-03, -1.7027e-01,  1.6862e-01,  7.9786e-02,\n",
       "                        -1.9915e-01,  4.6392e-02, -7.1841e-02,  1.0104e-01,  2.1979e-01,\n",
       "                         9.9288e-02,  3.3550e-02,  4.0755e-02, -1.0942e-01, -7.6839e-02,\n",
       "                        -1.5497e-01,  1.6216e-01,  1.9570e-01, -1.7680e-01, -1.8908e-01],\n",
       "                       [ 8.1760e-01,  1.1747e+00,  6.9845e-02, -2.8980e-02,  1.2703e+00,\n",
       "                         1.1017e+00, -1.5357e-01, -9.1709e-02, -1.0182e-01, -2.2172e-02,\n",
       "                         1.0701e+00, -1.8261e-02,  1.2327e+00, -1.4972e-01,  2.1412e-01,\n",
       "                         1.3993e+00,  1.1935e+00,  1.4037e+00,  1.3015e+00, -1.2772e-01],\n",
       "                       [ 6.5371e-02,  2.6495e-01,  9.2624e-02,  6.5136e-02, -1.9450e-02,\n",
       "                        -6.2750e-02,  1.9140e-01,  1.1874e-01,  1.3471e-01, -9.5442e-03,\n",
       "                        -5.2929e-01, -6.2280e-02,  2.4987e-01, -1.1313e-01, -1.3026e-01,\n",
       "                        -3.1673e-01, -1.6937e-01,  2.2256e-01, -3.9163e-01,  2.2979e-02]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.2.bias',\n",
       "               tensor([-0.0365,  0.1921,  0.0487, -0.1156,  0.1422, -0.0955, -0.0888,  0.1262,\n",
       "                        1.3369, -0.0932], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.361622004032135,\n",
       "   1.1774307218790054,\n",
       "   1.0682220284938813,\n",
       "   1.0134359432458877,\n",
       "   0.9557582337856293,\n",
       "   0.9252625764608383,\n",
       "   0.896571821808815,\n",
       "   0.8668007366657257,\n",
       "   0.8472098358869553,\n",
       "   0.8739602203369141,\n",
       "   0.8027454015016556,\n",
       "   0.790922427535057,\n",
       "   0.7696625510454178,\n",
       "   0.7480737142562867,\n",
       "   0.7457483853101731,\n",
       "   0.7329645273685456,\n",
       "   0.7137770148515701,\n",
       "   0.7022364659309387,\n",
       "   0.6944536420702935,\n",
       "   0.678084044277668,\n",
       "   0.6711137498617172,\n",
       "   0.6432430448532105,\n",
       "   0.6444714757800102,\n",
       "   0.6437381136417389,\n",
       "   0.6322268932461739,\n",
       "   0.7675004714727401,\n",
       "   0.6858330940008164,\n",
       "   0.6369115214347839,\n",
       "   0.6205476983785629,\n",
       "   0.6424601489901542,\n",
       "   0.6062124630808831,\n",
       "   0.6129032323956489,\n",
       "   0.6454385172724724,\n",
       "   0.6008061950802803,\n",
       "   0.6229772954583168,\n",
       "   0.5918961485624313,\n",
       "   0.5958339176774025,\n",
       "   0.6019341088533402,\n",
       "   0.5990372623801231,\n",
       "   0.5765040374994278,\n",
       "   0.5836926875710488,\n",
       "   0.575173718303442,\n",
       "   0.5905435925424098,\n",
       "   0.587688982129097,\n",
       "   0.5772113785147667,\n",
       "   0.5716481804251671,\n",
       "   0.7907439666390419,\n",
       "   0.606193328499794,\n",
       "   0.5810073080658913,\n",
       "   0.5803816887140274,\n",
       "   0.5827801483869552,\n",
       "   0.5594701299965381,\n",
       "   0.5675239782035351,\n",
       "   0.5574785028994084,\n",
       "   0.5821172350645065,\n",
       "   0.572479012131691,\n",
       "   0.5552550277411937,\n",
       "   0.6292399823069572,\n",
       "   0.5869806709885598,\n",
       "   0.5646496734023094,\n",
       "   0.5611681414842605,\n",
       "   0.5489646435678005,\n",
       "   0.5403351920247078,\n",
       "   0.5316672612428666,\n",
       "   0.547808075785637,\n",
       "   0.5346291369497777,\n",
       "   0.5325210520029068,\n",
       "   0.5130468677282334,\n",
       "   0.5175497198700905,\n",
       "   0.5460115510523319,\n",
       "   0.5095459462404252,\n",
       "   0.5113521621525288,\n",
       "   0.5150111964941024,\n",
       "   0.5099474284946919,\n",
       "   0.49857552760839463,\n",
       "   0.49722088885307314,\n",
       "   0.5020624770820141,\n",
       "   0.5039023125469685,\n",
       "   0.49570246279239655,\n",
       "   0.5044726549983024,\n",
       "   0.48876553937792777,\n",
       "   0.49015948405861853,\n",
       "   0.48760210585594177,\n",
       "   0.48937080571055414,\n",
       "   0.48270082703232764,\n",
       "   0.48500165647268295,\n",
       "   0.4837604705393314,\n",
       "   0.47981575956940653,\n",
       "   0.47343710049986837,\n",
       "   0.4680024850666523,\n",
       "   0.47328490269184115,\n",
       "   0.47831877300143244,\n",
       "   0.4573169270157814,\n",
       "   0.46452831718325616,\n",
       "   0.47274400290846824,\n",
       "   0.47608976206183434,\n",
       "   0.4707493006289005,\n",
       "   0.46580539804697035,\n",
       "   0.45470825970172885],\n",
       "  'train_loss_std': [0.15760631425230207,\n",
       "   0.13460466955083936,\n",
       "   0.15146414833100483,\n",
       "   0.14364851729528072,\n",
       "   0.1389922394975098,\n",
       "   0.1369571873499574,\n",
       "   0.14785146755809933,\n",
       "   0.13413648700154046,\n",
       "   0.14465316861115862,\n",
       "   0.15787905672436914,\n",
       "   0.13691639238397976,\n",
       "   0.14750697667014281,\n",
       "   0.13367896746429367,\n",
       "   0.14430841017623242,\n",
       "   0.1342308015198182,\n",
       "   0.14002720136409644,\n",
       "   0.1325191826057035,\n",
       "   0.13825487868786485,\n",
       "   0.14003931175731937,\n",
       "   0.1411454262578195,\n",
       "   0.13919954781913585,\n",
       "   0.1269709573564425,\n",
       "   0.13704759516908613,\n",
       "   0.13514228551529864,\n",
       "   0.12916899661670808,\n",
       "   0.23544735963598543,\n",
       "   0.13544957730247897,\n",
       "   0.1304847557673244,\n",
       "   0.13350003268784874,\n",
       "   0.14131885719601225,\n",
       "   0.13185041071234418,\n",
       "   0.13342135211783254,\n",
       "   0.14975434928835202,\n",
       "   0.13023210412548034,\n",
       "   0.13588161137608937,\n",
       "   0.13843272069531581,\n",
       "   0.12963593861839787,\n",
       "   0.13532709929398376,\n",
       "   0.13950879751689027,\n",
       "   0.13697043397909037,\n",
       "   0.13182726975343198,\n",
       "   0.1328639982920931,\n",
       "   0.1325888198522615,\n",
       "   0.13240833168487262,\n",
       "   0.13190938116976872,\n",
       "   0.13942539772334678,\n",
       "   0.23062122000698668,\n",
       "   0.132805798835033,\n",
       "   0.12853381337241354,\n",
       "   0.13191072003619783,\n",
       "   0.1318227394312389,\n",
       "   0.12297165568131242,\n",
       "   0.13406663164965715,\n",
       "   0.13314762755144663,\n",
       "   0.14043596239165218,\n",
       "   0.1342390529847867,\n",
       "   0.12841996427030639,\n",
       "   0.16978642186302176,\n",
       "   0.12825395997421235,\n",
       "   0.13082301298131938,\n",
       "   0.13471783292700518,\n",
       "   0.13596198244122065,\n",
       "   0.1380578753495257,\n",
       "   0.1321619185923792,\n",
       "   0.13451733482506162,\n",
       "   0.13150407458192687,\n",
       "   0.13160945331400967,\n",
       "   0.13680287754197146,\n",
       "   0.12327125635209571,\n",
       "   0.14938803565707603,\n",
       "   0.1194826546469831,\n",
       "   0.1255974547282668,\n",
       "   0.12104320335568049,\n",
       "   0.131712275865081,\n",
       "   0.1313008207101651,\n",
       "   0.12656590352170421,\n",
       "   0.13082851191154823,\n",
       "   0.12628226059671505,\n",
       "   0.1271287977403245,\n",
       "   0.13314781355187646,\n",
       "   0.12221343943221433,\n",
       "   0.13449480216469817,\n",
       "   0.1348814178028742,\n",
       "   0.1217728422082624,\n",
       "   0.13278561740513853,\n",
       "   0.12106947780984112,\n",
       "   0.1254040174696852,\n",
       "   0.12804199340265818,\n",
       "   0.12519335174240365,\n",
       "   0.12404039116505164,\n",
       "   0.1300050639144933,\n",
       "   0.12879062797434798,\n",
       "   0.12332243338846562,\n",
       "   0.12863197413845123,\n",
       "   0.13009441050199874,\n",
       "   0.1382745632889109,\n",
       "   0.11983601022911237,\n",
       "   0.12172995013749403,\n",
       "   0.11738596682182958],\n",
       "  'train_accuracy_mean': [0.4405066656470299,\n",
       "   0.5330399996638298,\n",
       "   0.5859599992036819,\n",
       "   0.6114799976348877,\n",
       "   0.6355066660046578,\n",
       "   0.6503333325386047,\n",
       "   0.663066666841507,\n",
       "   0.6769599989652634,\n",
       "   0.6815866663455963,\n",
       "   0.6705066667199134,\n",
       "   0.6989333317875862,\n",
       "   0.7032933332324028,\n",
       "   0.7124933324456215,\n",
       "   0.7215333334803581,\n",
       "   0.7214666662216187,\n",
       "   0.7288266673088074,\n",
       "   0.7338933345079423,\n",
       "   0.739773332953453,\n",
       "   0.7386799989938736,\n",
       "   0.7514533332586288,\n",
       "   0.7510133336782455,\n",
       "   0.7609733337163925,\n",
       "   0.7602666670084,\n",
       "   0.7594800000190735,\n",
       "   0.7638933342695237,\n",
       "   0.7116800007224083,\n",
       "   0.745373333454132,\n",
       "   0.7621866660118103,\n",
       "   0.7706000006198883,\n",
       "   0.765520000576973,\n",
       "   0.7755199990272522,\n",
       "   0.7757200000286102,\n",
       "   0.7648533329963684,\n",
       "   0.7821999988555908,\n",
       "   0.773053332567215,\n",
       "   0.7852133331298828,\n",
       "   0.7819466675519944,\n",
       "   0.781026666522026,\n",
       "   0.7850533329248428,\n",
       "   0.7894533336162567,\n",
       "   0.7866666666269302,\n",
       "   0.7905999997854233,\n",
       "   0.7859733345508575,\n",
       "   0.7872799997329711,\n",
       "   0.7917466661930084,\n",
       "   0.7920000003576279,\n",
       "   0.7050399999022484,\n",
       "   0.7812133324146271,\n",
       "   0.7907333332300186,\n",
       "   0.792519999742508,\n",
       "   0.7904933335781097,\n",
       "   0.8009066677093506,\n",
       "   0.7948133329153061,\n",
       "   0.8000533326864242,\n",
       "   0.790906665802002,\n",
       "   0.7937733331918716,\n",
       "   0.8007599996328354,\n",
       "   0.7723600000143052,\n",
       "   0.7890266660451889,\n",
       "   0.7943466671705246,\n",
       "   0.7935333335399628,\n",
       "   0.7980933334827424,\n",
       "   0.8011200007200241,\n",
       "   0.8042933332920075,\n",
       "   0.796706666469574,\n",
       "   0.8018400000333786,\n",
       "   0.8035466668605804,\n",
       "   0.8115333338975906,\n",
       "   0.8101599991321564,\n",
       "   0.7965199998617172,\n",
       "   0.8114133318662643,\n",
       "   0.8100133329629898,\n",
       "   0.8103466657400131,\n",
       "   0.8110266666412353,\n",
       "   0.8151599992513656,\n",
       "   0.8159200006723404,\n",
       "   0.8122000002861023,\n",
       "   0.8122800004482269,\n",
       "   0.8171066681146621,\n",
       "   0.8132799996137619,\n",
       "   0.8200266664028167,\n",
       "   0.8190666670799256,\n",
       "   0.818706667304039,\n",
       "   0.8197600010633469,\n",
       "   0.8213866665363312,\n",
       "   0.8198800004720688,\n",
       "   0.8217599998712539,\n",
       "   0.8220933334827423,\n",
       "   0.8257600013017654,\n",
       "   0.825973333120346,\n",
       "   0.8249733333587647,\n",
       "   0.8219866671562195,\n",
       "   0.8299200005531311,\n",
       "   0.8275333343744278,\n",
       "   0.8241199985742569,\n",
       "   0.8234666675329209,\n",
       "   0.826000000834465,\n",
       "   0.8268666652441025,\n",
       "   0.8301200000047684],\n",
       "  'train_accuracy_std': [0.07983043693360754,\n",
       "   0.06862265976066549,\n",
       "   0.0747567209000835,\n",
       "   0.07093932191495719,\n",
       "   0.06719117812037634,\n",
       "   0.06818650939483129,\n",
       "   0.06888586481646876,\n",
       "   0.06478822330166789,\n",
       "   0.06596475716465323,\n",
       "   0.07279933675869754,\n",
       "   0.06440924786187506,\n",
       "   0.06888120664240713,\n",
       "   0.063906224253305,\n",
       "   0.06783414414902081,\n",
       "   0.06351048770526453,\n",
       "   0.0640010505597935,\n",
       "   0.06411688889208492,\n",
       "   0.06302762667040834,\n",
       "   0.0653043449014169,\n",
       "   0.06488912820630388,\n",
       "   0.06194151111641609,\n",
       "   0.05930586695736313,\n",
       "   0.06381723920509595,\n",
       "   0.06000293157500874,\n",
       "   0.05954604047323673,\n",
       "   0.09700916415411907,\n",
       "   0.0626554467935191,\n",
       "   0.060205726776262826,\n",
       "   0.061404993565721926,\n",
       "   0.06462968555710999,\n",
       "   0.06104730838451907,\n",
       "   0.06146863186578526,\n",
       "   0.06505997861903173,\n",
       "   0.060817066079110575,\n",
       "   0.06091605496804217,\n",
       "   0.06389660944002858,\n",
       "   0.0574226962258441,\n",
       "   0.06132546491982719,\n",
       "   0.06303841440957585,\n",
       "   0.06313557768383013,\n",
       "   0.0596970149441467,\n",
       "   0.05849136681563204,\n",
       "   0.05933752062174577,\n",
       "   0.06019801859672452,\n",
       "   0.0590311995924288,\n",
       "   0.061486765399264315,\n",
       "   0.0994242235692022,\n",
       "   0.06150587321985237,\n",
       "   0.05968878521159052,\n",
       "   0.06007518172657642,\n",
       "   0.06051648935933851,\n",
       "   0.054670529738138345,\n",
       "   0.06096290980781924,\n",
       "   0.05957625771692975,\n",
       "   0.0641579659526408,\n",
       "   0.06006279738647803,\n",
       "   0.05861096029202174,\n",
       "   0.0728251136915521,\n",
       "   0.05920536203508506,\n",
       "   0.058663976734623174,\n",
       "   0.060235500547308425,\n",
       "   0.060988963103866416,\n",
       "   0.05876347182958441,\n",
       "   0.057434509371590205,\n",
       "   0.06022696594723091,\n",
       "   0.05722152982336496,\n",
       "   0.05851380839269789,\n",
       "   0.06061796562661331,\n",
       "   0.05353687137935982,\n",
       "   0.06538688862029382,\n",
       "   0.054157408020959655,\n",
       "   0.056318932330027195,\n",
       "   0.05424688286795283,\n",
       "   0.05717430757086448,\n",
       "   0.057649873123094816,\n",
       "   0.056780847811905705,\n",
       "   0.05706491639670968,\n",
       "   0.05539395825791732,\n",
       "   0.05604825315969658,\n",
       "   0.05840088937511512,\n",
       "   0.05419470562831422,\n",
       "   0.05953314853584953,\n",
       "   0.058674380452245614,\n",
       "   0.05506307710733761,\n",
       "   0.05747298380794343,\n",
       "   0.05182092023472708,\n",
       "   0.055124224924304153,\n",
       "   0.055467070522027796,\n",
       "   0.053823374465601995,\n",
       "   0.05228689543317499,\n",
       "   0.055437445949425485,\n",
       "   0.05771546923223388,\n",
       "   0.05368296183550681,\n",
       "   0.0561984980362622,\n",
       "   0.055066860865417476,\n",
       "   0.05980917994025287,\n",
       "   0.05104856364450462,\n",
       "   0.053488565588087164,\n",
       "   0.050749792705380416],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.3679926391442616,\n",
       "   1.2665635883808135,\n",
       "   1.187115044593811,\n",
       "   1.150152188539505,\n",
       "   1.1018906559546788,\n",
       "   1.103819659948349,\n",
       "   1.0518965985377629,\n",
       "   1.0638240402936936,\n",
       "   1.0246775339047114,\n",
       "   1.0404366920391719,\n",
       "   0.9956734057267507,\n",
       "   0.9974079296986262,\n",
       "   0.9795595763127009,\n",
       "   0.9822847622632981,\n",
       "   0.9686410011847814,\n",
       "   0.9727859516938527,\n",
       "   0.9796642202138901,\n",
       "   0.9395568052927653,\n",
       "   0.9345259749889374,\n",
       "   0.9407547829548518,\n",
       "   0.9327725533644359,\n",
       "   0.92769619901975,\n",
       "   0.9204771769046783,\n",
       "   0.9448835251728693,\n",
       "   0.9108933673302333,\n",
       "   1.0372760611772538,\n",
       "   0.9346874632438024,\n",
       "   0.9248945005734761,\n",
       "   0.9147397148609161,\n",
       "   0.9183905428647995,\n",
       "   0.9009467685222625,\n",
       "   0.9021474973360697,\n",
       "   0.9080566734075546,\n",
       "   0.9333889466524125,\n",
       "   0.9275669205188751,\n",
       "   0.9050271634260814,\n",
       "   0.926428096294403,\n",
       "   0.9209066055218379,\n",
       "   0.9143157025178273,\n",
       "   0.9049600623051326,\n",
       "   0.9200636585553487,\n",
       "   0.9034700530767441,\n",
       "   0.9263561564683914,\n",
       "   0.9148094246784846,\n",
       "   0.9136891599496205,\n",
       "   0.9303221921126048,\n",
       "   0.9509109348058701,\n",
       "   0.918716412782669,\n",
       "   0.9180389950672786,\n",
       "   0.9142436045408249,\n",
       "   0.9154155236482621,\n",
       "   0.912844978372256,\n",
       "   0.9149003094434738,\n",
       "   0.9255933437744777,\n",
       "   0.9092950361967087,\n",
       "   0.9149978530406951,\n",
       "   0.9086123383045197,\n",
       "   0.9526016773780187,\n",
       "   0.9112638721863429,\n",
       "   0.9188696434100468,\n",
       "   0.9226427108049393,\n",
       "   0.9266535631815592,\n",
       "   0.9162708167235056,\n",
       "   0.9089055428902308,\n",
       "   0.9085712881882986,\n",
       "   0.906133243838946,\n",
       "   0.9073711766799291,\n",
       "   0.9066981770594915,\n",
       "   0.9092002816994985,\n",
       "   0.9173020935058593,\n",
       "   0.9145328958829244,\n",
       "   0.910240751306216,\n",
       "   0.9013746041059494,\n",
       "   0.9056242448091507,\n",
       "   0.910866375764211,\n",
       "   0.9328299256165823,\n",
       "   0.915953133503596,\n",
       "   0.9131108146905899,\n",
       "   0.9050941954056422,\n",
       "   0.9124753997723262,\n",
       "   0.9091932680209478,\n",
       "   0.9005293403069178,\n",
       "   0.9159798089663188,\n",
       "   0.9014111695686976,\n",
       "   0.9044043095906575,\n",
       "   0.9150010774532954,\n",
       "   0.906214166879654,\n",
       "   0.9070675629377365,\n",
       "   0.9093568835655849,\n",
       "   0.9075004309415817,\n",
       "   0.8910978811979294,\n",
       "   0.9322220406929652,\n",
       "   0.9150334457556407,\n",
       "   0.916781660715739,\n",
       "   0.9202045565843582,\n",
       "   0.9247222755352656,\n",
       "   0.9162739940484365,\n",
       "   0.92395301481088,\n",
       "   0.9140932367245356],\n",
       "  'val_loss_std': [0.11062253486762083,\n",
       "   0.12608329885087627,\n",
       "   0.13131941896987437,\n",
       "   0.12630943183796767,\n",
       "   0.13536002487386312,\n",
       "   0.13685657709862972,\n",
       "   0.13878177838154124,\n",
       "   0.1383686870865279,\n",
       "   0.13720322655555808,\n",
       "   0.12860314528908715,\n",
       "   0.13815333407473412,\n",
       "   0.1421758131117974,\n",
       "   0.13386733018071642,\n",
       "   0.12966250406351876,\n",
       "   0.13357744954796044,\n",
       "   0.13349541100473403,\n",
       "   0.140908173021652,\n",
       "   0.1291422074901274,\n",
       "   0.13588970777468098,\n",
       "   0.1359635381114923,\n",
       "   0.1307441526091205,\n",
       "   0.13562906269947486,\n",
       "   0.12849118336466775,\n",
       "   0.13022329401405383,\n",
       "   0.13312905195577399,\n",
       "   0.13649817578369947,\n",
       "   0.13867301130289097,\n",
       "   0.14196561512850683,\n",
       "   0.13881678988797735,\n",
       "   0.13323774229912586,\n",
       "   0.13122538534586617,\n",
       "   0.13195144217630525,\n",
       "   0.13358238072944123,\n",
       "   0.13415383334812767,\n",
       "   0.13908654082221072,\n",
       "   0.13913592354338544,\n",
       "   0.1419006017656529,\n",
       "   0.13338726111796215,\n",
       "   0.127983224794475,\n",
       "   0.13657619077349314,\n",
       "   0.1337267671455435,\n",
       "   0.13576494334385047,\n",
       "   0.13480841403182175,\n",
       "   0.13959791513775216,\n",
       "   0.13455048243367818,\n",
       "   0.13895516274886816,\n",
       "   0.12765972479629564,\n",
       "   0.13244871181865622,\n",
       "   0.1384700298349217,\n",
       "   0.13553475226066147,\n",
       "   0.1321187246455792,\n",
       "   0.1349087152496928,\n",
       "   0.13273132806670207,\n",
       "   0.13119926739795806,\n",
       "   0.1308399592055531,\n",
       "   0.13082038301022048,\n",
       "   0.13619670907423428,\n",
       "   0.13080365290222037,\n",
       "   0.12991495994752314,\n",
       "   0.12928369431331915,\n",
       "   0.1313431692133802,\n",
       "   0.13269222598441546,\n",
       "   0.13535910695363415,\n",
       "   0.1311479739649875,\n",
       "   0.12970916182397046,\n",
       "   0.1298915017478646,\n",
       "   0.13029961477730842,\n",
       "   0.131087066579046,\n",
       "   0.1312596144226387,\n",
       "   0.13528126262760287,\n",
       "   0.13918127209012918,\n",
       "   0.13084975389907816,\n",
       "   0.1316753818475644,\n",
       "   0.1333483053487216,\n",
       "   0.135189479181418,\n",
       "   0.1379468648129042,\n",
       "   0.13711011185884397,\n",
       "   0.13607647717859778,\n",
       "   0.13891017475569256,\n",
       "   0.13336937330262763,\n",
       "   0.13614209821511872,\n",
       "   0.1383109894248505,\n",
       "   0.13990288098899295,\n",
       "   0.1389713895727852,\n",
       "   0.13523753270226518,\n",
       "   0.14538887695848646,\n",
       "   0.13556030651804227,\n",
       "   0.13572404193512486,\n",
       "   0.13647022773819134,\n",
       "   0.13807415748238314,\n",
       "   0.13605056056990947,\n",
       "   0.1455153016717191,\n",
       "   0.13680853300468027,\n",
       "   0.13582615283397945,\n",
       "   0.13537685480300057,\n",
       "   0.13958782437365516,\n",
       "   0.13426996273133024,\n",
       "   0.14062181746215174,\n",
       "   0.1415112652049066],\n",
       "  'val_accuracy_mean': [0.436266667842865,\n",
       "   0.49455555498600007,\n",
       "   0.5277555547157924,\n",
       "   0.5476666645208994,\n",
       "   0.5709555558363597,\n",
       "   0.5674444430073102,\n",
       "   0.5898444437980652,\n",
       "   0.5863333318630854,\n",
       "   0.6030444432298342,\n",
       "   0.5944888876875242,\n",
       "   0.6162444437543552,\n",
       "   0.613377777338028,\n",
       "   0.6197333340843518,\n",
       "   0.6195333326856295,\n",
       "   0.6224888883034388,\n",
       "   0.6216222226619721,\n",
       "   0.6227999993165334,\n",
       "   0.6370888885855674,\n",
       "   0.6362888886531194,\n",
       "   0.6372666656970978,\n",
       "   0.6399555552005768,\n",
       "   0.6410222221414248,\n",
       "   0.644288886586825,\n",
       "   0.6377555559078852,\n",
       "   0.6452888888120651,\n",
       "   0.5950888871153196,\n",
       "   0.6365999994675319,\n",
       "   0.6416888880729675,\n",
       "   0.6449777792890866,\n",
       "   0.6430666666229566,\n",
       "   0.6530444432298342,\n",
       "   0.6542444431781769,\n",
       "   0.6511555569370587,\n",
       "   0.6407333326339721,\n",
       "   0.642533330321312,\n",
       "   0.6517777766784032,\n",
       "   0.6462222211559614,\n",
       "   0.6476222203175227,\n",
       "   0.648599999944369,\n",
       "   0.6511555536588033,\n",
       "   0.6464666668574015,\n",
       "   0.6494888881842296,\n",
       "   0.643911110063394,\n",
       "   0.6507555549343427,\n",
       "   0.6467555563648542,\n",
       "   0.642244443098704,\n",
       "   0.6322666662931442,\n",
       "   0.646933331489563,\n",
       "   0.6479777769247691,\n",
       "   0.6480888870358467,\n",
       "   0.6460444438457489,\n",
       "   0.6480222209294637,\n",
       "   0.6481555548310279,\n",
       "   0.6429333325227101,\n",
       "   0.6502444432179133,\n",
       "   0.645333331724008,\n",
       "   0.6484666667381922,\n",
       "   0.6304222224156062,\n",
       "   0.6478444445133209,\n",
       "   0.6431333334247271,\n",
       "   0.6401777778069179,\n",
       "   0.6399999991059303,\n",
       "   0.644044444958369,\n",
       "   0.6459555547436079,\n",
       "   0.6494444440801939,\n",
       "   0.6477333321173986,\n",
       "   0.6481555543343226,\n",
       "   0.6468444443742434,\n",
       "   0.6466444428761801,\n",
       "   0.6424666671951612,\n",
       "   0.6446444436907768,\n",
       "   0.6450444427132607,\n",
       "   0.6498222217957179,\n",
       "   0.6472888892889023,\n",
       "   0.6438000010450681,\n",
       "   0.6373777782917023,\n",
       "   0.6438666646679242,\n",
       "   0.6466444430748621,\n",
       "   0.6507111120224,\n",
       "   0.6458000002304712,\n",
       "   0.6439555564522743,\n",
       "   0.6485333334406217,\n",
       "   0.6456444429357847,\n",
       "   0.6492666656772296,\n",
       "   0.6483777778347334,\n",
       "   0.6475777763128281,\n",
       "   0.645399997929732,\n",
       "   0.6480666647354761,\n",
       "   0.646222222050031,\n",
       "   0.6468444452683131,\n",
       "   0.6521555537978808,\n",
       "   0.6405999996264775,\n",
       "   0.6459111107389132,\n",
       "   0.6416666658719381,\n",
       "   0.6448666661977768,\n",
       "   0.6415555539727211,\n",
       "   0.6441555558641752,\n",
       "   0.6400222218036652,\n",
       "   0.643999999264876],\n",
       "  'val_accuracy_std': [0.060430455035726444,\n",
       "   0.06388289577098315,\n",
       "   0.06325842895999484,\n",
       "   0.06166366377455664,\n",
       "   0.0643978786712513,\n",
       "   0.06507763534319382,\n",
       "   0.06523443581669586,\n",
       "   0.0633815602184962,\n",
       "   0.06273806378523672,\n",
       "   0.060448920077195996,\n",
       "   0.06511050132917262,\n",
       "   0.0640115579957208,\n",
       "   0.06353245844644154,\n",
       "   0.06397862094152808,\n",
       "   0.06271178687947661,\n",
       "   0.06313446725762836,\n",
       "   0.06258514840523967,\n",
       "   0.0602043400579989,\n",
       "   0.06269271145081542,\n",
       "   0.061577497179440284,\n",
       "   0.06213961251578736,\n",
       "   0.06073738033016318,\n",
       "   0.05961958065011319,\n",
       "   0.05980337996898544,\n",
       "   0.059793703022176195,\n",
       "   0.0648898737064427,\n",
       "   0.06232646682088664,\n",
       "   0.06259422098077273,\n",
       "   0.0636759706130973,\n",
       "   0.06084165312811488,\n",
       "   0.05936508282159393,\n",
       "   0.06258315560253769,\n",
       "   0.06396293882999067,\n",
       "   0.06263518181630784,\n",
       "   0.06485617196315363,\n",
       "   0.06400193031610968,\n",
       "   0.06337971031823496,\n",
       "   0.062064286897766795,\n",
       "   0.0626640323884416,\n",
       "   0.06250568742802146,\n",
       "   0.06050984723414645,\n",
       "   0.06274373232613027,\n",
       "   0.06161619166925887,\n",
       "   0.0625875945980386,\n",
       "   0.06368899826319573,\n",
       "   0.060030852729306534,\n",
       "   0.06252977962714344,\n",
       "   0.06176601138548991,\n",
       "   0.06497794240377959,\n",
       "   0.0630582890420689,\n",
       "   0.06293251842764937,\n",
       "   0.06297159570381898,\n",
       "   0.06329944925723086,\n",
       "   0.06271147455473025,\n",
       "   0.0609220485960712,\n",
       "   0.06153830744291439,\n",
       "   0.06118476232659142,\n",
       "   0.060415593950262254,\n",
       "   0.06276544990866105,\n",
       "   0.061666642521865614,\n",
       "   0.06378411207250102,\n",
       "   0.0629497157791985,\n",
       "   0.06129417489870988,\n",
       "   0.06295040533254995,\n",
       "   0.0599690246182491,\n",
       "   0.0619633352324562,\n",
       "   0.061144682873389755,\n",
       "   0.060777414336705936,\n",
       "   0.061705390056926025,\n",
       "   0.060935827163214135,\n",
       "   0.06496426149654448,\n",
       "   0.06176327214370804,\n",
       "   0.06106073129405666,\n",
       "   0.06272029405870332,\n",
       "   0.06102765713501566,\n",
       "   0.061093234639174154,\n",
       "   0.06612119260389519,\n",
       "   0.06026484516730635,\n",
       "   0.060162233384282136,\n",
       "   0.06192460301335438,\n",
       "   0.059938718517751115,\n",
       "   0.0620662489249148,\n",
       "   0.06381332430152599,\n",
       "   0.06279345365243046,\n",
       "   0.06034866347697681,\n",
       "   0.06419345378936796,\n",
       "   0.06286984895973628,\n",
       "   0.06150431658399701,\n",
       "   0.06139690806647436,\n",
       "   0.06315388472536,\n",
       "   0.06218686213116309,\n",
       "   0.06247556038938242,\n",
       "   0.06198225466309179,\n",
       "   0.06360089845907092,\n",
       "   0.06046134919263145,\n",
       "   0.06224642468596385,\n",
       "   0.061425580603436905,\n",
       "   0.06280009203710868,\n",
       "   0.061299508423496414],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = 33\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "# print(\"=\"*10)\n",
    "# print(\"names_weights_copy == \",names_weights_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a30f19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.layer_dict.conv0.conv.weight\n",
      "classifier.layer_dict.conv0.conv.bias\n",
      "classifier.layer_dict.conv0.norm_layer.running_mean\n",
      "classifier.layer_dict.conv0.norm_layer.running_var\n",
      "classifier.layer_dict.conv0.norm_layer.bias\n",
      "classifier.layer_dict.conv0.norm_layer.weight\n",
      "classifier.layer_dict.conv1.conv.weight\n",
      "classifier.layer_dict.conv1.conv.bias\n",
      "classifier.layer_dict.conv1.norm_layer.running_mean\n",
      "classifier.layer_dict.conv1.norm_layer.running_var\n",
      "classifier.layer_dict.conv1.norm_layer.bias\n",
      "classifier.layer_dict.conv1.norm_layer.weight\n",
      "classifier.layer_dict.conv2.conv.weight\n",
      "classifier.layer_dict.conv2.conv.bias\n",
      "classifier.layer_dict.conv2.norm_layer.running_mean\n",
      "classifier.layer_dict.conv2.norm_layer.running_var\n",
      "classifier.layer_dict.conv2.norm_layer.bias\n",
      "classifier.layer_dict.conv2.norm_layer.weight\n",
      "classifier.layer_dict.conv3.conv.weight\n",
      "classifier.layer_dict.conv3.conv.bias\n",
      "classifier.layer_dict.conv3.norm_layer.running_mean\n",
      "classifier.layer_dict.conv3.norm_layer.running_var\n",
      "classifier.layer_dict.conv3.norm_layer.bias\n",
      "classifier.layer_dict.conv3.norm_layer.weight\n",
      "classifier.layer_dict.linear.weights\n",
      "classifier.layer_dict.linear.bias\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias\n",
      "arbiter.0.weight\n",
      "arbiter.0.bias\n",
      "arbiter.2.weight\n",
      "arbiter.2.bias\n"
     ]
    }
   ],
   "source": [
    "for key, value in maml_system.model.named_parameters():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "170a7604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==loss_landscape==\n",
      "layer_dict.conv0.conv.weight\n",
      "layer_dict.conv0.conv.bias\n",
      "layer_dict.conv0.norm_layer.running_mean\n",
      "layer_dict.conv0.norm_layer.running_var\n",
      "layer_dict.conv0.norm_layer.bias\n",
      "layer_dict.conv0.norm_layer.weight\n",
      "layer_dict.conv1.conv.weight\n",
      "layer_dict.conv1.conv.bias\n",
      "layer_dict.conv1.norm_layer.running_mean\n",
      "layer_dict.conv1.norm_layer.running_var\n",
      "layer_dict.conv1.norm_layer.bias\n",
      "layer_dict.conv1.norm_layer.weight\n",
      "layer_dict.conv2.conv.weight\n",
      "layer_dict.conv2.conv.bias\n",
      "layer_dict.conv2.norm_layer.running_mean\n",
      "layer_dict.conv2.norm_layer.running_var\n",
      "layer_dict.conv2.norm_layer.bias\n",
      "layer_dict.conv2.norm_layer.weight\n",
      "layer_dict.conv3.conv.weight\n",
      "layer_dict.conv3.conv.bias\n",
      "layer_dict.conv3.norm_layer.running_mean\n",
      "layer_dict.conv3.norm_layer.running_var\n",
      "layer_dict.conv3.norm_layer.bias\n",
      "layer_dict.conv3.norm_layer.weight\n",
      "layer_dict.linear.weights\n",
      "layer_dict.linear.bias\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "p ==  Parameter containing:\n",
      "tensor([[[[-1.5707e-03, -2.5832e-01,  1.7820e-01],\n",
      "          [-1.3748e-01, -6.9147e-02,  3.1331e-02],\n",
      "          [-6.8982e-02,  2.6458e-01, -8.9069e-03]],\n",
      "\n",
      "         [[ 1.4270e-01, -2.2754e-01,  1.8307e-01],\n",
      "          [-6.8883e-02,  3.6089e-02,  9.6674e-02],\n",
      "          [-1.5392e-01,  9.6749e-02, -9.6732e-02]],\n",
      "\n",
      "         [[ 2.0977e-01, -1.2438e-01, -8.0217e-02],\n",
      "          [ 1.0298e-01,  1.4493e-01, -1.7005e-01],\n",
      "          [-8.3693e-02,  1.1976e-01, -1.6073e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7137e-02,  3.3717e-01,  2.0318e-02],\n",
      "          [ 1.2752e-01, -9.1033e-02,  1.9680e-02],\n",
      "          [-1.5948e-01, -2.3552e-01, -6.1447e-02]],\n",
      "\n",
      "         [[-1.4350e-01,  1.4975e-01, -1.0675e-01],\n",
      "          [ 1.1053e-01, -1.4911e-01, -8.7883e-02],\n",
      "          [ 1.5145e-01, -1.5747e-01,  2.0006e-01]],\n",
      "\n",
      "         [[-1.5631e-01,  2.0872e-01, -1.8696e-01],\n",
      "          [-6.2811e-04,  6.4432e-02,  1.5282e-01],\n",
      "          [ 4.3215e-03, -1.0579e-01,  4.0867e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.2504e-04, -3.1052e-03,  1.2996e-01],\n",
      "          [ 4.3149e-02, -5.5917e-02, -4.1076e-02],\n",
      "          [ 7.5065e-02, -3.5752e-02, -1.8983e-01]],\n",
      "\n",
      "         [[-8.9421e-02, -4.6242e-02,  1.3393e-01],\n",
      "          [ 2.6513e-01,  7.3267e-05,  5.3320e-02],\n",
      "          [-1.5255e-01, -1.6220e-01,  3.9530e-02]],\n",
      "\n",
      "         [[ 6.2313e-02, -2.4308e-01,  3.2952e-02],\n",
      "          [ 2.2360e-01, -6.3872e-02, -1.7884e-01],\n",
      "          [-1.4942e-01, -7.7107e-02, -5.1498e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.5035e-02, -5.0993e-02, -5.4545e-02],\n",
      "          [-5.6864e-02, -3.5889e-01, -2.8949e-01],\n",
      "          [ 3.6489e-02, -1.9244e-01, -2.6829e-01]],\n",
      "\n",
      "         [[-1.0810e-01, -1.2368e-02,  5.3940e-02],\n",
      "          [-1.6190e-01, -1.6227e-01, -1.0322e-01],\n",
      "          [-3.1655e-02, -1.7322e-01, -2.0168e-01]],\n",
      "\n",
      "         [[ 2.0976e-01,  5.9735e-02,  1.7177e-01],\n",
      "          [ 9.6081e-02,  2.7967e-02,  1.1061e-01],\n",
      "          [-2.4330e-02, -1.2533e-01,  9.1291e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.9813e-02, -8.1347e-02, -1.2539e-01],\n",
      "          [ 1.3882e-01,  2.4584e-01, -7.4924e-02],\n",
      "          [ 1.0351e-02,  2.3417e-01,  2.9242e-01]],\n",
      "\n",
      "         [[ 4.6110e-02, -5.0671e-03, -2.3587e-01],\n",
      "          [-9.9369e-02, -8.7017e-02, -3.3888e-02],\n",
      "          [-1.2856e-01, -4.6593e-02, -3.0702e-02]],\n",
      "\n",
      "         [[ 1.2070e-01, -2.5048e-02, -2.1322e-01],\n",
      "          [ 9.3987e-02,  3.7978e-02, -5.8433e-02],\n",
      "          [-1.3207e-01,  2.0469e-01,  2.2441e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0348e-02, -1.5601e-02,  4.6318e-03],\n",
      "          [ 1.0038e-01,  1.8549e-01,  9.4030e-02],\n",
      "          [ 3.0773e-03, -6.0711e-03,  6.3132e-02]],\n",
      "\n",
      "         [[-2.7393e-01, -3.5946e-01, -7.2046e-02],\n",
      "          [-2.7470e-01, -3.5601e-01, -1.5984e-01],\n",
      "          [-8.8832e-02,  2.9364e-03,  1.7902e-01]],\n",
      "\n",
      "         [[ 8.7246e-02,  1.3247e-01,  1.6612e-01],\n",
      "          [ 4.0225e-02, -4.9835e-02,  6.0901e-02],\n",
      "          [ 7.5357e-02,  6.1459e-02,  1.5662e-02]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.2491,  0.1780,  0.0294, -0.0493, -0.0201, -0.0189, -0.0066,  0.0443,\n",
      "         0.0104,  0.0036, -0.0313, -0.0184,  0.1034,  0.0118, -0.0759, -0.0666,\n",
      "        -0.1952, -0.0600, -0.0214, -0.0129, -0.0480,  0.0081, -0.0456,  0.0510,\n",
      "        -0.0559,  0.0246,  0.0128,  0.0142,  0.0567,  0.0826, -0.0123,  0.0455,\n",
      "        -0.0779, -0.0139, -0.0232, -0.0106, -0.0253, -0.1090, -0.0428,  0.0816,\n",
      "         0.1015,  0.2538, -0.0456, -0.1551, -0.0409, -0.0188, -0.0456,  0.0082],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([ 0.2115,  0.1276, -0.5586, -0.0843, -0.0131,  0.2343, -0.6572,  0.5135,\n",
      "        -0.4966, -0.4306, -0.1452, -0.3932, -0.0031, -0.5557,  0.5804,  0.2421,\n",
      "         0.3746, -0.2550,  0.2540, -0.4660, -0.1339, -0.0538, -0.5804,  0.0273,\n",
      "         0.0902, -0.3972, -0.3181,  0.9918, -0.1642,  0.1640,  0.4759, -0.5277,\n",
      "         0.0587, -0.5559, -0.4383, -0.0718, -0.2505, -0.2495, -0.2502, -0.1197,\n",
      "         0.8410, -0.0156, -0.0092, -0.0252, -0.4610, -0.3380, -0.2034, -0.4786],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([1.1207, 1.1964, 0.5442, 1.0884, 0.7278, 1.0353, 0.8411, 1.3525, 0.5905,\n",
      "        0.8696, 0.9515, 0.5634, 1.0377, 0.6970, 1.0364, 0.8712, 0.8628, 0.8116,\n",
      "        1.3138, 0.9031, 0.8259, 0.7190, 0.5803, 1.0697, 0.8432, 0.8216, 0.7372,\n",
      "        0.9775, 0.8411, 0.8826, 1.1618, 0.7314, 1.4252, 0.5787, 0.7261, 1.1887,\n",
      "        0.6497, 1.3712, 0.7373, 0.8981, 0.8581, 1.3248, 0.8648, 1.3312, 0.6452,\n",
      "        0.7148, 0.6707, 0.9231], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([[[[ 0.1534, -0.1246,  0.0059],\n",
      "          [ 0.0587,  0.0940,  0.2769],\n",
      "          [-0.0969,  0.2831, -0.1458]],\n",
      "\n",
      "         [[-0.0577, -0.0651,  0.0677],\n",
      "          [-0.1302,  0.2155, -0.1159],\n",
      "          [ 0.1340,  0.1184, -0.2560]],\n",
      "\n",
      "         [[-0.0438,  0.0898,  0.0400],\n",
      "          [ 0.0177,  0.0718,  0.1219],\n",
      "          [-0.0036,  0.0745,  0.0027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2111, -0.0882,  0.0658],\n",
      "          [-0.2137,  0.0536,  0.0654],\n",
      "          [ 0.0706,  0.0302, -0.1045]],\n",
      "\n",
      "         [[ 0.0118,  0.0206,  0.0186],\n",
      "          [-0.0933, -0.1505, -0.1702],\n",
      "          [-0.0828, -0.1453, -0.0363]],\n",
      "\n",
      "         [[-0.0660, -0.1690, -0.1117],\n",
      "          [-0.1403, -0.1583, -0.0079],\n",
      "          [-0.0541, -0.0357, -0.1464]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2075,  0.0582, -0.1072],\n",
      "          [ 0.2159,  0.2360, -0.1414],\n",
      "          [ 0.1252,  0.2091,  0.0875]],\n",
      "\n",
      "         [[-0.0548, -0.2506, -0.1902],\n",
      "          [ 0.0979, -0.2266, -0.1672],\n",
      "          [-0.0044, -0.0343, -0.2183]],\n",
      "\n",
      "         [[ 0.0912,  0.0208, -0.1052],\n",
      "          [ 0.0889,  0.1561,  0.0046],\n",
      "          [-0.0265,  0.0809,  0.0484]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0879, -0.2648, -0.0268],\n",
      "          [ 0.0196, -0.1127, -0.1178],\n",
      "          [ 0.0225, -0.0199, -0.0220]],\n",
      "\n",
      "         [[ 0.0448, -0.0942, -0.0095],\n",
      "          [ 0.2407, -0.0882, -0.1859],\n",
      "          [ 0.0536,  0.1066, -0.0483]],\n",
      "\n",
      "         [[ 0.0923, -0.1951, -0.1808],\n",
      "          [ 0.0744, -0.2893, -0.3775],\n",
      "          [ 0.1501, -0.1423, -0.1307]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2110,  0.2436, -0.0743],\n",
      "          [ 0.1829,  0.2427,  0.2251],\n",
      "          [ 0.1321,  0.1579,  0.1919]],\n",
      "\n",
      "         [[ 0.2452,  0.3044,  0.2690],\n",
      "          [ 0.1529,  0.2397,  0.3189],\n",
      "          [ 0.1115,  0.0808,  0.1874]],\n",
      "\n",
      "         [[-0.1520, -0.1175,  0.0160],\n",
      "          [-0.0421, -0.0435, -0.0811],\n",
      "          [-0.1115, -0.1162, -0.0164]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1978, -0.1741, -0.0050],\n",
      "          [-0.1313, -0.2397, -0.0007],\n",
      "          [-0.0797, -0.1184, -0.1118]],\n",
      "\n",
      "         [[-0.1290, -0.1329, -0.0009],\n",
      "          [-0.0666, -0.1962, -0.0306],\n",
      "          [-0.0635, -0.1420, -0.1063]],\n",
      "\n",
      "         [[-0.0404, -0.2479,  0.0394],\n",
      "          [-0.1506, -0.1925, -0.0237],\n",
      "          [-0.1630, -0.0605, -0.0728]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0266, -0.1273,  0.1227],\n",
      "          [-0.0881,  0.0474,  0.1157],\n",
      "          [-0.0080,  0.0650,  0.0897]],\n",
      "\n",
      "         [[ 0.2080,  0.1026, -0.0301],\n",
      "          [ 0.0592,  0.0181,  0.0649],\n",
      "          [-0.1423, -0.2773, -0.0129]],\n",
      "\n",
      "         [[-0.0547, -0.0692, -0.1198],\n",
      "          [-0.1002,  0.0753, -0.0184],\n",
      "          [-0.0883, -0.0105,  0.0636]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1978,  0.1170,  0.0019],\n",
      "          [ 0.3494,  0.4068,  0.1813],\n",
      "          [ 0.3014,  0.2275,  0.3250]],\n",
      "\n",
      "         [[-0.0556, -0.0794, -0.2048],\n",
      "          [-0.1644, -0.2294, -0.1738],\n",
      "          [ 0.0031,  0.0344, -0.0381]],\n",
      "\n",
      "         [[ 0.0141,  0.0332, -0.1185],\n",
      "          [ 0.1451, -0.0078,  0.0175],\n",
      "          [ 0.1063,  0.0566,  0.3432]]],\n",
      "\n",
      "\n",
      "        [[[-0.1966, -0.3832,  0.0041],\n",
      "          [ 0.0980, -0.0902,  0.0847],\n",
      "          [ 0.2745, -0.0014, -0.0153]],\n",
      "\n",
      "         [[ 0.0126,  0.2387,  0.0088],\n",
      "          [-0.0857, -0.0824,  0.1581],\n",
      "          [-0.1651, -0.1040,  0.0801]],\n",
      "\n",
      "         [[ 0.0908,  0.1984,  0.1220],\n",
      "          [ 0.1322,  0.2554,  0.1642],\n",
      "          [ 0.1829,  0.2046,  0.0652]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0166,  0.1229,  0.1091],\n",
      "          [-0.0553, -0.0176,  0.1160],\n",
      "          [ 0.0468,  0.0073, -0.0682]],\n",
      "\n",
      "         [[ 0.0962,  0.0347, -0.0447],\n",
      "          [-0.0244, -0.0445,  0.1240],\n",
      "          [ 0.0384, -0.0078, -0.0120]],\n",
      "\n",
      "         [[ 0.0230,  0.2035,  0.1220],\n",
      "          [-0.0121, -0.1365, -0.0027],\n",
      "          [-0.0406, -0.1600, -0.0182]]],\n",
      "\n",
      "\n",
      "        [[[-0.1801, -0.0880, -0.2463],\n",
      "          [ 0.0439,  0.1334,  0.0526],\n",
      "          [-0.0824,  0.2746, -0.0068]],\n",
      "\n",
      "         [[ 0.0575, -0.0060, -0.0620],\n",
      "          [ 0.2031,  0.3164,  0.2225],\n",
      "          [-0.0425, -0.1512, -0.2046]],\n",
      "\n",
      "         [[-0.1664, -0.0316, -0.1543],\n",
      "          [-0.0493,  0.0245, -0.0230],\n",
      "          [-0.0709,  0.0862, -0.0630]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1210, -0.0262, -0.1943],\n",
      "          [ 0.0778,  0.0065, -0.1218],\n",
      "          [ 0.0824,  0.0413, -0.0148]],\n",
      "\n",
      "         [[-0.0105, -0.0439, -0.0284],\n",
      "          [ 0.0427, -0.0464,  0.0336],\n",
      "          [ 0.0173, -0.0880, -0.1129]],\n",
      "\n",
      "         [[ 0.1107,  0.2442,  0.0328],\n",
      "          [ 0.0580,  0.1131, -0.0461],\n",
      "          [ 0.1423,  0.1016,  0.0098]]]], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([ 0.0016,  0.0049,  0.0101, -0.0004, -0.0012, -0.0177,  0.0042,  0.0004,\n",
      "         0.0024,  0.0193,  0.0233, -0.0297,  0.0093, -0.0007, -0.0092,  0.0134,\n",
      "         0.0144,  0.0350, -0.0036,  0.0030, -0.0127,  0.0122, -0.0179, -0.0162,\n",
      "        -0.0066,  0.0388,  0.0112,  0.0040, -0.0256,  0.0064,  0.0115,  0.0111,\n",
      "         0.0024,  0.0105,  0.0090, -0.0082, -0.0124,  0.0106, -0.0249, -0.0094,\n",
      "        -0.0086,  0.0221,  0.0118,  0.0104, -0.0239, -0.0013,  0.0077, -0.0206],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.2565, -0.3726, -0.4054, -0.3625, -0.4137, -0.5008, -0.1775, -0.6069,\n",
      "        -0.3940, -0.5503, -0.2014, -0.6194, -0.3724, -0.4130, -0.5457, -0.4446,\n",
      "        -0.4979, -0.3875, -0.1840, -0.2695, -0.6760, -0.2627, -0.4194, -0.6555,\n",
      "        -0.4132, -0.5041, -0.2278, -0.3183, -0.3875, -0.4913, -0.4797, -0.1923,\n",
      "        -0.3370, -0.6027, -0.5468, -0.3690, -0.3803, -0.2908, -0.2563, -0.2654,\n",
      "        -0.2941, -0.5569, -0.4664, -0.2894, -0.5059, -0.4478, -0.4427, -0.1984],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([0.9177, 0.8832, 0.8448, 1.0406, 1.0257, 1.0022, 1.1157, 0.7906, 0.7547,\n",
      "        0.8948, 1.1182, 1.0589, 0.7598, 0.7876, 1.0296, 1.0151, 0.9489, 1.2162,\n",
      "        0.8784, 0.9061, 1.0395, 0.8995, 0.8664, 1.0246, 0.9742, 0.9668, 1.0212,\n",
      "        0.9051, 1.0941, 0.9682, 0.9440, 0.9219, 0.8243, 1.0067, 1.0394, 0.9452,\n",
      "        1.2436, 0.9018, 0.9305, 0.7080, 0.7266, 1.1290, 0.9851, 0.7811, 1.2469,\n",
      "        0.7777, 0.8055, 1.0439], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([[[[ 0.0969,  0.1765,  0.1515],\n",
      "          [ 0.0974,  0.1888,  0.1136],\n",
      "          [ 0.2171,  0.0861,  0.0038]],\n",
      "\n",
      "         [[-0.1099,  0.0427,  0.1209],\n",
      "          [ 0.0027,  0.0368, -0.0494],\n",
      "          [ 0.1241, -0.0476, -0.1435]],\n",
      "\n",
      "         [[-0.1099, -0.2007, -0.0550],\n",
      "          [-0.2285, -0.1212, -0.0148],\n",
      "          [-0.1207,  0.0054,  0.0542]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0423, -0.1250, -0.0748],\n",
      "          [-0.0240, -0.1807, -0.1030],\n",
      "          [-0.0790, -0.0533, -0.0750]],\n",
      "\n",
      "         [[-0.0210,  0.0076, -0.1453],\n",
      "          [ 0.0152,  0.0353, -0.0689],\n",
      "          [ 0.0200, -0.1009, -0.1617]],\n",
      "\n",
      "         [[ 0.0643, -0.1465,  0.0311],\n",
      "          [-0.0859, -0.1255, -0.0305],\n",
      "          [-0.0059,  0.0738,  0.0525]]],\n",
      "\n",
      "\n",
      "        [[[-0.1648, -0.0251, -0.0203],\n",
      "          [-0.0341,  0.2062,  0.2028],\n",
      "          [-0.1044,  0.1532,  0.0217]],\n",
      "\n",
      "         [[-0.1054,  0.1401,  0.0633],\n",
      "          [-0.1487, -0.0246,  0.0523],\n",
      "          [-0.1470, -0.0882, -0.1132]],\n",
      "\n",
      "         [[-0.1215, -0.0171, -0.0143],\n",
      "          [-0.2319,  0.1108,  0.1407],\n",
      "          [-0.2288, -0.0095,  0.1990]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0349,  0.0697, -0.1227],\n",
      "          [-0.0616, -0.1689, -0.1314],\n",
      "          [-0.0305, -0.1365, -0.3234]],\n",
      "\n",
      "         [[-0.1013, -0.1013, -0.1379],\n",
      "          [ 0.0547,  0.0701, -0.0875],\n",
      "          [ 0.0770,  0.0983, -0.1357]],\n",
      "\n",
      "         [[ 0.1024, -0.0150, -0.1279],\n",
      "          [-0.2290, -0.0353,  0.0241],\n",
      "          [ 0.0920,  0.2147,  0.1344]]],\n",
      "\n",
      "\n",
      "        [[[-0.0182, -0.0584,  0.0137],\n",
      "          [ 0.0679,  0.0537, -0.0867],\n",
      "          [ 0.1386,  0.0880, -0.1243]],\n",
      "\n",
      "         [[ 0.0297,  0.0410, -0.0631],\n",
      "          [ 0.0441, -0.0066, -0.1468],\n",
      "          [-0.1272, -0.0634,  0.0358]],\n",
      "\n",
      "         [[ 0.0654, -0.0412, -0.1205],\n",
      "          [-0.0820, -0.0092,  0.0524],\n",
      "          [ 0.0799,  0.0222,  0.1974]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0917, -0.1910,  0.0748],\n",
      "          [ 0.0673, -0.0352,  0.1719],\n",
      "          [-0.0641,  0.0124,  0.0399]],\n",
      "\n",
      "         [[-0.0571, -0.0562,  0.1612],\n",
      "          [-0.0988, -0.0957,  0.0140],\n",
      "          [-0.1091, -0.0691, -0.1685]],\n",
      "\n",
      "         [[ 0.0816,  0.1142,  0.1290],\n",
      "          [ 0.0783, -0.0598, -0.0030],\n",
      "          [ 0.1139, -0.2666, -0.0520]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2654, -0.0521, -0.1548],\n",
      "          [-0.0452, -0.1364,  0.1865],\n",
      "          [ 0.0448, -0.0208,  0.1298]],\n",
      "\n",
      "         [[ 0.0632, -0.1374, -0.2397],\n",
      "          [ 0.1702,  0.0875,  0.0737],\n",
      "          [ 0.0043, -0.0055, -0.1061]],\n",
      "\n",
      "         [[-0.0013,  0.0272,  0.0291],\n",
      "          [ 0.1501, -0.0592,  0.0022],\n",
      "          [ 0.1177,  0.0939,  0.0543]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0467,  0.0302,  0.0642],\n",
      "          [ 0.0266,  0.1546,  0.1913],\n",
      "          [-0.0472, -0.0010,  0.0152]],\n",
      "\n",
      "         [[-0.0301, -0.0459,  0.0857],\n",
      "          [ 0.1189,  0.0011,  0.0690],\n",
      "          [-0.0674, -0.0918, -0.0669]],\n",
      "\n",
      "         [[ 0.0406,  0.0156, -0.0180],\n",
      "          [ 0.0098, -0.1104,  0.0057],\n",
      "          [ 0.1691,  0.1189,  0.0734]]],\n",
      "\n",
      "\n",
      "        [[[-0.0054, -0.0155, -0.0348],\n",
      "          [ 0.2060, -0.0743, -0.0305],\n",
      "          [-0.2407,  0.0226, -0.0451]],\n",
      "\n",
      "         [[-0.3178, -0.2310, -0.1284],\n",
      "          [-0.2614, -0.1035, -0.0785],\n",
      "          [-0.0501,  0.0121,  0.0628]],\n",
      "\n",
      "         [[ 0.0423,  0.1901,  0.0839],\n",
      "          [-0.0504, -0.0759, -0.0213],\n",
      "          [ 0.0793, -0.0723, -0.0549]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1821,  0.0485,  0.1129],\n",
      "          [-0.1945, -0.0385,  0.2224],\n",
      "          [-0.0729,  0.1510,  0.2624]],\n",
      "\n",
      "         [[ 0.0189, -0.1726, -0.0392],\n",
      "          [-0.2015, -0.2252, -0.2615],\n",
      "          [-0.2092, -0.2089, -0.1065]],\n",
      "\n",
      "         [[ 0.0788,  0.1300,  0.0362],\n",
      "          [-0.0329,  0.1333,  0.0918],\n",
      "          [-0.1108,  0.1131, -0.1361]]],\n",
      "\n",
      "\n",
      "        [[[-0.0686,  0.0233,  0.1452],\n",
      "          [-0.0782,  0.0190,  0.0833],\n",
      "          [-0.1898, -0.1920, -0.2879]],\n",
      "\n",
      "         [[-0.1238,  0.0142,  0.0095],\n",
      "          [ 0.1675,  0.0807, -0.0055],\n",
      "          [ 0.2717,  0.1179, -0.0707]],\n",
      "\n",
      "         [[ 0.1714,  0.1265,  0.1470],\n",
      "          [-0.0986,  0.0350, -0.0558],\n",
      "          [-0.0725,  0.0475, -0.0362]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2657, -0.0155, -0.1745],\n",
      "          [ 0.2644, -0.0333, -0.1367],\n",
      "          [ 0.1927, -0.2982, -0.1966]],\n",
      "\n",
      "         [[-0.0261, -0.0225, -0.0724],\n",
      "          [-0.0685, -0.1148, -0.1041],\n",
      "          [ 0.0354, -0.1881, -0.0553]],\n",
      "\n",
      "         [[ 0.0836,  0.0691,  0.1550],\n",
      "          [ 0.0478,  0.1020,  0.2254],\n",
      "          [ 0.0730,  0.2066,  0.2626]]]], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([ 4.6797e-03, -2.7848e-02,  2.0921e-02,  5.6514e-03,  2.2449e-02,\n",
      "         3.4297e-03,  9.0140e-05,  3.4005e-02,  6.2501e-03, -1.8747e-02,\n",
      "        -2.3069e-02,  1.2220e-02,  1.8028e-03, -1.8847e-02,  8.0701e-03,\n",
      "         1.2551e-03, -3.1082e-02, -8.7124e-03, -3.7984e-03, -2.7988e-03,\n",
      "        -4.2684e-02,  3.7588e-02,  4.9385e-03,  2.8643e-02, -1.1775e-02,\n",
      "        -1.9628e-03, -1.0528e-02, -6.6784e-05, -2.4349e-03, -7.3820e-03,\n",
      "        -4.9919e-04, -2.6968e-03,  1.6067e-02,  9.0262e-03,  1.2372e-02,\n",
      "         8.0276e-03, -1.0513e-02, -5.9761e-03, -7.9432e-03,  3.9517e-02,\n",
      "        -3.1771e-02, -4.0168e-02, -4.0181e-03, -1.1663e-02,  2.7277e-03,\n",
      "        -2.8474e-02, -4.5315e-02, -4.6959e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.4925, -0.5699, -0.5517, -0.6658, -0.4979, -0.7727, -0.4690, -0.5960,\n",
      "        -0.4470, -0.4515, -0.5634, -0.4503, -0.6153, -0.6567, -0.3539, -0.6008,\n",
      "        -0.6378, -0.6275, -0.5148, -0.6807, -0.4783, -0.5610, -0.6452, -0.5353,\n",
      "        -0.5073, -0.6814, -0.5272, -0.8846, -0.5829, -0.9338, -0.7272, -0.4585,\n",
      "        -0.4503, -0.3549, -0.4363, -0.4604, -0.9601, -0.4361, -0.5242, -0.4797,\n",
      "        -0.6975, -0.6236, -0.5711, -0.7284, -0.6015, -0.5275, -0.4526, -0.5882],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([0.8607, 0.9741, 0.8169, 1.0186, 0.9191, 0.9619, 0.7814, 0.9199, 0.8598,\n",
      "        0.9243, 0.8946, 0.7636, 0.9258, 0.9634, 0.9229, 0.7296, 0.8315, 1.1071,\n",
      "        0.8243, 0.8980, 0.8095, 0.9100, 0.7447, 1.1476, 0.8485, 0.9612, 0.8168,\n",
      "        1.2696, 0.9880, 1.1204, 0.9548, 0.9426, 0.8238, 0.8730, 0.7502, 0.8088,\n",
      "        1.0032, 0.7436, 0.9786, 0.9716, 0.7514, 0.8736, 0.9905, 0.9081, 0.6871,\n",
      "        0.8485, 0.8788, 0.8877], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([[[[-4.5640e-02, -4.1175e-02, -1.9834e-02],\n",
      "          [-1.2550e-01, -8.8204e-02,  8.0529e-02],\n",
      "          [-5.3299e-02, -3.5713e-02,  9.4846e-02]],\n",
      "\n",
      "         [[-2.2738e-02,  2.2804e-02,  3.8118e-03],\n",
      "          [ 3.3837e-02,  8.1089e-02,  1.4925e-02],\n",
      "          [-6.6579e-02,  6.9574e-02, -6.1421e-02]],\n",
      "\n",
      "         [[-1.0693e-01,  4.0560e-02, -6.4710e-02],\n",
      "          [-2.4086e-02, -1.0410e-01, -2.5920e-02],\n",
      "          [-1.2764e-01,  1.3481e-02, -1.6666e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9768e-03, -4.2347e-02,  6.7394e-02],\n",
      "          [-5.1574e-02,  5.1517e-02,  4.0076e-02],\n",
      "          [-6.1437e-02, -1.1131e-02,  1.8982e-02]],\n",
      "\n",
      "         [[ 4.7593e-02, -1.0535e-01, -2.8322e-03],\n",
      "          [ 5.4531e-02,  9.2245e-02,  2.0356e-02],\n",
      "          [-4.2840e-02, -8.0307e-02, -1.9114e-01]],\n",
      "\n",
      "         [[-5.2188e-02,  4.1634e-02,  9.6728e-02],\n",
      "          [ 3.2906e-02,  7.8303e-03,  3.4751e-02],\n",
      "          [-3.2608e-02, -1.8341e-02, -6.8420e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.1581e-02,  2.4299e-02, -3.8789e-02],\n",
      "          [ 2.3691e-02, -8.0351e-02,  3.1567e-02],\n",
      "          [ 4.6836e-02,  1.0816e-01,  6.1362e-02]],\n",
      "\n",
      "         [[ 1.4457e-02,  1.9798e-02,  4.1581e-02],\n",
      "          [-2.3902e-02, -5.6596e-03,  1.0114e-01],\n",
      "          [-3.3503e-02,  1.5170e-01,  2.0833e-01]],\n",
      "\n",
      "         [[ 5.4470e-02,  1.0266e-01,  6.7109e-02],\n",
      "          [ 2.7143e-02,  6.0375e-02,  1.0256e-01],\n",
      "          [ 6.3186e-02,  4.6070e-02,  6.7429e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.7540e-02, -1.3205e-02, -1.4086e-02],\n",
      "          [-8.8549e-02, -1.5724e-01, -1.0478e-01],\n",
      "          [ 6.8114e-02,  6.3573e-02,  2.8371e-02]],\n",
      "\n",
      "         [[ 7.5995e-02,  6.6670e-02,  2.4592e-03],\n",
      "          [ 3.6954e-02,  1.4112e-02,  7.2606e-02],\n",
      "          [ 7.5322e-02, -2.8320e-02, -3.9340e-02]],\n",
      "\n",
      "         [[-5.0857e-02, -1.2942e-01, -2.7496e-03],\n",
      "          [-1.4060e-01, -1.8863e-01, -5.9909e-02],\n",
      "          [-6.2143e-02, -1.4408e-01,  1.4276e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.0426e-02, -1.0425e-01, -7.4805e-03],\n",
      "          [-5.0222e-02, -2.0485e-01, -1.3120e-01],\n",
      "          [ 1.2960e-01,  1.1312e-01, -6.4703e-02]],\n",
      "\n",
      "         [[-6.4745e-02, -4.4534e-03, -9.4135e-02],\n",
      "          [-5.1483e-02,  1.7306e-01, -6.9625e-02],\n",
      "          [ 5.5158e-02,  4.2527e-02, -4.4133e-03]],\n",
      "\n",
      "         [[-8.7738e-02, -1.4251e-02, -1.1715e-01],\n",
      "          [-4.2445e-02, -1.0592e-01, -1.9064e-01],\n",
      "          [ 8.3929e-02,  7.7866e-02,  8.2094e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4340e-01,  1.1277e-01,  5.5940e-03],\n",
      "          [-3.0396e-02,  4.2797e-02, -7.2887e-02],\n",
      "          [ 1.0814e-01,  1.4802e-01, -1.3846e-02]],\n",
      "\n",
      "         [[ 5.2776e-02,  9.6580e-02, -6.8554e-02],\n",
      "          [ 3.8198e-01,  2.1804e-01,  1.8465e-01],\n",
      "          [-2.2243e-01, -1.5402e-01, -2.3030e-01]],\n",
      "\n",
      "         [[ 9.9166e-02, -4.8675e-02,  1.0107e-01],\n",
      "          [ 5.4469e-02,  2.4618e-02,  4.5669e-02],\n",
      "          [-9.9300e-02,  6.6064e-02, -9.0739e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0463e-01,  6.6432e-02,  3.4125e-03],\n",
      "          [ 3.9948e-03,  1.9461e-01,  1.2335e-02],\n",
      "          [ 5.3210e-02,  1.9462e-01,  1.0495e-01]],\n",
      "\n",
      "         [[-1.2032e-01, -1.3170e-04, -8.2871e-02],\n",
      "          [ 1.0835e-01, -1.4311e-02, -8.4619e-02],\n",
      "          [-1.8151e-02, -1.0126e-01, -6.9328e-02]],\n",
      "\n",
      "         [[ 1.4754e-01,  4.4927e-02, -2.0501e-01],\n",
      "          [-7.4117e-02, -3.8085e-02, -1.6963e-01],\n",
      "          [-9.1811e-02, -1.2933e-02, -1.6990e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.9828e-02, -3.0172e-02,  4.5497e-02],\n",
      "          [ 1.6295e-01,  1.1592e-01,  5.4510e-02],\n",
      "          [ 7.7522e-02,  8.9341e-02,  5.9093e-03]],\n",
      "\n",
      "         [[-3.2998e-02, -1.0768e-01,  1.4430e-01],\n",
      "          [ 4.4946e-02, -1.2967e-01,  5.4693e-02],\n",
      "          [-7.1073e-02, -1.2688e-01, -9.9004e-04]],\n",
      "\n",
      "         [[-3.7495e-02, -6.1301e-02, -5.4111e-02],\n",
      "          [-8.8538e-02, -1.4798e-01, -1.4631e-01],\n",
      "          [-4.3679e-02, -1.0115e-01, -2.0299e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.2993e-02,  1.1450e-01,  1.8419e-01],\n",
      "          [ 1.0982e-01, -6.6783e-02, -3.5605e-03],\n",
      "          [-1.3280e-01, -4.9773e-02, -5.3685e-03]],\n",
      "\n",
      "         [[-1.1199e-01, -1.7870e-02, -6.2496e-02],\n",
      "          [ 1.5040e-02,  5.1426e-03, -1.7047e-03],\n",
      "          [ 2.7536e-02,  3.2812e-02,  5.4682e-02]],\n",
      "\n",
      "         [[ 2.2337e-01,  2.6839e-02, -4.8073e-02],\n",
      "          [ 1.0258e-01,  3.1376e-02, -5.4478e-02],\n",
      "          [-2.7655e-02, -1.0882e-01, -8.0135e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0562e-01,  1.6573e-01,  2.9849e-01],\n",
      "          [ 2.1932e-01,  9.9648e-02,  2.5494e-01],\n",
      "          [ 2.2588e-01,  1.9676e-01,  1.4557e-01]],\n",
      "\n",
      "         [[-5.3164e-02, -7.2637e-02, -4.8837e-02],\n",
      "          [-5.3502e-02, -4.7183e-02, -3.7362e-02],\n",
      "          [-3.1520e-02, -4.4656e-02,  5.6459e-02]],\n",
      "\n",
      "         [[-8.5155e-06, -2.5836e-02, -4.7716e-02],\n",
      "          [-1.7195e-01, -1.1149e-01, -5.7472e-02],\n",
      "          [-1.4635e-01, -1.5397e-02, -6.7162e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.3851e-02, -8.0660e-03,  3.4922e-02],\n",
      "          [-2.6560e-02, -4.4742e-02,  1.1295e-01],\n",
      "          [-3.0578e-03,  5.0902e-02,  2.3202e-02]],\n",
      "\n",
      "         [[ 3.7603e-02,  1.5068e-02,  1.1567e-02],\n",
      "          [ 3.5002e-03,  1.5338e-02,  3.3078e-02],\n",
      "          [ 1.0459e-01,  5.3448e-03,  1.6498e-01]],\n",
      "\n",
      "         [[ 3.5195e-02,  1.9011e-02, -4.2768e-02],\n",
      "          [ 5.5202e-02,  6.9494e-02,  4.3926e-02],\n",
      "          [-6.8030e-02,  4.4379e-02,  6.4735e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0067e-01, -8.4407e-02, -3.4908e-02],\n",
      "          [-5.6441e-02, -5.1928e-04, -8.3111e-02],\n",
      "          [ 3.5069e-02,  5.9770e-02, -6.6573e-02]],\n",
      "\n",
      "         [[ 4.5865e-02,  8.7347e-02, -2.5494e-02],\n",
      "          [ 6.8923e-02, -1.0468e-02,  1.0041e-03],\n",
      "          [ 5.1985e-02,  1.1928e-02, -3.5464e-02]],\n",
      "\n",
      "         [[-5.3967e-02, -1.0131e-01,  2.9840e-02],\n",
      "          [-5.9408e-02, -2.2586e-01, -1.1195e-01],\n",
      "          [-1.1900e-01, -5.8801e-02, -7.6145e-02]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.0771, -0.1722, -0.0193, -0.1231, -0.1573, -0.1233, -0.1109, -0.1175,\n",
      "        -0.0643, -0.1077, -0.2765, -0.0328, -0.1380, -0.1781, -0.0738, -0.0931,\n",
      "        -0.1107, -0.1006, -0.0087, -0.0949, -0.1747, -0.0663, -0.0028, -0.0224,\n",
      "        -0.1011, -0.0893, -0.0898, -0.0892, -0.1270, -0.0547, -0.0148, -0.0153,\n",
      "        -0.0004, -0.0540,  0.0014, -0.0670,  0.0032, -0.0073, -0.0618, -0.1172,\n",
      "        -0.1341, -0.0674, -0.1100, -0.1081, -0.1027, -0.0136, -0.0826, -0.1388],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.7452, -0.4614, -0.9732, -0.5632, -0.4272, -0.6645, -0.6547, -0.5296,\n",
      "        -0.7521, -0.7261, -0.4353, -0.2425, -0.5294, -0.4705, -0.7029, -0.5944,\n",
      "        -0.9825, -0.8394, -0.7006, -0.5789, -0.9276, -0.5445, -0.6761, -0.4957,\n",
      "        -0.6695, -0.5678, -0.8573, -0.6112, -0.5642, -0.6069, -0.6275, -0.7659,\n",
      "        -0.6143, -0.7128, -0.5577, -0.7317, -0.6254, -0.5058, -0.5688, -0.6440,\n",
      "        -0.5231, -0.4803, -0.7573, -0.4457, -0.4713, -0.6017, -0.5627, -0.5168],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([0.4327, 0.4864, 0.5201, 0.5317, 0.4727, 0.5323, 0.4952, 0.6216, 0.3962,\n",
      "        0.4628, 0.4814, 0.4449, 0.6615, 0.4950, 0.4458, 0.5224, 0.7510, 0.5628,\n",
      "        0.3219, 0.6292, 0.9001, 0.5661, 0.3108, 0.5593, 0.5496, 0.5867, 0.6321,\n",
      "        0.5800, 0.6937, 0.5924, 0.2911, 0.3531, 0.3178, 0.4182, 0.2175, 0.4813,\n",
      "        0.2826, 0.2305, 0.4924, 0.7562, 0.6295, 0.6081, 0.5078, 0.6046, 0.6596,\n",
      "        0.5337, 0.6557, 0.4768], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([[-0.0123,  0.1060,  0.0280,  ..., -0.0740, -0.0042, -0.0294],\n",
      "        [-0.0231,  0.0213, -0.0042,  ..., -0.0621, -0.1027, -0.0919],\n",
      "        [ 0.0090, -0.0443, -0.0072,  ..., -0.0407, -0.0364, -0.0287],\n",
      "        [ 0.0148, -0.0008, -0.0945,  ..., -0.0963, -0.0290, -0.0130],\n",
      "        [ 0.0244,  0.0005, -0.0111,  ...,  0.1707,  0.1722,  0.0834]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([ 0.0759, -0.0317,  0.1158,  0.1191,  0.0614], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "tensor([[[[ 2.0605e-01,  1.8848e-01,  2.1033e-01],\n",
      "          [ 1.6457e-01,  1.7608e-01,  2.1742e-01],\n",
      "          [ 1.5973e-01,  1.5694e-01,  1.4762e-01]],\n",
      "\n",
      "         [[ 1.7010e-01,  1.7861e-01,  2.0446e-01],\n",
      "          [ 1.3737e-01,  1.6922e-01,  2.1751e-01],\n",
      "          [ 1.1476e-01,  1.3509e-01,  1.3312e-01]],\n",
      "\n",
      "         [[ 1.4489e-03,  2.8734e-02,  4.6219e-02],\n",
      "          [-1.8815e-02,  1.2445e-02,  4.5535e-02],\n",
      "          [-2.9986e-02, -3.2158e-03, -1.1198e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6369e-01,  1.1473e-01,  1.2589e-01],\n",
      "          [ 1.3495e-01,  9.3856e-02,  1.2157e-01],\n",
      "          [ 1.8478e-01,  1.3272e-01,  2.0966e-01]],\n",
      "\n",
      "         [[ 6.7276e-02,  5.1448e-02,  5.4986e-02],\n",
      "          [ 5.0561e-02,  3.5970e-02,  4.9242e-02],\n",
      "          [ 9.1371e-02,  6.6519e-02,  1.3214e-01]],\n",
      "\n",
      "         [[-4.8959e-02, -3.1048e-02, -4.4303e-02],\n",
      "          [-6.7861e-02, -3.3217e-02, -4.7995e-02],\n",
      "          [-2.7933e-02, -2.0673e-03,  1.2609e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.5932e-02,  3.7034e-02,  4.5633e-02],\n",
      "          [ 5.8938e-02,  3.5563e-02,  4.0729e-02],\n",
      "          [ 5.7894e-02,  3.2003e-02,  3.3548e-02]],\n",
      "\n",
      "         [[ 3.6106e-02,  2.3000e-02,  3.1138e-02],\n",
      "          [ 3.8216e-02,  2.2310e-02,  2.8144e-02],\n",
      "          [ 3.9712e-02,  2.1605e-02,  2.4723e-02]],\n",
      "\n",
      "         [[ 1.2126e-02,  7.7073e-03,  1.2399e-02],\n",
      "          [ 1.2760e-02,  8.6436e-03,  1.2262e-02],\n",
      "          [ 1.5517e-02,  1.1258e-02,  1.4007e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.5884e-04,  2.2394e-03,  3.8294e-03],\n",
      "          [ 1.1816e-03,  1.0087e-03,  2.6059e-03],\n",
      "          [ 2.4771e-03,  2.4468e-03,  1.8159e-03]],\n",
      "\n",
      "         [[ 1.7726e-03,  3.1962e-03,  5.0854e-03],\n",
      "          [ 2.7764e-03,  7.4100e-04,  2.1041e-03],\n",
      "          [ 5.1376e-03,  2.6757e-03,  1.2247e-03]],\n",
      "\n",
      "         [[ 7.1563e-03,  6.3698e-03,  7.6499e-03],\n",
      "          [ 7.4796e-03,  4.1738e-03,  4.8207e-03],\n",
      "          [ 9.4683e-03,  6.0125e-03,  4.2246e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0836e-03,  1.4006e-02,  1.1519e-02],\n",
      "          [ 9.0275e-03,  1.3552e-02,  9.3189e-03],\n",
      "          [ 1.7274e-02,  1.7736e-02,  1.3368e-02]],\n",
      "\n",
      "         [[-6.3491e-03,  3.7032e-03,  3.7166e-03],\n",
      "          [-2.4859e-03, -1.3853e-03, -4.0085e-03],\n",
      "          [ 3.0775e-03, -1.1170e-04, -4.0403e-03]],\n",
      "\n",
      "         [[-1.4835e-02, -1.4734e-02, -1.5931e-02],\n",
      "          [-1.6386e-02, -2.2103e-02, -2.6743e-02],\n",
      "          [-1.4846e-02, -2.4549e-02, -3.0167e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.8515e-03, -3.1604e-03,  1.2016e-02],\n",
      "          [-5.6126e-03, -6.3232e-04,  1.9127e-02],\n",
      "          [ 4.5310e-03,  1.6659e-02,  3.3353e-02]],\n",
      "\n",
      "         [[-9.2864e-04,  4.3846e-03,  1.2736e-02],\n",
      "          [-6.5196e-05,  9.8421e-03,  2.3661e-02],\n",
      "          [ 7.2164e-03,  2.1266e-02,  3.1556e-02]],\n",
      "\n",
      "         [[-4.0535e-03,  1.8376e-03, -4.0917e-04],\n",
      "          [-1.3160e-03,  8.6565e-03,  1.0576e-02],\n",
      "          [ 4.0202e-03,  1.4138e-02,  1.4000e-02]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-0.0155, -0.0048,  0.0158,  0.0342,  0.0234,  0.0005,  0.0148,  0.0231,\n",
      "         0.0060,  0.0060,  0.0011, -0.0326,  0.0008,  0.0055,  0.0259, -0.0091,\n",
      "        -0.0006, -0.0120, -0.0066, -0.0138,  0.0153, -0.0070, -0.0566, -0.0032,\n",
      "         0.0151,  0.0219,  0.0037,  0.0110,  0.0080, -0.0145,  0.0012, -0.0039,\n",
      "         0.0296, -0.0002, -0.0102, -0.0170,  0.0015,  0.0061, -0.0167,  0.0277,\n",
      "         0.0025, -0.0245, -0.0018,  0.0081,  0.0118, -0.0048,  0.0133,  0.0041],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 1.4570e-02,  1.7866e-02,  1.7700e-02],\n",
      "          [ 1.3123e-02,  1.8958e-02,  1.3435e-02],\n",
      "          [ 1.0976e-02,  2.3147e-02,  1.8009e-02]],\n",
      "\n",
      "         [[ 1.7542e-02,  1.9880e-02,  2.2924e-02],\n",
      "          [ 2.3381e-02,  3.2664e-02,  2.1778e-02],\n",
      "          [ 3.2460e-02,  2.9342e-02,  2.2262e-02]],\n",
      "\n",
      "         [[ 3.8547e-04, -7.7770e-04, -1.2201e-04],\n",
      "          [ 7.5098e-04,  2.6039e-04,  8.0874e-04],\n",
      "          [ 5.0146e-04, -4.7405e-04,  7.1674e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.5648e-04,  9.2534e-05, -3.4623e-04],\n",
      "          [ 4.4900e-04, -3.7032e-04, -1.2337e-03],\n",
      "          [ 1.8537e-03,  8.2838e-04, -4.3024e-04]],\n",
      "\n",
      "         [[ 6.2033e-03,  4.2853e-03,  3.8019e-03],\n",
      "          [ 4.8392e-03,  4.2731e-03,  4.1137e-03],\n",
      "          [ 3.6890e-03,  3.3701e-03,  2.5775e-03]],\n",
      "\n",
      "         [[ 5.0382e-03,  4.1560e-03,  3.0363e-03],\n",
      "          [ 3.6012e-03,  2.6851e-03,  2.2547e-03],\n",
      "          [ 2.8224e-03,  4.0035e-03,  2.3768e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0677e-02, -1.2657e-02,  3.2481e-04],\n",
      "          [-1.4579e-02, -1.3131e-02, -3.1369e-03],\n",
      "          [-1.8887e-02, -1.6559e-02, -3.5275e-03]],\n",
      "\n",
      "         [[-1.2196e-02, -8.5588e-03, -8.2124e-03],\n",
      "          [-1.8990e-02, -1.6486e-02, -1.2125e-02],\n",
      "          [-1.6530e-02, -8.9672e-03, -1.3048e-02]],\n",
      "\n",
      "         [[-2.6676e-04, -9.7445e-05,  1.4847e-04],\n",
      "          [-3.7836e-04, -2.0725e-04,  1.1911e-04],\n",
      "          [-4.2398e-04, -3.0672e-04, -2.2138e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.1095e-04,  8.7913e-04,  1.2860e-03],\n",
      "          [-1.0312e-03,  6.6980e-04,  8.7352e-04],\n",
      "          [-1.0129e-03,  6.1514e-04,  3.3064e-04]],\n",
      "\n",
      "         [[-1.3757e-03, -2.1957e-03, -2.9121e-03],\n",
      "          [-3.1608e-03, -2.1166e-03, -1.3338e-03],\n",
      "          [-1.5134e-03, -2.1552e-03, -1.9886e-03]],\n",
      "\n",
      "         [[-3.2437e-03, -1.1894e-03, -6.1607e-04],\n",
      "          [-3.9511e-03, -2.8398e-04, -2.9860e-05],\n",
      "          [-4.9849e-03, -5.6867e-04, -2.1631e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5496e-02,  5.9879e-03,  1.0982e-02],\n",
      "          [ 1.2678e-02,  1.1756e-02,  1.2688e-02],\n",
      "          [ 1.3867e-02,  1.1969e-02,  1.2830e-02]],\n",
      "\n",
      "         [[ 1.8186e-02,  1.7460e-02,  1.9495e-02],\n",
      "          [ 1.8299e-02,  2.1220e-02,  2.1961e-02],\n",
      "          [ 1.4982e-02,  2.1454e-02,  2.0248e-02]],\n",
      "\n",
      "         [[ 1.1712e-03,  1.0684e-03,  1.3708e-03],\n",
      "          [ 1.0173e-03,  7.4978e-04,  8.5624e-04],\n",
      "          [ 7.8325e-04,  3.2369e-04, -3.0096e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1923e-03, -1.4303e-03, -1.2299e-03],\n",
      "          [-5.3954e-05, -9.8220e-04, -7.8702e-04],\n",
      "          [ 1.3194e-03,  9.3378e-04,  2.4461e-04]],\n",
      "\n",
      "         [[ 4.4408e-03,  4.0840e-03,  3.7714e-03],\n",
      "          [ 1.7432e-03,  6.0164e-04,  1.3796e-03],\n",
      "          [ 6.9834e-04, -3.3522e-03, -3.0221e-03]],\n",
      "\n",
      "         [[ 3.1316e-04, -4.7064e-05, -1.4506e-04],\n",
      "          [ 9.1561e-04,  1.2858e-04, -8.5266e-04],\n",
      "          [ 4.6104e-05, -9.5373e-04, -3.5146e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7.3415e-03,  8.1457e-03,  5.3000e-03],\n",
      "          [ 4.2943e-03,  4.8456e-03,  8.0654e-03],\n",
      "          [ 6.2106e-03,  1.2134e-02,  1.0301e-02]],\n",
      "\n",
      "         [[ 1.5389e-03,  3.2771e-03,  2.5141e-03],\n",
      "          [ 6.1411e-04,  2.9157e-03,  3.7743e-04],\n",
      "          [ 5.5527e-03,  4.8651e-03,  3.9147e-03]],\n",
      "\n",
      "         [[ 1.5186e-04,  5.7316e-04,  3.9101e-04],\n",
      "          [-9.5279e-05,  2.6849e-04,  2.5469e-04],\n",
      "          [ 3.7312e-05,  4.8998e-04,  3.7524e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.7353e-04, -1.2851e-03, -1.6699e-03],\n",
      "          [-1.7781e-03, -2.1776e-03, -2.3516e-03],\n",
      "          [-1.3747e-03, -2.1190e-03, -1.8491e-03]],\n",
      "\n",
      "         [[ 9.5982e-04, -4.4463e-04,  6.4009e-04],\n",
      "          [ 1.0208e-03,  1.0643e-03,  1.7191e-03],\n",
      "          [ 1.2066e-03,  1.6726e-03,  2.3177e-03]],\n",
      "\n",
      "         [[ 1.3327e-03,  2.7177e-04, -7.7271e-04],\n",
      "          [ 4.8552e-04, -2.8208e-04, -6.4804e-04],\n",
      "          [ 6.5881e-04, -2.0149e-04, -2.9215e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6627e-02,  4.4580e-03,  2.7269e-03],\n",
      "          [ 1.5254e-02,  4.8118e-03,  2.7211e-03],\n",
      "          [ 1.6577e-02,  4.8773e-03,  2.0961e-03]],\n",
      "\n",
      "         [[ 9.7888e-04, -2.4013e-03,  6.7865e-03],\n",
      "          [-2.0271e-03, -8.5981e-03,  1.1193e-03],\n",
      "          [-4.2429e-04,  4.6223e-03, -1.9376e-03]],\n",
      "\n",
      "         [[ 1.2865e-03,  1.5558e-03,  1.5992e-03],\n",
      "          [ 1.3627e-03,  1.1314e-03,  1.7632e-03],\n",
      "          [ 7.1952e-04,  4.5643e-04,  1.0951e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.1250e-04,  1.6429e-03,  2.6104e-03],\n",
      "          [ 1.3226e-03,  2.2654e-03,  3.1011e-03],\n",
      "          [ 9.7225e-04,  2.1339e-03,  3.1030e-03]],\n",
      "\n",
      "         [[ 1.1939e-03,  1.1294e-03,  7.0440e-04],\n",
      "          [ 3.1171e-03,  3.4026e-03, -1.0038e-03],\n",
      "          [ 6.4510e-04,  1.9277e-03,  5.5734e-04]],\n",
      "\n",
      "         [[ 2.5053e-03,  2.6694e-03,  3.1681e-03],\n",
      "          [ 7.7447e-04,  9.7398e-04,  6.3928e-04],\n",
      "          [ 2.1276e-04,  3.2160e-04, -2.5405e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.0511e-02, -2.3468e-03, -3.9885e-03],\n",
      "          [-1.0212e-02, -3.8377e-03, -1.8436e-03],\n",
      "          [-9.7983e-03, -1.0222e-02, -5.0620e-03]],\n",
      "\n",
      "         [[-5.2184e-03, -5.4628e-03,  9.9184e-04],\n",
      "          [-1.4639e-02, -1.4499e-02, -1.0348e-02],\n",
      "          [-1.5743e-02, -1.0518e-02, -9.2158e-03]],\n",
      "\n",
      "         [[ 2.1260e-04,  1.7305e-04,  6.0780e-04],\n",
      "          [-1.3249e-03, -1.0659e-03, -7.1539e-04],\n",
      "          [-6.5576e-04, -9.8288e-04, -1.2847e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8316e-04, -5.0972e-04, -1.0207e-04],\n",
      "          [ 5.7872e-04,  7.8624e-05,  1.6847e-04],\n",
      "          [-4.2600e-04, -1.2081e-03, -5.0776e-04]],\n",
      "\n",
      "         [[-9.5087e-04, -1.5009e-04, -7.5812e-04],\n",
      "          [-1.9837e-03, -2.2011e-04, -1.2302e-03],\n",
      "          [ 8.3555e-04,  1.3144e-03,  1.1579e-03]],\n",
      "\n",
      "         [[ 1.7877e-03,  1.2084e-03,  6.2045e-04],\n",
      "          [ 1.7390e-03,  2.2544e-03,  1.7469e-03],\n",
      "          [ 9.2863e-04,  1.3756e-03,  6.0869e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 0.0375, -0.0106,  0.0134,  0.0276,  0.0170,  0.0051,  0.0319,  0.0669,\n",
      "         0.0259, -0.0224,  0.0058, -0.0338,  0.0222, -0.0342,  0.0073,  0.0302,\n",
      "         0.0060, -0.0375, -0.0087, -0.0279,  0.0560,  0.0235,  0.0270, -0.0184,\n",
      "         0.0138,  0.0290,  0.0598, -0.0337, -0.0019, -0.0019,  0.0306, -0.0318,\n",
      "        -0.0011,  0.0086, -0.0015, -0.0046, -0.0189,  0.0092, -0.0140,  0.0178,\n",
      "        -0.0051,  0.0223,  0.0270, -0.0400, -0.0460,  0.0198,  0.0397,  0.0454],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([ 0.0782, -0.0221,  0.0316,  0.0231, -0.0071, -0.0091,  0.0110,  0.1708,\n",
      "         0.0333, -0.0248, -0.0539, -0.0682,  0.0646, -0.0564, -0.0187,  0.0444,\n",
      "        -0.0008, -0.0282, -0.0008,  0.0021,  0.0636, -0.0008,  0.0189, -0.1228,\n",
      "         0.0338,  0.0142,  0.0411, -0.0624,  0.0078,  0.0015,  0.0182, -0.0282,\n",
      "         0.0070,  0.0281,  0.0296, -0.0736, -0.0089,  0.0170,  0.0009,  0.0373,\n",
      "        -0.0280,  0.0228,  0.0270, -0.0333, -0.0577,  0.0278,  0.0655,  0.0163],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([-1.5716e-09,  1.2806e-09,  1.3970e-09, -2.3283e-09,  4.6566e-10,\n",
      "         2.3283e-10,  4.6566e-10, -9.3132e-10, -9.6043e-10, -9.3132e-10,\n",
      "         6.5193e-09,  4.7730e-09, -1.6298e-09, -2.3283e-10,  2.7940e-09,\n",
      "        -6.4028e-09,  1.1642e-09, -2.3283e-09,  3.7253e-09,  6.9849e-10,\n",
      "         5.4715e-09,  2.7940e-09,  1.0477e-09, -5.1223e-09,  2.0955e-09,\n",
      "        -2.7940e-09, -2.7940e-09, -1.3970e-09, -2.3283e-09,  1.3970e-09,\n",
      "         2.3283e-09, -1.6298e-09, -2.3283e-09, -1.6298e-09, -7.2177e-09,\n",
      "         4.6566e-10, -1.1642e-10,  3.6671e-09, -4.4238e-09, -2.0955e-09,\n",
      "         2.7940e-09,  2.3283e-09,  2.3283e-10,  1.1642e-09,  2.9104e-09,\n",
      "         6.9849e-10, -1.1642e-09,  1.3970e-09], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-0.0460,  0.0176,  0.0416, -0.0063,  0.0231, -0.0091,  0.0152, -0.0968,\n",
      "         0.0096, -0.0063,  0.0502,  0.0208,  0.0168,  0.0290,  0.0147, -0.0792,\n",
      "         0.0040,  0.0449,  0.0653,  0.0221,  0.0610,  0.0519,  0.0185,  0.0476,\n",
      "         0.0069, -0.1009,  0.0438,  0.0043, -0.0044,  0.0126,  0.0307, -0.0783,\n",
      "         0.0361,  0.0175, -0.0091,  0.0053, -0.0162,  0.0476,  0.0491,  0.0343,\n",
      "        -0.0113, -0.0178,  0.0117, -0.0219, -0.0281,  0.0068,  0.0125,  0.0357],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([-0.0761,  0.0099,  0.0756, -0.0126,  0.0959, -0.0124,  0.0165, -0.1930,\n",
      "        -0.0062, -0.0010,  0.0760,  0.0408,  0.0050,  0.0349,  0.0084, -0.1793,\n",
      "         0.0172,  0.0230,  0.1081,  0.0369,  0.1806,  0.0575,  0.0114,  0.0110,\n",
      "         0.0059, -0.0973,  0.0796,  0.0056, -0.0123,  0.0217,  0.0577, -0.0815,\n",
      "         0.0304, -0.0173, -0.0114, -0.0045, -0.0354,  0.0171,  0.0573,  0.0221,\n",
      "        -0.0745, -0.0244,  0.0075, -0.0303, -0.0684, -0.0274, -0.0097,  0.0310],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 2.9535e-05,  4.3092e-04,  9.4959e-05],\n",
      "          [ 9.3410e-05,  3.2603e-05,  2.7927e-04],\n",
      "          [-1.1500e-04, -2.5491e-06, -7.0966e-05]],\n",
      "\n",
      "         [[-3.2149e-05,  3.4612e-04, -1.1235e-04],\n",
      "          [-6.8932e-06,  5.8680e-05, -1.0152e-04],\n",
      "          [ 9.3882e-05,  2.1195e-04,  7.4284e-05]],\n",
      "\n",
      "         [[-1.1298e-04,  1.6718e-04,  7.5825e-05],\n",
      "          [-2.1505e-04, -7.9713e-05,  5.8837e-05],\n",
      "          [-2.0072e-04, -2.6668e-04, -1.0391e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.2049e-04,  6.0716e-04, -6.0987e-05],\n",
      "          [ 4.4250e-04,  8.4204e-04,  7.5126e-04],\n",
      "          [-6.8656e-08,  4.7909e-05,  3.7434e-05]],\n",
      "\n",
      "         [[ 1.7066e-04,  1.2097e-04,  9.7927e-04],\n",
      "          [-2.3298e-04, -2.4649e-04, -2.3336e-04],\n",
      "          [ 2.1159e-04,  2.4167e-04,  2.0694e-04]],\n",
      "\n",
      "         [[ 4.9007e-05, -1.2265e-05, -2.3188e-04],\n",
      "          [-7.4744e-04, -2.8004e-04, -6.3960e-04],\n",
      "          [-6.9289e-05,  1.3778e-04,  1.7883e-06]]],\n",
      "\n",
      "\n",
      "        [[[-2.7299e-02, -2.5258e-02, -4.2900e-02],\n",
      "          [-2.1921e-02, -1.8528e-02, -3.4416e-02],\n",
      "          [-1.6446e-02, -3.0147e-02, -2.4474e-02]],\n",
      "\n",
      "         [[ 3.8002e-03, -4.1002e-03, -1.4926e-02],\n",
      "          [ 1.0807e-05,  1.1713e-02,  1.0595e-02],\n",
      "          [ 1.1471e-02,  1.1202e-02,  2.3176e-02]],\n",
      "\n",
      "         [[ 4.0455e-03,  5.1620e-04, -1.3743e-02],\n",
      "          [-3.8468e-03, -1.4560e-02, -1.7014e-02],\n",
      "          [ 2.4507e-03,  5.8373e-03,  1.4454e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.0171e-03, -8.3233e-03,  6.1590e-03],\n",
      "          [-3.2063e-02, -6.3268e-03, -1.0045e-02],\n",
      "          [-1.6340e-02,  6.0401e-03, -1.9876e-02]],\n",
      "\n",
      "         [[-4.0323e-02, -2.2377e-02, -2.0025e-02],\n",
      "          [-3.2721e-02, -2.7800e-02, -2.8232e-02],\n",
      "          [-3.8597e-02, -2.9462e-02, -3.3520e-02]],\n",
      "\n",
      "         [[-1.2505e-02, -2.6215e-02, -8.0298e-03],\n",
      "          [-1.2957e-02, -4.7295e-03, -2.8479e-03],\n",
      "          [-9.1967e-03, -1.2117e-02, -1.6837e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9511e-04,  9.8408e-04,  3.9282e-04],\n",
      "          [ 2.3591e-03,  1.8156e-03,  3.2957e-03],\n",
      "          [ 4.8019e-05, -3.6518e-04, -8.0572e-04]],\n",
      "\n",
      "         [[-1.1135e-03, -1.7224e-03, -1.2692e-04],\n",
      "          [-2.6008e-03, -9.9882e-04, -5.8200e-04],\n",
      "          [-8.8491e-04, -1.5622e-03, -3.6324e-04]],\n",
      "\n",
      "         [[ 3.3577e-03,  3.3543e-03,  4.5739e-03],\n",
      "          [ 3.5427e-03,  1.0802e-04,  1.6522e-03],\n",
      "          [-5.5612e-04, -1.0122e-03, -1.0207e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0644e-04,  1.0799e-03, -8.6375e-05],\n",
      "          [ 6.7314e-04,  8.8279e-05,  8.6324e-04],\n",
      "          [ 4.9653e-04, -2.9390e-05, -6.5562e-04]],\n",
      "\n",
      "         [[ 1.1293e-03,  1.7670e-03, -9.8004e-05],\n",
      "          [ 1.5065e-03,  1.1327e-03, -1.8482e-04],\n",
      "          [-3.4686e-04, -1.5762e-04,  9.9267e-06]],\n",
      "\n",
      "         [[-6.4517e-05,  6.3423e-04,  2.5347e-04],\n",
      "          [-2.9781e-04, -1.5254e-04,  8.9364e-04],\n",
      "          [-4.7145e-04, -1.6857e-03, -1.2737e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.2775e-03, -1.5413e-03, -1.6228e-03],\n",
      "          [-7.4103e-04, -1.3604e-03, -4.4312e-04],\n",
      "          [ 2.7001e-05, -2.8768e-04, -9.2412e-04]],\n",
      "\n",
      "         [[ 1.3281e-03, -4.2836e-03,  7.0831e-04],\n",
      "          [-4.6392e-05, -5.3154e-03, -1.9814e-03],\n",
      "          [ 3.6781e-03, -5.0810e-03, -2.3031e-03]],\n",
      "\n",
      "         [[ 3.6960e-03, -1.5743e-03, -6.0061e-04],\n",
      "          [ 1.0470e-03, -2.4340e-03,  8.4209e-04],\n",
      "          [ 3.7595e-04, -1.1060e-03, -3.0743e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.9717e-04, -7.2802e-05, -1.2926e-03],\n",
      "          [-1.9682e-03, -1.5800e-03, -1.8192e-03],\n",
      "          [-3.7375e-04, -1.4992e-03, -1.3110e-03]],\n",
      "\n",
      "         [[ 1.8044e-03,  2.2550e-03,  3.2358e-03],\n",
      "          [ 1.3414e-03,  1.3944e-03, -6.9929e-05],\n",
      "          [ 1.1305e-03,  8.1650e-04,  4.2411e-04]],\n",
      "\n",
      "         [[-5.3542e-04, -1.7270e-03, -3.8286e-04],\n",
      "          [ 3.1086e-04, -1.2913e-03, -1.0434e-03],\n",
      "          [ 2.4291e-04, -5.8591e-04,  4.2589e-04]]],\n",
      "\n",
      "\n",
      "        [[[-8.2111e-03, -6.7760e-03, -2.0488e-03],\n",
      "          [-5.0022e-03, -7.4380e-03, -4.4201e-03],\n",
      "          [-1.7064e-03, -2.9525e-03, -2.1159e-03]],\n",
      "\n",
      "         [[-4.4925e-05, -6.9441e-03, -3.3791e-04],\n",
      "          [ 2.8367e-03, -7.8564e-03, -3.7093e-03],\n",
      "          [ 1.4738e-03, -4.8406e-03, -1.6781e-03]],\n",
      "\n",
      "         [[ 1.0088e-02,  2.4231e-03,  1.0153e-03],\n",
      "          [ 6.2593e-03,  5.3103e-03, -9.7185e-04],\n",
      "          [ 1.1503e-02,  2.9713e-03,  9.6796e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3938e-03, -5.1724e-03, -5.4690e-04],\n",
      "          [-4.9859e-03, -4.5386e-03, -4.7356e-03],\n",
      "          [-2.0405e-03, -1.2233e-03, -1.2884e-03]],\n",
      "\n",
      "         [[-6.9996e-03, -4.3749e-03, -4.8957e-03],\n",
      "          [-5.9166e-03, -7.4989e-03, -7.2475e-03],\n",
      "          [-1.6163e-03, -4.4729e-03, -3.5818e-03]],\n",
      "\n",
      "         [[ 1.4754e-03, -3.5137e-04,  4.2359e-03],\n",
      "          [ 7.2944e-04, -4.5652e-03, -3.2017e-04],\n",
      "          [ 1.8754e-03, -1.8301e-03,  1.1156e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.9994e-02, -6.4958e-03,  6.1299e-03],\n",
      "          [-1.7224e-02, -1.3893e-02, -1.6306e-02],\n",
      "          [-1.0692e-02, -2.0010e-02, -2.3748e-02]],\n",
      "\n",
      "         [[-9.4745e-03, -1.1497e-02,  2.5367e-02],\n",
      "          [ 1.8004e-03, -2.4186e-02,  2.0970e-02],\n",
      "          [-6.8975e-03, -5.6682e-03,  1.5043e-02]],\n",
      "\n",
      "         [[-2.1741e-02, -8.0177e-02, -2.0634e-02],\n",
      "          [-1.2525e-02, -3.7977e-02, -2.9505e-02],\n",
      "          [-1.5428e-02, -4.0089e-02, -6.7672e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1042e-02, -3.4789e-03, -7.1414e-03],\n",
      "          [ 1.6361e-02,  1.0771e-02,  1.4635e-02],\n",
      "          [ 1.7479e-02,  8.9833e-03, -3.3830e-03]],\n",
      "\n",
      "         [[ 2.9816e-02,  2.5584e-02,  1.9245e-02],\n",
      "          [ 2.9840e-02,  2.9174e-02,  1.6812e-02],\n",
      "          [ 4.0042e-03,  1.6106e-02,  2.0074e-02]],\n",
      "\n",
      "         [[-1.5656e-02,  9.9462e-04, -5.2482e-03],\n",
      "          [-1.6023e-02, -9.7555e-03,  1.5400e-02],\n",
      "          [ 3.6356e-03, -1.5175e-02,  1.3992e-02]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-1.4552e-10,  4.4703e-08,  1.6298e-09, -2.6543e-08, -4.6566e-09,\n",
      "         1.5134e-09, -1.3970e-09,  1.3970e-08,  3.4925e-10,  1.1642e-09,\n",
      "         8.3819e-09, -8.1491e-10,  4.3074e-09,  9.3132e-10,  1.8626e-09,\n",
      "         3.9581e-09,  2.4447e-09,  0.0000e+00,  8.7311e-11,  2.3283e-10,\n",
      "         3.2596e-09,  9.3132e-10, -1.2915e-10, -9.3132e-10, -2.3283e-09,\n",
      "         1.6298e-09,  2.0955e-09,  3.3760e-09, -2.4447e-09,  5.1223e-09,\n",
      "         1.4552e-11, -1.1642e-10, -8.0036e-11,  5.8208e-11, -2.5466e-11,\n",
      "         2.3283e-10, -2.9104e-11, -7.2760e-12, -1.8626e-09,  4.0745e-09,\n",
      "         1.8626e-09,  4.0745e-09,  0.0000e+00,  1.1176e-08,  7.6834e-09,\n",
      "         2.7940e-09,  4.6566e-10, -1.4901e-08], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-0.0031,  0.5535,  0.0237,  0.2638,  0.3809,  0.0785, -0.0019,  0.1640,\n",
      "         0.0033,  0.0453, -0.1920,  0.2375,  0.2597,  0.4052,  0.0522,  0.0800,\n",
      "         0.0365,  0.0082, -0.0073,  0.1024,  0.0957,  0.1922, -0.0074,  0.1992,\n",
      "         0.1017,  0.0923,  0.0743,  0.1253,  0.2362,  0.1901, -0.0067, -0.0087,\n",
      "        -0.0068,  0.0021, -0.0062, -0.0119, -0.0070, -0.0070,  0.0490,  0.1332,\n",
      "         0.1620,  0.2078,  0.0356,  0.2620,  0.1837,  0.0993,  0.1052, -0.4937],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([-0.0047,  0.7695,  0.0495,  0.4225,  0.5251,  0.1243, -0.0036,  0.2165,\n",
      "         0.0091,  0.0791, -0.1899,  0.3382,  0.3800,  0.6179,  0.0949,  0.1468,\n",
      "         0.0649,  0.0055, -0.0020,  0.1457,  0.1284,  0.2944, -0.0052,  0.2984,\n",
      "         0.1921,  0.1365,  0.1244,  0.2077,  0.3038,  0.2782, -0.0041, -0.0056,\n",
      "        -0.0042,  0.0042, -0.0037, -0.0173, -0.0037, -0.0047,  0.0596,  0.2126,\n",
      "         0.2018,  0.3174,  0.0605,  0.3708,  0.2531,  0.1477,  0.1383, -0.6982],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([[-3.5416e-04, -5.4801e-04, -6.6172e-04,  ...,  7.2640e-04,\n",
      "          2.0330e-03,  6.7962e-04],\n",
      "        [ 3.4549e-04,  3.0717e-04,  4.2256e-04,  ...,  8.2290e-04,\n",
      "          9.8606e-04,  1.3818e-03],\n",
      "        [-8.4791e-05, -9.2060e-06,  2.6016e-05,  ...,  3.7125e-03,\n",
      "          9.0847e-03,  1.7587e-03],\n",
      "        [-6.6787e-04, -6.7742e-04, -7.7555e-04,  ..., -2.1871e-03,\n",
      "          1.5423e-03, -4.3486e-03],\n",
      "        [ 7.6132e-04,  9.2747e-04,  9.8869e-04,  ..., -3.0748e-03,\n",
      "         -1.3646e-02,  5.2847e-04]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([ 0.1395, -0.1944,  0.0875,  0.1547, -0.1873], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p ==  Parameter containing:\n",
      "tensor([[[[-1.5707e-03, -2.5832e-01,  1.7820e-01],\n",
      "          [-1.3748e-01, -6.9147e-02,  3.1331e-02],\n",
      "          [-6.8982e-02,  2.6458e-01, -8.9069e-03]],\n",
      "\n",
      "         [[ 1.4270e-01, -2.2754e-01,  1.8307e-01],\n",
      "          [-6.8883e-02,  3.6089e-02,  9.6674e-02],\n",
      "          [-1.5392e-01,  9.6749e-02, -9.6732e-02]],\n",
      "\n",
      "         [[ 2.0977e-01, -1.2438e-01, -8.0217e-02],\n",
      "          [ 1.0298e-01,  1.4493e-01, -1.7005e-01],\n",
      "          [-8.3693e-02,  1.1976e-01, -1.6073e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7137e-02,  3.3717e-01,  2.0318e-02],\n",
      "          [ 1.2752e-01, -9.1033e-02,  1.9680e-02],\n",
      "          [-1.5948e-01, -2.3552e-01, -6.1447e-02]],\n",
      "\n",
      "         [[-1.4350e-01,  1.4975e-01, -1.0675e-01],\n",
      "          [ 1.1053e-01, -1.4911e-01, -8.7883e-02],\n",
      "          [ 1.5145e-01, -1.5747e-01,  2.0006e-01]],\n",
      "\n",
      "         [[-1.5631e-01,  2.0872e-01, -1.8696e-01],\n",
      "          [-6.2811e-04,  6.4432e-02,  1.5282e-01],\n",
      "          [ 4.3215e-03, -1.0579e-01,  4.0867e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.2504e-04, -3.1052e-03,  1.2996e-01],\n",
      "          [ 4.3149e-02, -5.5917e-02, -4.1076e-02],\n",
      "          [ 7.5065e-02, -3.5752e-02, -1.8983e-01]],\n",
      "\n",
      "         [[-8.9421e-02, -4.6242e-02,  1.3393e-01],\n",
      "          [ 2.6513e-01,  7.3267e-05,  5.3320e-02],\n",
      "          [-1.5255e-01, -1.6220e-01,  3.9530e-02]],\n",
      "\n",
      "         [[ 6.2313e-02, -2.4308e-01,  3.2952e-02],\n",
      "          [ 2.2360e-01, -6.3872e-02, -1.7884e-01],\n",
      "          [-1.4942e-01, -7.7107e-02, -5.1498e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.5035e-02, -5.0993e-02, -5.4545e-02],\n",
      "          [-5.6864e-02, -3.5889e-01, -2.8949e-01],\n",
      "          [ 3.6489e-02, -1.9244e-01, -2.6829e-01]],\n",
      "\n",
      "         [[-1.0810e-01, -1.2368e-02,  5.3940e-02],\n",
      "          [-1.6190e-01, -1.6227e-01, -1.0322e-01],\n",
      "          [-3.1655e-02, -1.7322e-01, -2.0168e-01]],\n",
      "\n",
      "         [[ 2.0976e-01,  5.9735e-02,  1.7177e-01],\n",
      "          [ 9.6081e-02,  2.7967e-02,  1.1061e-01],\n",
      "          [-2.4330e-02, -1.2533e-01,  9.1291e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.9813e-02, -8.1347e-02, -1.2539e-01],\n",
      "          [ 1.3882e-01,  2.4584e-01, -7.4924e-02],\n",
      "          [ 1.0351e-02,  2.3417e-01,  2.9242e-01]],\n",
      "\n",
      "         [[ 4.6110e-02, -5.0671e-03, -2.3587e-01],\n",
      "          [-9.9369e-02, -8.7017e-02, -3.3888e-02],\n",
      "          [-1.2856e-01, -4.6593e-02, -3.0702e-02]],\n",
      "\n",
      "         [[ 1.2070e-01, -2.5048e-02, -2.1322e-01],\n",
      "          [ 9.3987e-02,  3.7978e-02, -5.8433e-02],\n",
      "          [-1.3207e-01,  2.0469e-01,  2.2441e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0348e-02, -1.5601e-02,  4.6318e-03],\n",
      "          [ 1.0038e-01,  1.8549e-01,  9.4030e-02],\n",
      "          [ 3.0773e-03, -6.0711e-03,  6.3132e-02]],\n",
      "\n",
      "         [[-2.7393e-01, -3.5946e-01, -7.2046e-02],\n",
      "          [-2.7470e-01, -3.5601e-01, -1.5984e-01],\n",
      "          [-8.8832e-02,  2.9364e-03,  1.7902e-01]],\n",
      "\n",
      "         [[ 8.7246e-02,  1.3247e-01,  1.6612e-01],\n",
      "          [ 4.0225e-02, -4.9835e-02,  6.0901e-02],\n",
      "          [ 7.5357e-02,  6.1459e-02,  1.5662e-02]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.2491,  0.1780,  0.0294, -0.0493, -0.0201, -0.0189, -0.0066,  0.0443,\n",
      "         0.0104,  0.0036, -0.0313, -0.0184,  0.1034,  0.0118, -0.0759, -0.0666,\n",
      "        -0.1952, -0.0600, -0.0214, -0.0129, -0.0480,  0.0081, -0.0456,  0.0510,\n",
      "        -0.0559,  0.0246,  0.0128,  0.0142,  0.0567,  0.0826, -0.0123,  0.0455,\n",
      "        -0.0779, -0.0139, -0.0232, -0.0106, -0.0253, -0.1090, -0.0428,  0.0816,\n",
      "         0.1015,  0.2538, -0.0456, -0.1551, -0.0409, -0.0188, -0.0456,  0.0082],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([ 0.2115,  0.1276, -0.5586, -0.0843, -0.0131,  0.2343, -0.6572,  0.5135,\n",
      "        -0.4966, -0.4306, -0.1452, -0.3932, -0.0031, -0.5557,  0.5804,  0.2421,\n",
      "         0.3746, -0.2550,  0.2540, -0.4660, -0.1339, -0.0538, -0.5804,  0.0273,\n",
      "         0.0902, -0.3972, -0.3181,  0.9918, -0.1642,  0.1640,  0.4759, -0.5277,\n",
      "         0.0587, -0.5559, -0.4383, -0.0718, -0.2505, -0.2495, -0.2502, -0.1197,\n",
      "         0.8410, -0.0156, -0.0092, -0.0252, -0.4610, -0.3380, -0.2034, -0.4786],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([1.1207, 1.1964, 0.5442, 1.0884, 0.7278, 1.0353, 0.8411, 1.3525, 0.5905,\n",
      "        0.8696, 0.9515, 0.5634, 1.0377, 0.6970, 1.0364, 0.8712, 0.8628, 0.8116,\n",
      "        1.3138, 0.9031, 0.8259, 0.7190, 0.5803, 1.0697, 0.8432, 0.8216, 0.7372,\n",
      "        0.9775, 0.8411, 0.8826, 1.1618, 0.7314, 1.4252, 0.5787, 0.7261, 1.1887,\n",
      "        0.6497, 1.3712, 0.7373, 0.8981, 0.8581, 1.3248, 0.8648, 1.3312, 0.6452,\n",
      "        0.7148, 0.6707, 0.9231], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([[[[ 0.1534, -0.1246,  0.0059],\n",
      "          [ 0.0587,  0.0940,  0.2769],\n",
      "          [-0.0969,  0.2831, -0.1458]],\n",
      "\n",
      "         [[-0.0577, -0.0651,  0.0677],\n",
      "          [-0.1302,  0.2155, -0.1159],\n",
      "          [ 0.1340,  0.1184, -0.2560]],\n",
      "\n",
      "         [[-0.0438,  0.0898,  0.0400],\n",
      "          [ 0.0177,  0.0718,  0.1219],\n",
      "          [-0.0036,  0.0745,  0.0027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2111, -0.0882,  0.0658],\n",
      "          [-0.2137,  0.0536,  0.0654],\n",
      "          [ 0.0706,  0.0302, -0.1045]],\n",
      "\n",
      "         [[ 0.0118,  0.0206,  0.0186],\n",
      "          [-0.0933, -0.1505, -0.1702],\n",
      "          [-0.0828, -0.1453, -0.0363]],\n",
      "\n",
      "         [[-0.0660, -0.1690, -0.1117],\n",
      "          [-0.1403, -0.1583, -0.0079],\n",
      "          [-0.0541, -0.0357, -0.1464]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2075,  0.0582, -0.1072],\n",
      "          [ 0.2159,  0.2360, -0.1414],\n",
      "          [ 0.1252,  0.2091,  0.0875]],\n",
      "\n",
      "         [[-0.0548, -0.2506, -0.1902],\n",
      "          [ 0.0979, -0.2266, -0.1672],\n",
      "          [-0.0044, -0.0343, -0.2183]],\n",
      "\n",
      "         [[ 0.0912,  0.0208, -0.1052],\n",
      "          [ 0.0889,  0.1561,  0.0046],\n",
      "          [-0.0265,  0.0809,  0.0484]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0879, -0.2648, -0.0268],\n",
      "          [ 0.0196, -0.1127, -0.1178],\n",
      "          [ 0.0225, -0.0199, -0.0220]],\n",
      "\n",
      "         [[ 0.0448, -0.0942, -0.0095],\n",
      "          [ 0.2407, -0.0882, -0.1859],\n",
      "          [ 0.0536,  0.1066, -0.0483]],\n",
      "\n",
      "         [[ 0.0923, -0.1951, -0.1808],\n",
      "          [ 0.0744, -0.2893, -0.3775],\n",
      "          [ 0.1501, -0.1423, -0.1307]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2110,  0.2436, -0.0743],\n",
      "          [ 0.1829,  0.2427,  0.2251],\n",
      "          [ 0.1321,  0.1579,  0.1919]],\n",
      "\n",
      "         [[ 0.2452,  0.3044,  0.2690],\n",
      "          [ 0.1529,  0.2397,  0.3189],\n",
      "          [ 0.1115,  0.0808,  0.1874]],\n",
      "\n",
      "         [[-0.1520, -0.1175,  0.0160],\n",
      "          [-0.0421, -0.0435, -0.0811],\n",
      "          [-0.1115, -0.1162, -0.0164]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1978, -0.1741, -0.0050],\n",
      "          [-0.1313, -0.2397, -0.0007],\n",
      "          [-0.0797, -0.1184, -0.1118]],\n",
      "\n",
      "         [[-0.1290, -0.1329, -0.0009],\n",
      "          [-0.0666, -0.1962, -0.0306],\n",
      "          [-0.0635, -0.1420, -0.1063]],\n",
      "\n",
      "         [[-0.0404, -0.2479,  0.0394],\n",
      "          [-0.1506, -0.1925, -0.0237],\n",
      "          [-0.1630, -0.0605, -0.0728]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0266, -0.1273,  0.1227],\n",
      "          [-0.0881,  0.0474,  0.1157],\n",
      "          [-0.0080,  0.0650,  0.0897]],\n",
      "\n",
      "         [[ 0.2080,  0.1026, -0.0301],\n",
      "          [ 0.0592,  0.0181,  0.0649],\n",
      "          [-0.1423, -0.2773, -0.0129]],\n",
      "\n",
      "         [[-0.0547, -0.0692, -0.1198],\n",
      "          [-0.1002,  0.0753, -0.0184],\n",
      "          [-0.0883, -0.0105,  0.0636]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1978,  0.1170,  0.0019],\n",
      "          [ 0.3494,  0.4068,  0.1813],\n",
      "          [ 0.3014,  0.2275,  0.3250]],\n",
      "\n",
      "         [[-0.0556, -0.0794, -0.2048],\n",
      "          [-0.1644, -0.2294, -0.1738],\n",
      "          [ 0.0031,  0.0344, -0.0381]],\n",
      "\n",
      "         [[ 0.0141,  0.0332, -0.1185],\n",
      "          [ 0.1451, -0.0078,  0.0175],\n",
      "          [ 0.1063,  0.0566,  0.3432]]],\n",
      "\n",
      "\n",
      "        [[[-0.1966, -0.3832,  0.0041],\n",
      "          [ 0.0980, -0.0902,  0.0847],\n",
      "          [ 0.2745, -0.0014, -0.0153]],\n",
      "\n",
      "         [[ 0.0126,  0.2387,  0.0088],\n",
      "          [-0.0857, -0.0824,  0.1581],\n",
      "          [-0.1651, -0.1040,  0.0801]],\n",
      "\n",
      "         [[ 0.0908,  0.1984,  0.1220],\n",
      "          [ 0.1322,  0.2554,  0.1642],\n",
      "          [ 0.1829,  0.2046,  0.0652]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0166,  0.1229,  0.1091],\n",
      "          [-0.0553, -0.0176,  0.1160],\n",
      "          [ 0.0468,  0.0073, -0.0682]],\n",
      "\n",
      "         [[ 0.0962,  0.0347, -0.0447],\n",
      "          [-0.0244, -0.0445,  0.1240],\n",
      "          [ 0.0384, -0.0078, -0.0120]],\n",
      "\n",
      "         [[ 0.0230,  0.2035,  0.1220],\n",
      "          [-0.0121, -0.1365, -0.0027],\n",
      "          [-0.0406, -0.1600, -0.0182]]],\n",
      "\n",
      "\n",
      "        [[[-0.1801, -0.0880, -0.2463],\n",
      "          [ 0.0439,  0.1334,  0.0526],\n",
      "          [-0.0824,  0.2746, -0.0068]],\n",
      "\n",
      "         [[ 0.0575, -0.0060, -0.0620],\n",
      "          [ 0.2031,  0.3164,  0.2225],\n",
      "          [-0.0425, -0.1512, -0.2046]],\n",
      "\n",
      "         [[-0.1664, -0.0316, -0.1543],\n",
      "          [-0.0493,  0.0245, -0.0230],\n",
      "          [-0.0709,  0.0862, -0.0630]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1210, -0.0262, -0.1943],\n",
      "          [ 0.0778,  0.0065, -0.1218],\n",
      "          [ 0.0824,  0.0413, -0.0148]],\n",
      "\n",
      "         [[-0.0105, -0.0439, -0.0284],\n",
      "          [ 0.0427, -0.0464,  0.0336],\n",
      "          [ 0.0173, -0.0880, -0.1129]],\n",
      "\n",
      "         [[ 0.1107,  0.2442,  0.0328],\n",
      "          [ 0.0580,  0.1131, -0.0461],\n",
      "          [ 0.1423,  0.1016,  0.0098]]]], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([ 0.0016,  0.0049,  0.0101, -0.0004, -0.0012, -0.0177,  0.0042,  0.0004,\n",
      "         0.0024,  0.0193,  0.0233, -0.0297,  0.0093, -0.0007, -0.0092,  0.0134,\n",
      "         0.0144,  0.0350, -0.0036,  0.0030, -0.0127,  0.0122, -0.0179, -0.0162,\n",
      "        -0.0066,  0.0388,  0.0112,  0.0040, -0.0256,  0.0064,  0.0115,  0.0111,\n",
      "         0.0024,  0.0105,  0.0090, -0.0082, -0.0124,  0.0106, -0.0249, -0.0094,\n",
      "        -0.0086,  0.0221,  0.0118,  0.0104, -0.0239, -0.0013,  0.0077, -0.0206],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.2565, -0.3726, -0.4054, -0.3625, -0.4137, -0.5008, -0.1775, -0.6069,\n",
      "        -0.3940, -0.5503, -0.2014, -0.6194, -0.3724, -0.4130, -0.5457, -0.4446,\n",
      "        -0.4979, -0.3875, -0.1840, -0.2695, -0.6760, -0.2627, -0.4194, -0.6555,\n",
      "        -0.4132, -0.5041, -0.2278, -0.3183, -0.3875, -0.4913, -0.4797, -0.1923,\n",
      "        -0.3370, -0.6027, -0.5468, -0.3690, -0.3803, -0.2908, -0.2563, -0.2654,\n",
      "        -0.2941, -0.5569, -0.4664, -0.2894, -0.5059, -0.4478, -0.4427, -0.1984],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([0.9177, 0.8832, 0.8448, 1.0406, 1.0257, 1.0022, 1.1157, 0.7906, 0.7547,\n",
      "        0.8948, 1.1182, 1.0589, 0.7598, 0.7876, 1.0296, 1.0151, 0.9489, 1.2162,\n",
      "        0.8784, 0.9061, 1.0395, 0.8995, 0.8664, 1.0246, 0.9742, 0.9668, 1.0212,\n",
      "        0.9051, 1.0941, 0.9682, 0.9440, 0.9219, 0.8243, 1.0067, 1.0394, 0.9452,\n",
      "        1.2436, 0.9018, 0.9305, 0.7080, 0.7266, 1.1290, 0.9851, 0.7811, 1.2469,\n",
      "        0.7777, 0.8055, 1.0439], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([[[[ 0.0969,  0.1765,  0.1515],\n",
      "          [ 0.0974,  0.1888,  0.1136],\n",
      "          [ 0.2171,  0.0861,  0.0038]],\n",
      "\n",
      "         [[-0.1099,  0.0427,  0.1209],\n",
      "          [ 0.0027,  0.0368, -0.0494],\n",
      "          [ 0.1241, -0.0476, -0.1435]],\n",
      "\n",
      "         [[-0.1099, -0.2007, -0.0550],\n",
      "          [-0.2285, -0.1212, -0.0148],\n",
      "          [-0.1207,  0.0054,  0.0542]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0423, -0.1250, -0.0748],\n",
      "          [-0.0240, -0.1807, -0.1030],\n",
      "          [-0.0790, -0.0533, -0.0750]],\n",
      "\n",
      "         [[-0.0210,  0.0076, -0.1453],\n",
      "          [ 0.0152,  0.0353, -0.0689],\n",
      "          [ 0.0200, -0.1009, -0.1617]],\n",
      "\n",
      "         [[ 0.0643, -0.1465,  0.0311],\n",
      "          [-0.0859, -0.1255, -0.0305],\n",
      "          [-0.0059,  0.0738,  0.0525]]],\n",
      "\n",
      "\n",
      "        [[[-0.1648, -0.0251, -0.0203],\n",
      "          [-0.0341,  0.2062,  0.2028],\n",
      "          [-0.1044,  0.1532,  0.0217]],\n",
      "\n",
      "         [[-0.1054,  0.1401,  0.0633],\n",
      "          [-0.1487, -0.0246,  0.0523],\n",
      "          [-0.1470, -0.0882, -0.1132]],\n",
      "\n",
      "         [[-0.1215, -0.0171, -0.0143],\n",
      "          [-0.2319,  0.1108,  0.1407],\n",
      "          [-0.2288, -0.0095,  0.1990]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0349,  0.0697, -0.1227],\n",
      "          [-0.0616, -0.1689, -0.1314],\n",
      "          [-0.0305, -0.1365, -0.3234]],\n",
      "\n",
      "         [[-0.1013, -0.1013, -0.1379],\n",
      "          [ 0.0547,  0.0701, -0.0875],\n",
      "          [ 0.0770,  0.0983, -0.1357]],\n",
      "\n",
      "         [[ 0.1024, -0.0150, -0.1279],\n",
      "          [-0.2290, -0.0353,  0.0241],\n",
      "          [ 0.0920,  0.2147,  0.1344]]],\n",
      "\n",
      "\n",
      "        [[[-0.0182, -0.0584,  0.0137],\n",
      "          [ 0.0679,  0.0537, -0.0867],\n",
      "          [ 0.1386,  0.0880, -0.1243]],\n",
      "\n",
      "         [[ 0.0297,  0.0410, -0.0631],\n",
      "          [ 0.0441, -0.0066, -0.1468],\n",
      "          [-0.1272, -0.0634,  0.0358]],\n",
      "\n",
      "         [[ 0.0654, -0.0412, -0.1205],\n",
      "          [-0.0820, -0.0092,  0.0524],\n",
      "          [ 0.0799,  0.0222,  0.1974]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0917, -0.1910,  0.0748],\n",
      "          [ 0.0673, -0.0352,  0.1719],\n",
      "          [-0.0641,  0.0124,  0.0399]],\n",
      "\n",
      "         [[-0.0571, -0.0562,  0.1612],\n",
      "          [-0.0988, -0.0957,  0.0140],\n",
      "          [-0.1091, -0.0691, -0.1685]],\n",
      "\n",
      "         [[ 0.0816,  0.1142,  0.1290],\n",
      "          [ 0.0783, -0.0598, -0.0030],\n",
      "          [ 0.1139, -0.2666, -0.0520]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2654, -0.0521, -0.1548],\n",
      "          [-0.0452, -0.1364,  0.1865],\n",
      "          [ 0.0448, -0.0208,  0.1298]],\n",
      "\n",
      "         [[ 0.0632, -0.1374, -0.2397],\n",
      "          [ 0.1702,  0.0875,  0.0737],\n",
      "          [ 0.0043, -0.0055, -0.1061]],\n",
      "\n",
      "         [[-0.0013,  0.0272,  0.0291],\n",
      "          [ 0.1501, -0.0592,  0.0022],\n",
      "          [ 0.1177,  0.0939,  0.0543]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0467,  0.0302,  0.0642],\n",
      "          [ 0.0266,  0.1546,  0.1913],\n",
      "          [-0.0472, -0.0010,  0.0152]],\n",
      "\n",
      "         [[-0.0301, -0.0459,  0.0857],\n",
      "          [ 0.1189,  0.0011,  0.0690],\n",
      "          [-0.0674, -0.0918, -0.0669]],\n",
      "\n",
      "         [[ 0.0406,  0.0156, -0.0180],\n",
      "          [ 0.0098, -0.1104,  0.0057],\n",
      "          [ 0.1691,  0.1189,  0.0734]]],\n",
      "\n",
      "\n",
      "        [[[-0.0054, -0.0155, -0.0348],\n",
      "          [ 0.2060, -0.0743, -0.0305],\n",
      "          [-0.2407,  0.0226, -0.0451]],\n",
      "\n",
      "         [[-0.3178, -0.2310, -0.1284],\n",
      "          [-0.2614, -0.1035, -0.0785],\n",
      "          [-0.0501,  0.0121,  0.0628]],\n",
      "\n",
      "         [[ 0.0423,  0.1901,  0.0839],\n",
      "          [-0.0504, -0.0759, -0.0213],\n",
      "          [ 0.0793, -0.0723, -0.0549]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1821,  0.0485,  0.1129],\n",
      "          [-0.1945, -0.0385,  0.2224],\n",
      "          [-0.0729,  0.1510,  0.2624]],\n",
      "\n",
      "         [[ 0.0189, -0.1726, -0.0392],\n",
      "          [-0.2015, -0.2252, -0.2615],\n",
      "          [-0.2092, -0.2089, -0.1065]],\n",
      "\n",
      "         [[ 0.0788,  0.1300,  0.0362],\n",
      "          [-0.0329,  0.1333,  0.0918],\n",
      "          [-0.1108,  0.1131, -0.1361]]],\n",
      "\n",
      "\n",
      "        [[[-0.0686,  0.0233,  0.1452],\n",
      "          [-0.0782,  0.0190,  0.0833],\n",
      "          [-0.1898, -0.1920, -0.2879]],\n",
      "\n",
      "         [[-0.1238,  0.0142,  0.0095],\n",
      "          [ 0.1675,  0.0807, -0.0055],\n",
      "          [ 0.2717,  0.1179, -0.0707]],\n",
      "\n",
      "         [[ 0.1714,  0.1265,  0.1470],\n",
      "          [-0.0986,  0.0350, -0.0558],\n",
      "          [-0.0725,  0.0475, -0.0362]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2657, -0.0155, -0.1745],\n",
      "          [ 0.2644, -0.0333, -0.1367],\n",
      "          [ 0.1927, -0.2982, -0.1966]],\n",
      "\n",
      "         [[-0.0261, -0.0225, -0.0724],\n",
      "          [-0.0685, -0.1148, -0.1041],\n",
      "          [ 0.0354, -0.1881, -0.0553]],\n",
      "\n",
      "         [[ 0.0836,  0.0691,  0.1550],\n",
      "          [ 0.0478,  0.1020,  0.2254],\n",
      "          [ 0.0730,  0.2066,  0.2626]]]], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([ 4.6797e-03, -2.7848e-02,  2.0921e-02,  5.6514e-03,  2.2449e-02,\n",
      "         3.4297e-03,  9.0140e-05,  3.4005e-02,  6.2501e-03, -1.8747e-02,\n",
      "        -2.3069e-02,  1.2220e-02,  1.8028e-03, -1.8847e-02,  8.0701e-03,\n",
      "         1.2551e-03, -3.1082e-02, -8.7124e-03, -3.7984e-03, -2.7988e-03,\n",
      "        -4.2684e-02,  3.7588e-02,  4.9385e-03,  2.8643e-02, -1.1775e-02,\n",
      "        -1.9628e-03, -1.0528e-02, -6.6784e-05, -2.4349e-03, -7.3820e-03,\n",
      "        -4.9919e-04, -2.6968e-03,  1.6067e-02,  9.0262e-03,  1.2372e-02,\n",
      "         8.0276e-03, -1.0513e-02, -5.9761e-03, -7.9432e-03,  3.9517e-02,\n",
      "        -3.1771e-02, -4.0168e-02, -4.0181e-03, -1.1663e-02,  2.7277e-03,\n",
      "        -2.8474e-02, -4.5315e-02, -4.6959e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.4925, -0.5699, -0.5517, -0.6658, -0.4979, -0.7727, -0.4690, -0.5960,\n",
      "        -0.4470, -0.4515, -0.5634, -0.4503, -0.6153, -0.6567, -0.3539, -0.6008,\n",
      "        -0.6378, -0.6275, -0.5148, -0.6807, -0.4783, -0.5610, -0.6452, -0.5353,\n",
      "        -0.5073, -0.6814, -0.5272, -0.8846, -0.5829, -0.9338, -0.7272, -0.4585,\n",
      "        -0.4503, -0.3549, -0.4363, -0.4604, -0.9601, -0.4361, -0.5242, -0.4797,\n",
      "        -0.6975, -0.6236, -0.5711, -0.7284, -0.6015, -0.5275, -0.4526, -0.5882],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([0.8607, 0.9741, 0.8169, 1.0186, 0.9191, 0.9619, 0.7814, 0.9199, 0.8598,\n",
      "        0.9243, 0.8946, 0.7636, 0.9258, 0.9634, 0.9229, 0.7296, 0.8315, 1.1071,\n",
      "        0.8243, 0.8980, 0.8095, 0.9100, 0.7447, 1.1476, 0.8485, 0.9612, 0.8168,\n",
      "        1.2696, 0.9880, 1.1204, 0.9548, 0.9426, 0.8238, 0.8730, 0.7502, 0.8088,\n",
      "        1.0032, 0.7436, 0.9786, 0.9716, 0.7514, 0.8736, 0.9905, 0.9081, 0.6871,\n",
      "        0.8485, 0.8788, 0.8877], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([[[[-4.5640e-02, -4.1175e-02, -1.9834e-02],\n",
      "          [-1.2550e-01, -8.8204e-02,  8.0529e-02],\n",
      "          [-5.3299e-02, -3.5713e-02,  9.4846e-02]],\n",
      "\n",
      "         [[-2.2738e-02,  2.2804e-02,  3.8118e-03],\n",
      "          [ 3.3837e-02,  8.1089e-02,  1.4925e-02],\n",
      "          [-6.6579e-02,  6.9574e-02, -6.1421e-02]],\n",
      "\n",
      "         [[-1.0693e-01,  4.0560e-02, -6.4710e-02],\n",
      "          [-2.4086e-02, -1.0410e-01, -2.5920e-02],\n",
      "          [-1.2764e-01,  1.3481e-02, -1.6666e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9768e-03, -4.2347e-02,  6.7394e-02],\n",
      "          [-5.1574e-02,  5.1517e-02,  4.0076e-02],\n",
      "          [-6.1437e-02, -1.1131e-02,  1.8982e-02]],\n",
      "\n",
      "         [[ 4.7593e-02, -1.0535e-01, -2.8322e-03],\n",
      "          [ 5.4531e-02,  9.2245e-02,  2.0356e-02],\n",
      "          [-4.2840e-02, -8.0307e-02, -1.9114e-01]],\n",
      "\n",
      "         [[-5.2188e-02,  4.1634e-02,  9.6728e-02],\n",
      "          [ 3.2906e-02,  7.8303e-03,  3.4751e-02],\n",
      "          [-3.2608e-02, -1.8341e-02, -6.8420e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.1581e-02,  2.4299e-02, -3.8789e-02],\n",
      "          [ 2.3691e-02, -8.0351e-02,  3.1567e-02],\n",
      "          [ 4.6836e-02,  1.0816e-01,  6.1362e-02]],\n",
      "\n",
      "         [[ 1.4457e-02,  1.9798e-02,  4.1581e-02],\n",
      "          [-2.3902e-02, -5.6596e-03,  1.0114e-01],\n",
      "          [-3.3503e-02,  1.5170e-01,  2.0833e-01]],\n",
      "\n",
      "         [[ 5.4470e-02,  1.0266e-01,  6.7109e-02],\n",
      "          [ 2.7143e-02,  6.0375e-02,  1.0256e-01],\n",
      "          [ 6.3186e-02,  4.6070e-02,  6.7429e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.7540e-02, -1.3205e-02, -1.4086e-02],\n",
      "          [-8.8549e-02, -1.5724e-01, -1.0478e-01],\n",
      "          [ 6.8114e-02,  6.3573e-02,  2.8371e-02]],\n",
      "\n",
      "         [[ 7.5995e-02,  6.6670e-02,  2.4592e-03],\n",
      "          [ 3.6954e-02,  1.4112e-02,  7.2606e-02],\n",
      "          [ 7.5322e-02, -2.8320e-02, -3.9340e-02]],\n",
      "\n",
      "         [[-5.0857e-02, -1.2942e-01, -2.7496e-03],\n",
      "          [-1.4060e-01, -1.8863e-01, -5.9909e-02],\n",
      "          [-6.2143e-02, -1.4408e-01,  1.4276e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.0426e-02, -1.0425e-01, -7.4805e-03],\n",
      "          [-5.0222e-02, -2.0485e-01, -1.3120e-01],\n",
      "          [ 1.2960e-01,  1.1312e-01, -6.4703e-02]],\n",
      "\n",
      "         [[-6.4745e-02, -4.4534e-03, -9.4135e-02],\n",
      "          [-5.1483e-02,  1.7306e-01, -6.9625e-02],\n",
      "          [ 5.5158e-02,  4.2527e-02, -4.4133e-03]],\n",
      "\n",
      "         [[-8.7738e-02, -1.4251e-02, -1.1715e-01],\n",
      "          [-4.2445e-02, -1.0592e-01, -1.9064e-01],\n",
      "          [ 8.3929e-02,  7.7866e-02,  8.2094e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4340e-01,  1.1277e-01,  5.5940e-03],\n",
      "          [-3.0396e-02,  4.2797e-02, -7.2887e-02],\n",
      "          [ 1.0814e-01,  1.4802e-01, -1.3846e-02]],\n",
      "\n",
      "         [[ 5.2776e-02,  9.6580e-02, -6.8554e-02],\n",
      "          [ 3.8198e-01,  2.1804e-01,  1.8465e-01],\n",
      "          [-2.2243e-01, -1.5402e-01, -2.3030e-01]],\n",
      "\n",
      "         [[ 9.9166e-02, -4.8675e-02,  1.0107e-01],\n",
      "          [ 5.4469e-02,  2.4618e-02,  4.5669e-02],\n",
      "          [-9.9300e-02,  6.6064e-02, -9.0739e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0463e-01,  6.6432e-02,  3.4125e-03],\n",
      "          [ 3.9948e-03,  1.9461e-01,  1.2335e-02],\n",
      "          [ 5.3210e-02,  1.9462e-01,  1.0495e-01]],\n",
      "\n",
      "         [[-1.2032e-01, -1.3170e-04, -8.2871e-02],\n",
      "          [ 1.0835e-01, -1.4311e-02, -8.4619e-02],\n",
      "          [-1.8151e-02, -1.0126e-01, -6.9328e-02]],\n",
      "\n",
      "         [[ 1.4754e-01,  4.4927e-02, -2.0501e-01],\n",
      "          [-7.4117e-02, -3.8085e-02, -1.6963e-01],\n",
      "          [-9.1811e-02, -1.2933e-02, -1.6990e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.9828e-02, -3.0172e-02,  4.5497e-02],\n",
      "          [ 1.6295e-01,  1.1592e-01,  5.4510e-02],\n",
      "          [ 7.7522e-02,  8.9341e-02,  5.9093e-03]],\n",
      "\n",
      "         [[-3.2998e-02, -1.0768e-01,  1.4430e-01],\n",
      "          [ 4.4946e-02, -1.2967e-01,  5.4693e-02],\n",
      "          [-7.1073e-02, -1.2688e-01, -9.9004e-04]],\n",
      "\n",
      "         [[-3.7495e-02, -6.1301e-02, -5.4111e-02],\n",
      "          [-8.8538e-02, -1.4798e-01, -1.4631e-01],\n",
      "          [-4.3679e-02, -1.0115e-01, -2.0299e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.2993e-02,  1.1450e-01,  1.8419e-01],\n",
      "          [ 1.0982e-01, -6.6783e-02, -3.5605e-03],\n",
      "          [-1.3280e-01, -4.9773e-02, -5.3685e-03]],\n",
      "\n",
      "         [[-1.1199e-01, -1.7870e-02, -6.2496e-02],\n",
      "          [ 1.5040e-02,  5.1426e-03, -1.7047e-03],\n",
      "          [ 2.7536e-02,  3.2812e-02,  5.4682e-02]],\n",
      "\n",
      "         [[ 2.2337e-01,  2.6839e-02, -4.8073e-02],\n",
      "          [ 1.0258e-01,  3.1376e-02, -5.4478e-02],\n",
      "          [-2.7655e-02, -1.0882e-01, -8.0135e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0562e-01,  1.6573e-01,  2.9849e-01],\n",
      "          [ 2.1932e-01,  9.9648e-02,  2.5494e-01],\n",
      "          [ 2.2588e-01,  1.9676e-01,  1.4557e-01]],\n",
      "\n",
      "         [[-5.3164e-02, -7.2637e-02, -4.8837e-02],\n",
      "          [-5.3502e-02, -4.7183e-02, -3.7362e-02],\n",
      "          [-3.1520e-02, -4.4656e-02,  5.6459e-02]],\n",
      "\n",
      "         [[-8.5155e-06, -2.5836e-02, -4.7716e-02],\n",
      "          [-1.7195e-01, -1.1149e-01, -5.7472e-02],\n",
      "          [-1.4635e-01, -1.5397e-02, -6.7162e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.3851e-02, -8.0660e-03,  3.4922e-02],\n",
      "          [-2.6560e-02, -4.4742e-02,  1.1295e-01],\n",
      "          [-3.0578e-03,  5.0902e-02,  2.3202e-02]],\n",
      "\n",
      "         [[ 3.7603e-02,  1.5068e-02,  1.1567e-02],\n",
      "          [ 3.5002e-03,  1.5338e-02,  3.3078e-02],\n",
      "          [ 1.0459e-01,  5.3448e-03,  1.6498e-01]],\n",
      "\n",
      "         [[ 3.5195e-02,  1.9011e-02, -4.2768e-02],\n",
      "          [ 5.5202e-02,  6.9494e-02,  4.3926e-02],\n",
      "          [-6.8030e-02,  4.4379e-02,  6.4735e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0067e-01, -8.4407e-02, -3.4908e-02],\n",
      "          [-5.6441e-02, -5.1928e-04, -8.3111e-02],\n",
      "          [ 3.5069e-02,  5.9770e-02, -6.6573e-02]],\n",
      "\n",
      "         [[ 4.5865e-02,  8.7347e-02, -2.5494e-02],\n",
      "          [ 6.8923e-02, -1.0468e-02,  1.0041e-03],\n",
      "          [ 5.1985e-02,  1.1928e-02, -3.5464e-02]],\n",
      "\n",
      "         [[-5.3967e-02, -1.0131e-01,  2.9840e-02],\n",
      "          [-5.9408e-02, -2.2586e-01, -1.1195e-01],\n",
      "          [-1.1900e-01, -5.8801e-02, -7.6145e-02]]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.0771, -0.1722, -0.0193, -0.1231, -0.1573, -0.1233, -0.1109, -0.1175,\n",
      "        -0.0643, -0.1077, -0.2765, -0.0328, -0.1380, -0.1781, -0.0738, -0.0931,\n",
      "        -0.1107, -0.1006, -0.0087, -0.0949, -0.1747, -0.0663, -0.0028, -0.0224,\n",
      "        -0.1011, -0.0893, -0.0898, -0.0892, -0.1270, -0.0547, -0.0148, -0.0153,\n",
      "        -0.0004, -0.0540,  0.0014, -0.0670,  0.0032, -0.0073, -0.0618, -0.1172,\n",
      "        -0.1341, -0.0674, -0.1100, -0.1081, -0.1027, -0.0136, -0.0826, -0.1388],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([-0.7452, -0.4614, -0.9732, -0.5632, -0.4272, -0.6645, -0.6547, -0.5296,\n",
      "        -0.7521, -0.7261, -0.4353, -0.2425, -0.5294, -0.4705, -0.7029, -0.5944,\n",
      "        -0.9825, -0.8394, -0.7006, -0.5789, -0.9276, -0.5445, -0.6761, -0.4957,\n",
      "        -0.6695, -0.5678, -0.8573, -0.6112, -0.5642, -0.6069, -0.6275, -0.7659,\n",
      "        -0.6143, -0.7128, -0.5577, -0.7317, -0.6254, -0.5058, -0.5688, -0.6440,\n",
      "        -0.5231, -0.4803, -0.7573, -0.4457, -0.4713, -0.6017, -0.5627, -0.5168],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([0.4327, 0.4864, 0.5201, 0.5317, 0.4727, 0.5323, 0.4952, 0.6216, 0.3962,\n",
      "        0.4628, 0.4814, 0.4449, 0.6615, 0.4950, 0.4458, 0.5224, 0.7510, 0.5628,\n",
      "        0.3219, 0.6292, 0.9001, 0.5661, 0.3108, 0.5593, 0.5496, 0.5867, 0.6321,\n",
      "        0.5800, 0.6937, 0.5924, 0.2911, 0.3531, 0.3178, 0.4182, 0.2175, 0.4813,\n",
      "        0.2826, 0.2305, 0.4924, 0.7562, 0.6295, 0.6081, 0.5078, 0.6046, 0.6596,\n",
      "        0.5337, 0.6557, 0.4768], device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([[-0.0123,  0.1060,  0.0280,  ..., -0.0740, -0.0042, -0.0294],\n",
      "        [-0.0231,  0.0213, -0.0042,  ..., -0.0621, -0.1027, -0.0919],\n",
      "        [ 0.0090, -0.0443, -0.0072,  ..., -0.0407, -0.0364, -0.0287],\n",
      "        [ 0.0148, -0.0008, -0.0945,  ..., -0.0963, -0.0290, -0.0130],\n",
      "        [ 0.0244,  0.0005, -0.0111,  ...,  0.1707,  0.1722,  0.0834]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "p ==  Parameter containing:\n",
      "tensor([ 0.0759, -0.0317,  0.1158,  0.1191,  0.0614], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_eigenvector\n",
      "18\n",
      "m_orig shape ==  torch.Size([48, 3, 3, 3])\n",
      "m_perb shape ==  torch.Size([48, 3, 3, 3])\n",
      "m_orig shape ==  torch.Size([48])\n",
      "m_perb shape ==  torch.Size([48])\n",
      "m_orig shape ==  torch.Size([48])\n",
      "m_perb shape ==  torch.Size([48])\n",
      "m_orig shape ==  torch.Size([48])\n",
      "m_perb shape ==  torch.Size([48])\n",
      "m_orig shape ==  torch.Size([48])\n",
      "m_perb shape ==  torch.Size([48])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (48) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28988\\2736879412.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_landscape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlandscape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaml_system\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_support_set_task\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_support_set_task\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\MAML\\utils\\loss_landscape.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlam1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlams1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlam2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlams2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mmodel_perb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_perb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_eigenvector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                 \u001b[0mmodel_perb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_perb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_perb2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_eigenvector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlam1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_perb2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MAML\\utils\\loss_landscape.py\u001b[0m in \u001b[0;36mget_params\u001b[1;34m(self, model_orig, model_perb, direction, alpha)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"m_orig shape == \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_orig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"m_perb shape == \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_perb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mm_perb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm_orig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_perb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (48) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "test_data = maml_system.data.get_test_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "for sample_idx, test_sample in enumerate(test_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = test_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        # Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "        num_steps=5\n",
    "        for num_step in range(num_steps):            \n",
    "            support_loss, support_preds, out_feature_dict  = maml_system.model.net_forward(\n",
    "                    x=x_support_set_task,\n",
    "                    y=y_support_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    backup_running_statistics=num_step == 0,\n",
    "                    training=True,\n",
    "                    num_step=num_step,\n",
    "                )\n",
    "        \n",
    "            generated_alpha_params = {}\n",
    "\n",
    "            if maml_system.model.args.arbiter:\n",
    "                support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                        retain_graph=True)\n",
    "\n",
    "                names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "                per_step_task_embedding = []\n",
    "\n",
    "                for key, weight in names_weights_copy.items():\n",
    "                    weight_norm = torch.norm(weight, p=2)\n",
    "                    per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "                for key, grad in names_grads_copy.items():\n",
    "                    gradient_l2norm = torch.norm(grad, p=2)\n",
    "                    per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "                per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "                per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                            per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "                generated_gradient_rate = maml_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "                g = 0\n",
    "                for key in names_weights_copy.keys():\n",
    "                    generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "                    g += 1\n",
    "\n",
    "            names_weights_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                              names_weights_copy=names_weights_copy,\n",
    "                                                              out_feature_dict=out_feature_dict,\n",
    "                                                              alpha=generated_alpha_params,\n",
    "                                                              use_second_order=args.second_order,\n",
    "                                                              current_step_idx=num_step,\n",
    "                                                              current_iter=maml_system.state['current_iter'],\n",
    "                                                              training_phase='test')\n",
    "  \n",
    "        \n",
    "        for name, param in maml_system.model.classifier.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if \"norm_layer\" not in name:\n",
    "                    param = names_weights_copy[name].to(device=device)        \n",
    "                \n",
    "                \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        ls = loss_landscape.landscape(maml_system.model.classifier, criterion)\n",
    "        ls.show(x_support_set_task, y_support_set_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
