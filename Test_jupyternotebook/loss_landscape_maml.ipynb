{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"SWA\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f85286c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "arbiter.0.weight torch.Size([20, 20]) cuda:0 True\n",
      "arbiter.0.bias torch.Size([20]) cuda:0 True\n",
      "arbiter.2.weight torch.Size([10, 20]) cuda:0 True\n",
      "arbiter.2.bias torch.Size([10]) cuda:0 True\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6550666664044063,\n",
       " 'best_val_iter': 16500,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 33,\n",
       " 'train_loss_mean': 0.4869355769753456,\n",
       " 'train_loss_std': 0.12580969358408267,\n",
       " 'train_accuracy_mean': 0.8264800001382828,\n",
       " 'train_accuracy_std': 0.054369606032957596,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.9529854895671209,\n",
       " 'val_loss_std': 0.14067764332256763,\n",
       " 'val_accuracy_mean': 0.6284444446365038,\n",
       " 'val_accuracy_std': 0.06644704535564427,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-0.2229, -0.4079,  0.0229],\n",
       "                         [-0.4205, -0.3712, -0.1134],\n",
       "                         [-0.1798,  0.0387, -0.0479]],\n",
       "               \n",
       "                        [[ 0.0875,  0.1461,  0.5129],\n",
       "                         [-0.3017,  0.1216,  0.3612],\n",
       "                         [-0.3541,  0.0563,  0.1235]],\n",
       "               \n",
       "                        [[ 0.0737,  0.1099,  0.0783],\n",
       "                         [-0.3082,  0.0307, -0.0709],\n",
       "                         [-0.4206, -0.0144, -0.0508]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0198,  0.5814, -0.0080],\n",
       "                         [ 0.2629, -0.2386,  0.0913],\n",
       "                         [-0.2109, -0.4221, -0.0683]],\n",
       "               \n",
       "                        [[-0.2553,  0.2404, -0.1827],\n",
       "                         [ 0.2202, -0.3339, -0.0279],\n",
       "                         [ 0.2482, -0.1465,  0.2605]],\n",
       "               \n",
       "                        [[-0.2731,  0.2740, -0.2292],\n",
       "                         [-0.0269, -0.0361,  0.2049],\n",
       "                         [ 0.0167, -0.0107,  0.0535]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0299, -0.1038,  0.2057],\n",
       "                         [ 0.1687, -0.2716, -0.3158],\n",
       "                         [ 0.3400,  0.2465, -0.2081]],\n",
       "               \n",
       "                        [[-0.2896, -0.1635,  0.4933],\n",
       "                         [ 0.1406, -0.2526,  0.0727],\n",
       "                         [-0.2914, -0.0921,  0.2599]],\n",
       "               \n",
       "                        [[ 0.0506, -0.2271, -0.0086],\n",
       "                         [ 0.3426,  0.0461, -0.2961],\n",
       "                         [-0.0701,  0.1586,  0.0904]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.4581, -0.0384, -0.2547],\n",
       "                         [ 0.5722, -0.1844, -0.4678],\n",
       "                         [ 0.4209, -0.0859, -0.4157]],\n",
       "               \n",
       "                        [[ 0.0662,  0.0014, -0.0528],\n",
       "                         [ 0.0898, -0.0138, -0.1262],\n",
       "                         [ 0.1354, -0.0191, -0.1259]],\n",
       "               \n",
       "                        [[-0.0572, -0.0806,  0.1829],\n",
       "                         [-0.1003,  0.0039,  0.2054],\n",
       "                         [-0.2051, -0.0804,  0.2327]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0095, -0.0769,  0.0114],\n",
       "                         [ 0.0026,  0.1716, -0.0153],\n",
       "                         [-0.0260,  0.1488,  0.2865]],\n",
       "               \n",
       "                        [[-0.0198, -0.3633, -0.3081],\n",
       "                         [-0.4204, -0.5923, -0.0533],\n",
       "                         [-0.3964, -0.2715,  0.0382]],\n",
       "               \n",
       "                        [[ 0.4216,  0.0781,  0.0493],\n",
       "                         [ 0.1427, -0.0874,  0.1652],\n",
       "                         [-0.2659,  0.0831,  0.4186]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.4538, -0.6162, -0.0460],\n",
       "                         [-0.1943, -0.1862,  0.3005],\n",
       "                         [-0.0698,  0.2001,  0.4582]],\n",
       "               \n",
       "                        [[-0.2833, -0.2912,  0.0184],\n",
       "                         [-0.0533, -0.1320,  0.0959],\n",
       "                         [-0.0484,  0.2811,  0.2402]],\n",
       "               \n",
       "                        [[-0.0906, -0.0367,  0.1275],\n",
       "                         [ 0.0736, -0.0490,  0.1558],\n",
       "                         [ 0.0621,  0.2452, -0.0074]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.0006, -0.0152,  0.0159, -0.0041,  0.0055,  0.0073,  0.0015,  0.0019,\n",
       "                       -0.0025,  0.0037,  0.0086,  0.0011, -0.0069, -0.0022, -0.0094,  0.0068,\n",
       "                        0.0010, -0.0023,  0.0030, -0.0057, -0.0054,  0.0066, -0.0049,  0.0036,\n",
       "                        0.0215,  0.0061, -0.0054, -0.0105,  0.0121,  0.0012,  0.0025, -0.0007,\n",
       "                        0.0055, -0.0019, -0.0002,  0.0108,  0.0029,  0.0227, -0.0103, -0.0296,\n",
       "                        0.0045,  0.0310,  0.0023,  0.0029, -0.0009,  0.0070, -0.0075,  0.0082],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([-0.6625,  0.3722, -0.9954, -0.0411,  0.0047,  0.0956, -0.5131, -0.3034,\n",
       "                       -0.1839, -0.1969,  1.5331, -0.1288, -0.4148, -0.1719,  0.1362,  0.2457,\n",
       "                       -0.3061, -0.8333,  0.8492, -0.4338, -0.2871, -0.4858, -0.6078,  1.3922,\n",
       "                        0.7910, -0.8025, -0.8787, -0.2624, -0.0506,  1.1441, -0.0318, -0.0249,\n",
       "                        0.1848, -0.3849,  0.0065, -0.1025, -0.5109, -0.1119, -0.7590, -0.0587,\n",
       "                        0.0034, -0.0857, -0.7157, -0.0617, -0.7456, -0.1055, -0.5093, -0.8117],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.8325, 0.8915, 0.9018, 0.8677, 0.3670, 0.9607, 0.8851, 0.7135, 0.7859,\n",
       "                       1.2452, 0.7358, 1.1074, 0.7814, 0.4770, 1.2824, 0.7379, 0.6142, 0.9651,\n",
       "                       0.9260, 0.5504, 0.5593, 0.6499, 0.6584, 0.4491, 1.0249, 0.7382, 0.9084,\n",
       "                       0.7293, 1.2098, 0.9081, 1.1912, 0.4131, 1.0275, 1.1983, 0.4880, 1.0017,\n",
       "                       0.8400, 0.8717, 0.9610, 0.9765, 0.9199, 1.4345, 0.7718, 1.4513, 0.7413,\n",
       "                       0.9843, 0.8423, 0.8699], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-2.4550e-01, -7.0583e-02,  1.0165e-01],\n",
       "                         [-1.2881e-02,  1.1375e-01, -5.7472e-02],\n",
       "                         [ 2.2521e-01,  2.6243e-01, -9.7690e-02]],\n",
       "               \n",
       "                        [[ 3.2756e-01,  1.6647e-01, -5.7900e-02],\n",
       "                         [ 1.7206e-01, -2.6450e-01, -2.6424e-02],\n",
       "                         [-4.5610e-02, -6.5824e-02, -1.3930e-01]],\n",
       "               \n",
       "                        [[-2.6223e-01, -8.8651e-01, -1.7628e-01],\n",
       "                         [ 4.2582e-02, -4.5672e-02, -2.3364e-01],\n",
       "                         [ 8.9514e-02,  1.1614e-01,  1.4581e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.1153e-01,  2.8701e-01, -1.5680e-01],\n",
       "                         [ 2.0830e-01, -1.4658e-01, -2.5149e-01],\n",
       "                         [ 1.3287e-01, -2.3293e-01,  1.8406e-01]],\n",
       "               \n",
       "                        [[-2.1624e-01, -9.8587e-02,  4.5522e-02],\n",
       "                         [-1.2330e-01,  1.0650e-01, -2.6539e-01],\n",
       "                         [-1.0172e-01,  2.0036e-01, -2.2697e-01]],\n",
       "               \n",
       "                        [[ 3.2798e-01,  1.7341e-01, -1.5265e-01],\n",
       "                         [ 1.2728e-01, -3.6572e-01, -2.6068e-01],\n",
       "                         [-8.5376e-02,  3.0680e-02,  8.6793e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.3595e-02,  2.9863e-01,  1.2771e-01],\n",
       "                         [-4.6254e-02,  5.6705e-01,  5.5972e-01],\n",
       "                         [ 1.3490e-01,  6.8911e-01,  5.8283e-01]],\n",
       "               \n",
       "                        [[ 1.1154e-01, -6.8610e-02, -3.1506e-02],\n",
       "                         [ 7.6753e-02,  2.7033e-04, -2.3118e-01],\n",
       "                         [ 5.5881e-02,  2.9074e-01, -2.4195e-02]],\n",
       "               \n",
       "                        [[-3.0587e-02, -2.4537e-01,  1.2448e-02],\n",
       "                         [-1.9095e-01,  1.0947e-01,  1.2538e-01],\n",
       "                         [ 1.0883e-01,  2.0870e-01,  2.4730e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 5.9221e-02, -2.6003e-02, -4.1915e-01],\n",
       "                         [ 1.1460e-01,  3.1237e-01, -1.6163e-01],\n",
       "                         [ 1.2113e-01,  1.5183e-01,  2.4002e-01]],\n",
       "               \n",
       "                        [[-3.8509e-01, -2.0445e-01,  1.6797e-01],\n",
       "                         [-1.6160e-01,  1.4015e-01, -1.4026e-01],\n",
       "                         [ 2.5152e-01,  3.2599e-01,  4.5183e-01]],\n",
       "               \n",
       "                        [[ 1.8231e-01, -1.5691e-01,  1.3338e-02],\n",
       "                         [-1.3949e-01,  4.1793e-02,  1.1197e-01],\n",
       "                         [-4.7997e-02, -2.5364e-02,  2.1427e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.2290e-01, -5.4640e-01, -6.9958e-01],\n",
       "                         [ 1.0006e-01, -2.3374e-02, -9.9947e-02],\n",
       "                         [ 3.1562e-01,  2.2881e-01,  1.8421e-01]],\n",
       "               \n",
       "                        [[ 9.9917e-02,  1.3306e-01,  2.9111e-02],\n",
       "                         [ 2.3437e-01,  1.0363e-01, -3.5433e-02],\n",
       "                         [ 3.6510e-01,  1.2385e-01,  4.3573e-02]],\n",
       "               \n",
       "                        [[-4.5530e-01, -4.3313e-02,  1.9196e-01],\n",
       "                         [ 5.2069e-02,  6.9833e-03,  1.2879e-02],\n",
       "                         [-6.8186e-02,  3.3050e-01,  3.7478e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.4510e-02, -3.7710e-03, -1.5390e-01],\n",
       "                         [ 2.3174e-01, -2.7983e-01, -3.0388e-01],\n",
       "                         [ 3.0722e-01,  1.8152e-01, -6.1590e-01]],\n",
       "               \n",
       "                        [[-2.7028e-01, -5.1731e-01, -7.4208e-01],\n",
       "                         [ 1.4682e-02, -1.0948e-02, -9.1068e-02],\n",
       "                         [ 2.9309e-01,  3.3063e-01,  1.6263e-01]],\n",
       "               \n",
       "                        [[-4.5780e-01, -3.1244e-01, -2.4952e-01],\n",
       "                         [ 1.1341e-01, -2.7278e-01, -1.7275e-01],\n",
       "                         [ 2.9558e-01, -7.5688e-02,  5.6176e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.9877e-01,  9.9871e-02, -1.2622e-01],\n",
       "                         [ 3.6963e-01, -4.1987e-02, -1.8821e-01],\n",
       "                         [ 3.1455e-01, -3.2216e-02,  1.3562e-01]],\n",
       "               \n",
       "                        [[ 1.5572e-01, -9.8052e-02,  1.7208e-02],\n",
       "                         [-2.1017e-01,  1.7668e-01, -2.4732e-01],\n",
       "                         [ 1.0612e-01,  1.5801e-01,  1.6879e-01]],\n",
       "               \n",
       "                        [[-3.1632e-02, -1.2551e-01,  2.6513e-01],\n",
       "                         [-2.3340e-02,  1.0314e-01,  4.1187e-02],\n",
       "                         [ 4.0668e-01,  1.4175e-01, -9.9149e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.7531e-01,  1.1112e-01,  2.5969e-01],\n",
       "                         [ 3.3609e-02,  4.4629e-01, -1.4472e-01],\n",
       "                         [ 1.9912e-02,  3.3521e-01, -3.0833e-02]],\n",
       "               \n",
       "                        [[ 5.7873e-01,  3.8785e-01, -1.3167e-01],\n",
       "                         [ 4.5392e-01, -2.9415e-01, -2.3837e-01],\n",
       "                         [ 1.7466e-01, -2.6371e-01, -4.7010e-01]],\n",
       "               \n",
       "                        [[ 6.8131e-02,  3.0248e-01,  1.7722e-01],\n",
       "                         [ 3.5105e-01,  1.6343e-01,  5.4907e-01],\n",
       "                         [ 3.6277e-01, -8.0636e-02,  2.7709e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.5644e-01, -3.7770e-01, -4.4822e-01],\n",
       "                         [ 4.6227e-01, -2.5546e-01, -2.5212e-01],\n",
       "                         [ 7.6517e-01,  2.7184e-01, -4.1682e-02]],\n",
       "               \n",
       "                        [[-1.7068e-01,  2.0729e-02, -1.2397e-01],\n",
       "                         [-2.4835e-01, -4.3194e-01, -1.5399e-01],\n",
       "                         [-5.2940e-02,  2.7487e-01, -3.7107e-01]],\n",
       "               \n",
       "                        [[ 1.5742e-01,  5.5573e-01,  2.5187e-02],\n",
       "                         [-3.0237e-01,  4.1298e-01,  7.7266e-02],\n",
       "                         [-3.9530e-01, -2.7978e-01, -9.1191e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.9762e-02,  1.8042e-01, -3.9363e-01],\n",
       "                         [-1.1389e-01,  5.3562e-02, -8.5925e-02],\n",
       "                         [ 3.2643e-01,  2.0999e-01, -3.2451e-01]],\n",
       "               \n",
       "                        [[-1.5259e-01, -4.6726e-02, -3.7368e-02],\n",
       "                         [-6.0230e-01, -4.3001e-01, -4.1538e-01],\n",
       "                         [ 1.5315e-01,  2.1337e-01, -2.8751e-02]],\n",
       "               \n",
       "                        [[ 2.2063e-01,  1.8856e-01, -6.6229e-02],\n",
       "                         [ 6.0206e-02,  2.6171e-01, -1.5934e-02],\n",
       "                         [ 5.4239e-02, -1.2904e-01, -5.3348e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.1277e-01, -6.8714e-02,  4.7774e-02],\n",
       "                         [-1.3994e-01,  1.1240e-01, -3.5280e-02],\n",
       "                         [-1.7965e-01, -6.1642e-02, -1.3224e-01]],\n",
       "               \n",
       "                        [[-1.2082e-01, -2.0590e-01, -1.8080e-01],\n",
       "                         [-1.6017e-01, -4.7813e-03, -9.3882e-02],\n",
       "                         [-2.2095e-01, -3.2664e-01,  1.4929e-01]],\n",
       "               \n",
       "                        [[-1.2257e-01,  1.8162e-01, -2.6050e-01],\n",
       "                         [-2.4270e-01, -2.1884e-01, -8.9875e-03],\n",
       "                         [-7.0932e-01,  4.2061e-02, -3.1313e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3492e-01, -1.5707e-01, -2.3038e-01],\n",
       "                         [ 2.0263e-01, -5.1399e-03, -1.5371e-01],\n",
       "                         [ 1.4894e-01,  1.2968e-01, -6.0286e-02]],\n",
       "               \n",
       "                        [[-3.9678e-01, -5.3239e-02, -1.4896e-01],\n",
       "                         [-2.5842e-01,  1.3163e-01,  6.6864e-02],\n",
       "                         [-2.7437e-01, -3.1992e-02,  1.7977e-01]],\n",
       "               \n",
       "                        [[-3.5988e-02, -5.5333e-02, -2.0786e-01],\n",
       "                         [-1.3925e-01,  3.3345e-01, -7.5424e-02],\n",
       "                         [ 1.4782e-01, -1.6331e-01,  1.7908e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 3.4675e-07, -1.3822e-06,  3.6002e-05, -1.1692e-05,  5.6298e-05,\n",
       "                       -2.7983e-05,  4.3969e-04,  1.6799e-04, -1.4712e-05,  3.7967e-06,\n",
       "                       -1.6700e-05,  1.0681e-05, -1.8326e-05,  2.3052e-07, -6.3094e-02,\n",
       "                       -3.9045e-04, -1.0150e-06,  4.7115e-06,  9.7793e-06,  4.2916e-06,\n",
       "                       -1.4289e-05, -8.9777e-07, -7.7754e-06, -3.0376e-05, -1.6199e-06,\n",
       "                        3.4065e-08, -1.4880e-06, -4.5488e-08, -1.0447e-06, -2.9882e-05,\n",
       "                        8.2704e-06,  5.0056e-07, -6.8222e-06, -1.2699e-02,  8.6387e-08,\n",
       "                        7.1075e-06, -2.1831e-06, -3.8726e-06, -1.2091e-04,  9.3394e-05,\n",
       "                        2.3972e-06, -3.2954e-05, -3.8490e-05,  1.9007e-06,  2.8342e-06,\n",
       "                       -3.2070e-04,  7.4958e-06,  1.9534e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.5073, -0.6803, -0.3216, -0.5089, -0.7080, -0.4846, -0.4969, -0.4622,\n",
       "                       -0.3015, -0.5555, -0.6584, -0.5288, -0.3721, -0.5625, -0.5150, -0.4783,\n",
       "                       -0.2941, -0.6178, -0.3896, -0.6185, -0.6657, -0.3833, -0.3716, -0.3223,\n",
       "                       -0.7735, -0.4618, -0.4702, -0.5204, -0.2095, -0.6722, -0.3779, -0.4234,\n",
       "                       -0.5684, -0.5538, -0.4552, -0.4130, -0.4760, -0.5486, -0.3618, -0.5576,\n",
       "                       -0.0569, -0.6033, -0.6100, -0.3355, -0.4784, -0.5823, -0.3100, -0.4946],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9755, 0.7380, 0.7584, 0.9859, 0.9811, 1.0459, 1.1149, 0.9857, 0.7088,\n",
       "                       1.0036, 1.2683, 0.9150, 0.6523, 0.9748, 1.1094, 0.7687, 1.2251, 0.8960,\n",
       "                       0.7689, 0.7856, 1.1112, 0.8639, 1.0723, 1.3231, 1.0842, 0.8305, 1.1354,\n",
       "                       0.8854, 0.9567, 1.1720, 0.6357, 0.7800, 0.9924, 0.8931, 1.0498, 1.0436,\n",
       "                       0.7231, 0.9653, 1.1221, 0.8832, 0.9231, 0.9101, 1.3776, 0.9417, 0.8898,\n",
       "                       0.6269, 0.9028, 0.9212], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-0.0551,  0.0386, -0.2548],\n",
       "                         [-0.0836,  0.1459, -0.3847],\n",
       "                         [-0.1549,  0.0171,  0.1230]],\n",
       "               \n",
       "                        [[-0.1021,  0.1959, -0.0437],\n",
       "                         [ 0.0747, -0.1348, -0.2001],\n",
       "                         [-0.1679, -0.5819,  0.0532]],\n",
       "               \n",
       "                        [[ 0.0995,  0.1882, -0.0167],\n",
       "                         [ 0.2096,  0.1006, -0.0100],\n",
       "                         [ 0.1125, -0.0997,  0.1279]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.3700, -0.7690, -0.6538],\n",
       "                         [-0.1612, -0.2821, -0.0579],\n",
       "                         [-0.0506, -0.3480, -0.2915]],\n",
       "               \n",
       "                        [[ 0.0827,  0.2635,  0.2415],\n",
       "                         [ 0.0458, -0.2100,  0.1349],\n",
       "                         [-0.0598, -0.2657,  0.1379]],\n",
       "               \n",
       "                        [[ 0.4273,  0.0273, -0.0490],\n",
       "                         [ 0.1972, -0.1230, -0.0062],\n",
       "                         [ 0.2973,  0.2393,  0.5921]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.2870,  0.2035, -0.1604],\n",
       "                         [-0.0384, -0.2472, -0.0143],\n",
       "                         [-0.1714, -0.5509, -0.1046]],\n",
       "               \n",
       "                        [[-0.2980, -0.1562,  0.1516],\n",
       "                         [ 0.0349, -0.0638,  0.1129],\n",
       "                         [ 0.0273, -0.0478, -0.0555]],\n",
       "               \n",
       "                        [[-0.3174, -0.2526, -0.2561],\n",
       "                         [-0.0692,  0.3524, -0.1677],\n",
       "                         [ 0.0941,  0.2443,  0.1078]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1601, -0.0299, -0.0410],\n",
       "                         [-0.1526, -0.2685,  0.3021],\n",
       "                         [-0.3314, -0.0891, -0.0585]],\n",
       "               \n",
       "                        [[-0.7138,  0.0470, -0.0622],\n",
       "                         [-0.4234,  0.1162,  0.2022],\n",
       "                         [ 0.0897,  0.2663,  0.0168]],\n",
       "               \n",
       "                        [[ 0.0965, -0.4280, -0.3450],\n",
       "                         [ 0.2361,  0.0708,  0.2990],\n",
       "                         [-0.1464, -0.1316, -0.2739]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1009, -0.1944,  0.0183],\n",
       "                         [ 0.2331, -0.1083, -0.1692],\n",
       "                         [-0.2568, -0.1608, -0.2752]],\n",
       "               \n",
       "                        [[ 0.1822,  0.2075,  0.1133],\n",
       "                         [ 0.0738, -0.0447, -0.4281],\n",
       "                         [-0.0140,  0.1066,  0.0137]],\n",
       "               \n",
       "                        [[-0.0526, -0.3042, -0.1734],\n",
       "                         [-0.1675, -0.1860,  0.2161],\n",
       "                         [ 0.2241,  0.0564,  0.5510]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0011, -0.4297, -0.2971],\n",
       "                         [ 0.3855, -0.2051, -0.3019],\n",
       "                         [ 0.0034, -0.1980, -0.3490]],\n",
       "               \n",
       "                        [[ 0.5324,  0.2993,  0.3033],\n",
       "                         [-0.0986,  0.1608,  0.1256],\n",
       "                         [-0.1107, -0.2541, -0.4343]],\n",
       "               \n",
       "                        [[ 0.0366,  0.1533, -0.2988],\n",
       "                         [ 0.1007, -0.0845, -0.2157],\n",
       "                         [-0.0098,  0.0398, -0.1780]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.2956, -0.2389, -0.3466],\n",
       "                         [ 0.0589,  0.1177,  0.0698],\n",
       "                         [ 0.0139, -0.1299, -0.1228]],\n",
       "               \n",
       "                        [[-0.0276,  0.1440, -0.4216],\n",
       "                         [ 0.0282, -0.2313, -0.4467],\n",
       "                         [-0.1419, -0.1280,  0.4738]],\n",
       "               \n",
       "                        [[ 0.0739, -0.2851, -0.4027],\n",
       "                         [ 0.0277, -0.0399, -0.3409],\n",
       "                         [-0.1397, -0.0254, -0.3392]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0610,  0.3317, -0.2912],\n",
       "                         [-0.0746, -0.2666, -0.0813],\n",
       "                         [ 0.2096, -0.0097, -0.3440]],\n",
       "               \n",
       "                        [[ 0.2202,  0.4956,  0.0582],\n",
       "                         [-0.0024,  0.3627, -0.0543],\n",
       "                         [-0.0682,  0.2893, -0.0248]],\n",
       "               \n",
       "                        [[-0.0135, -0.3340, -0.2262],\n",
       "                         [ 0.2038, -0.1177,  0.0312],\n",
       "                         [ 0.0852,  0.3841, -0.0802]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.2081,  0.2873, -0.0601],\n",
       "                         [-0.1575, -0.1335,  0.0296],\n",
       "                         [ 0.2671,  0.2290, -0.4022]],\n",
       "               \n",
       "                        [[-0.0090, -0.2006, -0.3800],\n",
       "                         [-0.2363, -0.0966, -0.2390],\n",
       "                         [-0.2024, -0.3317, -0.0149]],\n",
       "               \n",
       "                        [[ 0.0867, -0.0202, -0.0143],\n",
       "                         [ 0.2898,  0.1937,  0.1191],\n",
       "                         [ 0.2533, -0.0765, -0.3535]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.2507, -0.2394, -0.0038],\n",
       "                         [-0.0118, -0.0126, -0.1463],\n",
       "                         [ 0.1954, -0.1247,  0.1838]],\n",
       "               \n",
       "                        [[-0.0472, -0.1389, -0.0031],\n",
       "                         [-0.0042, -0.0094, -0.2507],\n",
       "                         [ 0.1166,  0.0937,  0.3499]],\n",
       "               \n",
       "                        [[-0.4724, -0.0689,  0.0020],\n",
       "                         [-0.5414, -0.1167, -0.0732],\n",
       "                         [ 0.2248, -0.0482,  0.3121]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0054,  0.1255, -0.0034],\n",
       "                         [ 0.0862, -0.2127, -0.2793],\n",
       "                         [-0.2717, -0.1859,  0.0695]],\n",
       "               \n",
       "                        [[-0.3568, -0.3591, -0.2921],\n",
       "                         [-0.1340, -0.1198, -0.0012],\n",
       "                         [-0.0049, -0.1523,  0.2611]],\n",
       "               \n",
       "                        [[-0.0319, -0.3560, -0.4190],\n",
       "                         [ 0.0418,  0.1293, -0.0649],\n",
       "                         [ 0.3580,  0.1353,  0.1102]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1890, -0.0299, -0.0055],\n",
       "                         [-0.2736,  0.1236, -0.0349],\n",
       "                         [-0.0298,  0.0247,  0.0491]],\n",
       "               \n",
       "                        [[-0.2591, -0.0104,  0.3085],\n",
       "                         [-0.3485,  0.0615, -0.2684],\n",
       "                         [-0.0247, -0.1887, -0.0024]],\n",
       "               \n",
       "                        [[-0.0888, -0.3364, -0.0901],\n",
       "                         [ 0.2140, -0.7166, -0.1792],\n",
       "                         [-0.1012, -0.0672, -0.0027]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-2.3633e-06,  1.0469e-06,  7.8472e-02,  2.4988e-04, -5.1936e-05,\n",
       "                        1.3128e-04,  5.1106e-06,  2.4732e-06,  7.7365e-05,  3.2479e-04,\n",
       "                       -3.5754e-05,  7.4134e-05,  6.4770e-06,  2.5295e-07, -8.7708e-04,\n",
       "                       -1.8077e-06,  2.8812e-06, -7.9885e-06, -2.8181e-05,  1.3798e-05,\n",
       "                       -2.5233e-05,  4.6732e-06,  7.9271e-06, -1.2385e-05,  3.0971e-05,\n",
       "                       -6.7944e-06,  3.6456e-05,  6.6352e-04,  9.7610e-06,  5.5136e-05,\n",
       "                        1.1681e-06, -3.7099e-05,  1.4990e-06,  2.4114e-05, -3.4525e-06,\n",
       "                       -8.9465e-06,  5.5668e-04,  6.1941e-05,  6.6495e-06,  2.6177e-06,\n",
       "                       -4.6854e-04,  2.0417e-05,  7.2599e-05,  9.6248e-05,  2.3471e-06,\n",
       "                        1.7190e-06,  1.4704e-06, -1.1330e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.7243, -0.7349, -0.6957, -0.9244, -0.5872, -0.6202, -1.5405, -0.5988,\n",
       "                       -0.8811, -0.3065, -0.7493, -0.2038, -0.4154, -0.6770, -0.7211, -0.3975,\n",
       "                       -0.5538, -0.3849, -0.7044, -0.8573, -0.7443, -0.8489, -0.8606, -0.6636,\n",
       "                       -1.1020, -0.7373, -0.6095, -0.7444, -0.7555, -1.2167, -0.9588, -0.6580,\n",
       "                       -0.7397, -0.7734, -0.4599, -1.1118, -0.8484, -0.7505, -0.4869, -0.3458,\n",
       "                       -0.8483, -0.5976, -1.2578, -0.5037, -1.6591, -0.4866, -0.7794, -1.0678],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.9002, 0.8365, 0.7665, 0.9873, 0.8324, 0.7809, 1.3866, 0.6501, 0.7894,\n",
       "                       0.5731, 0.7609, 0.4583, 0.7319, 0.5952, 0.9627, 0.4154, 0.6756, 0.4618,\n",
       "                       0.8304, 0.9394, 0.8271, 0.9145, 0.8206, 0.8766, 1.0868, 0.6922, 0.7333,\n",
       "                       0.8003, 0.7533, 1.0916, 0.8175, 1.0859, 0.8594, 0.7810, 0.7084, 0.9367,\n",
       "                       1.1040, 0.9170, 0.5442, 0.4612, 0.9007, 0.7373, 1.3747, 0.5808, 1.0918,\n",
       "                       0.6811, 0.7711, 1.1068], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-2.8310e-01, -2.0914e-01, -6.5285e-02],\n",
       "                         [ 3.4184e-02, -2.1975e-02,  2.8535e-02],\n",
       "                         [-6.8668e-02,  1.8950e-01, -4.4843e-01]],\n",
       "               \n",
       "                        [[ 4.0438e-01,  1.6353e-01,  1.8204e-01],\n",
       "                         [ 2.6569e-01,  6.7784e-02,  3.4256e-03],\n",
       "                         [ 6.0006e-01,  5.3307e-01,  1.2031e-01]],\n",
       "               \n",
       "                        [[ 1.3756e-01,  1.7761e-01,  3.6982e-01],\n",
       "                         [ 9.2377e-02, -8.1022e-02, -2.1848e-01],\n",
       "                         [ 1.3620e-01,  1.0908e-01,  8.1515e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.6709e-01,  3.3983e-01,  5.7269e-01],\n",
       "                         [ 3.1856e-01,  1.3676e-01, -1.8426e-02],\n",
       "                         [ 1.5065e-01,  7.7990e-02, -1.9569e-02]],\n",
       "               \n",
       "                        [[-1.2466e-01,  9.7893e-03, -2.6080e-02],\n",
       "                         [ 2.4879e-01,  6.8848e-03, -4.4428e-02],\n",
       "                         [-9.6185e-02,  4.5038e-01,  1.4433e-01]],\n",
       "               \n",
       "                        [[-2.3295e-01, -2.4837e-01, -3.4987e-01],\n",
       "                         [-1.5908e-01, -2.6149e-01, -2.2362e-01],\n",
       "                         [-9.7676e-02, -2.1808e-01, -3.8856e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 9.4815e-02,  3.5949e-01,  3.5619e-01],\n",
       "                         [ 2.1663e-02,  4.6284e-02,  7.6257e-02],\n",
       "                         [-1.4976e-01, -6.8081e-02, -1.2647e-01]],\n",
       "               \n",
       "                        [[-4.7403e-02, -4.2210e-01,  2.9898e-02],\n",
       "                         [ 4.2945e-02, -2.1895e-01, -2.3066e-01],\n",
       "                         [-1.8456e-01,  1.9031e-01, -8.5742e-02]],\n",
       "               \n",
       "                        [[-1.5009e-02, -1.1156e-01,  2.5760e-01],\n",
       "                         [ 1.7109e-02, -1.2930e-02, -4.5699e-02],\n",
       "                         [-3.5880e-01, -4.2921e-02, -3.3210e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.8035e-02, -7.0373e-02, -2.3561e-01],\n",
       "                         [-5.2749e-01,  2.8336e-03, -1.5097e-01],\n",
       "                         [-1.6993e-01, -3.0056e-01, -3.1711e-01]],\n",
       "               \n",
       "                        [[ 7.8038e-02, -5.9466e-01,  8.4680e-02],\n",
       "                         [-1.5156e-01, -1.7239e-03, -1.9964e-01],\n",
       "                         [-1.7114e-01,  1.0609e-01,  1.4588e-01]],\n",
       "               \n",
       "                        [[ 1.0817e-01,  2.0990e-01,  1.1826e-01],\n",
       "                         [ 2.0106e-01,  5.5757e-02,  1.4667e-01],\n",
       "                         [ 2.2534e-01,  1.2010e-02, -3.8204e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.2592e-01,  6.3338e-01, -4.2481e-02],\n",
       "                         [-2.0465e-01,  7.1708e-01,  4.2461e-01],\n",
       "                         [-4.0162e-02,  9.3341e-02,  1.4045e-01]],\n",
       "               \n",
       "                        [[-1.7965e-01, -6.8501e-02, -2.1684e-01],\n",
       "                         [-6.3048e-01, -5.1482e-01, -3.9200e-02],\n",
       "                         [-5.3577e-01, -5.9225e-01, -3.0859e-01]],\n",
       "               \n",
       "                        [[ 1.0686e-01,  2.7417e-01,  6.5716e-02],\n",
       "                         [ 7.5422e-02,  1.9976e-01,  1.3875e-02],\n",
       "                         [-2.0662e-01, -1.0873e-01,  6.3057e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.2341e-01, -3.8039e-01, -2.6171e-01],\n",
       "                         [-3.7921e-01, -3.7799e-01, -5.4610e-01],\n",
       "                         [-4.8741e-01, -4.4255e-01, -3.0622e-01]],\n",
       "               \n",
       "                        [[ 1.1612e-01, -4.9641e-01, -1.2872e-01],\n",
       "                         [-9.2748e-02,  1.4422e-01, -1.2655e-01],\n",
       "                         [ 9.3112e-02, -9.5108e-02, -3.6291e-01]],\n",
       "               \n",
       "                        [[ 1.9802e-01,  3.5046e-02,  5.7140e-02],\n",
       "                         [-4.4809e-01,  9.2866e-02,  1.8988e-02],\n",
       "                         [-2.9893e-01, -1.0889e-02, -5.3089e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-7.8406e-02,  4.9826e-02,  7.0721e-02],\n",
       "                         [-5.7641e-02, -4.9315e-02, -4.9498e-03],\n",
       "                         [-5.3969e-02,  7.9533e-02, -8.5407e-02]],\n",
       "               \n",
       "                        [[-3.2632e-01,  1.5888e-01,  1.1490e-01],\n",
       "                         [-6.9644e-02,  5.1899e-03,  1.1905e-01],\n",
       "                         [-4.7614e-02, -1.4216e-01, -9.1484e-02]],\n",
       "               \n",
       "                        [[-2.6486e-02, -3.0101e-01, -7.8414e-02],\n",
       "                         [ 7.3886e-02,  1.1270e-01, -6.5406e-02],\n",
       "                         [ 2.5990e-02, -7.8440e-02,  1.5450e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.6765e-01,  2.5716e-01, -2.6811e-02],\n",
       "                         [ 1.8537e-01,  1.0663e-01, -8.4637e-02],\n",
       "                         [-9.6355e-02,  1.1779e-02, -1.6459e-01]],\n",
       "               \n",
       "                        [[-9.3641e-03, -9.5721e-03, -2.3495e-01],\n",
       "                         [-6.4733e-02, -4.4944e-01, -3.6356e-01],\n",
       "                         [ 1.4458e-01, -1.1898e-01,  1.4731e-01]],\n",
       "               \n",
       "                        [[ 1.3736e-01,  8.4425e-02,  2.2426e-01],\n",
       "                         [-2.0682e-01, -6.5508e-02,  2.5239e-02],\n",
       "                         [-1.4133e-01,  6.8302e-02, -1.1116e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.7145e-03,  7.3694e-03,  7.0604e-02],\n",
       "                         [ 2.3968e-02,  9.1227e-02, -2.2583e-02],\n",
       "                         [ 5.6406e-02,  5.1781e-02, -9.5081e-02]],\n",
       "               \n",
       "                        [[-1.3195e-02, -5.3338e-02, -4.3132e-03],\n",
       "                         [ 1.3948e-01, -6.9245e-02, -2.2298e-03],\n",
       "                         [ 2.2535e-01,  8.9276e-02, -5.9099e-02]],\n",
       "               \n",
       "                        [[-7.3223e-02,  8.4277e-02, -7.8187e-03],\n",
       "                         [ 4.1766e-02,  1.1646e-02,  6.7504e-02],\n",
       "                         [-4.5926e-02, -1.7276e-02, -4.1100e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.2332e-01,  1.2797e-01,  1.3707e-01],\n",
       "                         [ 6.3272e-03,  5.0328e-02,  3.7540e-02],\n",
       "                         [-3.8476e-02,  8.7270e-02,  3.2585e-02]],\n",
       "               \n",
       "                        [[-6.6257e-02, -1.6422e-02,  6.5831e-02],\n",
       "                         [ 3.7520e-03,  1.4050e-02,  2.7304e-02],\n",
       "                         [ 2.2253e-03,  1.0966e-01, -6.3325e-03]],\n",
       "               \n",
       "                        [[ 2.0294e-02,  9.6318e-02, -1.2270e-01],\n",
       "                         [ 1.1732e-02,  4.9912e-02, -3.3140e-02],\n",
       "                         [-3.6757e-02, -4.8870e-03, -1.2425e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1599e-02, -4.2979e-02,  1.0060e-03],\n",
       "                         [ 5.9407e-02,  1.8185e-02,  2.2814e-04],\n",
       "                         [ 9.5778e-02, -2.0595e-02, -3.9948e-02]],\n",
       "               \n",
       "                        [[-5.0383e-02, -6.7711e-02, -4.0695e-03],\n",
       "                         [ 1.0549e-01, -2.0189e-02, -5.2930e-02],\n",
       "                         [ 1.9371e-01,  1.2850e-01, -3.5729e-02]],\n",
       "               \n",
       "                        [[-1.0376e-01,  1.6325e-01,  6.6750e-03],\n",
       "                         [ 5.5700e-02,  2.9303e-03,  8.7001e-02],\n",
       "                         [-6.1027e-02,  3.2602e-02,  1.3172e-04]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.4478e-02,  5.3473e-02,  9.1972e-02],\n",
       "                         [-1.0676e-02,  5.8015e-02,  1.2178e-01],\n",
       "                         [-2.8701e-02,  6.7987e-02,  1.1676e-01]],\n",
       "               \n",
       "                        [[-2.0196e-02,  1.6919e-02,  4.3047e-02],\n",
       "                         [-3.7918e-02, -1.2018e-02,  3.2968e-02],\n",
       "                         [ 4.1434e-02,  1.0655e-01, -2.7636e-02]],\n",
       "               \n",
       "                        [[ 6.2689e-02,  1.0456e-01,  2.7545e-03],\n",
       "                         [ 3.6871e-02,  7.5025e-02,  6.4456e-02],\n",
       "                         [-1.0447e-02, -1.4049e-02, -8.7212e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 0.2010,  0.2236,  0.0486,  0.2353,  0.0010,  0.0091,  0.1996, -0.0036,\n",
       "                        0.1802,  0.1719,  0.1533,  0.1640,  0.0031,  0.0027,  0.0048,  0.1159,\n",
       "                        0.1487,  0.2060,  0.0016,  0.1909,  0.1915, -0.0082,  0.0669,  0.2673,\n",
       "                        0.1338,  0.1493,  0.0645,  0.1237, -0.0065, -0.0085,  0.1718,  0.2095,\n",
       "                       -0.0068, -0.0045, -0.0039, -0.0078,  0.0311,  0.0232,  0.1952, -0.0109,\n",
       "                        0.1167,  0.0860,  0.1613,  0.0374, -0.0316,  0.0182,  0.1428,  0.1329],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.4066, -0.4206, -0.3590, -0.6283, -0.7477, -0.6760, -0.5424, -0.4238,\n",
       "                       -0.5798, -0.4787, -0.4408, -0.4739, -0.5400, -0.5663, -0.3085, -0.4903,\n",
       "                       -0.8057, -0.4369, -0.6719, -0.4819, -0.2953, -0.7309, -0.1803, -0.7399,\n",
       "                       -0.6059, -0.6555, -0.5482, -0.2849, -0.8180, -0.6749, -0.6550, -0.6527,\n",
       "                       -0.2826, -0.5522, -0.1158, -0.5288, -0.8311, -0.1753, -0.5979, -0.5782,\n",
       "                       -0.5606, -0.5134, -0.3055, -0.2297, -0.7966, -0.2473, -0.5253, -0.5131],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.4355, 0.5044, 0.2624, 0.4089, 0.2649, 0.3203, 0.5236, 0.1470, 0.5827,\n",
       "                       0.4505, 0.4273, 0.4006, 0.2180, 0.1924, 0.1078, 0.3850, 0.6482, 0.4722,\n",
       "                       0.2644, 0.5090, 0.3512, 0.2718, 0.1048, 0.5889, 0.3515, 0.3442, 0.2368,\n",
       "                       0.3057, 0.2594, 0.2971, 0.5312, 0.5481, 0.0856, 0.1698, 0.0461, 0.2474,\n",
       "                       0.5142, 0.0805, 0.4602, 0.2276, 0.5240, 0.3991, 0.3274, 0.1164, 0.3288,\n",
       "                       0.0700, 0.4186, 0.4076], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[ 0.0047, -0.0215, -0.0624,  ..., -0.2331, -0.1596, -0.1752],\n",
       "                       [-0.0128, -0.0402, -0.0117,  ..., -0.2256, -0.1424, -0.1187],\n",
       "                       [-0.0512, -0.0676, -0.0439,  ..., -0.1770, -0.1248, -0.1140],\n",
       "                       [ 0.1212,  0.1377,  0.0317,  ..., -0.2126, -0.1075, -0.1941],\n",
       "                       [ 0.0609,  0.1498,  0.0380,  ...,  0.4780,  0.3586,  0.4213]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0251, -0.0171, -0.0427, -0.0276, -0.0232], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('arbiter.0.weight',\n",
       "               tensor([[-2.2247e-01, -1.5931e-01,  9.6521e-02,  1.1918e-01, -7.0226e-02,\n",
       "                        -1.1884e-01, -7.2531e-02,  2.5318e-01, -2.2093e-01, -4.7102e-02,\n",
       "                         1.7146e-01,  8.8539e-02,  3.5216e-02,  9.6989e-03,  4.9386e-02,\n",
       "                         4.2055e-02, -1.7720e-01,  1.4861e-01,  1.3337e-01, -1.0836e-01],\n",
       "                       [ 1.1339e+00, -1.3940e+00,  6.2818e-01, -6.6696e-01,  9.5432e-01,\n",
       "                        -6.7190e-01,  1.0401e+00, -7.8026e-01,  5.8989e-01, -6.8534e-01,\n",
       "                        -1.2912e+00, -7.0929e-01, -5.5973e-01, -5.9905e-01, -7.1229e-01,\n",
       "                        -8.5548e-01, -3.7023e-01, -5.2063e-01,  4.0425e-01, -8.8635e-01],\n",
       "                       [-1.7650e-01, -7.4263e-02, -1.9603e-01,  1.7395e-01, -1.2017e-01,\n",
       "                         1.5592e-01, -2.8765e-03,  7.1014e-02,  7.4614e-02,  1.7035e-01,\n",
       "                        -1.8075e-01, -3.1389e-03, -6.3424e-02,  4.2344e-02, -8.9193e-02,\n",
       "                         1.8377e-01, -5.3476e-02,  1.4562e-01, -1.4323e-01, -1.9829e-01],\n",
       "                       [ 6.8407e-01, -8.2890e-01,  6.8174e-01, -7.5535e-01,  8.4494e-01,\n",
       "                        -7.3308e-01,  9.2288e-01, -6.0184e-01,  4.0720e-01, -6.2824e-01,\n",
       "                        -8.6108e-01, -7.1613e-01, -4.6364e-01, -8.8631e-01, -6.2843e-01,\n",
       "                        -5.6570e-01, -5.5475e-01, -8.5190e-01, -1.6904e-01, -5.8001e-01],\n",
       "                       [ 6.1037e-01, -1.6474e+00,  9.8530e-01, -6.7701e-01,  6.2578e-01,\n",
       "                        -7.1100e-01,  9.6076e-01, -1.3746e+00,  2.6624e-01, -8.7000e-01,\n",
       "                         1.8483e-01, -8.4143e-01, -6.7921e-01, -8.0056e-01, -6.3000e-01,\n",
       "                        -5.7448e-01, -3.4261e-01, -8.1264e-01,  3.0343e-01, -6.4937e-01],\n",
       "                       [-1.3744e-01,  8.8859e-02, -1.4254e-01, -2.0478e-02,  9.0880e-03,\n",
       "                         1.4116e-01,  1.2301e-02, -6.6963e-02, -1.7384e-01,  1.4978e-01,\n",
       "                         1.8470e-01, -1.6821e-01, -6.7825e-02, -9.9556e-02, -1.1804e-01,\n",
       "                         1.8543e-01,  5.4935e-02, -1.3889e-01, -8.5513e-02, -7.6293e-02],\n",
       "                       [-3.1918e-03,  1.6702e-01, -2.5259e-02, -5.8045e-02, -1.9568e-01,\n",
       "                        -1.8738e-01, -1.0919e-01, -2.0999e-01, -1.4216e-01,  3.0850e-02,\n",
       "                        -1.5226e-01,  1.8288e-01,  1.7433e-01, -8.3347e-02, -3.3173e-03,\n",
       "                         2.0585e-01,  3.2247e-02, -1.9702e-02, -1.5549e-01, -3.7919e-02],\n",
       "                       [-5.2119e-02,  5.8366e-02, -9.5376e-02,  4.1151e-02, -1.4242e-02,\n",
       "                        -1.3989e-01, -2.9618e-02, -4.6825e-02,  1.5251e-02, -5.1655e-03,\n",
       "                        -1.6485e-01,  1.5387e-01,  1.4536e-01,  3.0741e-02, -1.5371e-01,\n",
       "                         2.2002e-02, -5.1757e-02, -5.1059e-02, -1.4909e-02,  2.1917e-01],\n",
       "                       [ 8.6941e-01, -6.9592e-01,  8.4204e-01, -5.3590e-01,  9.1796e-01,\n",
       "                        -7.4439e-01,  8.4390e-01, -8.8405e-01,  7.3470e-01, -5.9659e-01,\n",
       "                        -9.6316e-01, -8.0717e-01, -4.9581e-01, -4.9376e-01, -8.4781e-01,\n",
       "                        -7.7259e-01, -5.9439e-01, -5.1684e-01, -1.1509e-01, -5.7943e-01],\n",
       "                       [ 9.6380e-01, -1.0576e+00,  7.2722e-01, -5.1482e-01,  8.9669e-01,\n",
       "                        -6.0853e-01,  7.4861e-01, -9.0947e-01,  6.6959e-01, -6.9792e-01,\n",
       "                        -9.8941e-01, -7.3063e-01, -1.0287e+00, -8.4421e-01, -8.5455e-01,\n",
       "                        -8.7129e-01, -6.4758e-01, -7.1543e-01,  1.1148e-01, -6.0509e-01],\n",
       "                       [ 9.4558e-01, -9.7096e-01,  6.5784e-01, -7.6383e-01,  7.3809e-01,\n",
       "                        -5.9879e-01,  9.0801e-01, -5.3441e-01,  1.3402e+00, -4.8692e-01,\n",
       "                        -1.7795e+00, -4.4081e-01, -1.0958e+00, -3.7528e-01, -1.1250e+00,\n",
       "                        -4.9806e-01, -9.2816e-01, -4.2413e-01,  2.9029e-01, -7.8440e-01],\n",
       "                       [-1.6299e-01,  1.9557e-01, -1.6206e-01,  2.2474e-01,  3.2993e-02,\n",
       "                         2.3002e-02,  9.7205e-02, -1.7096e-01,  1.7325e-01, -1.4765e-01,\n",
       "                        -7.1639e-02, -7.4825e-02,  4.6794e-02,  1.8881e-01, -1.0527e-01,\n",
       "                         9.9648e-02,  4.8222e-02, -1.8196e-02, -3.7068e-02,  2.4631e-01],\n",
       "                       [ 1.2785e-03,  7.8483e-02, -1.6075e-01,  3.5523e-03, -5.6337e-02,\n",
       "                         1.3809e-01, -1.3024e-01,  3.6182e-02,  1.8668e-01, -1.7668e-01,\n",
       "                        -1.1430e-01, -1.3822e-01,  1.0208e-01,  1.4600e-01, -1.7406e-01,\n",
       "                        -1.6048e-01,  1.0708e-01,  6.5007e-03,  3.6957e-02, -1.8922e-01],\n",
       "                       [ 5.0203e-02, -3.2657e-03, -1.4001e-01, -5.2389e-02,  8.2383e-02,\n",
       "                         1.4948e-02, -1.9413e-01,  2.1511e-01, -2.2565e-01, -2.6085e-02,\n",
       "                        -1.4133e-01,  6.7594e-02,  1.2138e-01, -1.7762e-01, -2.2753e-01,\n",
       "                        -2.7562e-02,  5.4655e-02, -2.3453e-02,  1.2776e-01, -1.9326e-01],\n",
       "                       [ 2.1353e-01,  2.0006e-01, -1.8942e-01,  7.9496e-02,  4.1478e-02,\n",
       "                        -1.1179e-01,  5.3119e-02,  1.8549e-01, -3.3323e-03, -1.2598e-01,\n",
       "                         2.5711e-02, -2.9604e-03, -2.0496e-02,  1.7962e-01, -1.9416e-02,\n",
       "                        -4.8471e-02, -7.4401e-02,  2.2016e-01, -8.8840e-03, -1.8109e-01],\n",
       "                       [ 6.9884e-01, -1.9052e+00,  8.9536e-01, -8.5159e-01,  8.8327e-01,\n",
       "                        -5.6440e-01,  6.9266e-01, -1.2121e+00,  3.1834e-02, -9.6104e-01,\n",
       "                        -2.1650e-01, -7.3143e-01, -6.7014e-01, -9.5005e-01, -6.9524e-01,\n",
       "                        -6.0115e-01, -2.8879e-01, -9.6045e-01,  3.7924e-02, -7.1621e-01],\n",
       "                       [ 8.7824e-01, -8.6416e-01,  6.7471e-01, -6.9491e-01,  7.6214e-01,\n",
       "                        -8.0100e-01,  5.8003e-01, -1.0440e+00,  5.8952e-01, -5.3776e-01,\n",
       "                        -1.6916e-01, -7.3688e-01, -6.3667e-01, -8.5702e-01, -4.9527e-01,\n",
       "                        -7.1978e-01, -5.0668e-01, -6.7977e-01, -2.2152e-02, -4.3600e-01],\n",
       "                       [ 1.5881e-01,  1.8694e-01, -9.5665e-02, -1.6845e-01,  7.3781e-02,\n",
       "                         1.4416e-01, -1.7335e-01, -1.3803e-01,  1.9204e-01, -1.5731e-01,\n",
       "                        -4.9296e-02,  2.4749e-02,  1.6346e-02,  1.5047e-01,  8.7614e-02,\n",
       "                        -4.4198e-02,  3.5807e-02, -1.8876e-01,  6.7670e-02,  1.8616e-01],\n",
       "                       [ 8.4664e-01, -1.2164e+00,  6.2694e-01, -6.7528e-01,  8.9729e-01,\n",
       "                        -7.6841e-01,  7.6079e-01, -1.1419e+00,  2.6913e-01, -7.6785e-01,\n",
       "                         7.7611e-02, -4.7037e-01, -5.2229e-01, -5.5774e-01, -5.6529e-01,\n",
       "                        -5.2306e-01, -4.3142e-01, -7.8037e-01,  1.5382e-01, -6.6937e-01],\n",
       "                       [ 9.0034e-02, -1.4732e-01,  1.2791e-01,  1.8062e-01, -2.2643e-01,\n",
       "                        -1.6065e-01,  4.1732e-02, -1.9450e-01, -1.8327e-01,  1.2446e-01,\n",
       "                        -1.2507e-01, -1.0608e-01, -5.7903e-02,  8.1527e-02,  1.6932e-01,\n",
       "                         2.4688e-02,  1.3024e-01,  1.7540e-01,  7.2658e-02,  2.2244e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.0.bias',\n",
       "               tensor([ 0.0201,  0.9119,  0.0644,  0.7064,  0.8057, -0.0581,  0.0711, -0.2082,\n",
       "                        0.8379,  0.7319,  0.8915, -0.1592,  0.0547, -0.0025, -0.0097,  0.6112,\n",
       "                        0.8677, -0.1443,  0.7415,  0.0019], device='cuda:0')),\n",
       "              ('arbiter.2.weight',\n",
       "               tensor([[-1.6730e-01, -1.1959e-01, -1.9416e-01,  7.7802e-02,  1.2669e-01,\n",
       "                         1.5178e-01,  3.6239e-02,  2.3965e-02,  1.5949e-02, -1.0364e-02,\n",
       "                        -4.2260e-01, -2.8065e-02, -4.9352e-02, -1.2828e-01, -1.1449e-01,\n",
       "                         4.1683e-02,  1.4762e-01, -1.8167e-02,  1.8168e-01, -1.9080e-01],\n",
       "                       [ 1.0153e-01,  2.1473e+00,  2.0148e-02,  1.8247e+00,  2.1405e+00,\n",
       "                         1.4975e-01,  1.4924e-01,  1.6985e-02,  2.0014e+00,  1.8177e+00,\n",
       "                         2.2863e+00, -9.3935e-02,  1.9514e-01, -1.6500e-01,  1.4378e-01,\n",
       "                         2.1581e+00,  2.2681e+00,  1.1773e-01,  2.0554e+00,  1.6454e-01],\n",
       "                       [ 7.5235e-03, -2.5820e-01, -2.0589e-01, -1.1547e-01, -3.5018e-01,\n",
       "                         2.1607e-01,  1.5543e-01, -1.1048e-01,  1.7357e-02, -1.8394e-03,\n",
       "                        -4.0284e-01,  5.9812e-03, -7.0057e-02, -1.9036e-01,  3.9366e-03,\n",
       "                        -2.6834e-01, -1.7219e-01, -2.9052e-02, -1.1525e-01, -1.1373e-01],\n",
       "                       [ 6.7662e-02, -2.4105e-01,  1.6478e-01, -6.3654e-02, -2.1666e-01,\n",
       "                        -2.4171e-01,  6.4815e-02, -1.5311e-01,  1.6604e-02, -3.1266e-02,\n",
       "                        -1.3559e-01, -1.6420e-02,  1.8723e-01, -6.1182e-02, -1.8708e-01,\n",
       "                        -7.4751e-02, -2.9204e-01,  6.4915e-02, -1.0951e-01,  9.2902e-02],\n",
       "                       [ 9.9647e-02,  1.7909e-01,  2.1410e-01,  2.1573e-01, -2.7112e-02,\n",
       "                         1.1768e-01,  6.1226e-02, -5.4656e-02,  2.0588e-01,  1.6151e-01,\n",
       "                         3.0651e-01,  1.2463e-01, -4.2923e-02,  4.1856e-02,  9.8967e-02,\n",
       "                         2.3818e-03,  1.5596e-01,  8.1670e-02,  2.1747e-02,  2.3783e-02],\n",
       "                       [ 1.2777e-02, -1.0278e-01,  9.4219e-02,  3.3310e-02, -1.2960e-01,\n",
       "                         2.0553e-01,  1.8008e-01, -1.9471e-01,  2.7876e-02, -2.2569e-01,\n",
       "                         2.3035e-01, -1.4089e-01,  1.7361e-01,  1.2626e-01,  2.2125e-01,\n",
       "                         6.3840e-02, -2.5975e-01, -4.3063e-02, -3.2873e-01, -1.7439e-01],\n",
       "                       [-3.4667e-02,  8.3479e-01, -2.1800e-01,  7.7901e-01,  6.7884e-01,\n",
       "                        -1.2479e-01,  7.4694e-02, -2.1838e-01,  8.7846e-01,  9.8893e-01,\n",
       "                         1.0813e+00, -2.2720e-01, -6.6055e-02, -2.0785e-02, -8.1823e-02,\n",
       "                         7.3780e-01,  8.3614e-01, -2.1515e-01,  9.1527e-01, -9.1618e-02],\n",
       "                       [-1.5452e-01,  1.4643e+00, -1.7027e-01,  1.6270e+00,  1.5611e+00,\n",
       "                        -2.1366e-01,  4.6392e-02, -7.1841e-02,  1.5739e+00,  1.6726e+00,\n",
       "                         1.6515e+00,  3.3550e-02,  3.4054e-02, -1.1829e-01, -7.6839e-02,\n",
       "                         1.2949e+00,  1.6491e+00,  1.9570e-01,  1.2938e+00, -2.0401e-01],\n",
       "                       [-1.5696e-01,  1.1218e+00,  6.9845e-02,  1.0845e+00,  1.2435e+00,\n",
       "                        -8.0150e-02, -1.7155e-01, -9.1709e-02,  9.8733e-01,  1.0914e+00,\n",
       "                         7.9755e-01, -4.0581e-02,  2.8706e-02, -1.7417e-01,  2.1412e-01,\n",
       "                         1.3397e+00,  1.1466e+00,  1.6001e-01,  1.3013e+00, -1.4447e-01],\n",
       "                       [-7.8591e-02, -5.9629e-02,  9.2624e-02, -8.9901e-02, -7.2837e-02,\n",
       "                         2.1472e-01,  1.8529e-01,  1.1874e-01, -4.1833e-02, -1.8833e-01,\n",
       "                        -6.6415e-01, -6.1061e-02,  2.0867e-01, -1.1245e-01, -1.3026e-01,\n",
       "                        -3.7516e-01, -1.5701e-01,  2.0398e-01, -2.7524e-01,  2.8911e-02]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.2.bias',\n",
       "               tensor([-0.0093,  2.2027, -0.1076, -0.1869,  0.1249, -0.2151,  0.2933,  1.5668,\n",
       "                        1.1532, -0.2841], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3880010995864869,\n",
       "   1.1681266173124314,\n",
       "   1.0543024513721466,\n",
       "   1.0028827078342437,\n",
       "   0.9615960403680801,\n",
       "   0.9269297261238099,\n",
       "   0.9020256378650665,\n",
       "   0.872249094247818,\n",
       "   0.8478070604205131,\n",
       "   0.8314304674863815,\n",
       "   0.7922845501303672,\n",
       "   0.7835283778309822,\n",
       "   0.7652127574682236,\n",
       "   0.7428346804976463,\n",
       "   0.7332226045131683,\n",
       "   0.7308824400901794,\n",
       "   0.705058721601963,\n",
       "   0.6948017406463624,\n",
       "   0.6930058258771896,\n",
       "   0.6839108386039734,\n",
       "   0.680693097770214,\n",
       "   0.6602493169307708,\n",
       "   0.6527665069699288,\n",
       "   0.6603941802978516,\n",
       "   0.6491406707167625,\n",
       "   0.6506650331020355,\n",
       "   0.6211030874848366,\n",
       "   0.631516472697258,\n",
       "   0.6266675333380699,\n",
       "   0.6418148913383483,\n",
       "   0.6225920960903167,\n",
       "   0.6224332730770111,\n",
       "   0.6151731034517288,\n",
       "   0.5966151193976402,\n",
       "   0.6072314867973327,\n",
       "   0.6112588542401791,\n",
       "   0.5932681545615196,\n",
       "   0.588482424557209,\n",
       "   0.5891214713454247,\n",
       "   0.5740478490591049,\n",
       "   0.5895799670219422,\n",
       "   0.5787645511031151,\n",
       "   0.5740910849571228,\n",
       "   0.5875121900439262,\n",
       "   0.5626180595755577,\n",
       "   0.5696809201836586,\n",
       "   0.5647759016156196,\n",
       "   0.5609338417649269,\n",
       "   0.569571004062891,\n",
       "   0.559442238688469,\n",
       "   0.5664950123429299,\n",
       "   0.5425387865006923,\n",
       "   0.5519939655363559,\n",
       "   0.5421509275436401,\n",
       "   0.5614877280592918,\n",
       "   0.550574865013361,\n",
       "   0.5462881864309311,\n",
       "   0.5507248792946339,\n",
       "   0.5487677071094513,\n",
       "   0.5512309635877609,\n",
       "   0.5514772172570228,\n",
       "   0.5507675117850304,\n",
       "   0.5388385071754456,\n",
       "   0.5326644408702851,\n",
       "   0.5503542674779892,\n",
       "   0.5595048902034759,\n",
       "   0.5605040855705739,\n",
       "   0.5318098911345005,\n",
       "   0.5330302594304085,\n",
       "   0.5450326809883118,\n",
       "   0.5329351550042629,\n",
       "   0.5452175827622414,\n",
       "   0.5499126724302769,\n",
       "   0.5484648705124855,\n",
       "   0.5291688284873962,\n",
       "   0.5271694138050079,\n",
       "   0.5388342508673668,\n",
       "   0.5393127513229847,\n",
       "   0.5420061250030994,\n",
       "   0.5373164128363133,\n",
       "   0.5268589859306813,\n",
       "   0.5365027299523354,\n",
       "   0.5262212351560592,\n",
       "   0.5327360532879829,\n",
       "   0.5246948049068451,\n",
       "   0.531257542192936,\n",
       "   0.5197355850934983,\n",
       "   0.5251022400856018,\n",
       "   0.5219416041076184,\n",
       "   0.5181742776036262,\n",
       "   0.520197881191969,\n",
       "   0.5178069486916065,\n",
       "   0.4986466233730316,\n",
       "   0.5120488120913506,\n",
       "   0.5180639134347439,\n",
       "   0.5253360925018787,\n",
       "   0.5145994372069835,\n",
       "   0.5116343019604683,\n",
       "   0.5055164863467216],\n",
       "  'train_loss_std': [0.1718720553380148,\n",
       "   0.13543342014482987,\n",
       "   0.15233810823942176,\n",
       "   0.13925313618004417,\n",
       "   0.13911213106800488,\n",
       "   0.13885642837270176,\n",
       "   0.14440579753894686,\n",
       "   0.1342315987283224,\n",
       "   0.14564704997778782,\n",
       "   0.14082242392747174,\n",
       "   0.13822568008662908,\n",
       "   0.14435348531644207,\n",
       "   0.13171308891804998,\n",
       "   0.14125151223621724,\n",
       "   0.13212036752060216,\n",
       "   0.14017007146898922,\n",
       "   0.1308188378766614,\n",
       "   0.13588048037026065,\n",
       "   0.13902349559961594,\n",
       "   0.1407540970569683,\n",
       "   0.1373276417993746,\n",
       "   0.12415524278795766,\n",
       "   0.1339184239042131,\n",
       "   0.13771383734837706,\n",
       "   0.13035643681054182,\n",
       "   0.14151314597366113,\n",
       "   0.13065243016184436,\n",
       "   0.12869313522516423,\n",
       "   0.12998550101484732,\n",
       "   0.13807770276899353,\n",
       "   0.13168075685302483,\n",
       "   0.13202116127793864,\n",
       "   0.13416423718600223,\n",
       "   0.13453603043511175,\n",
       "   0.13201436556185295,\n",
       "   0.14201132765948005,\n",
       "   0.1261596100997624,\n",
       "   0.1367771301803189,\n",
       "   0.1376009683822406,\n",
       "   0.13796446632692544,\n",
       "   0.1328359633874348,\n",
       "   0.13372205079298455,\n",
       "   0.12972398676590488,\n",
       "   0.13856240163147013,\n",
       "   0.13297252534249657,\n",
       "   0.1400199451309861,\n",
       "   0.13784142286408618,\n",
       "   0.1294204858446371,\n",
       "   0.1357055057001211,\n",
       "   0.13289227407452023,\n",
       "   0.13383902257712776,\n",
       "   0.12533856388999665,\n",
       "   0.13222042406819773,\n",
       "   0.13198655038527093,\n",
       "   0.1353321137332332,\n",
       "   0.1310135787218593,\n",
       "   0.13202928021903101,\n",
       "   0.13532369591149776,\n",
       "   0.12770903065083572,\n",
       "   0.13304869953271548,\n",
       "   0.13163240756649658,\n",
       "   0.1374003643186059,\n",
       "   0.13291842341969776,\n",
       "   0.13085078345380485,\n",
       "   0.13287801800555007,\n",
       "   0.13297659427306505,\n",
       "   0.13399650606488195,\n",
       "   0.14036222894527328,\n",
       "   0.1273058538908108,\n",
       "   0.1369425225172201,\n",
       "   0.12579883064567987,\n",
       "   0.12757776142672758,\n",
       "   0.12763509750743818,\n",
       "   0.13682456694201378,\n",
       "   0.13074319763275172,\n",
       "   0.12862558184315884,\n",
       "   0.13675631764982907,\n",
       "   0.13179777112812996,\n",
       "   0.12743522955126366,\n",
       "   0.133190334132086,\n",
       "   0.12505726341673326,\n",
       "   0.13566212177020245,\n",
       "   0.13614259223117275,\n",
       "   0.12625567748313332,\n",
       "   0.13594198072512936,\n",
       "   0.1259720741810102,\n",
       "   0.12833837227812459,\n",
       "   0.13533703070255057,\n",
       "   0.12535293822315055,\n",
       "   0.13385830637236623,\n",
       "   0.1316696916636857,\n",
       "   0.1353652657511138,\n",
       "   0.12529006849307425,\n",
       "   0.13136219308808625,\n",
       "   0.13185292255685188,\n",
       "   0.13791414438109006,\n",
       "   0.12165253599215675,\n",
       "   0.12873833993298953,\n",
       "   0.12616680230800362],\n",
       "  'train_accuracy_mean': [0.4233066675364971,\n",
       "   0.5390666657090187,\n",
       "   0.59481333142519,\n",
       "   0.6148399997353554,\n",
       "   0.6336266662478447,\n",
       "   0.6479866663217545,\n",
       "   0.659893333017826,\n",
       "   0.6721333324313163,\n",
       "   0.6828533319830894,\n",
       "   0.6881466675400734,\n",
       "   0.7052000002264976,\n",
       "   0.7074266669154167,\n",
       "   0.7155999996066094,\n",
       "   0.7252266653180123,\n",
       "   0.7277200006246567,\n",
       "   0.7313733336925506,\n",
       "   0.7405066672563553,\n",
       "   0.7441199996471405,\n",
       "   0.7469200004339218,\n",
       "   0.7481066665649414,\n",
       "   0.7511333339214324,\n",
       "   0.7609066662788391,\n",
       "   0.7601466664075851,\n",
       "   0.7574666669368744,\n",
       "   0.7627066668272019,\n",
       "   0.7620266656875611,\n",
       "   0.775120000243187,\n",
       "   0.7727733331918717,\n",
       "   0.7716933337450027,\n",
       "   0.7667866671085357,\n",
       "   0.7756533328294754,\n",
       "   0.7737999999523163,\n",
       "   0.7772266652584076,\n",
       "   0.7842000005245209,\n",
       "   0.7803199998140335,\n",
       "   0.7784400005340576,\n",
       "   0.7871199995279312,\n",
       "   0.7897466659545899,\n",
       "   0.7880399997234344,\n",
       "   0.7939333324432373,\n",
       "   0.7867066669464111,\n",
       "   0.791426666021347,\n",
       "   0.7953066686391831,\n",
       "   0.7888533326387406,\n",
       "   0.7986133317947388,\n",
       "   0.795106665968895,\n",
       "   0.7972266665697098,\n",
       "   0.7984666662216187,\n",
       "   0.7960800006389618,\n",
       "   0.7987200003862381,\n",
       "   0.7975200002193451,\n",
       "   0.8062000006437302,\n",
       "   0.8019066665172577,\n",
       "   0.8044266678094864,\n",
       "   0.7996133328676224,\n",
       "   0.8038933326005936,\n",
       "   0.8049999990463257,\n",
       "   0.8013066655397415,\n",
       "   0.8033866665363312,\n",
       "   0.8012399994134903,\n",
       "   0.8026266663074494,\n",
       "   0.8034933333396912,\n",
       "   0.8078533325195313,\n",
       "   0.8083866667747498,\n",
       "   0.8038666669130325,\n",
       "   0.800573331952095,\n",
       "   0.7989600002765656,\n",
       "   0.8101599998474122,\n",
       "   0.8092800005674362,\n",
       "   0.8054799995422364,\n",
       "   0.8114533323049545,\n",
       "   0.80664000082016,\n",
       "   0.8021466664075851,\n",
       "   0.8030266677141189,\n",
       "   0.8117600002288818,\n",
       "   0.8121599991321564,\n",
       "   0.8081066673994064,\n",
       "   0.8081066660881042,\n",
       "   0.8070399987697602,\n",
       "   0.8062800006866455,\n",
       "   0.8129333328008652,\n",
       "   0.8059733326435089,\n",
       "   0.8130933339595795,\n",
       "   0.8083600002527237,\n",
       "   0.8117466689348221,\n",
       "   0.8093733336925507,\n",
       "   0.8143200014829636,\n",
       "   0.813666667342186,\n",
       "   0.811933333516121,\n",
       "   0.81567999958992,\n",
       "   0.8139199993610382,\n",
       "   0.8152666662931443,\n",
       "   0.8225999999046326,\n",
       "   0.8172533321380615,\n",
       "   0.8137999997138977,\n",
       "   0.8116133325099945,\n",
       "   0.8165200002193451,\n",
       "   0.8162133325338363,\n",
       "   0.8200666670799255],\n",
       "  'train_accuracy_std': [0.09250441017521906,\n",
       "   0.06813854998784334,\n",
       "   0.07728869919849311,\n",
       "   0.06829443626941314,\n",
       "   0.06984111023670288,\n",
       "   0.06882467210071173,\n",
       "   0.06691545022536181,\n",
       "   0.06469195523092033,\n",
       "   0.06831196972052894,\n",
       "   0.06719547643557558,\n",
       "   0.064569548494271,\n",
       "   0.066387750168935,\n",
       "   0.06237250086954995,\n",
       "   0.06712818380391292,\n",
       "   0.06020890683181795,\n",
       "   0.06459568654633001,\n",
       "   0.0613384863060337,\n",
       "   0.06339157832572556,\n",
       "   0.0641070491663232,\n",
       "   0.06448198498182356,\n",
       "   0.06307424152310069,\n",
       "   0.05659583882831817,\n",
       "   0.06115827042228327,\n",
       "   0.0625884599457373,\n",
       "   0.058472848037456136,\n",
       "   0.06383575528384056,\n",
       "   0.05856209400402444,\n",
       "   0.0588349645170612,\n",
       "   0.058361890902662125,\n",
       "   0.061831914805562896,\n",
       "   0.061086058580134765,\n",
       "   0.05980601957238949,\n",
       "   0.05854340794498474,\n",
       "   0.06056075063222086,\n",
       "   0.05837263263372414,\n",
       "   0.06199040879298005,\n",
       "   0.05766854577583528,\n",
       "   0.060695435593681825,\n",
       "   0.0606584479842574,\n",
       "   0.06327432487687529,\n",
       "   0.06067983834927738,\n",
       "   0.059172142290492585,\n",
       "   0.05794571647831502,\n",
       "   0.06248641402300734,\n",
       "   0.058651791215897676,\n",
       "   0.06222512764154951,\n",
       "   0.060548216095650684,\n",
       "   0.05661295487057218,\n",
       "   0.06078240164827344,\n",
       "   0.05866936993192973,\n",
       "   0.05955300723013841,\n",
       "   0.056268050277295575,\n",
       "   0.057080140388256845,\n",
       "   0.05813360330850108,\n",
       "   0.058076821173439576,\n",
       "   0.057275531290293155,\n",
       "   0.057075195416284544,\n",
       "   0.05982106152478251,\n",
       "   0.056336266675819445,\n",
       "   0.05837632987437048,\n",
       "   0.05750469093541408,\n",
       "   0.06093399966165079,\n",
       "   0.058235658077786835,\n",
       "   0.05750958929785716,\n",
       "   0.05761311691815642,\n",
       "   0.058849187251826585,\n",
       "   0.058469237034720144,\n",
       "   0.06129180648372267,\n",
       "   0.0558371996246257,\n",
       "   0.05970829763804626,\n",
       "   0.056525501649062934,\n",
       "   0.05636862277362295,\n",
       "   0.056774922914718515,\n",
       "   0.06072447225725964,\n",
       "   0.05581409914511799,\n",
       "   0.05691046572530966,\n",
       "   0.05959319237257311,\n",
       "   0.05995900276061789,\n",
       "   0.05655945367929914,\n",
       "   0.058734294462033106,\n",
       "   0.05489723799039951,\n",
       "   0.06024143427399667,\n",
       "   0.059562368968577244,\n",
       "   0.055483925037965655,\n",
       "   0.057904655644531865,\n",
       "   0.05479868823745104,\n",
       "   0.056715311587304466,\n",
       "   0.05789626937941102,\n",
       "   0.05486383158699988,\n",
       "   0.05903373620158079,\n",
       "   0.0580866248537325,\n",
       "   0.05641036190375222,\n",
       "   0.05485897107152519,\n",
       "   0.057202276624876695,\n",
       "   0.05644942810074001,\n",
       "   0.06141134081481692,\n",
       "   0.05378703518614399,\n",
       "   0.056446386230667916,\n",
       "   0.05493669109213588],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.3638961803913117,\n",
       "   1.2229010911782583,\n",
       "   1.1647925506035486,\n",
       "   1.139654997388522,\n",
       "   1.113648825287819,\n",
       "   1.0629060816764833,\n",
       "   1.0590756314992904,\n",
       "   1.050997693737348,\n",
       "   1.013101231654485,\n",
       "   0.9954779247442881,\n",
       "   0.9761733635266622,\n",
       "   0.9652349529663722,\n",
       "   0.9637797162930171,\n",
       "   0.9637210756540299,\n",
       "   0.9739676823218664,\n",
       "   0.9613964915275574,\n",
       "   0.9383996158838273,\n",
       "   0.9271624314785004,\n",
       "   0.9681146003802618,\n",
       "   0.9345628271500269,\n",
       "   0.9574245816469192,\n",
       "   0.924825358192126,\n",
       "   0.9099760947624842,\n",
       "   0.9220849227905273,\n",
       "   0.9042441620429357,\n",
       "   0.9115960707267126,\n",
       "   0.9247081176439921,\n",
       "   0.9176876652240753,\n",
       "   0.9108298009634018,\n",
       "   0.9138818176587423,\n",
       "   0.9138319814205169,\n",
       "   0.9772489287455877,\n",
       "   0.9007962890466055,\n",
       "   0.916221942504247,\n",
       "   0.906806378364563,\n",
       "   0.9111019708712895,\n",
       "   0.908636908531189,\n",
       "   0.9187039248148601,\n",
       "   0.9300190496444702,\n",
       "   0.9174943564335505,\n",
       "   0.925990361769994,\n",
       "   0.9284041764338812,\n",
       "   0.9129143816232681,\n",
       "   0.9187601880232493,\n",
       "   0.9091062966982524,\n",
       "   0.920066639582316,\n",
       "   0.9259215766191482,\n",
       "   0.9952064357201258,\n",
       "   0.9087821064392726,\n",
       "   0.9124377131462097,\n",
       "   0.9128633306423823,\n",
       "   0.9305381552378337,\n",
       "   0.9132787841558456,\n",
       "   0.9143071681261062,\n",
       "   0.9157892527182897,\n",
       "   0.9324130817254385,\n",
       "   0.9162640319267908,\n",
       "   0.9254227860768636,\n",
       "   0.9330246579647065,\n",
       "   0.9400067675113678,\n",
       "   0.9340177313486735,\n",
       "   0.9320868148406347,\n",
       "   0.9525225881735484,\n",
       "   0.9482289747397105,\n",
       "   0.9265881925821304,\n",
       "   0.9236228664716085,\n",
       "   0.9470560677846273,\n",
       "   0.9329888266324997,\n",
       "   0.9437516049544017,\n",
       "   0.9403902192910513,\n",
       "   0.9602256021896998,\n",
       "   0.9389830219745636,\n",
       "   0.9568384102980296,\n",
       "   0.9422046653429668,\n",
       "   0.9681998399893442,\n",
       "   0.9543885354200999,\n",
       "   0.9464990103244781,\n",
       "   0.9502060796817143,\n",
       "   0.9459489307800929,\n",
       "   0.9593949955701828,\n",
       "   0.9555120392640432,\n",
       "   0.9443522866566976,\n",
       "   0.9535773112376531,\n",
       "   0.9516563880443573,\n",
       "   0.9455244183540344,\n",
       "   0.9667045164108277,\n",
       "   0.9599228282769521,\n",
       "   0.9518605438868205,\n",
       "   0.9412835854291915,\n",
       "   0.9627139083544414,\n",
       "   0.9615793863932292,\n",
       "   0.946319442987442,\n",
       "   0.9537743016084035,\n",
       "   0.974657630721728,\n",
       "   0.9483506641785304,\n",
       "   0.951670036315918,\n",
       "   0.9397315577665964,\n",
       "   0.9537831139564514,\n",
       "   0.9555253247419994],\n",
       "  'val_loss_std': [0.10239758349535745,\n",
       "   0.11866311920411354,\n",
       "   0.1306360657056468,\n",
       "   0.1333222818529221,\n",
       "   0.13876616641253603,\n",
       "   0.13236389371519758,\n",
       "   0.13777845607154837,\n",
       "   0.13513394329444267,\n",
       "   0.1292642122075696,\n",
       "   0.1293804102528254,\n",
       "   0.12882529755154007,\n",
       "   0.12924482490042055,\n",
       "   0.135525014409551,\n",
       "   0.1288272688530453,\n",
       "   0.12418143917879028,\n",
       "   0.1239191561531761,\n",
       "   0.12898017790288954,\n",
       "   0.13127132864553434,\n",
       "   0.13275107467698585,\n",
       "   0.13073412037938256,\n",
       "   0.12470883856811306,\n",
       "   0.1256644364414366,\n",
       "   0.12737831345182746,\n",
       "   0.12579512202309362,\n",
       "   0.12581353794693906,\n",
       "   0.12745391830176708,\n",
       "   0.12467283173704864,\n",
       "   0.1288788358037708,\n",
       "   0.13118940811468152,\n",
       "   0.1229335274615152,\n",
       "   0.13116700095034095,\n",
       "   0.12424667309354273,\n",
       "   0.12441023115603621,\n",
       "   0.12559014950639996,\n",
       "   0.1290592829462789,\n",
       "   0.13207456941409587,\n",
       "   0.1304668032177877,\n",
       "   0.12745929000599368,\n",
       "   0.1347886662577716,\n",
       "   0.13074526732423308,\n",
       "   0.13177432725144522,\n",
       "   0.1219544659635333,\n",
       "   0.1278525072523663,\n",
       "   0.12817855015878588,\n",
       "   0.12565686352386343,\n",
       "   0.1276531991406745,\n",
       "   0.12841630730243736,\n",
       "   0.11759603649471859,\n",
       "   0.13618490079711543,\n",
       "   0.13182464797432772,\n",
       "   0.12696886058011111,\n",
       "   0.13615627761753127,\n",
       "   0.12683988633267376,\n",
       "   0.12397767130940682,\n",
       "   0.13455501594354236,\n",
       "   0.12974344761183013,\n",
       "   0.13452024880098706,\n",
       "   0.12795267878873548,\n",
       "   0.12885507349838046,\n",
       "   0.13310465772640892,\n",
       "   0.12810861466666887,\n",
       "   0.13234160869627581,\n",
       "   0.13017980365624346,\n",
       "   0.13058247581479845,\n",
       "   0.13001907142668065,\n",
       "   0.13262728545493738,\n",
       "   0.13584498807255505,\n",
       "   0.12750972898283053,\n",
       "   0.1381385434014239,\n",
       "   0.1331765437011908,\n",
       "   0.13318257524168908,\n",
       "   0.12691321869804828,\n",
       "   0.13398790897921317,\n",
       "   0.12451727967036623,\n",
       "   0.13785298679111255,\n",
       "   0.1325953943453727,\n",
       "   0.12663011824512427,\n",
       "   0.1297897864624669,\n",
       "   0.13181410133322902,\n",
       "   0.1362259832173056,\n",
       "   0.13153849873457057,\n",
       "   0.13451571142973134,\n",
       "   0.13992757200730452,\n",
       "   0.14289648490571477,\n",
       "   0.12741409237974613,\n",
       "   0.1316415949120241,\n",
       "   0.1315493391493938,\n",
       "   0.135725609846602,\n",
       "   0.13640001260995727,\n",
       "   0.13258777613345726,\n",
       "   0.13995084248222087,\n",
       "   0.1335404110772166,\n",
       "   0.14055777429895724,\n",
       "   0.13846317862945065,\n",
       "   0.14088242172688836,\n",
       "   0.13840149291044754,\n",
       "   0.13778243641490254,\n",
       "   0.13851464415597958,\n",
       "   0.13226622872251972],\n",
       "  'val_accuracy_mean': [0.43920000036557516,\n",
       "   0.513955555955569,\n",
       "   0.5412666650613149,\n",
       "   0.5509555546442667,\n",
       "   0.5619555549820264,\n",
       "   0.5849777761101723,\n",
       "   0.5892222210764885,\n",
       "   0.5891777747869491,\n",
       "   0.609466665883859,\n",
       "   0.6142888884743055,\n",
       "   0.6231111108263334,\n",
       "   0.6281111116210619,\n",
       "   0.6277555550138155,\n",
       "   0.627644444902738,\n",
       "   0.6233333312471707,\n",
       "   0.6279111100236575,\n",
       "   0.640399999221166,\n",
       "   0.6448888886968295,\n",
       "   0.6257111098368963,\n",
       "   0.6421333333849907,\n",
       "   0.632088886698087,\n",
       "   0.6468444437781969,\n",
       "   0.6511111116409302,\n",
       "   0.6453333336114884,\n",
       "   0.6499111103018125,\n",
       "   0.6517777775724729,\n",
       "   0.6475333309173584,\n",
       "   0.6481777769327164,\n",
       "   0.652555555899938,\n",
       "   0.6486222232381503,\n",
       "   0.648933333158493,\n",
       "   0.6216222219665846,\n",
       "   0.6550666664044063,\n",
       "   0.6487777760624885,\n",
       "   0.650888887445132,\n",
       "   0.6512666644652685,\n",
       "   0.6510888870557149,\n",
       "   0.6479777775208155,\n",
       "   0.6425555549065272,\n",
       "   0.6479777759313583,\n",
       "   0.6439555558562279,\n",
       "   0.6441555554668109,\n",
       "   0.6482666647434234,\n",
       "   0.6485111115376154,\n",
       "   0.650466668109099,\n",
       "   0.6461111116409302,\n",
       "   0.6453333327174187,\n",
       "   0.6152222200234732,\n",
       "   0.6519777767856916,\n",
       "   0.6514000005523364,\n",
       "   0.6501555563012759,\n",
       "   0.6436222221453984,\n",
       "   0.6450222206115722,\n",
       "   0.6462666644652685,\n",
       "   0.648022221326828,\n",
       "   0.6387555555502573,\n",
       "   0.6483999999364217,\n",
       "   0.6427777783075969,\n",
       "   0.6391777761777242,\n",
       "   0.6379555541276932,\n",
       "   0.639422222574552,\n",
       "   0.6415999986728033,\n",
       "   0.6313555545608203,\n",
       "   0.6311111097534498,\n",
       "   0.6429555557171504,\n",
       "   0.6459555552403132,\n",
       "   0.6361111102501551,\n",
       "   0.6411555561423302,\n",
       "   0.6348666664958,\n",
       "   0.6380222205320994,\n",
       "   0.6276444437106451,\n",
       "   0.6392444445689519,\n",
       "   0.6329555549224217,\n",
       "   0.6370888884862264,\n",
       "   0.626733333170414,\n",
       "   0.6309555545449257,\n",
       "   0.6333111110329628,\n",
       "   0.6336444439490636,\n",
       "   0.6319333329796791,\n",
       "   0.6260222225387891,\n",
       "   0.628666666050752,\n",
       "   0.6319333319862683,\n",
       "   0.6339111090699832,\n",
       "   0.6315555555621782,\n",
       "   0.6325333323081335,\n",
       "   0.6221333328882853,\n",
       "   0.6266444443662962,\n",
       "   0.6305999989310901,\n",
       "   0.6344222214818,\n",
       "   0.626466666162014,\n",
       "   0.6254888895153999,\n",
       "   0.6331333327293396,\n",
       "   0.633444443444411,\n",
       "   0.6249777776996295,\n",
       "   0.6343555554747582,\n",
       "   0.6322666666905086,\n",
       "   0.6357777770360311,\n",
       "   0.6307777764399847,\n",
       "   0.6295333321889242],\n",
       "  'val_accuracy_std': [0.05540380990769798,\n",
       "   0.060228438185702705,\n",
       "   0.06400019562609857,\n",
       "   0.06419511603599963,\n",
       "   0.06339784665539014,\n",
       "   0.0636201051942842,\n",
       "   0.06494774681815277,\n",
       "   0.06359179571136145,\n",
       "   0.061743890944465266,\n",
       "   0.06340094172902971,\n",
       "   0.06257518949130769,\n",
       "   0.0596122243407247,\n",
       "   0.061813335556880775,\n",
       "   0.06162436616343117,\n",
       "   0.06298853554539659,\n",
       "   0.06137142164960313,\n",
       "   0.06270084175988012,\n",
       "   0.060636950692598834,\n",
       "   0.0649544988312707,\n",
       "   0.06267999743813801,\n",
       "   0.06069418960277866,\n",
       "   0.06061876909436977,\n",
       "   0.0631338008964403,\n",
       "   0.060745981142224625,\n",
       "   0.06274102881691714,\n",
       "   0.061513825320116705,\n",
       "   0.06179287833327865,\n",
       "   0.062418644405241766,\n",
       "   0.06135134707783905,\n",
       "   0.060538554680592126,\n",
       "   0.061301102223439374,\n",
       "   0.06554301617864175,\n",
       "   0.06094043368491554,\n",
       "   0.06094675003741202,\n",
       "   0.06269227179487509,\n",
       "   0.06373808640823317,\n",
       "   0.06491133255863522,\n",
       "   0.06256895136750189,\n",
       "   0.0664629320245928,\n",
       "   0.06241248385757635,\n",
       "   0.06331858088220954,\n",
       "   0.060175592965309325,\n",
       "   0.06191119156521017,\n",
       "   0.0650789118198778,\n",
       "   0.06041526809045952,\n",
       "   0.06350843211501762,\n",
       "   0.06133575016364911,\n",
       "   0.06451519238994377,\n",
       "   0.061331638289794606,\n",
       "   0.062498320882509076,\n",
       "   0.06402411204272013,\n",
       "   0.06325075014842858,\n",
       "   0.060346832962845307,\n",
       "   0.061380320885398236,\n",
       "   0.06338430346968271,\n",
       "   0.0636047356646884,\n",
       "   0.06517863881846517,\n",
       "   0.06304721084830671,\n",
       "   0.05909745608225456,\n",
       "   0.06268296213097893,\n",
       "   0.06179505846223131,\n",
       "   0.06396378787694844,\n",
       "   0.06587822374874754,\n",
       "   0.06300460475365206,\n",
       "   0.06422649596632221,\n",
       "   0.0625892995685028,\n",
       "   0.06366453261543414,\n",
       "   0.06539111810909536,\n",
       "   0.06498219809225353,\n",
       "   0.06394047967749321,\n",
       "   0.06149922722455358,\n",
       "   0.06120535873532953,\n",
       "   0.06279424184814865,\n",
       "   0.06321251391184396,\n",
       "   0.062464177569439554,\n",
       "   0.063673749350074,\n",
       "   0.06329589180230587,\n",
       "   0.063583539199474,\n",
       "   0.062058242012673986,\n",
       "   0.06350059302992492,\n",
       "   0.06381106412381227,\n",
       "   0.06594697224152704,\n",
       "   0.06445641471309749,\n",
       "   0.064340145460968,\n",
       "   0.061107895376712025,\n",
       "   0.0648659931339262,\n",
       "   0.0676318979708645,\n",
       "   0.06671839362336646,\n",
       "   0.06371347807404341,\n",
       "   0.0638826827636937,\n",
       "   0.06564510173984796,\n",
       "   0.0660939058838478,\n",
       "   0.0641097645988135,\n",
       "   0.06600196183759525,\n",
       "   0.06283069280479699,\n",
       "   0.06774497079898716,\n",
       "   0.06493491699861399,\n",
       "   0.06341972794862683,\n",
       "   0.06411278451417123],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = 33\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key, value)\n",
    "# print(\"=\"*10)\n",
    "# print(\"names_weights_copy == \",names_weights_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "170a7604",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'num_step'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25068\\1448501900.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_landscape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlandscape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaml_system\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_support_set_task\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_support_set_task\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\MAML\\utils\\loss_landscape.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mhessian_comp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# get the top eigenvector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\pyhessian\\hessian.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, criterion, data, dataloader, cuda)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;31m# if we only compute the Hessian information for a single batch data, we can re-use the gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'num_step'"
     ]
    }
   ],
   "source": [
    "test_data = maml_system.data.get_test_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "for sample_idx, test_sample in enumerate(test_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = test_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        # Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "        num_steps=5\n",
    "        for num_step in range(num_steps):            \n",
    "            support_loss, support_preds, out_feature_dict  = maml_system.model.net_forward(\n",
    "                    x=x_support_set_task,\n",
    "                    y=y_support_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    backup_running_statistics=num_step == 0,\n",
    "                    training=True,\n",
    "                    num_step=num_step,\n",
    "                )\n",
    "        \n",
    "            generated_alpha_params = {}\n",
    "\n",
    "            if maml_system.model.args.arbiter:\n",
    "                support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                        retain_graph=True)\n",
    "\n",
    "                names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "                per_step_task_embedding = []\n",
    "\n",
    "                for key, weight in names_weights_copy.items():\n",
    "                    weight_norm = torch.norm(weight, p=2)\n",
    "                    per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "                for key, grad in names_grads_copy.items():\n",
    "                    gradient_l2norm = torch.norm(grad, p=2)\n",
    "                    per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "                per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "                per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                            per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "                generated_gradient_rate = maml_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "                g = 0\n",
    "                for key in names_weights_copy.keys():\n",
    "                    generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "                    g += 1\n",
    "\n",
    "            names_weights_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                              names_weights_copy=names_weights_copy,\n",
    "                                                              out_feature_dict=out_feature_dict,\n",
    "                                                              alpha=generated_alpha_params,\n",
    "                                                              use_second_order=args.second_order,\n",
    "                                                              current_step_idx=num_step,\n",
    "                                                              current_iter=maml_system.state['current_iter'],\n",
    "                                                              training_phase='test')\n",
    "  \n",
    "        \n",
    "        for name, param in maml_system.model.classifier.named_parameters():\n",
    "            if \"norm_layer\" not in name:\n",
    "                param = names_weights_copy[name].to(device=device)        \n",
    "                \n",
    "                \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        ls = loss_landscape.landscape(maml_system.model.classifier, criterion)\n",
    "        ls.show(x_support_set_task, y_support_set_task)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
