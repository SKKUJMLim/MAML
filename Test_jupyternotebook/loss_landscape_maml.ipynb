{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"SWA\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f85286c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "arbiter.0.weight torch.Size([20, 20]) cuda:0 True\n",
      "arbiter.0.bias torch.Size([20]) cuda:0 True\n",
      "arbiter.2.weight torch.Size([10, 20]) cuda:0 True\n",
      "arbiter.2.bias torch.Size([10]) cuda:0 True\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6542444431781769,\n",
       " 'best_val_iter': 16000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 32,\n",
       " 'train_loss_mean': 0.4496441180706024,\n",
       " 'train_loss_std': 0.12430861797929893,\n",
       " 'train_accuracy_mean': 0.8344133331775665,\n",
       " 'train_accuracy_std': 0.05293675929356405,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.9212178971370061,\n",
       " 'val_loss_std': 0.1378287118752748,\n",
       " 'val_accuracy_mean': 0.6422444424033165,\n",
       " 'val_accuracy_std': 0.05979594587457209,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-9.2420e-03, -4.2841e-01,  3.0116e-01],\n",
       "                         [-1.7578e-01, -8.3019e-02,  8.1573e-02],\n",
       "                         [-9.1437e-02,  3.7958e-01, -2.7273e-03]],\n",
       "               \n",
       "                        [[ 2.2214e-01, -3.4631e-01,  2.9974e-01],\n",
       "                         [-9.1582e-02,  5.1024e-02,  9.6501e-02],\n",
       "                         [-2.5836e-01,  1.4183e-01, -1.4209e-01]],\n",
       "               \n",
       "                        [[ 3.1487e-01, -1.9859e-01, -1.0493e-01],\n",
       "                         [ 1.5040e-01,  2.1750e-01, -2.3930e-01],\n",
       "                         [-1.0072e-01,  2.1121e-01, -2.3503e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.8902e-02,  5.4940e-01,  7.9530e-03],\n",
       "                         [ 1.8810e-01, -1.4940e-01,  6.4084e-02],\n",
       "                         [-2.0655e-01, -3.8260e-01, -9.7125e-02]],\n",
       "               \n",
       "                        [[-2.5672e-01,  2.5995e-01, -2.0516e-01],\n",
       "                         [ 1.7166e-01, -2.2283e-01, -3.7939e-02],\n",
       "                         [ 2.4555e-01, -1.7969e-01,  2.6681e-01]],\n",
       "               \n",
       "                        [[-2.6352e-01,  2.4822e-01, -2.7551e-01],\n",
       "                         [ 1.7703e-02,  1.6849e-02,  1.9993e-01],\n",
       "                         [ 4.8506e-02, -1.0896e-01,  9.6475e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-7.0600e-04, -3.8632e-02,  3.0753e-01],\n",
       "                         [ 1.2012e-01, -3.8486e-02,  6.2404e-03],\n",
       "                         [ 7.6611e-02, -2.5255e-01, -4.4790e-01]],\n",
       "               \n",
       "                        [[-1.1252e-01, -9.4092e-02,  3.0871e-01],\n",
       "                         [ 3.7256e-01,  4.8387e-02,  1.0759e-01],\n",
       "                         [-1.8726e-01, -2.9239e-01, -1.1086e-01]],\n",
       "               \n",
       "                        [[ 6.4373e-02, -4.8488e-01,  4.1492e-02],\n",
       "                         [ 2.6376e-01, -1.6067e-01, -2.3686e-01],\n",
       "                         [-1.6881e-01, -9.5936e-02, -1.1421e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.2307e-01, -8.5983e-02, -9.3396e-02],\n",
       "                         [-2.8775e-01, -8.0900e-01, -4.5745e-01],\n",
       "                         [ 2.1824e-01, -1.2336e-01, -4.3534e-01]],\n",
       "               \n",
       "                        [[-1.9882e-01, -7.5247e-02,  9.7369e-02],\n",
       "                         [-2.3072e-01, -3.1824e-01, -6.6113e-02],\n",
       "                         [ 2.1019e-02, -2.2340e-01, -1.8445e-01]],\n",
       "               \n",
       "                        [[ 1.7712e-01,  5.1526e-02,  3.3039e-01],\n",
       "                         [-5.5094e-02, -1.7805e-01,  1.4264e-01],\n",
       "                         [-9.6980e-02, -3.0537e-01,  4.9012e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.5232e-02, -1.7595e-01, -2.7444e-01],\n",
       "                         [ 1.9038e-01,  4.4885e-01, -1.0017e-01],\n",
       "                         [ 5.8903e-02,  4.6424e-01,  4.5375e-01]],\n",
       "               \n",
       "                        [[ 7.7671e-02, -1.1619e-02, -3.1148e-01],\n",
       "                         [-1.5840e-01, -4.6056e-03, -1.0808e-01],\n",
       "                         [-2.8773e-01, -1.3547e-02, -2.9761e-02]],\n",
       "               \n",
       "                        [[ 2.1034e-01, -5.3589e-04, -3.4048e-01],\n",
       "                         [ 6.7773e-02,  1.5574e-01, -1.5827e-01],\n",
       "                         [-2.4198e-01,  3.3417e-01,  2.3253e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.1863e-03, -8.2525e-03, -8.0905e-02],\n",
       "                         [ 1.5910e-01,  2.3668e-01,  9.4835e-02],\n",
       "                         [-3.0047e-02,  2.8073e-02,  1.0984e-01]],\n",
       "               \n",
       "                        [[-3.7171e-01, -5.7232e-01,  1.5735e-02],\n",
       "                         [-3.7619e-01, -6.7938e-01, -1.7892e-01],\n",
       "                         [-6.5298e-03,  7.2885e-02,  3.8755e-01]],\n",
       "               \n",
       "                        [[ 2.3895e-01,  1.8046e-01,  3.0757e-01],\n",
       "                         [ 1.0552e-01, -1.4696e-01,  4.7012e-02],\n",
       "                         [ 3.2209e-02, -2.3370e-04, -4.3250e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.2507,  0.1374,  0.0695,  0.0283, -0.0307,  0.1600,  0.0303, -0.0098,\n",
       "                        0.0735, -0.2094,  0.0175, -0.0736,  0.1567,  0.0019,  0.1251, -0.0624,\n",
       "                       -0.3373,  0.0223,  0.1915, -0.0383, -0.2383,  0.0709, -0.0694,  0.1056,\n",
       "                        0.0442,  0.0433,  0.0092, -0.0286,  0.1730,  0.1829,  0.0188,  0.0606,\n",
       "                       -0.0855, -0.0420, -0.0013,  0.0216,  0.0127, -0.3873, -0.1163, -0.1251,\n",
       "                        0.1177,  0.4540, -0.0203, -0.1445, -0.0362, -0.0095, -0.0290, -0.0847],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.2829, -0.0338, -0.7348, -0.1689,  0.0022,  0.3701, -0.7843,  0.5674,\n",
       "                       -0.6233, -0.4914, -0.0046, -0.8196, -0.0761, -0.5953,  0.6437,  0.5376,\n",
       "                        0.3812, -0.4640,  0.0473, -0.4245, -0.2213, -0.0577, -0.7362, -0.0937,\n",
       "                        0.1132, -0.4043, -0.6306,  1.9772, -0.2074,  0.3804,  0.1493, -1.0453,\n",
       "                       -0.0466, -0.7914, -0.5520, -0.0699, -0.6277, -0.2622, -0.3442, -0.0447,\n",
       "                        1.3243, -0.0037, -0.0049,  0.0188, -0.6432, -0.3625, -0.4323, -0.4544],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.1480, 1.0570, 0.6975, 0.9909, 0.5764, 0.8663, 0.9620, 1.2472, 0.7542,\n",
       "                       0.9784, 0.9442, 0.8364, 0.8942, 0.6853, 1.0219, 0.6726, 0.6724, 0.7618,\n",
       "                       1.3292, 0.7181, 0.8642, 0.5643, 0.6341, 0.9987, 0.8297, 0.8218, 0.7115,\n",
       "                       0.8301, 0.7223, 0.6974, 1.1947, 0.8512, 1.3384, 0.8482, 0.8702, 1.0977,\n",
       "                       0.8214, 1.4203, 0.8242, 0.7655, 0.9957, 1.3179, 0.7956, 1.3415, 0.7822,\n",
       "                       0.7100, 0.6686, 0.9323], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 2.1677e-01, -1.4459e-01, -9.2818e-05],\n",
       "                         [ 2.1092e-01,  6.5329e-02,  3.5458e-01],\n",
       "                         [-1.8996e-01,  3.3358e-01, -3.8276e-01]],\n",
       "               \n",
       "                        [[-1.6349e-01, -2.2558e-01,  3.3899e-02],\n",
       "                         [-3.0898e-01,  2.7362e-01,  2.8541e-02],\n",
       "                         [ 8.0974e-02,  8.9332e-02, -4.8683e-01]],\n",
       "               \n",
       "                        [[-4.6083e-01,  1.6621e-01,  3.5297e-01],\n",
       "                         [ 2.0278e-01,  2.2274e-01,  2.9703e-01],\n",
       "                         [ 7.4974e-02,  7.6536e-02, -3.3798e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.4031e-01, -1.6943e-01,  1.1019e-01],\n",
       "                         [-1.9915e-01,  1.0008e-01,  1.4754e-01],\n",
       "                         [ 2.5032e-01,  1.4321e-01, -1.9474e-01]],\n",
       "               \n",
       "                        [[ 1.5385e-01,  1.7343e-01, -2.4566e-01],\n",
       "                         [ 4.2820e-02, -1.4889e-01, -3.1402e-01],\n",
       "                         [-1.1345e-01, -1.4720e-01,  5.2237e-02]],\n",
       "               \n",
       "                        [[ 6.9268e-02, -2.0699e-01, -2.0551e-01],\n",
       "                         [-2.3838e-01, -1.3198e-01,  1.8776e-01],\n",
       "                         [-1.5885e-01,  7.1989e-02,  4.9657e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.4132e-01, -1.7715e-01, -2.2230e-01],\n",
       "                         [ 3.8240e-01,  2.1546e-01, -2.7562e-01],\n",
       "                         [ 5.7365e-02,  3.0601e-01,  6.4926e-02]],\n",
       "               \n",
       "                        [[ 1.1830e-01, -3.8392e-01,  1.8111e-03],\n",
       "                         [ 2.0859e-01, -3.2235e-01, -3.1126e-01],\n",
       "                         [-6.2049e-02,  4.7597e-02, -2.1432e-01]],\n",
       "               \n",
       "                        [[ 2.3831e-03, -5.8735e-02,  2.8086e-01],\n",
       "                         [-6.1121e-02,  4.7561e-01,  3.6996e-01],\n",
       "                         [ 3.7668e-01,  4.5231e-01,  3.9194e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.2943e-02, -4.6555e-01,  4.1514e-02],\n",
       "                         [-3.1925e-02, -3.3106e-01, -2.8237e-01],\n",
       "                         [-1.4268e-03, -1.8111e-01, -1.9724e-01]],\n",
       "               \n",
       "                        [[ 1.7832e-01, -4.1938e-02, -5.8581e-02],\n",
       "                         [ 4.8968e-01, -6.7898e-03, -2.0483e-01],\n",
       "                         [ 3.1400e-01,  4.5429e-01, -1.6762e-01]],\n",
       "               \n",
       "                        [[ 3.1722e-01,  4.1150e-02, -1.7313e-01],\n",
       "                         [ 2.2682e-01, -1.4492e-01, -6.1364e-01],\n",
       "                         [ 1.6961e-01,  5.6562e-02, -1.5405e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.0585e-01,  3.6552e-01, -2.6263e-01],\n",
       "                         [ 1.5414e-01,  5.1289e-01,  1.3036e-01],\n",
       "                         [ 1.3017e-01,  6.3586e-02,  3.7991e-01]],\n",
       "               \n",
       "                        [[ 5.0689e-02,  3.4917e-01,  2.3254e-01],\n",
       "                         [ 1.1185e-01,  3.5573e-01,  3.6269e-01],\n",
       "                         [ 1.9367e-01,  2.6071e-01,  1.7051e-01]],\n",
       "               \n",
       "                        [[-4.9240e-01, -1.3882e-01, -1.3019e-01],\n",
       "                         [-3.0796e-02, -2.2895e-01, -1.5431e-01],\n",
       "                         [-1.6306e-01, -7.3120e-01, -2.3248e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.1854e-01, -2.4350e-01,  1.8162e-01],\n",
       "                         [-2.9123e-01, -4.2484e-01,  2.4024e-01],\n",
       "                         [-3.0722e-01, -4.9224e-01, -7.2411e-04]],\n",
       "               \n",
       "                        [[-4.1266e-01, -4.5840e-01,  4.9604e-02],\n",
       "                         [-2.0925e-01, -3.9451e-01, -2.3259e-01],\n",
       "                         [-4.9139e-02, -3.2523e-01, -1.4385e-01]],\n",
       "               \n",
       "                        [[ 9.1693e-02, -3.3221e-01,  1.7044e-01],\n",
       "                         [-1.7955e-01, -1.7821e-01,  5.1324e-02],\n",
       "                         [-2.5713e-01,  6.5665e-03,  1.6393e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.2099e-01, -1.2562e-01,  4.0438e-01],\n",
       "                         [ 1.0820e-01, -1.3490e-01, -1.7520e-02],\n",
       "                         [-1.3801e-01, -1.9744e-02, -5.3576e-03]],\n",
       "               \n",
       "                        [[ 3.5093e-01,  1.0029e-01, -1.3084e-01],\n",
       "                         [ 3.6878e-02, -9.7811e-02,  2.6315e-02],\n",
       "                         [-2.1996e-01, -1.3221e-01,  3.3208e-02]],\n",
       "               \n",
       "                        [[-2.4909e-01, -2.6592e-01, -2.0645e-01],\n",
       "                         [-1.0762e-01, -6.6946e-02,  8.7811e-03],\n",
       "                         [-6.0665e-01, -3.9572e-01,  2.6259e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.5713e-01,  2.0598e-02, -2.7367e-01],\n",
       "                         [ 6.5564e-01,  7.0835e-01,  3.2843e-01],\n",
       "                         [ 5.3082e-01,  4.2170e-01,  4.4941e-01]],\n",
       "               \n",
       "                        [[-2.3635e-01,  1.3916e-01, -6.9457e-02],\n",
       "                         [-2.0772e-01, -2.0619e-01,  1.1433e-01],\n",
       "                         [-9.7579e-02,  1.3461e-01, -1.9435e-01]],\n",
       "               \n",
       "                        [[ 1.1243e-01,  7.3248e-02, -5.8005e-01],\n",
       "                         [ 4.1625e-01,  1.5802e-01, -1.5220e-01],\n",
       "                         [ 1.0954e-01,  2.1287e-01,  6.4479e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.8514e-01, -6.7767e-01, -6.2687e-03],\n",
       "                         [ 7.9792e-02, -1.5496e-01,  7.1891e-02],\n",
       "                         [ 4.1729e-01,  8.8093e-02,  1.1415e-02]],\n",
       "               \n",
       "                        [[ 1.0472e-01,  3.1409e-01, -8.2013e-02],\n",
       "                         [-1.7413e-01,  4.8220e-02,  2.3044e-01],\n",
       "                         [-2.2661e-01, -3.1395e-01,  3.5782e-03]],\n",
       "               \n",
       "                        [[ 1.0326e-02,  9.5574e-02,  2.7251e-01],\n",
       "                         [ 1.0675e-01,  6.2986e-01,  6.7351e-02],\n",
       "                         [ 2.0625e-01, -1.3773e-01, -2.4176e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0384e-01,  1.9216e-01,  4.9292e-01],\n",
       "                         [-6.2377e-02, -4.8770e-02,  2.5647e-01],\n",
       "                         [ 8.8091e-03, -2.2785e-01, -2.9479e-01]],\n",
       "               \n",
       "                        [[-1.9465e-01,  2.0875e-01,  1.3799e-01],\n",
       "                         [-7.9501e-02, -1.2001e-01,  8.5799e-02],\n",
       "                         [ 4.1575e-02, -1.1280e-01, -1.3597e-01]],\n",
       "               \n",
       "                        [[-8.6483e-02,  2.1124e-01, -4.1575e-02],\n",
       "                         [ 4.4607e-02, -3.8374e-01, -8.8831e-02],\n",
       "                         [-6.3992e-03, -3.0904e-01,  1.4238e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-9.5150e-02, -1.2026e-01, -4.7868e-01],\n",
       "                         [ 1.1366e-01,  3.2316e-01, -4.8886e-02],\n",
       "                         [-2.2111e-01,  4.1699e-01, -1.3402e-01]],\n",
       "               \n",
       "                        [[-4.6084e-02, -9.5220e-02,  1.3270e-01],\n",
       "                         [ 7.3283e-02,  4.6227e-01,  6.0677e-02],\n",
       "                         [ 8.0982e-02,  5.3135e-02, -3.8011e-01]],\n",
       "               \n",
       "                        [[-3.9134e-02, -4.4792e-02, -1.6132e-01],\n",
       "                         [-1.9085e-02,  2.2989e-01,  2.8438e-01],\n",
       "                         [-1.2687e-01, -1.6844e-01, -1.1649e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.2950e-02,  1.6138e-01, -1.3951e-01],\n",
       "                         [ 1.9325e-01,  2.2958e-01,  1.3838e-04],\n",
       "                         [ 1.5654e-01,  1.9797e-01,  2.3748e-02]],\n",
       "               \n",
       "                        [[-4.3690e-02,  2.0704e-01,  1.5514e-01],\n",
       "                         [-1.4521e-02,  1.1969e-01,  1.7547e-01],\n",
       "                         [-1.4499e-01, -1.1951e-01, -8.5195e-02]],\n",
       "               \n",
       "                        [[ 1.1795e-01,  2.9087e-01, -9.9679e-03],\n",
       "                         [ 1.4717e-03,  3.8095e-02,  1.6303e-02],\n",
       "                         [-1.9939e-01, -2.4597e-02,  5.6245e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-0.0329,  0.0076, -0.0186, -0.0108, -0.0339, -0.0197, -0.0016, -0.0328,\n",
       "                       -0.0221,  0.0538,  0.1613, -0.0712,  0.0406, -0.0192,  0.0004,  0.0020,\n",
       "                       -0.0230,  0.0113, -0.0054,  0.0304, -0.0420, -0.0046, -0.0610,  0.0347,\n",
       "                       -0.0150,  0.0948, -0.0050,  0.0200, -0.0487, -0.0260,  0.0248,  0.0243,\n",
       "                        0.0470, -0.0028, -0.0245,  0.0253,  0.0163,  0.0624, -0.0293, -0.0285,\n",
       "                        0.0053,  0.0136,  0.0167,  0.0048, -0.0406,  0.0204,  0.0439, -0.0006],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.4304, -0.4345, -0.5333, -0.5142, -0.5146, -0.4574, -0.2607, -0.6476,\n",
       "                       -0.5226, -0.7339, -0.3557, -0.6526, -0.3827, -0.5060, -0.6920, -0.5347,\n",
       "                       -0.5618, -0.5161, -0.3714, -0.5171, -0.8430, -0.3703, -0.4904, -0.5754,\n",
       "                       -0.4580, -0.5587, -0.2553, -0.3038, -0.5594, -0.5349, -0.5377, -0.2308,\n",
       "                       -0.3876, -0.6925, -0.6291, -0.4366, -0.4763, -0.2601, -0.2796, -0.3697,\n",
       "                       -0.4186, -0.6934, -0.5950, -0.3696, -0.5702, -0.4626, -0.4917, -0.3401],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9212, 0.8697, 0.8523, 1.0124, 1.0174, 0.9678, 1.1767, 0.7103, 0.8055,\n",
       "                       0.8389, 1.1517, 1.0097, 0.6234, 0.8290, 1.1433, 1.0160, 1.0019, 1.1706,\n",
       "                       0.8001, 1.0345, 1.1402, 0.8585, 0.9387, 0.9455, 0.8976, 0.9676, 1.0437,\n",
       "                       0.7782, 1.1580, 0.9720, 1.0089, 0.8770, 0.7915, 1.0672, 1.0063, 0.9985,\n",
       "                       1.3480, 0.9780, 0.9090, 0.6284, 0.7256, 1.1761, 1.0711, 0.7730, 1.2416,\n",
       "                       0.6414, 0.6992, 1.0323], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[ 2.4549e-01,  2.5847e-01,  1.0454e-01],\n",
       "                         [ 7.7286e-02,  3.3138e-01,  3.2097e-01],\n",
       "                         [ 4.8473e-01,  3.3912e-01,  7.7782e-02]],\n",
       "               \n",
       "                        [[-2.0340e-01,  2.1817e-01,  7.4440e-02],\n",
       "                         [ 9.8932e-02,  8.8508e-02,  1.4834e-01],\n",
       "                         [ 2.4463e-01, -2.8272e-02, -3.2642e-01]],\n",
       "               \n",
       "                        [[-1.4590e-03, -4.1734e-01, -3.7396e-02],\n",
       "                         [-2.9451e-01, -1.6112e-01, -6.6461e-02],\n",
       "                         [-3.1281e-01,  1.0927e-02, -1.9482e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0687e-01, -1.4027e-01, -9.8042e-02],\n",
       "                         [ 1.0702e-01, -4.5900e-01, -2.3066e-01],\n",
       "                         [-2.6252e-01, -2.0689e-01, -1.4543e-01]],\n",
       "               \n",
       "                        [[-5.5574e-02,  1.9709e-01, -4.1687e-01],\n",
       "                         [-6.4237e-02, -5.1425e-02, -1.8742e-01],\n",
       "                         [-6.8694e-02, -6.1672e-02, -2.0805e-01]],\n",
       "               \n",
       "                        [[-2.2694e-02, -1.5993e-01,  6.5287e-02],\n",
       "                         [-1.6701e-01, -4.1218e-01, -1.7733e-01],\n",
       "                         [-7.9389e-02,  3.0221e-01,  2.6804e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.1293e-02,  8.5440e-02,  6.2510e-02],\n",
       "                         [-5.2402e-01,  4.0840e-01,  3.4895e-01],\n",
       "                         [-1.8414e-01,  4.1813e-02, -1.8922e-02]],\n",
       "               \n",
       "                        [[-1.8183e-01,  2.8712e-01,  3.6843e-02],\n",
       "                         [-1.4976e-01,  3.9988e-02,  3.1769e-02],\n",
       "                         [-2.2052e-01, -1.3178e-01, -8.2666e-03]],\n",
       "               \n",
       "                        [[-1.0987e-02,  1.1520e-01,  6.5045e-02],\n",
       "                         [-2.8627e-01,  2.8054e-01,  1.9490e-01],\n",
       "                         [-3.3616e-01, -1.7016e-01,  1.0980e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7812e-01,  2.2633e-01, -1.5931e-01],\n",
       "                         [ 1.6444e-01, -1.2592e-01,  1.0437e-01],\n",
       "                         [-7.8552e-02, -1.3336e-01, -2.2701e-01]],\n",
       "               \n",
       "                        [[-4.9333e-02, -2.6297e-01, -2.3004e-01],\n",
       "                         [ 6.8446e-02,  1.2761e-01, -1.6410e-01],\n",
       "                         [ 1.6785e-01,  5.0741e-02, -1.6915e-01]],\n",
       "               \n",
       "                        [[-3.7354e-02,  1.4744e-01, -2.8789e-01],\n",
       "                         [-2.9584e-01,  7.3733e-02,  2.9048e-01],\n",
       "                         [-2.5140e-01,  9.0306e-02,  1.0614e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.9974e-02,  4.4222e-02,  1.1808e-01],\n",
       "                         [-4.7956e-02,  2.7788e-01, -2.1017e-01],\n",
       "                         [ 1.3672e-01,  2.9520e-01,  7.1339e-02]],\n",
       "               \n",
       "                        [[ 2.7446e-01,  8.9280e-02, -2.7824e-01],\n",
       "                         [ 2.0821e-01, -1.7941e-01, -4.1485e-01],\n",
       "                         [-4.8224e-01, -3.6463e-01, -1.9661e-01]],\n",
       "               \n",
       "                        [[ 2.7581e-01, -7.6286e-02, -2.8250e-01],\n",
       "                         [ 1.3885e-02, -7.9687e-03,  1.3981e-01],\n",
       "                         [ 2.5039e-01, -1.2190e-01,  2.3697e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.3907e-01, -8.1180e-02,  3.2926e-01],\n",
       "                         [-1.7035e-03,  8.2414e-02,  2.0983e-01],\n",
       "                         [-3.1340e-02, -1.2375e-01,  1.3808e-01]],\n",
       "               \n",
       "                        [[-2.2722e-01, -6.5160e-02,  1.3883e-01],\n",
       "                         [-8.3940e-02,  1.3297e-04,  3.5131e-02],\n",
       "                         [-1.7716e-02, -1.7864e-01, -3.5251e-01]],\n",
       "               \n",
       "                        [[ 1.6763e-01,  4.8312e-02,  9.2901e-02],\n",
       "                         [-2.9436e-02, -6.5738e-02, -7.9424e-02],\n",
       "                         [-4.0691e-02, -1.9156e-01,  1.3348e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.5346e-01, -1.2185e-01, -3.5528e-02],\n",
       "                         [ 1.1681e-01, -3.4166e-01,  3.0948e-01],\n",
       "                         [ 3.7600e-01, -1.4377e-01,  8.0332e-02]],\n",
       "               \n",
       "                        [[ 1.9662e-01, -2.8229e-01, -9.5155e-02],\n",
       "                         [ 2.1560e-01, -4.9343e-02,  2.4265e-01],\n",
       "                         [-6.3908e-02,  9.7519e-02, -2.6488e-01]],\n",
       "               \n",
       "                        [[ 1.9296e-01,  4.1335e-01,  1.8412e-02],\n",
       "                         [ 1.5878e-01, -1.0945e-01,  2.0183e-02],\n",
       "                         [ 3.6425e-02,  1.3105e-01,  4.5901e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 5.0332e-02,  1.3929e-01,  1.1186e-02],\n",
       "                         [-1.6881e-01,  2.0558e-01,  3.0479e-01],\n",
       "                         [-1.9465e-01, -1.5111e-01, -1.7442e-01]],\n",
       "               \n",
       "                        [[ 1.7245e-01, -6.3128e-02,  1.0966e-01],\n",
       "                         [ 1.4785e-01, -1.1358e-02,  4.4072e-02],\n",
       "                         [-2.3047e-01, -1.9480e-01, -3.6843e-01]],\n",
       "               \n",
       "                        [[ 6.5583e-02,  4.3287e-02,  9.5875e-02],\n",
       "                         [ 1.5306e-02, -1.9196e-01,  9.7573e-02],\n",
       "                         [ 3.3543e-01,  1.3394e-01,  5.5534e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.8403e-01, -6.7915e-02, -8.3255e-02],\n",
       "                         [ 2.8147e-01, -2.2715e-01,  1.4711e-01],\n",
       "                         [-4.9215e-01,  3.3187e-02, -1.0947e-01]],\n",
       "               \n",
       "                        [[-4.1156e-01, -4.6810e-01, -1.4943e-01],\n",
       "                         [-2.4137e-01, -8.1182e-02,  7.9947e-03],\n",
       "                         [ 1.2040e-01,  9.5524e-03,  1.7108e-01]],\n",
       "               \n",
       "                        [[ 1.1197e-01,  1.6591e-01,  3.9222e-01],\n",
       "                         [-1.0676e-01, -7.7532e-02,  3.5692e-02],\n",
       "                         [ 1.0571e-01, -1.7920e-01, -4.9146e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0443e-01,  1.6609e-01,  1.7897e-01],\n",
       "                         [-4.4188e-01, -2.1222e-01,  3.4465e-01],\n",
       "                         [-1.2874e-01,  1.3213e-01,  4.2678e-01]],\n",
       "               \n",
       "                        [[-1.8101e-01, -3.1850e-01, -1.4375e-01],\n",
       "                         [-1.7774e-01, -2.4946e-01, -4.4879e-01],\n",
       "                         [-3.3877e-01, -3.1757e-01, -1.2390e-01]],\n",
       "               \n",
       "                        [[ 1.2054e-01,  2.3060e-02,  3.2912e-01],\n",
       "                         [-1.6222e-01, -2.1900e-02,  4.5365e-02],\n",
       "                         [-2.6235e-01,  3.4127e-02, -1.1586e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4132e-01,  1.0333e-01,  1.6047e-01],\n",
       "                         [-4.6080e-02,  6.5285e-02, -2.7473e-02],\n",
       "                         [-7.9620e-02, -1.6466e-01, -4.5081e-01]],\n",
       "               \n",
       "                        [[-1.2032e-01, -1.4584e-01,  2.9436e-02],\n",
       "                         [ 2.4179e-01,  3.5906e-02, -1.7551e-01],\n",
       "                         [ 3.5838e-01,  2.0579e-01, -2.2640e-01]],\n",
       "               \n",
       "                        [[ 2.8517e-01,  1.4383e-01,  1.6230e-01],\n",
       "                         [ 5.4796e-03, -1.5349e-01, -9.0931e-02],\n",
       "                         [ 1.3547e-02, -1.6966e-01, -3.7585e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.0525e-01,  1.3899e-01, -2.2681e-01],\n",
       "                         [ 2.7978e-01, -1.2026e-01, -3.0919e-01],\n",
       "                         [ 1.7165e-01, -3.6785e-01, -1.6713e-01]],\n",
       "               \n",
       "                        [[-1.8233e-01, -2.3156e-01,  1.5796e-02],\n",
       "                         [ 1.0380e-01, -2.1537e-01, -2.3803e-02],\n",
       "                         [-1.6771e-01, -3.0854e-01, -3.4553e-01]],\n",
       "               \n",
       "                        [[ 2.7610e-01, -4.7905e-02,  1.8626e-01],\n",
       "                         [ 2.1234e-01,  1.4442e-01,  2.5251e-01],\n",
       "                         [ 3.8522e-02,  3.0156e-01,  2.4454e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-0.0877,  0.0023, -0.0376, -0.0640, -0.0558, -0.0492, -0.0712,  0.0384,\n",
       "                        0.0128, -0.0420, -0.0930, -0.0566, -0.0319, -0.1364,  0.0349,  0.0320,\n",
       "                       -0.0986,  0.2366, -0.0639, -0.0719, -0.1615,  0.0126, -0.0164,  0.2751,\n",
       "                       -0.0989,  0.1213, -0.0041, -0.1039, -0.0417, -0.0902, -0.0538, -0.0886,\n",
       "                        0.0083, -0.0346,  0.1271, -0.0450, -0.1059,  0.0044, -0.1248, -0.0260,\n",
       "                       -0.0834, -0.0914, -0.0490, -0.0264, -0.0739,  0.0183, -0.1078, -0.1302],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.5949, -0.7788, -0.7880, -0.7466, -0.6135, -1.0065, -0.5644, -0.7884,\n",
       "                       -0.6787, -0.5656, -0.7406, -0.5386, -0.8404, -0.8894, -0.4902, -0.5821,\n",
       "                       -0.7360, -0.8093, -0.5860, -0.8698, -0.5723, -0.6899, -0.7785, -0.8635,\n",
       "                       -0.7334, -0.7971, -0.6033, -1.0569, -0.9189, -1.1420, -0.9178, -0.6445,\n",
       "                       -0.5795, -0.4786, -0.3683, -0.6311, -1.5928, -0.4780, -0.6913, -0.4625,\n",
       "                       -0.6796, -0.7789, -0.7732, -0.8903, -0.7531, -0.6598, -0.6171, -0.7321],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.7071, 0.8695, 0.7773, 0.8650, 0.8455, 0.9980, 0.6671, 0.8089, 0.9074,\n",
       "                       0.8966, 0.8511, 0.7180, 0.9046, 0.9215, 0.7185, 0.6047, 0.8742, 1.2465,\n",
       "                       0.6919, 0.8145, 0.6539, 0.7987, 0.6534, 1.0370, 0.8965, 0.8471, 0.8757,\n",
       "                       1.2521, 0.9317, 1.1705, 0.9870, 0.9418, 0.7088, 0.7082, 0.6715, 0.8272,\n",
       "                       1.2326, 0.5916, 0.9352, 0.7657, 0.6323, 0.8456, 1.0063, 0.9364, 0.6543,\n",
       "                       0.8201, 0.7544, 0.8710], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-2.0459e-01, -8.4365e-02, -1.0680e-01],\n",
       "                         [-2.4442e-01, -5.0273e-02, -7.8724e-02],\n",
       "                         [-9.6904e-02, -1.3160e-01, -3.2295e-02]],\n",
       "               \n",
       "                        [[-2.2729e-01, -2.7357e-01, -3.1115e-01],\n",
       "                         [-1.1110e-01,  2.6167e-02, -2.3969e-01],\n",
       "                         [-3.0304e-01, -2.1493e-01, -8.8380e-02]],\n",
       "               \n",
       "                        [[-1.6748e-01, -1.0030e-01, -2.4359e-01],\n",
       "                         [-1.5814e-01, -2.0185e-01, -9.2237e-02],\n",
       "                         [-1.9136e-01, -1.2386e-01, -4.0967e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.6445e-02, -4.1455e-02,  2.2409e-01],\n",
       "                         [ 2.2754e-01,  3.9414e-01,  2.7529e-01],\n",
       "                         [-4.0295e-02,  2.0674e-02,  6.1777e-02]],\n",
       "               \n",
       "                        [[ 1.1443e-01, -3.6887e-02,  1.0964e-01],\n",
       "                         [ 4.6583e-02,  6.8231e-02, -2.7080e-03],\n",
       "                         [ 1.0169e-01,  3.5468e-02, -1.5149e-01]],\n",
       "               \n",
       "                        [[-1.2192e-01,  1.9149e-01,  9.8602e-02],\n",
       "                         [ 6.8092e-02,  2.6787e-01,  1.9773e-01],\n",
       "                         [ 1.3956e-01,  2.7847e-01,  2.5147e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.1813e-02,  7.9722e-02, -4.6994e-03],\n",
       "                         [-2.1382e-02,  9.2675e-03,  2.7499e-02],\n",
       "                         [ 1.5760e-02,  8.7869e-03,  5.6520e-02]],\n",
       "               \n",
       "                        [[-9.9726e-03, -2.6515e-02, -3.0455e-02],\n",
       "                         [ 4.5747e-02,  8.0527e-02,  9.8227e-02],\n",
       "                         [ 6.4579e-02,  1.1608e-01,  1.7990e-01]],\n",
       "               \n",
       "                        [[ 1.3058e-01,  7.1330e-02, -7.5656e-02],\n",
       "                         [ 7.9242e-02,  3.2647e-02,  8.4565e-02],\n",
       "                         [ 1.1707e-01, -8.4182e-04,  1.7320e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.1825e-02, -3.9066e-03, -7.3329e-02],\n",
       "                         [-5.4164e-02, -2.7161e-02, -1.2240e-01],\n",
       "                         [ 6.6366e-02,  6.7999e-02, -2.1571e-02]],\n",
       "               \n",
       "                        [[ 7.6220e-02,  7.6708e-02,  3.7360e-02],\n",
       "                         [ 7.4579e-03,  2.6419e-02,  3.0145e-02],\n",
       "                         [-4.9123e-02, -1.2312e-01, -2.8876e-02]],\n",
       "               \n",
       "                        [[-1.0599e-01, -2.1741e-01, -1.4791e-02],\n",
       "                         [-1.6360e-01, -1.3339e-01, -6.0747e-02],\n",
       "                         [-5.3684e-02, -1.0150e-01,  9.3847e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.2344e-01,  7.5209e-02, -3.2955e-01],\n",
       "                         [-2.4891e-01, -4.1140e-01, -3.4098e-01],\n",
       "                         [ 1.9807e-01, -1.0983e-01, -1.4272e-01]],\n",
       "               \n",
       "                        [[-1.0951e-01,  5.4531e-02, -2.4592e-01],\n",
       "                         [-2.6688e-02,  4.5938e-02, -3.0180e-01],\n",
       "                         [ 2.1480e-01,  2.3019e-01,  2.1506e-01]],\n",
       "               \n",
       "                        [[-4.4562e-01, -3.7466e-01, -2.1436e-01],\n",
       "                         [-2.0995e-01, -1.7431e-01, -4.7698e-01],\n",
       "                         [-2.5191e-02,  1.6022e-02, -6.4040e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3041e-01,  6.7363e-02, -1.2642e-01],\n",
       "                         [ 5.1967e-01,  1.3314e-01,  2.0036e-01],\n",
       "                         [ 4.0107e-03,  6.3314e-02, -4.8053e-03]],\n",
       "               \n",
       "                        [[ 2.7523e-01,  3.9019e-01, -1.0661e-01],\n",
       "                         [ 4.1568e-01,  2.4950e-01,  1.8807e-01],\n",
       "                         [-6.0319e-01, -2.8670e-01, -2.0880e-01]],\n",
       "               \n",
       "                        [[-1.4294e-02, -4.0188e-01,  3.9233e-01],\n",
       "                         [ 3.5388e-02,  2.4834e-02,  3.0181e-01],\n",
       "                         [-1.5131e-03,  1.7115e-01, -1.5098e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.2414e-01, -4.7446e-02, -1.2911e-01],\n",
       "                         [ 1.2241e-01,  1.3555e-01,  2.1511e-01],\n",
       "                         [ 3.4014e-01,  2.4265e-01,  3.3169e-01]],\n",
       "               \n",
       "                        [[-4.0369e-02,  1.5118e-01,  8.1829e-02],\n",
       "                         [-5.3496e-02,  2.5203e-02, -1.4704e-01],\n",
       "                         [-3.0604e-01, -1.1004e-01, -1.2336e-01]],\n",
       "               \n",
       "                        [[-6.4162e-02, -1.8585e-02, -2.1969e-01],\n",
       "                         [-1.8833e-01, -9.2162e-02, -2.8038e-01],\n",
       "                         [-3.3636e-02, -1.5890e-01, -9.7827e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.6249e-02, -4.9382e-01, -1.5533e-01],\n",
       "                         [-1.3802e-01, -2.0947e-02,  3.1026e-02],\n",
       "                         [-7.3374e-02, -2.0758e-01, -2.3607e-01]],\n",
       "               \n",
       "                        [[-7.8566e-02, -6.7067e-02,  6.4306e-02],\n",
       "                         [-6.0312e-02, -2.9894e-01,  5.9674e-02],\n",
       "                         [-1.3209e-01, -4.7107e-01, -9.8594e-02]],\n",
       "               \n",
       "                        [[-3.6677e-02,  7.1678e-02,  7.5430e-02],\n",
       "                         [-5.8716e-02, -1.5807e-01,  1.7849e-01],\n",
       "                         [-2.7926e-01, -2.0088e-01,  7.8701e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.1853e-01, -6.0608e-02,  3.9033e-01],\n",
       "                         [ 1.6900e-01,  1.4960e-01, -2.0439e-02],\n",
       "                         [-6.9034e-02,  1.4096e-01,  3.0732e-01]],\n",
       "               \n",
       "                        [[-2.2637e-01, -3.3940e-01, -1.0124e-01],\n",
       "                         [ 2.6204e-04, -1.0727e-01,  1.5555e-02],\n",
       "                         [ 1.0444e-01, -3.0837e-01,  7.3571e-04]],\n",
       "               \n",
       "                        [[ 1.0017e-01, -2.9949e-01, -8.3789e-02],\n",
       "                         [-1.3365e-02,  1.4483e-02, -1.5821e-01],\n",
       "                         [-1.2575e-01, -2.7508e-01, -3.0131e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.3556e-01,  1.8656e-01,  4.4582e-01],\n",
       "                         [ 3.5240e-01,  1.4942e-01,  5.4494e-01],\n",
       "                         [ 5.9869e-01,  2.4622e-01,  1.8589e-01]],\n",
       "               \n",
       "                        [[-3.5133e-01, -1.0746e-01, -1.6267e-01],\n",
       "                         [-1.0638e-01, -2.8611e-01,  1.2333e-01],\n",
       "                         [-2.6930e-01, -1.6662e-01,  7.8572e-02]],\n",
       "               \n",
       "                        [[-2.2341e-01, -3.5009e-01, -3.2329e-01],\n",
       "                         [-3.8637e-01, -3.3331e-01, -3.3028e-03],\n",
       "                         [-2.6943e-01,  6.2298e-02,  2.3679e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.6665e-02,  6.4546e-02,  1.2089e-02],\n",
       "                         [ 9.6224e-03, -6.2301e-02,  1.6918e-02],\n",
       "                         [ 3.4178e-02,  1.5356e-02,  5.9921e-02]],\n",
       "               \n",
       "                        [[ 5.7247e-02,  5.3222e-03,  5.9031e-02],\n",
       "                         [ 1.7299e-02,  1.1680e-01,  1.2865e-01],\n",
       "                         [ 4.9161e-02,  7.1454e-02,  6.7179e-02]],\n",
       "               \n",
       "                        [[ 7.7367e-02,  3.4430e-02, -6.0610e-02],\n",
       "                         [ 4.6197e-02, -6.6605e-03,  2.7399e-02],\n",
       "                         [ 1.1000e-01, -6.0356e-03, -2.6387e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.9626e-02, -5.6386e-02, -4.8871e-03],\n",
       "                         [ 2.3261e-02, -1.0355e-01, -3.5961e-02],\n",
       "                         [ 2.2009e-02,  6.8089e-02, -4.3653e-02]],\n",
       "               \n",
       "                        [[ 8.1906e-02,  7.1204e-02,  2.1098e-02],\n",
       "                         [ 9.6310e-02,  1.2330e-03,  4.0216e-02],\n",
       "                         [-1.5326e-02,  2.5112e-02,  2.4048e-02]],\n",
       "               \n",
       "                        [[-1.4966e-01, -8.3312e-02,  3.3522e-02],\n",
       "                         [-1.1627e-01, -2.3901e-01, -3.5298e-02],\n",
       "                         [-1.1663e-01, -1.3862e-02,  7.0965e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-7.7400e-02, -1.6487e+00, -1.5310e-01, -2.5010e-01, -1.1542e+00,\n",
       "                       -2.0747e-01, -3.1312e-01, -2.5296e-01, -6.6167e-02, -1.0743e-01,\n",
       "                       -1.6641e+00, -1.0022e-01, -3.2794e-01, -4.5677e-01, -1.9883e-01,\n",
       "                       -1.7019e-01, -5.7371e-01, -3.9004e-01, -7.8508e-03, -3.1320e-01,\n",
       "                       -6.0012e-01, -2.4004e-01, -3.9910e-03, -1.4160e-01, -3.3410e-01,\n",
       "                       -4.1296e-01, -2.4801e-01, -3.2825e-01, -4.1676e-01, -2.6253e-01,\n",
       "                       -1.6345e-02, -1.7900e-02,  1.3540e-03, -5.4109e-02,  1.3195e-03,\n",
       "                       -6.7602e-02,  3.4870e-03, -7.5968e-03, -1.2228e-01, -4.2025e-01,\n",
       "                       -3.9132e-01, -2.6037e-01, -2.8956e-01, -3.4559e-01, -2.3766e-01,\n",
       "                       -1.5480e-01, -2.7266e-01, -1.1641e+00], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.8522, -0.3410, -1.4733, -0.7980, -0.2966, -0.5938, -0.7963, -0.4635,\n",
       "                       -0.8682, -0.8026, -0.2516, -0.1343, -0.4746, -0.3484, -0.9831, -0.6640,\n",
       "                       -1.3425, -1.4309, -0.9185, -0.5757, -1.0268, -0.5835, -0.8744, -0.4689,\n",
       "                       -0.7214, -0.5428, -0.7791, -0.7104, -0.6917, -0.3308, -0.8832, -0.8812,\n",
       "                       -0.9696, -0.8198, -0.7112, -0.9641, -0.8182, -0.7399, -0.7880, -0.8582,\n",
       "                       -0.3953, -0.2718, -0.8091, -0.3548, -0.4451, -0.9478, -0.7033, -0.2702],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.3098, 0.3202, 0.7028, 0.5855, 0.2599, 0.6091, 0.5385, 0.7165, 0.2697,\n",
       "                       0.3252, 0.2638, 0.4211, 0.7873, 0.2843, 0.4916, 0.5110, 0.9202, 0.8916,\n",
       "                       0.2327, 0.5921, 0.8864, 0.6685, 0.1929, 0.5606, 0.4550, 0.5141, 0.5519,\n",
       "                       0.6143, 0.8405, 0.5100, 0.1843, 0.2259, 0.2590, 0.3242, 0.1427, 0.3078,\n",
       "                       0.2185, 0.1363, 0.6754, 0.8303, 0.6430, 0.4632, 0.5864, 0.5430, 0.7119,\n",
       "                       0.7958, 0.8594, 0.2817], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[ 0.0775,  0.0360, -0.0757,  ..., -0.1118, -0.1221, -0.0472],\n",
       "                       [-0.1407,  0.1495,  0.2562,  ..., -0.1349, -0.0663, -0.1425],\n",
       "                       [-0.2792, -0.1754, -0.2051,  ..., -0.1163, -0.0203, -0.0997],\n",
       "                       [ 0.3169,  0.1465, -0.0330,  ..., -0.1304, -0.0937, -0.1754],\n",
       "                       [ 0.0515,  0.1285,  0.2233,  ...,  0.3056,  0.2616,  0.2186]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([ 0.0935, -0.3501,  0.2727,  0.0914, -0.2260], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('arbiter.0.weight',\n",
       "               tensor([[-5.9029e-01, -5.3795e-01, -1.5661e-01, -2.5986e-01, -3.1954e-01,\n",
       "                        -4.9768e-01, -3.2275e-01, -1.2587e-01, -4.6661e-01, -4.2619e-01,\n",
       "                         3.9449e-01, -2.9062e-01,  3.1589e-01, -3.6946e-01,  8.7661e-01,\n",
       "                        -3.3710e-01,  4.6817e-01, -2.3055e-01,  1.6675e-01, -2.6510e+00],\n",
       "                       [-8.7019e-01, -1.0105e+00, -1.1579e+00, -9.3530e-01, -8.2814e-01,\n",
       "                        -9.3626e-01, -7.4222e-01, -8.1796e-01, -1.1467e+00, -9.3128e-01,\n",
       "                         1.1218e+00, -9.5300e-01,  1.1893e+00, -8.4276e-01,  1.6632e+00,\n",
       "                        -1.0992e+00,  1.1598e+00, -7.6434e-01,  1.4629e-01, -5.6038e+00],\n",
       "                       [-1.7650e-01, -7.4263e-02, -1.9603e-01,  1.7395e-01, -1.2017e-01,\n",
       "                         1.5592e-01, -2.8765e-03,  7.1014e-02,  7.4614e-02,  1.7035e-01,\n",
       "                        -1.8075e-01, -3.1389e-03, -6.3424e-02,  4.2344e-02, -8.9193e-02,\n",
       "                         1.8377e-01, -5.3476e-02,  1.4562e-01, -1.4323e-01, -1.9829e-01],\n",
       "                       [-5.7256e-02,  3.0856e-02, -5.4239e-02, -8.6291e-02,  1.0822e-01,\n",
       "                        -6.2852e-02,  1.8812e-01,  1.9100e-01, -1.6266e-01,  6.1604e-02,\n",
       "                        -1.5123e-01, -2.5679e-02,  1.4868e-01, -1.9586e-01, -1.3195e-02,\n",
       "                         1.2474e-01, -8.8378e-02, -1.6145e-01, -2.0578e-01,  1.1818e-01],\n",
       "                       [-7.3623e-01, -6.0644e-01, -3.1303e-01, -4.3669e-01, -6.6999e-01,\n",
       "                        -4.7027e-01, -3.3417e-01, -5.6876e-01, -3.0895e-01, -6.1169e-01,\n",
       "                         1.0986e+00, -5.8277e-01,  1.2235e+00, -5.4190e-01,  1.3720e+00,\n",
       "                        -3.1581e-01,  2.7115e-01, -5.5397e-01, -8.2026e-01, -6.4324e+00],\n",
       "                       [-6.1735e-01, -3.8340e-01, -5.5814e-01, -4.9269e-01, -4.0294e-01,\n",
       "                        -3.3116e-01, -4.0873e-01, -5.3923e-01, -5.8814e-01, -3.2241e-01,\n",
       "                         1.2334e+00, -6.4057e-01,  1.1687e+00, -5.7191e-01,  1.2466e+00,\n",
       "                        -2.8693e-01,  1.4586e-01, -6.1125e-01, -1.1817e+00, -7.0028e+00],\n",
       "                       [ 1.0714e-02,  1.8095e-01, -1.1328e-02, -4.4111e-02, -1.8175e-01,\n",
       "                        -1.7345e-01, -9.5249e-02, -1.9606e-01, -1.2823e-01,  4.4785e-02,\n",
       "                        -1.6487e-01,  1.9681e-01,  1.6048e-01, -6.9412e-02, -1.7510e-02,\n",
       "                         2.1979e-01,  1.7670e-02, -5.7673e-03, -1.7022e-01, -2.3977e-02],\n",
       "                       [-5.2119e-02,  5.8366e-02, -9.5376e-02,  4.1151e-02, -1.4242e-02,\n",
       "                        -1.3989e-01, -2.9618e-02, -4.6825e-02,  1.5251e-02, -5.1655e-03,\n",
       "                        -1.6485e-01,  1.5387e-01,  1.4536e-01,  3.0741e-02, -1.5371e-01,\n",
       "                         2.2002e-02, -5.1757e-02, -5.1059e-02, -1.4909e-02,  2.1917e-01],\n",
       "                       [ 1.1688e-01,  2.1151e-01,  1.4778e-01,  1.1768e-01,  2.2312e-01,\n",
       "                        -8.8966e-02,  1.5089e-01, -1.1936e-01,  1.0614e-01,  7.8510e-02,\n",
       "                        -7.6256e-03, -1.3070e-01,  1.3243e-01,  1.8272e-01, -2.0383e-01,\n",
       "                        -9.6117e-02, -1.3217e-01,  1.5964e-01, -1.7313e-01,  1.2106e-01],\n",
       "                       [ 8.7365e-02,  9.4045e-02, -5.1892e-02,  1.6762e-01,  1.1671e-01,\n",
       "                         7.5187e-02, -2.6933e-02, -8.9412e-03,  7.3725e-02,  6.3618e-03,\n",
       "                         6.2247e-03, -2.5323e-02, -2.1953e-01, -1.3890e-01, -4.5655e-02,\n",
       "                        -1.6598e-01, -1.5479e-01, -1.0123e-02, -4.2731e-02,  1.1125e-01],\n",
       "                       [-4.1115e-01, -1.6570e-01, -3.3532e-01, -4.6333e-01, -2.4726e-01,\n",
       "                        -2.9219e-01, -8.4968e-02, -2.7568e-01, -1.3568e-01, -1.6816e-01,\n",
       "                         1.0562e+00, -1.1782e-01,  9.7169e-01, -5.2290e-02,  1.2544e+00,\n",
       "                        -1.7507e-01, -2.2543e-01, -1.0114e-01, -1.5591e+00, -7.6597e+00],\n",
       "                       [-1.1983e-01,  1.5614e-01, -1.2172e-01,  1.8576e-01,  7.3353e-02,\n",
       "                        -1.6203e-02,  1.3759e-01, -2.0994e-01,  2.1366e-01, -1.8662e-01,\n",
       "                        -1.1727e-01, -1.1385e-01, -3.9996e-03,  1.4979e-01, -1.4717e-01,\n",
       "                         6.0627e-02,  1.8551e-02, -5.7218e-02, -5.6653e-02,  2.0733e-01],\n",
       "                       [-8.6713e-01, -7.1265e-01, -9.3696e-01, -7.8758e-01, -8.2849e-01,\n",
       "                        -6.5304e-01, -9.0559e-01, -7.5495e-01, -6.0328e-01, -9.6782e-01,\n",
       "                         7.5313e-01, -9.2936e-01,  8.5076e-01, -6.4514e-01,  1.2284e+00,\n",
       "                        -9.5162e-01,  1.1903e+00, -7.8464e-01, -3.1248e-02, -5.3264e+00],\n",
       "                       [ 9.6642e-02,  3.9319e-02, -6.7998e-02, -9.8098e-03,  1.5438e-01,\n",
       "                         5.7538e-02, -1.2209e-01,  2.5769e-01, -1.5013e-01,  1.6490e-02,\n",
       "                        -1.7372e-01,  1.1015e-01,  8.8218e-02, -1.3506e-01, -2.6332e-01,\n",
       "                         1.4997e-02,  1.3280e-02,  1.9106e-02,  6.1276e-02, -1.5640e-01],\n",
       "                       [ 2.1353e-01,  2.0006e-01, -1.8942e-01,  7.9496e-02,  4.1478e-02,\n",
       "                        -1.1179e-01,  5.3119e-02,  1.8549e-01, -3.3323e-03, -1.2598e-01,\n",
       "                         2.5711e-02, -2.9604e-03, -2.0496e-02,  1.7962e-01, -1.9416e-02,\n",
       "                        -4.8471e-02, -7.4401e-02,  2.2016e-01, -8.8840e-03, -1.8109e-01],\n",
       "                       [-8.5593e-01, -8.0253e-01, -5.3770e-01, -7.1620e-01, -5.4736e-01,\n",
       "                        -4.2729e-01, -7.3555e-01, -4.9435e-01, -7.6810e-01, -8.0610e-01,\n",
       "                         1.1936e+00, -5.7552e-01,  1.1435e+00, -7.9414e-01,  1.3673e+00,\n",
       "                        -4.4523e-01,  5.3758e-01, -8.0454e-01, -9.2493e-01, -6.7744e+00],\n",
       "                       [-2.7364e-01, -4.3885e-01, -4.4961e-01, -4.8198e-01, -3.5917e-01,\n",
       "                        -5.8706e-01, -5.4683e-01, -6.3895e-01, -2.5407e-01, -3.0597e-01,\n",
       "                         1.1502e+00, -5.0423e-01,  1.1282e+00, -6.2438e-01,  1.3824e+00,\n",
       "                        -4.8713e-01, -2.2746e-02, -4.4713e-01, -1.0334e+00, -6.2128e+00],\n",
       "                       [-5.9038e-01, -5.0077e-01, -7.8565e-01, -8.5617e-01, -6.1241e-01,\n",
       "                        -5.4356e-01, -8.6500e-01, -8.2575e-01, -4.9568e-01, -8.4503e-01,\n",
       "                         1.0357e+00, -6.6297e-01,  1.2667e+00, -5.3725e-01,  1.6418e+00,\n",
       "                        -7.3191e-01,  5.3606e-01, -8.7648e-01, -4.6736e-01, -6.0921e+00],\n",
       "                       [-2.2760e-01, -3.4731e-01, -4.2479e-01, -3.2570e-01, -1.5208e-01,\n",
       "                        -4.1844e-01, -2.9004e-01, -3.8252e-01, -2.1515e-01, -4.0105e-01,\n",
       "                         8.9929e-01, -1.0297e-01,  1.1806e+00, -1.9034e-01,  1.1976e+00,\n",
       "                        -1.5566e-01, -8.1065e-02, -4.1297e-01, -1.0365e+00, -6.1294e+00],\n",
       "                       [ 1.2256e-01, -1.2452e-01,  2.0354e-01,  2.0327e-01, -1.5081e-01,\n",
       "                        -1.3786e-01,  1.1737e-01, -1.7185e-01, -1.0510e-01,  1.4711e-01,\n",
       "                        -1.4424e-01, -8.3434e-02, -7.8773e-02,  1.0417e-01,  1.4677e-01,\n",
       "                         4.7335e-02,  1.0653e-01,  1.9805e-01,  3.6867e-02,  2.4315e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.0.bias',\n",
       "               tensor([ 0.5779,  1.4630,  0.0644, -0.0222,  1.0881,  1.0257,  0.0573, -0.2082,\n",
       "                        0.0964, -0.0406,  1.0319, -0.1191,  1.2168, -0.0490, -0.0097,  0.9679,\n",
       "                        1.1930,  1.0564,  0.9237, -0.0218], device='cuda:0')),\n",
       "              ('arbiter.2.weight',\n",
       "               tensor([[-2.7696e-01, -2.0690e-01, -1.9416e-01,  8.0315e-02,  9.9638e-02,\n",
       "                         1.1186e-01,  4.5160e-02,  2.3965e-02,  1.4061e-02,  2.2705e-02,\n",
       "                         1.2049e-01, -1.1446e-02, -2.8662e-01, -1.1019e-01, -1.1449e-01,\n",
       "                         1.0774e-02,  1.3232e-01, -4.3487e-02,  1.7941e-01, -1.8308e-01],\n",
       "                       [ 8.9045e-02,  1.0461e-01,  2.0148e-02, -1.9925e-01,  9.4846e-02,\n",
       "                         1.3727e-01,  1.4924e-01,  1.6985e-02, -3.1922e-02, -1.8997e-01,\n",
       "                         1.2385e-01, -9.4876e-02,  1.8950e-01, -1.7378e-01,  1.4378e-01,\n",
       "                         1.1648e-01,  2.1102e-01,  1.1773e-01,  1.3853e-02,  1.5089e-01],\n",
       "                       [-1.3032e-01, -1.1176e-01, -2.0589e-01,  5.7043e-02, -1.6127e-01,\n",
       "                         1.3371e-01,  1.7142e-01, -1.1048e-01,  1.9873e-01,  1.9033e-01,\n",
       "                        -2.0928e-01, -8.5642e-03, -3.7499e-01, -1.7186e-01,  3.9366e-03,\n",
       "                        -1.1968e-01,  1.2172e-02, -5.6698e-02,  8.3250e-02, -1.0491e-01],\n",
       "                       [ 9.8387e-02, -1.3402e-01,  1.6478e-01,  9.6023e-02, -1.0133e-01,\n",
       "                        -2.2337e-01,  6.4815e-02, -1.5311e-01,  1.4839e-01,  1.4678e-01,\n",
       "                         2.7220e-02, -1.6420e-02,  1.9317e-01, -5.1189e-02, -1.8708e-01,\n",
       "                         8.1884e-02, -1.8736e-01,  6.4915e-02,  2.0665e-02,  1.1247e-01],\n",
       "                       [-1.3734e-01, -4.3429e-02,  2.1410e-01,  6.7579e-02, -1.2165e-01,\n",
       "                        -1.5032e-02,  7.8945e-02, -5.4656e-02,  8.8937e-02, -1.9746e-02,\n",
       "                        -7.2334e-01,  1.3736e-01, -1.1583e-01,  6.0569e-02,  9.8967e-02,\n",
       "                        -2.0337e-01, -7.0027e-03, -2.6471e-02, -5.6458e-02,  3.6071e-02],\n",
       "                       [ 2.0000e-03,  1.2369e-02,  9.4219e-02,  1.6797e-01, -1.1251e-02,\n",
       "                         1.9300e-01,  1.8008e-01, -1.9471e-01,  1.4875e-01, -8.0597e-02,\n",
       "                         2.1163e-01, -1.4089e-01,  1.6965e-01,  1.1741e-01,  2.2125e-01,\n",
       "                         1.9232e-01, -1.4408e-01, -4.3063e-02, -2.0308e-01, -1.8770e-01],\n",
       "                       [ 1.6617e-01,  8.0233e-01, -2.1800e-01,  3.4234e-03,  1.0386e+00,\n",
       "                         1.1546e+00,  9.3585e-02, -2.1838e-01,  1.5790e-01,  1.6616e-01,\n",
       "                         1.5319e+00, -2.0903e-01,  1.1677e+00,  1.4261e-03, -8.1823e-02,\n",
       "                         1.1348e+00,  1.1217e+00,  7.9287e-01,  1.3367e+00, -7.9305e-02],\n",
       "                       [-1.1812e-01, -9.0398e-03, -1.7027e-01,  1.6862e-01,  7.9786e-02,\n",
       "                        -1.9915e-01,  4.6392e-02, -7.1841e-02,  1.0104e-01,  2.1979e-01,\n",
       "                         9.9288e-02,  3.3550e-02,  4.0755e-02, -1.0942e-01, -7.6839e-02,\n",
       "                        -1.5497e-01,  1.6216e-01,  1.9570e-01, -1.7680e-01, -1.8908e-01],\n",
       "                       [ 8.1760e-01,  1.1747e+00,  6.9845e-02, -2.8980e-02,  1.2703e+00,\n",
       "                         1.1017e+00, -1.5357e-01, -9.1709e-02, -1.0182e-01, -2.2172e-02,\n",
       "                         1.0701e+00, -1.8261e-02,  1.2327e+00, -1.4972e-01,  2.1412e-01,\n",
       "                         1.3993e+00,  1.1935e+00,  1.4037e+00,  1.3015e+00, -1.2772e-01],\n",
       "                       [ 6.5371e-02,  2.6495e-01,  9.2624e-02,  6.5136e-02, -1.9450e-02,\n",
       "                        -6.2750e-02,  1.9140e-01,  1.1874e-01,  1.3471e-01, -9.5442e-03,\n",
       "                        -5.2929e-01, -6.2280e-02,  2.4987e-01, -1.1313e-01, -1.3026e-01,\n",
       "                        -3.1673e-01, -1.6937e-01,  2.2256e-01, -3.9163e-01,  2.2979e-02]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.2.bias',\n",
       "               tensor([-0.0365,  0.1921,  0.0487, -0.1156,  0.1422, -0.0955, -0.0888,  0.1262,\n",
       "                        1.3369, -0.0932], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.361622004032135,\n",
       "   1.1774307218790054,\n",
       "   1.0682220284938813,\n",
       "   1.0134359432458877,\n",
       "   0.9557582337856293,\n",
       "   0.9252625764608383,\n",
       "   0.896571821808815,\n",
       "   0.8668007366657257,\n",
       "   0.8472098358869553,\n",
       "   0.8739602203369141,\n",
       "   0.8027454015016556,\n",
       "   0.790922427535057,\n",
       "   0.7696625510454178,\n",
       "   0.7480737142562867,\n",
       "   0.7457483853101731,\n",
       "   0.7329645273685456,\n",
       "   0.7137770148515701,\n",
       "   0.7022364659309387,\n",
       "   0.6944536420702935,\n",
       "   0.678084044277668,\n",
       "   0.6711137498617172,\n",
       "   0.6432430448532105,\n",
       "   0.6444714757800102,\n",
       "   0.6437381136417389,\n",
       "   0.6322268932461739,\n",
       "   0.7675004714727401,\n",
       "   0.6858330940008164,\n",
       "   0.6369115214347839,\n",
       "   0.6205476983785629,\n",
       "   0.6424601489901542,\n",
       "   0.6062124630808831,\n",
       "   0.6129032323956489,\n",
       "   0.6454385172724724,\n",
       "   0.6008061950802803,\n",
       "   0.6229772954583168,\n",
       "   0.5918961485624313,\n",
       "   0.5958339176774025,\n",
       "   0.6019341088533402,\n",
       "   0.5990372623801231,\n",
       "   0.5765040374994278,\n",
       "   0.5836926875710488,\n",
       "   0.575173718303442,\n",
       "   0.5905435925424098,\n",
       "   0.587688982129097,\n",
       "   0.5772113785147667,\n",
       "   0.5716481804251671,\n",
       "   0.7907439666390419,\n",
       "   0.606193328499794,\n",
       "   0.5810073080658913,\n",
       "   0.5803816887140274,\n",
       "   0.5827801483869552,\n",
       "   0.5594701299965381,\n",
       "   0.5675239782035351,\n",
       "   0.5574785028994084,\n",
       "   0.5821172350645065,\n",
       "   0.572479012131691,\n",
       "   0.5552550277411937,\n",
       "   0.6292399823069572,\n",
       "   0.5869806709885598,\n",
       "   0.5646496734023094,\n",
       "   0.5611681414842605,\n",
       "   0.5489646435678005,\n",
       "   0.5403351920247078,\n",
       "   0.5316672612428666,\n",
       "   0.547808075785637,\n",
       "   0.5346291369497777,\n",
       "   0.5325210520029068,\n",
       "   0.5130468677282334,\n",
       "   0.5175497198700905,\n",
       "   0.5460115510523319,\n",
       "   0.5095459462404252,\n",
       "   0.5113521621525288,\n",
       "   0.5150111964941024,\n",
       "   0.5099474284946919,\n",
       "   0.49857552760839463,\n",
       "   0.49722088885307314,\n",
       "   0.5020624770820141,\n",
       "   0.5039023125469685,\n",
       "   0.49570246279239655,\n",
       "   0.5044726549983024,\n",
       "   0.48876553937792777,\n",
       "   0.49015948405861853,\n",
       "   0.48760210585594177,\n",
       "   0.48937080571055414,\n",
       "   0.48270082703232764,\n",
       "   0.48500165647268295,\n",
       "   0.4837604705393314,\n",
       "   0.47981575956940653,\n",
       "   0.47343710049986837,\n",
       "   0.4680024850666523,\n",
       "   0.47328490269184115,\n",
       "   0.47831877300143244,\n",
       "   0.4573169270157814,\n",
       "   0.46452831718325616,\n",
       "   0.47274400290846824,\n",
       "   0.47608976206183434,\n",
       "   0.4707493006289005,\n",
       "   0.46580539804697035,\n",
       "   0.45470825970172885],\n",
       "  'train_loss_std': [0.15760631425230207,\n",
       "   0.13460466955083936,\n",
       "   0.15146414833100483,\n",
       "   0.14364851729528072,\n",
       "   0.1389922394975098,\n",
       "   0.1369571873499574,\n",
       "   0.14785146755809933,\n",
       "   0.13413648700154046,\n",
       "   0.14465316861115862,\n",
       "   0.15787905672436914,\n",
       "   0.13691639238397976,\n",
       "   0.14750697667014281,\n",
       "   0.13367896746429367,\n",
       "   0.14430841017623242,\n",
       "   0.1342308015198182,\n",
       "   0.14002720136409644,\n",
       "   0.1325191826057035,\n",
       "   0.13825487868786485,\n",
       "   0.14003931175731937,\n",
       "   0.1411454262578195,\n",
       "   0.13919954781913585,\n",
       "   0.1269709573564425,\n",
       "   0.13704759516908613,\n",
       "   0.13514228551529864,\n",
       "   0.12916899661670808,\n",
       "   0.23544735963598543,\n",
       "   0.13544957730247897,\n",
       "   0.1304847557673244,\n",
       "   0.13350003268784874,\n",
       "   0.14131885719601225,\n",
       "   0.13185041071234418,\n",
       "   0.13342135211783254,\n",
       "   0.14975434928835202,\n",
       "   0.13023210412548034,\n",
       "   0.13588161137608937,\n",
       "   0.13843272069531581,\n",
       "   0.12963593861839787,\n",
       "   0.13532709929398376,\n",
       "   0.13950879751689027,\n",
       "   0.13697043397909037,\n",
       "   0.13182726975343198,\n",
       "   0.1328639982920931,\n",
       "   0.1325888198522615,\n",
       "   0.13240833168487262,\n",
       "   0.13190938116976872,\n",
       "   0.13942539772334678,\n",
       "   0.23062122000698668,\n",
       "   0.132805798835033,\n",
       "   0.12853381337241354,\n",
       "   0.13191072003619783,\n",
       "   0.1318227394312389,\n",
       "   0.12297165568131242,\n",
       "   0.13406663164965715,\n",
       "   0.13314762755144663,\n",
       "   0.14043596239165218,\n",
       "   0.1342390529847867,\n",
       "   0.12841996427030639,\n",
       "   0.16978642186302176,\n",
       "   0.12825395997421235,\n",
       "   0.13082301298131938,\n",
       "   0.13471783292700518,\n",
       "   0.13596198244122065,\n",
       "   0.1380578753495257,\n",
       "   0.1321619185923792,\n",
       "   0.13451733482506162,\n",
       "   0.13150407458192687,\n",
       "   0.13160945331400967,\n",
       "   0.13680287754197146,\n",
       "   0.12327125635209571,\n",
       "   0.14938803565707603,\n",
       "   0.1194826546469831,\n",
       "   0.1255974547282668,\n",
       "   0.12104320335568049,\n",
       "   0.131712275865081,\n",
       "   0.1313008207101651,\n",
       "   0.12656590352170421,\n",
       "   0.13082851191154823,\n",
       "   0.12628226059671505,\n",
       "   0.1271287977403245,\n",
       "   0.13314781355187646,\n",
       "   0.12221343943221433,\n",
       "   0.13449480216469817,\n",
       "   0.1348814178028742,\n",
       "   0.1217728422082624,\n",
       "   0.13278561740513853,\n",
       "   0.12106947780984112,\n",
       "   0.1254040174696852,\n",
       "   0.12804199340265818,\n",
       "   0.12519335174240365,\n",
       "   0.12404039116505164,\n",
       "   0.1300050639144933,\n",
       "   0.12879062797434798,\n",
       "   0.12332243338846562,\n",
       "   0.12863197413845123,\n",
       "   0.13009441050199874,\n",
       "   0.1382745632889109,\n",
       "   0.11983601022911237,\n",
       "   0.12172995013749403,\n",
       "   0.11738596682182958],\n",
       "  'train_accuracy_mean': [0.4405066656470299,\n",
       "   0.5330399996638298,\n",
       "   0.5859599992036819,\n",
       "   0.6114799976348877,\n",
       "   0.6355066660046578,\n",
       "   0.6503333325386047,\n",
       "   0.663066666841507,\n",
       "   0.6769599989652634,\n",
       "   0.6815866663455963,\n",
       "   0.6705066667199134,\n",
       "   0.6989333317875862,\n",
       "   0.7032933332324028,\n",
       "   0.7124933324456215,\n",
       "   0.7215333334803581,\n",
       "   0.7214666662216187,\n",
       "   0.7288266673088074,\n",
       "   0.7338933345079423,\n",
       "   0.739773332953453,\n",
       "   0.7386799989938736,\n",
       "   0.7514533332586288,\n",
       "   0.7510133336782455,\n",
       "   0.7609733337163925,\n",
       "   0.7602666670084,\n",
       "   0.7594800000190735,\n",
       "   0.7638933342695237,\n",
       "   0.7116800007224083,\n",
       "   0.745373333454132,\n",
       "   0.7621866660118103,\n",
       "   0.7706000006198883,\n",
       "   0.765520000576973,\n",
       "   0.7755199990272522,\n",
       "   0.7757200000286102,\n",
       "   0.7648533329963684,\n",
       "   0.7821999988555908,\n",
       "   0.773053332567215,\n",
       "   0.7852133331298828,\n",
       "   0.7819466675519944,\n",
       "   0.781026666522026,\n",
       "   0.7850533329248428,\n",
       "   0.7894533336162567,\n",
       "   0.7866666666269302,\n",
       "   0.7905999997854233,\n",
       "   0.7859733345508575,\n",
       "   0.7872799997329711,\n",
       "   0.7917466661930084,\n",
       "   0.7920000003576279,\n",
       "   0.7050399999022484,\n",
       "   0.7812133324146271,\n",
       "   0.7907333332300186,\n",
       "   0.792519999742508,\n",
       "   0.7904933335781097,\n",
       "   0.8009066677093506,\n",
       "   0.7948133329153061,\n",
       "   0.8000533326864242,\n",
       "   0.790906665802002,\n",
       "   0.7937733331918716,\n",
       "   0.8007599996328354,\n",
       "   0.7723600000143052,\n",
       "   0.7890266660451889,\n",
       "   0.7943466671705246,\n",
       "   0.7935333335399628,\n",
       "   0.7980933334827424,\n",
       "   0.8011200007200241,\n",
       "   0.8042933332920075,\n",
       "   0.796706666469574,\n",
       "   0.8018400000333786,\n",
       "   0.8035466668605804,\n",
       "   0.8115333338975906,\n",
       "   0.8101599991321564,\n",
       "   0.7965199998617172,\n",
       "   0.8114133318662643,\n",
       "   0.8100133329629898,\n",
       "   0.8103466657400131,\n",
       "   0.8110266666412353,\n",
       "   0.8151599992513656,\n",
       "   0.8159200006723404,\n",
       "   0.8122000002861023,\n",
       "   0.8122800004482269,\n",
       "   0.8171066681146621,\n",
       "   0.8132799996137619,\n",
       "   0.8200266664028167,\n",
       "   0.8190666670799256,\n",
       "   0.818706667304039,\n",
       "   0.8197600010633469,\n",
       "   0.8213866665363312,\n",
       "   0.8198800004720688,\n",
       "   0.8217599998712539,\n",
       "   0.8220933334827423,\n",
       "   0.8257600013017654,\n",
       "   0.825973333120346,\n",
       "   0.8249733333587647,\n",
       "   0.8219866671562195,\n",
       "   0.8299200005531311,\n",
       "   0.8275333343744278,\n",
       "   0.8241199985742569,\n",
       "   0.8234666675329209,\n",
       "   0.826000000834465,\n",
       "   0.8268666652441025,\n",
       "   0.8301200000047684],\n",
       "  'train_accuracy_std': [0.07983043693360754,\n",
       "   0.06862265976066549,\n",
       "   0.0747567209000835,\n",
       "   0.07093932191495719,\n",
       "   0.06719117812037634,\n",
       "   0.06818650939483129,\n",
       "   0.06888586481646876,\n",
       "   0.06478822330166789,\n",
       "   0.06596475716465323,\n",
       "   0.07279933675869754,\n",
       "   0.06440924786187506,\n",
       "   0.06888120664240713,\n",
       "   0.063906224253305,\n",
       "   0.06783414414902081,\n",
       "   0.06351048770526453,\n",
       "   0.0640010505597935,\n",
       "   0.06411688889208492,\n",
       "   0.06302762667040834,\n",
       "   0.0653043449014169,\n",
       "   0.06488912820630388,\n",
       "   0.06194151111641609,\n",
       "   0.05930586695736313,\n",
       "   0.06381723920509595,\n",
       "   0.06000293157500874,\n",
       "   0.05954604047323673,\n",
       "   0.09700916415411907,\n",
       "   0.0626554467935191,\n",
       "   0.060205726776262826,\n",
       "   0.061404993565721926,\n",
       "   0.06462968555710999,\n",
       "   0.06104730838451907,\n",
       "   0.06146863186578526,\n",
       "   0.06505997861903173,\n",
       "   0.060817066079110575,\n",
       "   0.06091605496804217,\n",
       "   0.06389660944002858,\n",
       "   0.0574226962258441,\n",
       "   0.06132546491982719,\n",
       "   0.06303841440957585,\n",
       "   0.06313557768383013,\n",
       "   0.0596970149441467,\n",
       "   0.05849136681563204,\n",
       "   0.05933752062174577,\n",
       "   0.06019801859672452,\n",
       "   0.0590311995924288,\n",
       "   0.061486765399264315,\n",
       "   0.0994242235692022,\n",
       "   0.06150587321985237,\n",
       "   0.05968878521159052,\n",
       "   0.06007518172657642,\n",
       "   0.06051648935933851,\n",
       "   0.054670529738138345,\n",
       "   0.06096290980781924,\n",
       "   0.05957625771692975,\n",
       "   0.0641579659526408,\n",
       "   0.06006279738647803,\n",
       "   0.05861096029202174,\n",
       "   0.0728251136915521,\n",
       "   0.05920536203508506,\n",
       "   0.058663976734623174,\n",
       "   0.060235500547308425,\n",
       "   0.060988963103866416,\n",
       "   0.05876347182958441,\n",
       "   0.057434509371590205,\n",
       "   0.06022696594723091,\n",
       "   0.05722152982336496,\n",
       "   0.05851380839269789,\n",
       "   0.06061796562661331,\n",
       "   0.05353687137935982,\n",
       "   0.06538688862029382,\n",
       "   0.054157408020959655,\n",
       "   0.056318932330027195,\n",
       "   0.05424688286795283,\n",
       "   0.05717430757086448,\n",
       "   0.057649873123094816,\n",
       "   0.056780847811905705,\n",
       "   0.05706491639670968,\n",
       "   0.05539395825791732,\n",
       "   0.05604825315969658,\n",
       "   0.05840088937511512,\n",
       "   0.05419470562831422,\n",
       "   0.05953314853584953,\n",
       "   0.058674380452245614,\n",
       "   0.05506307710733761,\n",
       "   0.05747298380794343,\n",
       "   0.05182092023472708,\n",
       "   0.055124224924304153,\n",
       "   0.055467070522027796,\n",
       "   0.053823374465601995,\n",
       "   0.05228689543317499,\n",
       "   0.055437445949425485,\n",
       "   0.05771546923223388,\n",
       "   0.05368296183550681,\n",
       "   0.0561984980362622,\n",
       "   0.055066860865417476,\n",
       "   0.05980917994025287,\n",
       "   0.05104856364450462,\n",
       "   0.053488565588087164,\n",
       "   0.050749792705380416],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.3679926391442616,\n",
       "   1.2665635883808135,\n",
       "   1.187115044593811,\n",
       "   1.150152188539505,\n",
       "   1.1018906559546788,\n",
       "   1.103819659948349,\n",
       "   1.0518965985377629,\n",
       "   1.0638240402936936,\n",
       "   1.0246775339047114,\n",
       "   1.0404366920391719,\n",
       "   0.9956734057267507,\n",
       "   0.9974079296986262,\n",
       "   0.9795595763127009,\n",
       "   0.9822847622632981,\n",
       "   0.9686410011847814,\n",
       "   0.9727859516938527,\n",
       "   0.9796642202138901,\n",
       "   0.9395568052927653,\n",
       "   0.9345259749889374,\n",
       "   0.9407547829548518,\n",
       "   0.9327725533644359,\n",
       "   0.92769619901975,\n",
       "   0.9204771769046783,\n",
       "   0.9448835251728693,\n",
       "   0.9108933673302333,\n",
       "   1.0372760611772538,\n",
       "   0.9346874632438024,\n",
       "   0.9248945005734761,\n",
       "   0.9147397148609161,\n",
       "   0.9183905428647995,\n",
       "   0.9009467685222625,\n",
       "   0.9021474973360697,\n",
       "   0.9080566734075546,\n",
       "   0.9333889466524125,\n",
       "   0.9275669205188751,\n",
       "   0.9050271634260814,\n",
       "   0.926428096294403,\n",
       "   0.9209066055218379,\n",
       "   0.9143157025178273,\n",
       "   0.9049600623051326,\n",
       "   0.9200636585553487,\n",
       "   0.9034700530767441,\n",
       "   0.9263561564683914,\n",
       "   0.9148094246784846,\n",
       "   0.9136891599496205,\n",
       "   0.9303221921126048,\n",
       "   0.9509109348058701,\n",
       "   0.918716412782669,\n",
       "   0.9180389950672786,\n",
       "   0.9142436045408249,\n",
       "   0.9154155236482621,\n",
       "   0.912844978372256,\n",
       "   0.9149003094434738,\n",
       "   0.9255933437744777,\n",
       "   0.9092950361967087,\n",
       "   0.9149978530406951,\n",
       "   0.9086123383045197,\n",
       "   0.9526016773780187,\n",
       "   0.9112638721863429,\n",
       "   0.9188696434100468,\n",
       "   0.9226427108049393,\n",
       "   0.9266535631815592,\n",
       "   0.9162708167235056,\n",
       "   0.9089055428902308,\n",
       "   0.9085712881882986,\n",
       "   0.906133243838946,\n",
       "   0.9073711766799291,\n",
       "   0.9066981770594915,\n",
       "   0.9092002816994985,\n",
       "   0.9173020935058593,\n",
       "   0.9145328958829244,\n",
       "   0.910240751306216,\n",
       "   0.9013746041059494,\n",
       "   0.9056242448091507,\n",
       "   0.910866375764211,\n",
       "   0.9328299256165823,\n",
       "   0.915953133503596,\n",
       "   0.9131108146905899,\n",
       "   0.9050941954056422,\n",
       "   0.9124753997723262,\n",
       "   0.9091932680209478,\n",
       "   0.9005293403069178,\n",
       "   0.9159798089663188,\n",
       "   0.9014111695686976,\n",
       "   0.9044043095906575,\n",
       "   0.9150010774532954,\n",
       "   0.906214166879654,\n",
       "   0.9070675629377365,\n",
       "   0.9093568835655849,\n",
       "   0.9075004309415817,\n",
       "   0.8910978811979294,\n",
       "   0.9322220406929652,\n",
       "   0.9150334457556407,\n",
       "   0.916781660715739,\n",
       "   0.9202045565843582,\n",
       "   0.9247222755352656,\n",
       "   0.9162739940484365,\n",
       "   0.92395301481088,\n",
       "   0.9140932367245356],\n",
       "  'val_loss_std': [0.11062253486762083,\n",
       "   0.12608329885087627,\n",
       "   0.13131941896987437,\n",
       "   0.12630943183796767,\n",
       "   0.13536002487386312,\n",
       "   0.13685657709862972,\n",
       "   0.13878177838154124,\n",
       "   0.1383686870865279,\n",
       "   0.13720322655555808,\n",
       "   0.12860314528908715,\n",
       "   0.13815333407473412,\n",
       "   0.1421758131117974,\n",
       "   0.13386733018071642,\n",
       "   0.12966250406351876,\n",
       "   0.13357744954796044,\n",
       "   0.13349541100473403,\n",
       "   0.140908173021652,\n",
       "   0.1291422074901274,\n",
       "   0.13588970777468098,\n",
       "   0.1359635381114923,\n",
       "   0.1307441526091205,\n",
       "   0.13562906269947486,\n",
       "   0.12849118336466775,\n",
       "   0.13022329401405383,\n",
       "   0.13312905195577399,\n",
       "   0.13649817578369947,\n",
       "   0.13867301130289097,\n",
       "   0.14196561512850683,\n",
       "   0.13881678988797735,\n",
       "   0.13323774229912586,\n",
       "   0.13122538534586617,\n",
       "   0.13195144217630525,\n",
       "   0.13358238072944123,\n",
       "   0.13415383334812767,\n",
       "   0.13908654082221072,\n",
       "   0.13913592354338544,\n",
       "   0.1419006017656529,\n",
       "   0.13338726111796215,\n",
       "   0.127983224794475,\n",
       "   0.13657619077349314,\n",
       "   0.1337267671455435,\n",
       "   0.13576494334385047,\n",
       "   0.13480841403182175,\n",
       "   0.13959791513775216,\n",
       "   0.13455048243367818,\n",
       "   0.13895516274886816,\n",
       "   0.12765972479629564,\n",
       "   0.13244871181865622,\n",
       "   0.1384700298349217,\n",
       "   0.13553475226066147,\n",
       "   0.1321187246455792,\n",
       "   0.1349087152496928,\n",
       "   0.13273132806670207,\n",
       "   0.13119926739795806,\n",
       "   0.1308399592055531,\n",
       "   0.13082038301022048,\n",
       "   0.13619670907423428,\n",
       "   0.13080365290222037,\n",
       "   0.12991495994752314,\n",
       "   0.12928369431331915,\n",
       "   0.1313431692133802,\n",
       "   0.13269222598441546,\n",
       "   0.13535910695363415,\n",
       "   0.1311479739649875,\n",
       "   0.12970916182397046,\n",
       "   0.1298915017478646,\n",
       "   0.13029961477730842,\n",
       "   0.131087066579046,\n",
       "   0.1312596144226387,\n",
       "   0.13528126262760287,\n",
       "   0.13918127209012918,\n",
       "   0.13084975389907816,\n",
       "   0.1316753818475644,\n",
       "   0.1333483053487216,\n",
       "   0.135189479181418,\n",
       "   0.1379468648129042,\n",
       "   0.13711011185884397,\n",
       "   0.13607647717859778,\n",
       "   0.13891017475569256,\n",
       "   0.13336937330262763,\n",
       "   0.13614209821511872,\n",
       "   0.1383109894248505,\n",
       "   0.13990288098899295,\n",
       "   0.1389713895727852,\n",
       "   0.13523753270226518,\n",
       "   0.14538887695848646,\n",
       "   0.13556030651804227,\n",
       "   0.13572404193512486,\n",
       "   0.13647022773819134,\n",
       "   0.13807415748238314,\n",
       "   0.13605056056990947,\n",
       "   0.1455153016717191,\n",
       "   0.13680853300468027,\n",
       "   0.13582615283397945,\n",
       "   0.13537685480300057,\n",
       "   0.13958782437365516,\n",
       "   0.13426996273133024,\n",
       "   0.14062181746215174,\n",
       "   0.1415112652049066],\n",
       "  'val_accuracy_mean': [0.436266667842865,\n",
       "   0.49455555498600007,\n",
       "   0.5277555547157924,\n",
       "   0.5476666645208994,\n",
       "   0.5709555558363597,\n",
       "   0.5674444430073102,\n",
       "   0.5898444437980652,\n",
       "   0.5863333318630854,\n",
       "   0.6030444432298342,\n",
       "   0.5944888876875242,\n",
       "   0.6162444437543552,\n",
       "   0.613377777338028,\n",
       "   0.6197333340843518,\n",
       "   0.6195333326856295,\n",
       "   0.6224888883034388,\n",
       "   0.6216222226619721,\n",
       "   0.6227999993165334,\n",
       "   0.6370888885855674,\n",
       "   0.6362888886531194,\n",
       "   0.6372666656970978,\n",
       "   0.6399555552005768,\n",
       "   0.6410222221414248,\n",
       "   0.644288886586825,\n",
       "   0.6377555559078852,\n",
       "   0.6452888888120651,\n",
       "   0.5950888871153196,\n",
       "   0.6365999994675319,\n",
       "   0.6416888880729675,\n",
       "   0.6449777792890866,\n",
       "   0.6430666666229566,\n",
       "   0.6530444432298342,\n",
       "   0.6542444431781769,\n",
       "   0.6511555569370587,\n",
       "   0.6407333326339721,\n",
       "   0.642533330321312,\n",
       "   0.6517777766784032,\n",
       "   0.6462222211559614,\n",
       "   0.6476222203175227,\n",
       "   0.648599999944369,\n",
       "   0.6511555536588033,\n",
       "   0.6464666668574015,\n",
       "   0.6494888881842296,\n",
       "   0.643911110063394,\n",
       "   0.6507555549343427,\n",
       "   0.6467555563648542,\n",
       "   0.642244443098704,\n",
       "   0.6322666662931442,\n",
       "   0.646933331489563,\n",
       "   0.6479777769247691,\n",
       "   0.6480888870358467,\n",
       "   0.6460444438457489,\n",
       "   0.6480222209294637,\n",
       "   0.6481555548310279,\n",
       "   0.6429333325227101,\n",
       "   0.6502444432179133,\n",
       "   0.645333331724008,\n",
       "   0.6484666667381922,\n",
       "   0.6304222224156062,\n",
       "   0.6478444445133209,\n",
       "   0.6431333334247271,\n",
       "   0.6401777778069179,\n",
       "   0.6399999991059303,\n",
       "   0.644044444958369,\n",
       "   0.6459555547436079,\n",
       "   0.6494444440801939,\n",
       "   0.6477333321173986,\n",
       "   0.6481555543343226,\n",
       "   0.6468444443742434,\n",
       "   0.6466444428761801,\n",
       "   0.6424666671951612,\n",
       "   0.6446444436907768,\n",
       "   0.6450444427132607,\n",
       "   0.6498222217957179,\n",
       "   0.6472888892889023,\n",
       "   0.6438000010450681,\n",
       "   0.6373777782917023,\n",
       "   0.6438666646679242,\n",
       "   0.6466444430748621,\n",
       "   0.6507111120224,\n",
       "   0.6458000002304712,\n",
       "   0.6439555564522743,\n",
       "   0.6485333334406217,\n",
       "   0.6456444429357847,\n",
       "   0.6492666656772296,\n",
       "   0.6483777778347334,\n",
       "   0.6475777763128281,\n",
       "   0.645399997929732,\n",
       "   0.6480666647354761,\n",
       "   0.646222222050031,\n",
       "   0.6468444452683131,\n",
       "   0.6521555537978808,\n",
       "   0.6405999996264775,\n",
       "   0.6459111107389132,\n",
       "   0.6416666658719381,\n",
       "   0.6448666661977768,\n",
       "   0.6415555539727211,\n",
       "   0.6441555558641752,\n",
       "   0.6400222218036652,\n",
       "   0.643999999264876],\n",
       "  'val_accuracy_std': [0.060430455035726444,\n",
       "   0.06388289577098315,\n",
       "   0.06325842895999484,\n",
       "   0.06166366377455664,\n",
       "   0.0643978786712513,\n",
       "   0.06507763534319382,\n",
       "   0.06523443581669586,\n",
       "   0.0633815602184962,\n",
       "   0.06273806378523672,\n",
       "   0.060448920077195996,\n",
       "   0.06511050132917262,\n",
       "   0.0640115579957208,\n",
       "   0.06353245844644154,\n",
       "   0.06397862094152808,\n",
       "   0.06271178687947661,\n",
       "   0.06313446725762836,\n",
       "   0.06258514840523967,\n",
       "   0.0602043400579989,\n",
       "   0.06269271145081542,\n",
       "   0.061577497179440284,\n",
       "   0.06213961251578736,\n",
       "   0.06073738033016318,\n",
       "   0.05961958065011319,\n",
       "   0.05980337996898544,\n",
       "   0.059793703022176195,\n",
       "   0.0648898737064427,\n",
       "   0.06232646682088664,\n",
       "   0.06259422098077273,\n",
       "   0.0636759706130973,\n",
       "   0.06084165312811488,\n",
       "   0.05936508282159393,\n",
       "   0.06258315560253769,\n",
       "   0.06396293882999067,\n",
       "   0.06263518181630784,\n",
       "   0.06485617196315363,\n",
       "   0.06400193031610968,\n",
       "   0.06337971031823496,\n",
       "   0.062064286897766795,\n",
       "   0.0626640323884416,\n",
       "   0.06250568742802146,\n",
       "   0.06050984723414645,\n",
       "   0.06274373232613027,\n",
       "   0.06161619166925887,\n",
       "   0.0625875945980386,\n",
       "   0.06368899826319573,\n",
       "   0.060030852729306534,\n",
       "   0.06252977962714344,\n",
       "   0.06176601138548991,\n",
       "   0.06497794240377959,\n",
       "   0.0630582890420689,\n",
       "   0.06293251842764937,\n",
       "   0.06297159570381898,\n",
       "   0.06329944925723086,\n",
       "   0.06271147455473025,\n",
       "   0.0609220485960712,\n",
       "   0.06153830744291439,\n",
       "   0.06118476232659142,\n",
       "   0.060415593950262254,\n",
       "   0.06276544990866105,\n",
       "   0.061666642521865614,\n",
       "   0.06378411207250102,\n",
       "   0.0629497157791985,\n",
       "   0.06129417489870988,\n",
       "   0.06295040533254995,\n",
       "   0.0599690246182491,\n",
       "   0.0619633352324562,\n",
       "   0.061144682873389755,\n",
       "   0.060777414336705936,\n",
       "   0.061705390056926025,\n",
       "   0.060935827163214135,\n",
       "   0.06496426149654448,\n",
       "   0.06176327214370804,\n",
       "   0.06106073129405666,\n",
       "   0.06272029405870332,\n",
       "   0.06102765713501566,\n",
       "   0.061093234639174154,\n",
       "   0.06612119260389519,\n",
       "   0.06026484516730635,\n",
       "   0.060162233384282136,\n",
       "   0.06192460301335438,\n",
       "   0.059938718517751115,\n",
       "   0.0620662489249148,\n",
       "   0.06381332430152599,\n",
       "   0.06279345365243046,\n",
       "   0.06034866347697681,\n",
       "   0.06419345378936796,\n",
       "   0.06286984895973628,\n",
       "   0.06150431658399701,\n",
       "   0.06139690806647436,\n",
       "   0.06315388472536,\n",
       "   0.06218686213116309,\n",
       "   0.06247556038938242,\n",
       "   0.06198225466309179,\n",
       "   0.06360089845907092,\n",
       "   0.06046134919263145,\n",
       "   0.06224642468596385,\n",
       "   0.061425580603436905,\n",
       "   0.06280009203710868,\n",
       "   0.061299508423496414],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = 31\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key, value)\n",
    "# print(\"=\"*10)\n",
    "# print(\"names_weights_copy == \",names_weights_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "170a7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = maml_system.data.get_test_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "for sample_idx, test_sample in enumerate(test_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = test_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        # Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "        num_steps=5\n",
    "        for num_step in range(num_steps):            \n",
    "            support_loss, support_preds, out_feature_dict  = maml_system.model.net_forward(\n",
    "                    x=x_support_set_task,\n",
    "                    y=y_support_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    backup_running_statistics=num_step == 0,\n",
    "                    training=True,\n",
    "                    num_step=num_step,\n",
    "                )\n",
    "        \n",
    "            generated_alpha_params = {}\n",
    "\n",
    "            if maml_system.model.args.arbiter:\n",
    "                support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                        retain_graph=True)\n",
    "\n",
    "                names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "                per_step_task_embedding = []\n",
    "\n",
    "                for key, weight in names_weights_copy.items():\n",
    "                    weight_norm = torch.norm(weight, p=2)\n",
    "                    per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "                for key, grad in names_grads_copy.items():\n",
    "                    gradient_l2norm = torch.norm(grad, p=2)\n",
    "                    per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "                per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "                per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                            per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "                generated_gradient_rate = maml_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "                g = 0\n",
    "                for key in names_weights_copy.keys():\n",
    "                    generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "                    g += 1\n",
    "\n",
    "            names_weights_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                              names_weights_copy=names_weights_copy,\n",
    "                                                              out_feature_dict=out_feature_dict,\n",
    "                                                              alpha=generated_alpha_params,\n",
    "                                                              use_second_order=args.second_order,\n",
    "                                                              current_step_idx=num_step,\n",
    "                                                              current_iter=maml_system.state['current_iter'],\n",
    "                                                              training_phase='test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
