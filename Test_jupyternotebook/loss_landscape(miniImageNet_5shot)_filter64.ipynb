{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16249129",
   "metadata": {},
   "source": [
    "## [참고]\n",
    "### https://cocoa-t.tistory.com/entry/PyHessian-Loss-Landscape-%EC%8B%9C%EA%B0%81%ED%99%94-PyHessian-Neural-Networks-Through-the-Lens-of-the-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af476",
   "metadata": {},
   "source": [
    "# 0. Dataset 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7235fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"mini_imagenet_full_size\"\n",
    "# dataset=\"tiered_imagenet\"\n",
    "# dataset=\"CIFAR_FS\"\n",
    "# dataset=\"CUB\"\n",
    "\n",
    "title = 'miniImageNet'\n",
    "# title = 'tieredImageNet'\n",
    "# title = 'CIFAR-FS'\n",
    "# title = 'CUB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005193c",
   "metadata": {},
   "source": [
    "# 1. MAML 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f0d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_maml = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_5way_5shot_filter64\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":64,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_maml.im_shape = (2, 3, args_maml.image_height, args_maml.image_width)\n",
    "\n",
    "args_maml.use_cuda = torch.cuda.is_available()\n",
    "args_maml.seed = 104\n",
    "args_maml.reverse_channels=False\n",
    "args_maml.labels_as_int=False\n",
    "args_maml.reset_stored_filepaths=False\n",
    "args_maml.num_of_gpus=1\n",
    "\n",
    "args_maml.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9052a",
   "metadata": {},
   "source": [
    "## 2. Arbiter 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_arbiter = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot_filter64\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 150,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":64,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_arbiter.im_shape = (2, 3, args_arbiter.image_height, args_arbiter.image_width)\n",
    "\n",
    "args_arbiter.use_cuda = torch.cuda.is_available()\n",
    "args_arbiter.seed = 104\n",
    "args_arbiter.reverse_channels=False\n",
    "args_arbiter.labels_as_int=False\n",
    "args_arbiter.reset_stored_filepaths=False\n",
    "args_arbiter.num_of_gpus=1\n",
    "\n",
    "args_arbiter.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f7d8",
   "metadata": {},
   "source": [
    "## 3. Model 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803156ee",
   "metadata": {},
   "source": [
    "### 3.1. MAML Model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f85286c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_5way_5shot_filter64\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_maml = MAMLFewShotClassifier(args=args_maml, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_maml.image_height, args_maml.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model_maml, data=data, args=args_maml, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3acf",
   "metadata": {},
   "source": [
    "### 3.2.  Arbiter 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25651dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot_filter64\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_arbiter = MAMLFewShotClassifier(args=args_arbiter, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_arbiter.image_height, args_arbiter.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "arbiter_system = ExperimentBuilder(model=model_arbiter, data=data, args=args_arbiter, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a2ff6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6116666648785273,\n",
       " 'best_val_iter': 45500,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 91,\n",
       " 'train_loss_mean': 0.5506827815175056,\n",
       " 'train_loss_std': 0.1347330680555223,\n",
       " 'train_accuracy_mean': 0.7948933341503144,\n",
       " 'train_accuracy_std': 0.05953420825581363,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 1.0046450330813725,\n",
       " 'val_loss_std': 0.1571665716406741,\n",
       " 'val_accuracy_mean': 0.6093777750929197,\n",
       " 'val_accuracy_std': 0.0643605435900373,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.1425e-01, -4.4933e-01,  1.7018e-01],\n",
       "                         [-1.7254e-01, -2.7310e-02,  9.3902e-02],\n",
       "                         [-1.1717e-01,  4.5088e-01, -4.0048e-02]],\n",
       "               \n",
       "                        [[ 2.4871e-01, -3.8809e-01,  2.3791e-01],\n",
       "                         [-1.0773e-01,  5.8160e-02,  1.5152e-01],\n",
       "                         [-1.9190e-01,  2.8830e-01, -1.5601e-01]],\n",
       "               \n",
       "                        [[ 3.4503e-01, -2.4130e-01,  1.1107e-02],\n",
       "                         [ 5.8856e-02,  1.1183e-01, -1.6049e-01],\n",
       "                         [-1.7040e-01,  1.9904e-01, -2.8729e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.5724e-01,  3.5376e-01,  2.6145e-01],\n",
       "                         [ 2.2268e-01, -2.9287e-01, -4.0536e-02],\n",
       "                         [-2.4458e-01, -3.9195e-01, -2.7559e-01]],\n",
       "               \n",
       "                        [[-1.5052e-01, -1.0293e-01, -1.1279e-01],\n",
       "                         [ 1.3595e-01, -2.1226e-01, -8.7453e-02],\n",
       "                         [ 1.9349e-01,  3.9514e-02,  2.1235e-01]],\n",
       "               \n",
       "                        [[-2.9972e-01, -8.9993e-03, -1.9661e-01],\n",
       "                         [-1.0305e-01,  1.5704e-01,  1.7131e-01],\n",
       "                         [ 6.2591e-03,  2.5766e-01,  8.3135e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.5220e-01, -8.7272e-02,  3.6978e-01],\n",
       "                         [-1.5598e-01, -3.5780e-01, -2.0308e-01],\n",
       "                         [-5.8473e-02,  5.9604e-02, -2.4067e-01]],\n",
       "               \n",
       "                        [[-6.0130e-02, -2.1631e-01,  1.1681e-02],\n",
       "                         [ 1.8765e-01, -2.9733e-01, -2.9815e-01],\n",
       "                         [ 1.0875e-01,  1.0133e-01,  7.0407e-03]],\n",
       "               \n",
       "                        [[ 1.9262e-01, -1.5284e-01,  1.1929e-01],\n",
       "                         [ 2.3647e-01, -3.4826e-01, -3.8164e-01],\n",
       "                         [ 2.1311e-01,  8.4489e-02, -7.2797e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-8.4951e-03, -1.5571e-01, -1.9053e-01],\n",
       "                         [ 6.1550e-02,  3.8455e-01,  4.6697e-01],\n",
       "                         [-8.5795e-02, -2.0821e-01, -2.6607e-01]],\n",
       "               \n",
       "                        [[-1.0180e-01, -2.6297e-01, -2.6089e-01],\n",
       "                         [ 1.9029e-01,  3.1206e-01,  4.0120e-01],\n",
       "                         [ 5.5657e-03, -1.0900e-01, -1.8492e-01]],\n",
       "               \n",
       "                        [[ 4.2985e-02, -7.1504e-02, -5.0023e-02],\n",
       "                         [-1.5526e-02,  1.8296e-01,  1.3825e-01],\n",
       "                         [-6.2402e-02, -9.4225e-02, -6.3561e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.0798e-01,  1.0266e-01, -2.5602e-01],\n",
       "                         [ 2.3282e-01,  4.3741e-01,  3.7466e-02],\n",
       "                         [-4.5110e-01,  2.6200e-01, -6.6773e-02]],\n",
       "               \n",
       "                        [[-2.8795e-01, -5.8905e-02, -7.1594e-02],\n",
       "                         [ 4.0443e-01,  1.5080e-01,  2.2495e-02],\n",
       "                         [-4.3889e-01,  1.3316e-01, -1.2865e-01]],\n",
       "               \n",
       "                        [[-7.4229e-02, -3.3152e-02,  1.6747e-04],\n",
       "                         [ 3.1752e-01, -8.1946e-02,  1.3254e-01],\n",
       "                         [-8.4182e-02, -9.3233e-02, -8.5916e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.3777e-02,  2.0849e-01,  3.1493e-01],\n",
       "                         [-2.8116e-01,  1.9551e-01,  1.1700e-01],\n",
       "                         [-7.7545e-02,  5.5102e-02, -1.8253e-01]],\n",
       "               \n",
       "                        [[ 3.3258e-01, -1.3814e-03, -8.9905e-02],\n",
       "                         [ 1.7441e-01, -2.3459e-01, -2.7387e-01],\n",
       "                         [-7.7950e-02, -2.8888e-01, -1.0685e-01]],\n",
       "               \n",
       "                        [[-2.2439e-01, -2.8082e-01, -2.6374e-01],\n",
       "                         [ 1.8899e-01,  1.0728e-01,  2.1259e-01],\n",
       "                         [ 9.9554e-02,  1.6593e-01,  2.4591e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-2.1974e-01, -1.0048e-02,  5.4799e-02, -1.6550e-01,  4.5056e-02,\n",
       "                        4.6803e-02, -4.2921e-02, -4.6358e-02, -7.3666e-03, -1.0152e-02,\n",
       "                       -3.3494e-02,  3.7006e-02, -5.0691e-02, -4.0396e-02, -1.9255e-01,\n",
       "                       -3.0390e-02,  1.3884e-01, -7.9025e-03,  3.2168e-02, -3.7331e-03,\n",
       "                        2.3686e-02,  2.3907e-03,  1.7946e-02, -1.6191e-02, -1.1648e-01,\n",
       "                        1.5137e-02,  3.0031e-02,  5.5802e-02,  3.0414e-02,  9.4580e-02,\n",
       "                        8.5888e-03, -3.5225e-03,  2.7190e-03,  1.4011e-02, -1.0153e-01,\n",
       "                        8.7998e-03,  1.8984e-02, -7.8792e-02, -6.4151e-03,  3.4397e-02,\n",
       "                       -2.4677e-01, -2.7835e-01,  4.7902e-02, -6.6826e-03, -7.2411e-02,\n",
       "                       -9.7357e-02,  8.7957e-02,  1.0613e-03,  6.6394e-02, -3.0103e-02,\n",
       "                       -8.2059e-02,  1.7308e-01, -2.3401e-01,  1.8221e-04,  8.4949e-02,\n",
       "                        9.2546e-03, -5.6294e-02, -5.7813e-02, -7.6219e-02,  1.3246e-02,\n",
       "                        6.3603e-02,  2.2194e-02,  8.0497e-02,  1.1162e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.0768, -0.2647, -0.5560, -0.1482, -0.1001, -0.5294, -0.5443,  0.5139,\n",
       "                       -0.7352, -0.5212, -0.2146,  0.3618, -0.0911, -0.4789, -0.2472, -0.0073,\n",
       "                        0.1666, -0.6075, -0.8713, -0.0098, -0.8245, -0.5607, -0.4030, -0.8242,\n",
       "                       -0.6997, -0.4335, -0.3676, -0.5829, -0.1611, -0.0993,  0.3263, -0.2027,\n",
       "                       -0.0942, -0.5569, -0.0435, -0.6011, -0.2119,  0.3112, -0.4785, -0.6165,\n",
       "                       -0.1936,  1.5433, -0.3875,  0.3764,  0.1528, -0.2944, -0.5504, -0.3287,\n",
       "                       -0.2695, -0.6077, -0.7200,  0.0224,  0.9720, -0.3319, -0.6574, -0.7891,\n",
       "                        0.3938,  0.4673,  0.1754, -0.0224, -0.0829,  0.1660, -0.1263, -0.8972],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.1060, 1.0835, 0.5497, 1.6271, 1.0290, 0.6296, 0.5750, 1.5240, 0.6860,\n",
       "                       0.6148, 1.1041, 1.0063, 0.9291, 0.8256, 1.6631, 0.8922, 1.0038, 1.0168,\n",
       "                       0.9262, 0.6544, 0.9209, 0.5810, 1.0364, 0.6603, 0.8955, 0.6233, 0.6575,\n",
       "                       0.9348, 0.5648, 0.7168, 0.8408, 0.5489, 1.7692, 0.5362, 0.5860, 0.7129,\n",
       "                       0.6302, 0.8030, 0.7014, 1.2564, 1.3981, 1.2509, 0.9444, 1.0753, 0.9766,\n",
       "                       1.0994, 0.9111, 0.6100, 0.9571, 0.9412, 0.9195, 1.0522, 0.9301, 0.6162,\n",
       "                       0.9221, 0.6261, 0.9075, 1.1481, 1.0851, 0.9191, 1.3969, 1.3580, 0.7608,\n",
       "                       1.0898], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-0.0552, -0.0518, -0.0169],\n",
       "                         [-0.1781, -0.2258, -0.0171],\n",
       "                         [ 0.0013, -0.0646,  0.2396]],\n",
       "               \n",
       "                        [[ 0.1803,  0.0029, -0.1969],\n",
       "                         [-0.0629, -0.2176, -0.1148],\n",
       "                         [ 0.1275,  0.2232,  0.4426]],\n",
       "               \n",
       "                        [[-0.1052, -0.1239, -0.2432],\n",
       "                         [ 0.0550, -0.0748, -0.1509],\n",
       "                         [-0.1113, -0.0903, -0.1133]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0363, -0.2232, -0.1212],\n",
       "                         [-0.4827, -0.3564,  0.2465],\n",
       "                         [-0.1428, -0.1749,  0.3656]],\n",
       "               \n",
       "                        [[ 0.1821, -0.0083, -0.0814],\n",
       "                         [ 0.1046,  0.0648, -0.0229],\n",
       "                         [ 0.1235,  0.0611, -0.0104]],\n",
       "               \n",
       "                        [[-0.4950, -0.3531, -0.3406],\n",
       "                         [-0.3248, -0.0720, -0.0386],\n",
       "                         [ 0.2774,  0.5066,  0.3938]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0398,  0.2929, -0.0359],\n",
       "                         [ 0.0497, -0.0617, -0.1706],\n",
       "                         [ 0.0607, -0.0114,  0.2030]],\n",
       "               \n",
       "                        [[ 0.1720,  0.0889,  0.3912],\n",
       "                         [-0.3584, -0.3514, -0.4373],\n",
       "                         [-0.1198,  0.2985,  0.3948]],\n",
       "               \n",
       "                        [[ 0.3818,  0.4086,  0.4432],\n",
       "                         [ 0.2988,  0.2066,  0.3699],\n",
       "                         [ 0.1554,  0.2125,  0.3941]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.3745, -0.2107, -0.2418],\n",
       "                         [ 0.0668,  0.0723, -0.1193],\n",
       "                         [ 0.1658,  0.1627,  0.1326]],\n",
       "               \n",
       "                        [[-0.2157,  0.1014,  0.1957],\n",
       "                         [-0.3181,  0.0211,  0.0886],\n",
       "                         [-0.2779,  0.0960,  0.3412]],\n",
       "               \n",
       "                        [[-0.2660,  0.0486,  0.1427],\n",
       "                         [-0.7072, -0.5646,  0.1724],\n",
       "                         [ 0.0995, -0.0541,  0.3805]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0232, -0.2219, -0.1077],\n",
       "                         [-0.1118, -0.2023,  0.1575],\n",
       "                         [-0.2803, -0.1475,  0.1692]],\n",
       "               \n",
       "                        [[ 0.0268, -0.0475, -0.1938],\n",
       "                         [ 0.0765,  0.0412, -0.0289],\n",
       "                         [ 0.1761,  0.1244,  0.0261]],\n",
       "               \n",
       "                        [[-0.1348, -0.3150, -0.4037],\n",
       "                         [-0.4065, -0.2704, -0.1299],\n",
       "                         [-0.2877, -0.1691, -0.0063]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0530, -0.1342, -0.1449],\n",
       "                         [-0.2913, -0.2642, -0.3326],\n",
       "                         [-0.1452, -0.2391, -0.2708]],\n",
       "               \n",
       "                        [[-0.5667, -0.6442, -0.3987],\n",
       "                         [-0.3698, -0.7739, -0.5180],\n",
       "                         [-0.2811, -0.4799, -0.3650]],\n",
       "               \n",
       "                        [[ 0.0851,  0.2213, -0.0459],\n",
       "                         [-0.0345,  0.0758, -0.0379],\n",
       "                         [ 0.1332, -0.0888, -0.1837]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0603,  0.0942,  0.1174],\n",
       "                         [ 0.3295, -0.0303,  0.2227],\n",
       "                         [ 0.0760, -0.1590, -0.0065]],\n",
       "               \n",
       "                        [[ 0.0974, -0.1822, -0.2166],\n",
       "                         [ 0.0057, -0.1355, -0.1014],\n",
       "                         [ 0.0107, -0.0575,  0.3266]],\n",
       "               \n",
       "                        [[-0.0316, -0.0129,  0.0799],\n",
       "                         [ 0.0446, -0.1721,  0.0043],\n",
       "                         [ 0.1083, -0.1404, -0.1183]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1889,  0.0471,  0.2256],\n",
       "                         [ 0.0098, -0.1343, -0.0266],\n",
       "                         [ 0.0194,  0.0791,  0.0756]],\n",
       "               \n",
       "                        [[-0.1381,  0.0616,  0.0579],\n",
       "                         [ 0.1207,  0.0075, -0.0396],\n",
       "                         [ 0.1610,  0.2292,  0.0773]],\n",
       "               \n",
       "                        [[-0.8341, -1.0011, -0.0856],\n",
       "                         [-0.3181, -0.6751, -0.2187],\n",
       "                         [ 0.0054, -0.0658,  0.2669]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0481,  0.2880,  0.0227],\n",
       "                         [ 0.1393,  0.3853, -0.1632],\n",
       "                         [ 0.1718, -0.0191, -0.2305]],\n",
       "               \n",
       "                        [[ 0.2171, -0.0119,  0.0777],\n",
       "                         [ 0.1122, -0.0118, -0.3004],\n",
       "                         [ 0.2159, -0.1331, -0.2630]],\n",
       "               \n",
       "                        [[ 0.1100,  0.2175,  0.2207],\n",
       "                         [-0.0384,  0.2904,  0.2453],\n",
       "                         [ 0.1433,  0.2702,  0.2972]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0672, -0.3800, -0.2988],\n",
       "                         [-0.1442, -0.2913, -0.1250],\n",
       "                         [-0.0353, -0.2721, -0.3683]],\n",
       "               \n",
       "                        [[-0.1636, -0.2470, -0.2344],\n",
       "                         [-0.3119, -0.2328, -0.1382],\n",
       "                         [-0.2240, -0.1746, -0.0839]],\n",
       "               \n",
       "                        [[ 0.1797, -0.0133, -0.0865],\n",
       "                         [ 0.1685, -0.1825,  0.1089],\n",
       "                         [ 0.2382,  0.0653,  0.1454]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1568,  0.0459, -0.2911],\n",
       "                         [-0.1245,  0.3810,  0.0741],\n",
       "                         [ 0.0992, -0.0885, -0.0811]],\n",
       "               \n",
       "                        [[ 0.0303, -0.0527, -0.1356],\n",
       "                         [ 0.0211,  0.2111, -0.0050],\n",
       "                         [-0.1571,  0.0879,  0.0663]],\n",
       "               \n",
       "                        [[-0.1415,  0.1957,  0.4150],\n",
       "                         [ 0.0401, -0.0891, -0.0463],\n",
       "                         [-0.1121, -0.1589, -0.2461]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.4485, -0.1227,  0.0774],\n",
       "                         [ 0.2247, -0.0581,  0.2494],\n",
       "                         [ 0.1815,  0.0386,  0.2631]],\n",
       "               \n",
       "                        [[ 0.0708, -0.0612, -0.1494],\n",
       "                         [-0.4773, -0.3249,  0.1699],\n",
       "                         [-0.2335, -0.1831, -0.1130]],\n",
       "               \n",
       "                        [[ 0.2165,  0.1236, -0.0844],\n",
       "                         [ 0.4394,  0.1509, -0.1244],\n",
       "                         [ 0.0283, -0.1923,  0.2237]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-0.0087, -0.0092,  0.0071,  0.0222, -0.0091, -0.0087, -0.0070, -0.0127,\n",
       "                        0.0080,  0.0183, -0.0104, -0.0016,  0.0342,  0.0221, -0.0132,  0.0103,\n",
       "                       -0.0004,  0.0005, -0.0046,  0.0048, -0.0297, -0.0077, -0.0106, -0.0171,\n",
       "                       -0.0163,  0.0102, -0.0218, -0.0176,  0.0129, -0.0115,  0.0075,  0.0166,\n",
       "                        0.0260,  0.0119,  0.0018, -0.0074,  0.0103,  0.0019, -0.0238, -0.0043,\n",
       "                        0.0044,  0.0177,  0.0220, -0.0128, -0.0046, -0.0046,  0.0065,  0.0227,\n",
       "                       -0.0189, -0.0592,  0.0047,  0.0046, -0.0014,  0.0279,  0.0152, -0.0207,\n",
       "                       -0.0181,  0.0128,  0.0132,  0.0062,  0.0071,  0.0219,  0.0362,  0.0067],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.7223, -0.5630, -0.5199, -0.6213, -0.5990, -0.5255, -0.6143, -0.3874,\n",
       "                       -0.4352, -0.5472, -0.4839, -0.2642, -0.6268, -0.5430, -0.4818, -0.1934,\n",
       "                       -0.3193, -0.3709, -0.5040, -0.4137, -0.6651, -0.3381, -0.3062, -0.6898,\n",
       "                       -0.5156, -0.3250, -0.5559, -0.4577, -0.5768, -0.3916, -0.7649, -0.6429,\n",
       "                       -0.4531, -0.7730, -0.3959, -0.5099, -0.2668, -0.5467, -0.6896, -1.0173,\n",
       "                       -0.4411, -0.4857, -0.4778, -0.7194, -0.4136, -0.3690, -0.4334, -0.5637,\n",
       "                       -0.6931, -0.9836, -0.8214, -0.3821, -0.4799, -0.5513, -0.4797, -0.5274,\n",
       "                       -0.5071, -0.5073, -0.5482, -0.4414, -0.6790, -0.4656, -0.5285, -0.8491],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.0377, 0.7067, 0.8707, 1.1156, 1.1022, 1.1092, 1.0487, 1.0471, 0.7609,\n",
       "                       0.7529, 0.7082, 1.2359, 0.8646, 0.8491, 0.9485, 0.8643, 0.9119, 1.0798,\n",
       "                       0.8896, 0.9068, 1.0101, 0.9584, 0.8229, 1.0330, 0.8362, 1.0023, 0.8208,\n",
       "                       0.8197, 0.7373, 1.0089, 0.7694, 0.9002, 0.9060, 0.9308, 0.7601, 0.7411,\n",
       "                       1.0108, 1.1356, 0.8865, 1.2111, 0.9471, 0.8133, 0.6516, 1.0893, 0.8124,\n",
       "                       0.6728, 0.7589, 0.8695, 0.9530, 1.4721, 1.1368, 0.7226, 0.9697, 0.7728,\n",
       "                       0.9932, 1.1010, 1.0558, 0.9981, 0.8223, 0.8246, 1.1807, 0.9223, 0.8480,\n",
       "                       0.9001], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-8.8947e-02, -9.6673e-02, -3.9948e-03],\n",
       "                         [-1.5530e-02, -1.1609e-01, -9.4092e-02],\n",
       "                         [ 5.6432e-02, -9.1169e-02,  9.1794e-02]],\n",
       "               \n",
       "                        [[-1.2121e-01, -2.2285e-01, -1.0858e-01],\n",
       "                         [-2.6728e-01, -2.6124e-02,  1.9724e-01],\n",
       "                         [ 5.2824e-02, -7.1375e-02, -8.0869e-02]],\n",
       "               \n",
       "                        [[-1.1086e-03, -3.0246e-02,  1.4816e-01],\n",
       "                         [ 6.0217e-02, -1.4101e-01,  5.9081e-02],\n",
       "                         [ 1.2289e-01, -1.4247e-01,  1.1244e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.8901e-01,  6.8924e-02,  1.0366e-01],\n",
       "                         [-7.8722e-02, -3.6566e-01, -3.8332e-01],\n",
       "                         [ 9.4160e-02, -7.4724e-02, -1.3604e-01]],\n",
       "               \n",
       "                        [[-3.1832e-01, -2.2688e-01,  2.7675e-02],\n",
       "                         [-3.4762e-01, -2.4889e-01,  2.8266e-01],\n",
       "                         [-7.2806e-03,  1.3155e-01,  7.8612e-02]],\n",
       "               \n",
       "                        [[ 2.7913e-01,  2.8868e-01,  2.6588e-01],\n",
       "                         [ 2.5647e-01,  8.9225e-02,  2.4742e-01],\n",
       "                         [ 2.9269e-02,  2.7606e-02,  2.8122e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1088e-01, -7.3156e-01, -1.0811e+00],\n",
       "                         [ 4.5897e-01, -4.2384e-01, -7.5408e-01],\n",
       "                         [ 1.4177e-01, -4.4486e-01, -7.0054e-01]],\n",
       "               \n",
       "                        [[ 9.9812e-02, -2.3903e-01, -1.3354e-01],\n",
       "                         [-1.7921e-01, -2.8784e-01, -2.5782e-01],\n",
       "                         [-8.4262e-02,  1.2109e-01,  3.9641e-02]],\n",
       "               \n",
       "                        [[ 1.4976e-01,  3.2172e-02, -6.3651e-02],\n",
       "                         [ 1.8591e-01,  1.7183e-01,  4.1664e-03],\n",
       "                         [ 8.6497e-02,  2.9568e-01,  2.2874e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0941e-01, -1.0850e-01, -1.1813e-01],\n",
       "                         [-1.5011e-01,  7.3631e-02, -9.9876e-02],\n",
       "                         [-2.1692e-01, -8.1490e-02, -1.9925e-01]],\n",
       "               \n",
       "                        [[-5.9069e-02, -1.2191e-01, -2.1176e-01],\n",
       "                         [-2.2837e-01,  6.0312e-03,  2.9756e-02],\n",
       "                         [-2.8850e-01,  1.5011e-02, -2.1544e-01]],\n",
       "               \n",
       "                        [[ 6.8947e-02,  3.3931e-01,  1.7286e-01],\n",
       "                         [-3.3684e-01, -1.0952e-01,  1.3482e-01],\n",
       "                         [-2.0515e-01,  1.4422e-01,  8.0730e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1866e-01,  5.3614e-02,  6.5315e-02],\n",
       "                         [-2.8851e-01, -2.7413e-01, -1.8471e-01],\n",
       "                         [-2.0930e-01, -1.4442e-01, -2.8129e-01]],\n",
       "               \n",
       "                        [[-2.6725e-02, -2.0578e-01, -1.5601e-01],\n",
       "                         [ 2.1598e-01, -2.1026e-02, -3.5384e-01],\n",
       "                         [ 7.8481e-02, -9.7343e-02, -2.5198e-01]],\n",
       "               \n",
       "                        [[ 1.2725e-01, -1.2808e-01,  2.0151e-01],\n",
       "                         [-2.8450e-02,  5.7556e-02,  2.1256e-01],\n",
       "                         [-5.9615e-02, -1.5819e-01, -9.0206e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.0288e-01, -2.0952e-02,  5.5108e-02],\n",
       "                         [-1.0897e-01, -1.4218e-01, -1.5323e-02],\n",
       "                         [-7.3600e-02, -2.7061e-01, -8.1098e-02]],\n",
       "               \n",
       "                        [[-5.1316e-02,  1.4601e-01,  1.1121e-01],\n",
       "                         [-6.2586e-02, -6.3342e-02, -7.1862e-02],\n",
       "                         [-2.0287e-01, -8.3704e-02, -3.5511e-02]],\n",
       "               \n",
       "                        [[ 5.8441e-02,  2.2368e-01,  2.0253e-01],\n",
       "                         [ 2.0511e-01,  7.8082e-02,  4.3241e-02],\n",
       "                         [-1.7961e-02,  1.1176e-01, -1.1735e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.5865e-03,  2.0013e-01, -1.7829e-01],\n",
       "                         [-6.5540e-02,  2.5779e-03, -1.1012e-01],\n",
       "                         [-1.6716e-01, -7.0334e-01, -1.4243e-01]],\n",
       "               \n",
       "                        [[-4.2519e-01, -4.2180e-01, -2.5056e-01],\n",
       "                         [-4.5262e-01, -3.7827e-01, -5.7711e-02],\n",
       "                         [ 2.5666e-01, -8.7549e-02,  2.2904e-01]],\n",
       "               \n",
       "                        [[ 4.2328e-02, -2.6974e-02,  2.4277e-01],\n",
       "                         [ 4.1234e-02, -9.2512e-02,  1.3896e-01],\n",
       "                         [ 8.6447e-02, -1.2768e-01,  9.8358e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5787e-01,  2.2987e-01, -1.5920e-01],\n",
       "                         [ 2.1384e-01, -3.7154e-01, -4.5396e-01],\n",
       "                         [ 1.4524e-01, -6.8086e-01, -3.3288e-01]],\n",
       "               \n",
       "                        [[-1.8669e-01, -9.2962e-02,  8.9025e-02],\n",
       "                         [ 6.4377e-02,  2.5184e-01,  4.7494e-02],\n",
       "                         [ 1.0999e-01, -9.7002e-02,  3.2453e-01]],\n",
       "               \n",
       "                        [[ 1.1896e-01,  3.6262e-02, -4.5817e-01],\n",
       "                         [ 2.2589e-01, -3.1837e-01, -2.3807e-01],\n",
       "                         [ 1.5791e-01, -4.9304e-01,  9.9954e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.0212e-01, -2.6165e-01, -2.1175e-01],\n",
       "                         [-3.3859e-03,  3.3830e-02,  8.2777e-02],\n",
       "                         [-7.6809e-02,  3.8688e-01,  3.4634e-01]],\n",
       "               \n",
       "                        [[-4.1318e-01,  6.0619e-02,  1.6295e-02],\n",
       "                         [-3.6345e-01, -3.5207e-01,  1.0681e-01],\n",
       "                         [-4.6782e-02, -3.0050e-01,  2.1442e-01]],\n",
       "               \n",
       "                        [[ 1.3472e-01,  2.5350e-01,  1.2164e-01],\n",
       "                         [ 2.2068e-02,  2.6988e-01,  5.3131e-02],\n",
       "                         [-2.3855e-01,  1.7927e-01, -2.5914e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2600e-01,  5.0094e-03, -3.9816e-01],\n",
       "                         [-3.2816e-02,  2.0372e-01, -1.8098e-01],\n",
       "                         [ 6.3464e-02,  2.1729e-01,  9.6025e-03]],\n",
       "               \n",
       "                        [[-6.4930e-02, -1.6863e-01, -8.4929e-02],\n",
       "                         [ 2.3301e-02, -1.4895e-01, -2.9705e-01],\n",
       "                         [-7.1422e-02, -1.4142e-01, -1.2580e-01]],\n",
       "               \n",
       "                        [[ 6.5445e-01,  2.5667e-01,  2.0544e-01],\n",
       "                         [-4.3627e-02, -7.6820e-02, -2.0133e-01],\n",
       "                         [-2.3129e-01, -2.3626e-01,  1.5683e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.9024e-01,  6.8006e-02,  3.1571e-01],\n",
       "                         [ 7.4362e-02, -2.1441e-01,  3.3295e-01],\n",
       "                         [ 4.5818e-02, -1.4404e-01,  1.5520e-02]],\n",
       "               \n",
       "                        [[ 2.5976e-02, -5.8042e-02, -7.5722e-02],\n",
       "                         [-1.3456e-01, -2.2077e-01, -1.3269e-01],\n",
       "                         [-1.5131e-01, -2.2756e-01, -2.7982e-01]],\n",
       "               \n",
       "                        [[-2.9049e-01, -1.1146e-01,  1.5746e-01],\n",
       "                         [ 2.6669e-02, -2.1598e-01, -3.5773e-01],\n",
       "                         [-2.8724e-01, -1.7407e-01, -4.3434e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.1657e-02,  4.6368e-02, -1.5153e-01],\n",
       "                         [-1.5693e-01,  1.7010e-02, -1.2736e-01],\n",
       "                         [-1.2911e-01,  9.1136e-02, -1.0788e-01]],\n",
       "               \n",
       "                        [[-7.6717e-02, -5.1377e-01, -1.9287e-01],\n",
       "                         [-1.6914e-01, -2.1414e-01, -8.6451e-04],\n",
       "                         [-4.1195e-02, -2.0961e-01, -1.8368e-01]],\n",
       "               \n",
       "                        [[ 2.5996e-01,  3.2128e-01,  9.2766e-02],\n",
       "                         [ 3.5690e-01,  4.2198e-01,  1.5806e-01],\n",
       "                         [-3.1720e-02,  1.2629e-01,  1.0463e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 4.1250e-03,  3.7986e-02, -2.2236e-02, -4.9676e-02, -1.4213e-02,\n",
       "                        1.3658e-02, -4.7387e-02, -2.7228e-02, -1.7521e-02, -5.6051e-02,\n",
       "                       -5.9123e-03, -7.0252e-03,  3.1749e-03,  2.9750e-02,  7.7557e-03,\n",
       "                        8.5977e-03, -3.9357e-03, -2.2359e-02, -2.5177e-02, -1.0459e-02,\n",
       "                        5.4773e-03, -3.2566e-02, -1.6284e-02, -2.4308e-02,  1.3500e-02,\n",
       "                       -1.7912e-02,  2.3080e-03,  3.6633e-02, -3.0188e-03,  2.3883e-02,\n",
       "                       -7.3679e-03,  7.3331e-03, -5.1378e-03,  7.0333e-02,  8.1474e-03,\n",
       "                        4.2475e-02, -1.6598e-03, -1.6736e-02, -6.3913e-03,  1.9204e-02,\n",
       "                       -4.9940e-02, -1.2806e-02,  6.7781e-02,  4.2965e-02,  1.2168e-02,\n",
       "                       -4.9881e-02, -1.0214e-01,  1.0897e-02,  3.5558e-02,  1.0061e-01,\n",
       "                       -4.3778e-02, -1.8507e-02, -2.0792e-02,  8.2172e-03,  1.2072e-02,\n",
       "                        6.1623e-02, -1.1852e-02, -1.8381e-02,  3.1606e-03, -3.7534e-03,\n",
       "                        2.8484e-05, -2.8674e-02, -3.4160e-02,  3.2097e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.6017, -0.4780, -0.4892, -0.8492, -1.0702, -0.7887, -0.8295, -0.6969,\n",
       "                       -0.3358, -0.6247, -0.9939, -0.5285, -0.9254, -0.5851, -0.9856, -0.5461,\n",
       "                       -0.8158, -0.4743, -0.8483, -0.6238, -0.6817, -0.4769, -0.4211, -1.0376,\n",
       "                       -0.9802, -0.6869, -0.6768, -0.8752, -0.7253, -0.6898, -0.6401, -0.6538,\n",
       "                       -0.8286, -0.6262, -0.4836, -0.6692, -0.8096, -0.5973, -0.5231, -0.4769,\n",
       "                       -0.7063, -1.5868, -0.6896, -1.8907, -0.7027, -1.0255, -0.7266, -0.5361,\n",
       "                       -0.6326, -0.8311, -1.1398, -0.7910, -0.8678, -0.6213, -0.5821, -1.0058,\n",
       "                       -0.8971, -0.6464, -0.3720, -0.6176, -0.9121, -0.6968, -0.6827, -0.6903],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.6571, 0.7139, 0.7157, 0.8339, 0.9776, 0.8622, 1.0132, 0.7052, 0.6320,\n",
       "                       0.8380, 0.8747, 0.8164, 0.9219, 0.7002, 0.8030, 0.6590, 0.8126, 0.6803,\n",
       "                       0.9719, 0.6998, 0.8286, 0.7268, 0.6386, 1.0455, 0.9501, 0.8360, 0.6325,\n",
       "                       0.9914, 0.8301, 0.9838, 0.8084, 0.7438, 0.9850, 1.0082, 0.7621, 0.7778,\n",
       "                       0.9250, 0.7411, 0.7639, 0.7104, 0.7910, 0.7173, 0.9910, 1.0829, 0.7336,\n",
       "                       0.9676, 0.8398, 0.7753, 0.7975, 0.8226, 0.9543, 0.8884, 0.9038, 0.7794,\n",
       "                       0.8419, 0.9781, 0.9874, 0.7961, 0.6324, 0.8170, 0.7838, 0.7380, 0.9042,\n",
       "                       0.9094], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-1.7067e-01, -1.3066e-02, -1.7701e-01],\n",
       "                         [-1.1037e-01, -9.3368e-02, -1.0649e-01],\n",
       "                         [ 3.1819e-02, -8.6203e-02, -3.0124e-02]],\n",
       "               \n",
       "                        [[-1.6657e-01,  7.2936e-02,  1.0613e-01],\n",
       "                         [-2.4644e-02,  3.7754e-02,  1.1152e-01],\n",
       "                         [ 1.7000e-01,  2.5106e-01,  3.5388e-01]],\n",
       "               \n",
       "                        [[ 9.5263e-02,  1.5322e-01,  1.0976e-01],\n",
       "                         [-2.1412e-02, -3.0098e-02,  2.0796e-02],\n",
       "                         [ 2.3298e-01, -1.3165e-01,  2.1547e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.4961e-01, -1.9217e-01, -4.2393e-01],\n",
       "                         [-1.7386e-01, -1.5587e-01, -2.5135e-01],\n",
       "                         [-1.4756e-01, -1.6373e-01, -2.9216e-01]],\n",
       "               \n",
       "                        [[ 1.9778e-01, -1.4796e-01,  1.2410e-01],\n",
       "                         [ 4.6801e-02, -2.4166e-01,  1.0877e-01],\n",
       "                         [-2.4004e-03, -1.8757e-01, -2.1509e-01]],\n",
       "               \n",
       "                        [[ 2.0079e-01,  1.4096e-01,  2.9325e-01],\n",
       "                         [ 2.2767e-01,  3.1308e-02,  1.7275e-01],\n",
       "                         [ 2.5716e-01,  9.4600e-02,  1.9197e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.0705e-01,  1.3311e-01,  1.1247e-01],\n",
       "                         [-5.5667e-02,  1.1988e-01,  8.3343e-03],\n",
       "                         [-1.5005e-01,  9.3824e-02, -7.1935e-02]],\n",
       "               \n",
       "                        [[-1.6594e-01, -2.0657e-01, -2.3828e-01],\n",
       "                         [ 2.0826e-02, -1.6480e-01, -1.2982e-01],\n",
       "                         [-1.2433e-01, -2.0697e-01, -3.3013e-01]],\n",
       "               \n",
       "                        [[ 1.2100e-01, -6.9613e-02, -3.4620e-02],\n",
       "                         [-5.2852e-02, -7.1432e-02, -3.5363e-02],\n",
       "                         [-2.0287e-02, -2.2322e-01, -2.7510e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7025e-01,  6.0503e-03,  1.5624e-01],\n",
       "                         [ 6.2056e-02,  2.3598e-02,  7.6279e-02],\n",
       "                         [-8.7558e-02, -6.1394e-02, -1.8677e-02]],\n",
       "               \n",
       "                        [[ 1.8419e-01,  2.6918e-01,  6.2228e-01],\n",
       "                         [ 1.6490e-01,  8.7511e-02,  4.5316e-01],\n",
       "                         [ 1.8446e-01,  3.7751e-02,  6.4550e-01]],\n",
       "               \n",
       "                        [[ 1.1532e-01, -4.7652e-03, -1.9973e-01],\n",
       "                         [ 2.4383e-01, -4.9781e-02, -5.2043e-02],\n",
       "                         [-1.1410e-01, -1.9317e-01, -1.5489e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4215e-02,  4.1816e-02, -1.0965e-01],\n",
       "                         [ 2.4125e-01,  2.3535e-01, -6.3894e-02],\n",
       "                         [ 1.2636e-01,  1.6674e-01,  1.4834e-01]],\n",
       "               \n",
       "                        [[ 7.8367e-03,  1.8051e-01, -6.5671e-02],\n",
       "                         [ 9.8078e-03,  1.7806e-01,  1.0109e-01],\n",
       "                         [-2.2050e-01, -2.4391e-01, -1.4126e-01]],\n",
       "               \n",
       "                        [[-5.9175e-02, -2.6013e-01, -2.0994e-01],\n",
       "                         [ 1.0115e-01, -2.5451e-01, -2.1587e-01],\n",
       "                         [-2.5809e-01, -2.3031e-01, -2.7370e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.3602e-02, -9.6916e-02, -1.5441e-01],\n",
       "                         [ 1.5569e-01,  2.3636e-01, -1.3609e-01],\n",
       "                         [-2.9290e-02,  2.1860e-01, -2.6863e-02]],\n",
       "               \n",
       "                        [[-2.3563e-02, -2.7303e-01,  1.0055e-01],\n",
       "                         [-2.7075e-01, -2.7049e-01, -2.1870e-02],\n",
       "                         [-2.5540e-01, -2.9596e-01, -2.5480e-02]],\n",
       "               \n",
       "                        [[-2.3743e-01, -1.8762e-01,  1.3507e-01],\n",
       "                         [ 1.5412e-02, -8.7452e-02,  8.3685e-02],\n",
       "                         [ 1.0808e-01, -2.1368e-01,  1.0315e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3657e-02,  2.1707e-01, -1.1502e-01],\n",
       "                         [ 4.6054e-02, -3.3525e-02, -1.6309e-01],\n",
       "                         [-1.2973e-01, -2.6311e-01, -2.5469e-01]],\n",
       "               \n",
       "                        [[ 1.8950e-01,  1.1237e-01,  1.7514e-01],\n",
       "                         [ 9.3067e-02,  7.5957e-02,  2.4685e-01],\n",
       "                         [ 2.6706e-01,  1.1448e-01,  2.6876e-01]],\n",
       "               \n",
       "                        [[ 1.0918e-01, -7.5168e-02, -2.5756e-01],\n",
       "                         [-1.5880e-01, -1.2850e-01, -4.1996e-01],\n",
       "                         [-1.8728e-01, -5.1427e-01, -4.1481e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.2109e-02, -7.7912e-02, -1.6402e-02],\n",
       "                         [-1.9518e-02,  5.8579e-02,  5.4888e-04],\n",
       "                         [-1.5492e-02,  6.8988e-03,  2.0721e-02]],\n",
       "               \n",
       "                        [[-2.1506e-01, -1.8461e-02,  5.1667e-02],\n",
       "                         [-7.9244e-02, -2.2890e-01, -5.7329e-02],\n",
       "                         [-3.9364e-01, -6.3303e-01, -4.0065e-01]],\n",
       "               \n",
       "                        [[ 2.7630e-01,  1.4794e-01, -1.9473e-02],\n",
       "                         [-4.2248e-02,  5.8114e-02, -1.9232e-01],\n",
       "                         [ 5.0234e-02,  2.0128e-01,  7.8026e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.6807e-01,  3.7591e-02,  2.9190e-01],\n",
       "                         [-3.5815e-01,  1.9374e-01,  7.9991e-02],\n",
       "                         [-3.8051e-01,  5.4600e-02,  2.7299e-01]],\n",
       "               \n",
       "                        [[ 4.9207e-02, -3.2392e-01, -2.3834e-01],\n",
       "                         [ 2.4520e-01,  1.8751e-01,  1.8917e-01],\n",
       "                         [ 3.2089e-04, -5.4633e-02, -2.7677e-02]],\n",
       "               \n",
       "                        [[ 5.5989e-02, -1.9251e-01, -2.9557e-01],\n",
       "                         [ 2.5636e-01, -2.4829e-02, -1.3211e-01],\n",
       "                         [ 1.4171e-01, -2.3610e-02,  5.8787e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-9.2428e-02, -2.0456e-01,  8.3478e-02],\n",
       "                         [-2.3927e-01, -3.9887e-01, -1.5866e-01],\n",
       "                         [-1.1212e-01, -1.1777e-01, -1.2569e-01]],\n",
       "               \n",
       "                        [[ 5.7468e-01,  2.6531e-01,  1.4976e-02],\n",
       "                         [ 7.2242e-01,  4.4700e-01, -1.3288e-01],\n",
       "                         [ 4.9329e-01,  3.5156e-01, -5.7665e-02]],\n",
       "               \n",
       "                        [[ 1.6617e-03,  7.8952e-02, -9.3825e-03],\n",
       "                         [ 1.9070e-01,  2.5640e-01,  9.5074e-02],\n",
       "                         [ 1.7011e-02,  1.3433e-01,  8.2523e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.5604e-01, -3.7159e-01, -2.4709e-01],\n",
       "                         [-3.8478e-01, -1.7239e-01, -3.7673e-01],\n",
       "                         [-7.0394e-01, -4.8626e-01, -6.1311e-01]],\n",
       "               \n",
       "                        [[-1.4100e-01, -1.4097e-01, -8.7667e-02],\n",
       "                         [ 6.1109e-02, -1.6246e-01,  4.6355e-02],\n",
       "                         [-2.1542e-01, -2.9154e-01, -1.4383e-01]],\n",
       "               \n",
       "                        [[-8.5900e-03, -5.7570e-02, -3.8405e-02],\n",
       "                         [-6.1285e-02, -2.2596e-01, -1.1702e-01],\n",
       "                         [ 1.6398e-01, -1.0532e-01, -1.7420e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.6589e-01, -1.7009e-01, -4.8729e-02],\n",
       "                         [ 1.5189e-03, -2.2084e-01, -1.2078e-01],\n",
       "                         [ 1.4541e-01, -1.7820e-01, -7.6363e-02]],\n",
       "               \n",
       "                        [[-1.1031e-01, -2.2752e-01, -3.9630e-02],\n",
       "                         [-7.5092e-02, -3.0361e-03,  1.2115e-01],\n",
       "                         [-1.0173e-01, -3.3766e-01,  6.9194e-02]],\n",
       "               \n",
       "                        [[ 7.6225e-02,  1.3244e-01,  4.0945e-01],\n",
       "                         [-2.5315e-01,  3.1414e-02,  7.7669e-02],\n",
       "                         [-1.7007e-01,  1.5014e-01,  1.8012e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 4.1641e-02, -1.0669e-01, -1.3972e-02,  2.8114e-02, -9.2975e-03,\n",
       "                        2.0619e-03, -1.1585e-01,  3.8805e-02,  1.1243e-02,  2.0440e-02,\n",
       "                        1.9745e-02, -4.5448e-03,  3.0958e-02, -4.6014e-01, -7.8435e-02,\n",
       "                       -8.6226e-02, -3.3750e-02,  4.3969e-02, -3.9211e-03,  1.7982e-02,\n",
       "                        7.4458e-02,  2.0272e-03,  7.9179e-03,  1.1332e-02,  1.9535e-03,\n",
       "                        2.8692e-02,  1.3725e-02,  6.9201e-03,  1.5172e-02,  7.2591e-03,\n",
       "                       -1.0934e-01, -7.1052e-03, -2.1104e-02, -8.9992e-02, -7.2162e-02,\n",
       "                        2.3039e-02,  1.0898e-02, -2.0060e-02,  2.4481e-02,  7.0455e-04,\n",
       "                        1.7038e-03, -1.4990e-01,  7.0736e-02,  8.8229e-03,  1.6725e-01,\n",
       "                       -2.2029e-02, -9.4136e-02,  3.2394e-02,  4.1300e-02, -6.4366e-05,\n",
       "                        4.6533e-02, -7.4872e-03,  3.0531e-02, -3.9437e-02, -1.5623e-02,\n",
       "                        1.2684e-02,  1.7539e-02,  1.3746e-02,  1.2018e-02, -5.1134e-03,\n",
       "                       -4.8543e-03,  1.1518e-02,  9.7071e-03,  1.8686e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-1.0896, -2.1507, -1.8048, -0.8596, -1.0478, -1.0102, -1.4288, -1.4060,\n",
       "                       -0.9663, -1.5368, -1.5156, -0.8173, -1.8204, -2.2456, -0.9085, -1.1909,\n",
       "                       -0.7711, -1.3301, -0.8088, -1.0439, -1.5628, -0.9181, -1.1588, -1.2810,\n",
       "                       -1.0020, -0.8865, -1.0150, -1.0155, -0.9656, -1.5015, -2.3037, -0.9375,\n",
       "                       -1.1398, -1.6902, -1.1948, -1.7713, -0.9115, -1.5968, -1.3182, -0.7613,\n",
       "                       -1.7297, -2.1894, -2.0172, -1.3212, -1.4256, -2.0764, -1.6747, -0.9523,\n",
       "                       -1.5801, -0.9332, -1.1985, -1.2180, -1.6818, -1.5722, -1.6079, -1.3869,\n",
       "                       -0.9416, -0.9241, -1.2179, -1.3441, -0.8788, -1.0140, -1.1072, -0.7275],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([3.7842, 4.4471, 0.7056, 3.7946, 0.2785, 0.2738, 3.4043, 0.4481, 0.2609,\n",
       "                       0.7169, 3.9364, 0.2088, 0.4314, 5.0101, 3.3203, 3.2529, 3.5226, 0.6003,\n",
       "                       0.2146, 0.2651, 0.5497, 0.3230, 0.4740, 3.5539, 0.2871, 0.2918, 0.2972,\n",
       "                       0.3825, 3.2005, 0.3990, 3.9184, 0.2533, 3.2587, 3.6326, 3.3870, 0.8362,\n",
       "                       0.2742, 0.7150, 0.4313, 0.2199, 0.6827, 3.8503, 1.3627, 0.4611, 3.0792,\n",
       "                       3.3972, 3.5064, 0.2283, 0.9893, 0.2625, 3.3467, 3.1768, 0.6578, 3.8792,\n",
       "                       3.3881, 3.4488, 0.3257, 0.2208, 0.3248, 0.5026, 0.2992, 0.3435, 0.3278,\n",
       "                       2.9382], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0348, -0.0104, -0.0042,  ..., -0.0009,  0.0163,  0.0054],\n",
       "                       [-0.0141, -0.0083,  0.0053,  ...,  0.0157,  0.0218,  0.0393],\n",
       "                       [-0.0223,  0.0105,  0.0090,  ...,  0.0104,  0.0185,  0.0036],\n",
       "                       [-0.0093, -0.0106,  0.0012,  ...,  0.0246,  0.0343,  0.0062],\n",
       "                       [-0.0192,  0.0020,  0.0106,  ...,  0.0193,  0.0290, -0.0057]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.2154,  0.3053, -0.2302,  0.4531, -0.2380], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3821220500469207,\n",
       "   1.3021315078735352,\n",
       "   1.222449166893959,\n",
       "   1.1697446973323822,\n",
       "   1.126164562702179,\n",
       "   1.103199450969696,\n",
       "   1.0799383276700973,\n",
       "   1.0510324746370316,\n",
       "   1.0330799340009689,\n",
       "   1.0169329599142074,\n",
       "   0.9822105876207352,\n",
       "   0.9770566730499267,\n",
       "   0.9588655294179916,\n",
       "   0.9414263654947281,\n",
       "   0.9363355795145035,\n",
       "   0.9240059593915939,\n",
       "   0.8977977560758591,\n",
       "   0.8909568021297455,\n",
       "   0.8813100074529647,\n",
       "   0.8655572018027305,\n",
       "   0.8559504778385162,\n",
       "   0.8309543271064759,\n",
       "   0.8299630028605461,\n",
       "   0.8273259307146072,\n",
       "   0.8180275123119354,\n",
       "   0.8168431396484375,\n",
       "   0.7906347794532775,\n",
       "   0.7917161728739739,\n",
       "   0.7819899005889892,\n",
       "   0.7928046478033066,\n",
       "   0.7694970924854279,\n",
       "   0.76863211196661,\n",
       "   0.7561018224954605,\n",
       "   0.7385065902471543,\n",
       "   0.7591250451207161,\n",
       "   0.7319793934226037,\n",
       "   0.7312492675185204,\n",
       "   0.7306893970966339,\n",
       "   0.728378259241581,\n",
       "   0.7039069735407829,\n",
       "   0.7223251054286957,\n",
       "   0.7047018485069275,\n",
       "   0.700988232254982,\n",
       "   0.7032113144397736,\n",
       "   0.7018483471274376,\n",
       "   0.6943742069602012,\n",
       "   0.6895885868668556,\n",
       "   0.6779186080098152,\n",
       "   0.6803666565418244,\n",
       "   0.6810621865987778,\n",
       "   0.6812637149095535,\n",
       "   0.6557760425209999,\n",
       "   0.6690531409978867,\n",
       "   0.6529473427534104,\n",
       "   0.6755649086236953,\n",
       "   0.6613797073364258,\n",
       "   0.651308987736702,\n",
       "   0.6519375024437905,\n",
       "   0.6481107335090637,\n",
       "   0.6441545747518539,\n",
       "   0.6521966953873635,\n",
       "   0.645221735715866,\n",
       "   0.6315558581948281,\n",
       "   0.6255988692045211,\n",
       "   0.6410764248371125,\n",
       "   0.639128813803196,\n",
       "   0.6378846968412399,\n",
       "   0.6167481243610382,\n",
       "   0.6208667230606079,\n",
       "   0.6332033781409263,\n",
       "   0.6127314854860306,\n",
       "   0.6143889218568802,\n",
       "   0.619949030160904,\n",
       "   0.6137805005311966,\n",
       "   0.6032597521543502,\n",
       "   0.6025395417213439,\n",
       "   0.6121120390892029,\n",
       "   0.6128589915633201,\n",
       "   0.6039318307638168,\n",
       "   0.6086812917888165,\n",
       "   0.5973714723587036,\n",
       "   0.5980873594284057,\n",
       "   0.602192023396492,\n",
       "   0.5940385815501213,\n",
       "   0.5822140692174435,\n",
       "   0.5875552316308021,\n",
       "   0.5863675386309624,\n",
       "   0.5852665336132049,\n",
       "   0.5740595889687539,\n",
       "   0.5694547915458679,\n",
       "   0.5712518365979194,\n",
       "   0.5806549159288407,\n",
       "   0.5540645588338375,\n",
       "   0.5712025994658471,\n",
       "   0.5659674203395844,\n",
       "   0.5841565625071525,\n",
       "   0.5712108993530274,\n",
       "   0.5693458061218262,\n",
       "   0.5553330751657486],\n",
       "  'train_loss_std': [0.12801127557390604,\n",
       "   0.12336082410684515,\n",
       "   0.13635924083318587,\n",
       "   0.12554661332111852,\n",
       "   0.1268564065009036,\n",
       "   0.13544711002593973,\n",
       "   0.1497437622719848,\n",
       "   0.13938298808071456,\n",
       "   0.14253826310312293,\n",
       "   0.13639411986157854,\n",
       "   0.14389784873787342,\n",
       "   0.14896825266708055,\n",
       "   0.13228377645501033,\n",
       "   0.1470495758560407,\n",
       "   0.1430183231496096,\n",
       "   0.15262180547142867,\n",
       "   0.13596626441587675,\n",
       "   0.14440958256944372,\n",
       "   0.1449276504620866,\n",
       "   0.15165752260781132,\n",
       "   0.14392671275930444,\n",
       "   0.1333633977546889,\n",
       "   0.14625585779382916,\n",
       "   0.14823901382468152,\n",
       "   0.13968552510589854,\n",
       "   0.1522297783757191,\n",
       "   0.139405518945719,\n",
       "   0.14179371742156666,\n",
       "   0.14704305629774894,\n",
       "   0.152598378794665,\n",
       "   0.1427770380679407,\n",
       "   0.1422912184337323,\n",
       "   0.14663150556614643,\n",
       "   0.14574725713996067,\n",
       "   0.1483526925693327,\n",
       "   0.15155342968202987,\n",
       "   0.138991792476493,\n",
       "   0.15067771600291702,\n",
       "   0.15138181971458303,\n",
       "   0.146991086240644,\n",
       "   0.14832017308575354,\n",
       "   0.1430487216754083,\n",
       "   0.14266094133821608,\n",
       "   0.147983208705066,\n",
       "   0.1494397335129483,\n",
       "   0.14487249153418494,\n",
       "   0.1473558352941536,\n",
       "   0.1469758579229944,\n",
       "   0.14450294436820507,\n",
       "   0.14548173583634944,\n",
       "   0.14596319301365335,\n",
       "   0.13592101273973412,\n",
       "   0.14748962133254923,\n",
       "   0.14327280437489404,\n",
       "   0.15428859628335892,\n",
       "   0.14342698484877522,\n",
       "   0.13279665585419934,\n",
       "   0.14725479781096096,\n",
       "   0.1428647687169155,\n",
       "   0.1430698060436431,\n",
       "   0.14754276981507053,\n",
       "   0.14657789115111505,\n",
       "   0.14556498210199287,\n",
       "   0.13780205059561573,\n",
       "   0.1408331643027919,\n",
       "   0.1440561725858388,\n",
       "   0.1391446882355011,\n",
       "   0.14457839140728956,\n",
       "   0.13772532684458066,\n",
       "   0.14952820124358085,\n",
       "   0.1326991718957292,\n",
       "   0.13927216564107353,\n",
       "   0.13496276646947006,\n",
       "   0.14204750006733052,\n",
       "   0.13878890014447665,\n",
       "   0.13925840098856135,\n",
       "   0.1463542889185346,\n",
       "   0.14727189113373543,\n",
       "   0.13884061377155377,\n",
       "   0.140134462923132,\n",
       "   0.13697797506455173,\n",
       "   0.1421513839630234,\n",
       "   0.1507406849901963,\n",
       "   0.13563639436913083,\n",
       "   0.14141437078255903,\n",
       "   0.13027424436714932,\n",
       "   0.13702290075988652,\n",
       "   0.141461953506257,\n",
       "   0.13455943290498604,\n",
       "   0.1350746401142183,\n",
       "   0.1372223097718178,\n",
       "   0.14106655462301085,\n",
       "   0.1333278437591555,\n",
       "   0.14081110589591758,\n",
       "   0.14252111427568634,\n",
       "   0.15208774617171844,\n",
       "   0.1296614593295469,\n",
       "   0.13488985153868005,\n",
       "   0.13450310665712903],\n",
       "  'train_accuracy_mean': [0.428586667239666,\n",
       "   0.46876000064611434,\n",
       "   0.5082266674637794,\n",
       "   0.5347999989390373,\n",
       "   0.5569733330011368,\n",
       "   0.5652266656160354,\n",
       "   0.5748399984240532,\n",
       "   0.5897466659545898,\n",
       "   0.5980399978160859,\n",
       "   0.6031999994516373,\n",
       "   0.6182266660332679,\n",
       "   0.6209733339548111,\n",
       "   0.6296800004839898,\n",
       "   0.6376800001859665,\n",
       "   0.6394933329820633,\n",
       "   0.643906665623188,\n",
       "   0.6543466662764549,\n",
       "   0.6586666657924652,\n",
       "   0.6621999987959861,\n",
       "   0.6686000021100045,\n",
       "   0.6736666663885117,\n",
       "   0.6834000002741814,\n",
       "   0.6836266658306122,\n",
       "   0.6845733318924904,\n",
       "   0.6883866664171219,\n",
       "   0.6905466667413711,\n",
       "   0.7011066663265229,\n",
       "   0.699306666970253,\n",
       "   0.7040133326053619,\n",
       "   0.6979733331799507,\n",
       "   0.7090266666412354,\n",
       "   0.7082266661524773,\n",
       "   0.7145599992275238,\n",
       "   0.723213332414627,\n",
       "   0.7112800000309945,\n",
       "   0.7249866659641266,\n",
       "   0.7247466676235199,\n",
       "   0.7239200010299682,\n",
       "   0.7231600009799004,\n",
       "   0.7358133323192596,\n",
       "   0.7275066646337509,\n",
       "   0.733600000500679,\n",
       "   0.737253333568573,\n",
       "   0.73394666659832,\n",
       "   0.7349733336567879,\n",
       "   0.7385333330631256,\n",
       "   0.7398000009655953,\n",
       "   0.747106665790081,\n",
       "   0.7452399985790252,\n",
       "   0.7435200006961823,\n",
       "   0.7448666675090789,\n",
       "   0.7541599999666214,\n",
       "   0.7489999997615814,\n",
       "   0.7552933340072632,\n",
       "   0.7470666662454605,\n",
       "   0.7514000004529953,\n",
       "   0.7563733341693878,\n",
       "   0.7560800001621246,\n",
       "   0.7567066668272019,\n",
       "   0.7578666648864746,\n",
       "   0.7545333336591721,\n",
       "   0.7562799990177155,\n",
       "   0.7636000000238419,\n",
       "   0.7673466669321061,\n",
       "   0.7597600008249283,\n",
       "   0.7598533321619034,\n",
       "   0.760466667354107,\n",
       "   0.768413333773613,\n",
       "   0.7678533318042755,\n",
       "   0.762693333029747,\n",
       "   0.771853333234787,\n",
       "   0.76832000041008,\n",
       "   0.7675333330631257,\n",
       "   0.7673066663742065,\n",
       "   0.77378666639328,\n",
       "   0.7763466665744782,\n",
       "   0.770813333272934,\n",
       "   0.7688933311700821,\n",
       "   0.7746933327913285,\n",
       "   0.7726800001859665,\n",
       "   0.7767466663122177,\n",
       "   0.7782399986982346,\n",
       "   0.7753333333730698,\n",
       "   0.7784266669750214,\n",
       "   0.7823733344078064,\n",
       "   0.7794933326244354,\n",
       "   0.7792000001668931,\n",
       "   0.7819199997186661,\n",
       "   0.7860533322095871,\n",
       "   0.7847333332300186,\n",
       "   0.7863866666555405,\n",
       "   0.782346665263176,\n",
       "   0.7938666670322418,\n",
       "   0.7850533335208892,\n",
       "   0.7874933331012726,\n",
       "   0.7814933341145516,\n",
       "   0.7867866657972336,\n",
       "   0.787053332567215,\n",
       "   0.7921199990510941],\n",
       "  'train_accuracy_std': [0.06737921064308171,\n",
       "   0.06546089731424419,\n",
       "   0.07591200735479188,\n",
       "   0.07011723498560915,\n",
       "   0.06977420191150185,\n",
       "   0.07100464496552186,\n",
       "   0.07526041310192337,\n",
       "   0.06868432095195662,\n",
       "   0.07019197900943841,\n",
       "   0.06691075363206772,\n",
       "   0.07250785206299383,\n",
       "   0.07322436897907708,\n",
       "   0.06662922227591263,\n",
       "   0.07164942274759843,\n",
       "   0.06940580430219782,\n",
       "   0.0718933174217093,\n",
       "   0.06660960776813454,\n",
       "   0.06906872676789784,\n",
       "   0.07113964345970662,\n",
       "   0.07332057560832278,\n",
       "   0.06952649420760879,\n",
       "   0.06423633533996384,\n",
       "   0.06960972717209561,\n",
       "   0.0704157190726784,\n",
       "   0.0663577613850617,\n",
       "   0.07172517827477634,\n",
       "   0.06586263243017439,\n",
       "   0.06547779474399565,\n",
       "   0.0666185151056179,\n",
       "   0.06941776521895708,\n",
       "   0.06731969610555617,\n",
       "   0.06555446989055239,\n",
       "   0.06657398393011765,\n",
       "   0.06919013288992641,\n",
       "   0.06936830381008505,\n",
       "   0.06972724431358072,\n",
       "   0.06377357565278177,\n",
       "   0.06774273398247575,\n",
       "   0.07040496401767392,\n",
       "   0.06643346390057893,\n",
       "   0.06753900069055288,\n",
       "   0.06572616708853805,\n",
       "   0.0642986969376107,\n",
       "   0.06556406050065372,\n",
       "   0.06624583245498923,\n",
       "   0.06603689107462543,\n",
       "   0.06738845841963909,\n",
       "   0.067234132523043,\n",
       "   0.06644770786686237,\n",
       "   0.06480370827344462,\n",
       "   0.0679755192991285,\n",
       "   0.061495663828149104,\n",
       "   0.06595604721424432,\n",
       "   0.0648072573083374,\n",
       "   0.0676449547330563,\n",
       "   0.06722099399001859,\n",
       "   0.062083837073735756,\n",
       "   0.06706489578205446,\n",
       "   0.06540606995340235,\n",
       "   0.06512947782046564,\n",
       "   0.06579415680983716,\n",
       "   0.06756943331576573,\n",
       "   0.06590916264878602,\n",
       "   0.06270197530957357,\n",
       "   0.06317478355433129,\n",
       "   0.06662049013487682,\n",
       "   0.0641959152124166,\n",
       "   0.0642084645950181,\n",
       "   0.0633699255823971,\n",
       "   0.06622009844306413,\n",
       "   0.05965259327993763,\n",
       "   0.06364013259978547,\n",
       "   0.05988325642204962,\n",
       "   0.06437072605152275,\n",
       "   0.06192733178884599,\n",
       "   0.06410397822224918,\n",
       "   0.06599633620465825,\n",
       "   0.06694423799991532,\n",
       "   0.06169544590897662,\n",
       "   0.06374546444778599,\n",
       "   0.06177444981938501,\n",
       "   0.06325901069822962,\n",
       "   0.06506680188215687,\n",
       "   0.06094379915776649,\n",
       "   0.061853326116929626,\n",
       "   0.05853858174752532,\n",
       "   0.06114812610247444,\n",
       "   0.06247810472215801,\n",
       "   0.059936089199080715,\n",
       "   0.05936774229448623,\n",
       "   0.06139281197916323,\n",
       "   0.06396530333496596,\n",
       "   0.05951268750344206,\n",
       "   0.06326081301518738,\n",
       "   0.06458366690977631,\n",
       "   0.06706872914991535,\n",
       "   0.05716318765840403,\n",
       "   0.06011937615162815,\n",
       "   0.059697340007060846],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.4313842721780141,\n",
       "   1.3996887254714965,\n",
       "   1.3112757643063864,\n",
       "   1.26250301917394,\n",
       "   1.2876383022467295,\n",
       "   1.2545889856417973,\n",
       "   1.233648990591367,\n",
       "   1.1869175463914872,\n",
       "   1.187744747598966,\n",
       "   1.182660307486852,\n",
       "   1.1714224723974864,\n",
       "   1.1686604909102123,\n",
       "   1.1638708635171255,\n",
       "   1.1429610453049341,\n",
       "   1.1356699309746425,\n",
       "   1.1218904346227645,\n",
       "   1.1724198426802952,\n",
       "   1.091321399609248,\n",
       "   1.113779217004776,\n",
       "   1.0932742349306743,\n",
       "   1.0896364412705104,\n",
       "   1.074805858929952,\n",
       "   1.0746053276459375,\n",
       "   1.0673917526006698,\n",
       "   1.0747260477145513,\n",
       "   1.072311089038849,\n",
       "   1.056406568288803,\n",
       "   1.0801214994986852,\n",
       "   1.0519764280319215,\n",
       "   1.0616912080844243,\n",
       "   1.0431558632850646,\n",
       "   1.0556800566116968,\n",
       "   1.0451338219642639,\n",
       "   1.0661295268932978,\n",
       "   1.0382146974404654,\n",
       "   1.0442357323567073,\n",
       "   1.0441136995951334,\n",
       "   1.0391528894503912,\n",
       "   1.0459215968847275,\n",
       "   1.0423232239484788,\n",
       "   1.0364221445719402,\n",
       "   1.047435855269432,\n",
       "   1.0448797887563706,\n",
       "   1.0483581193288167,\n",
       "   1.0156011585394542,\n",
       "   1.0090239785114925,\n",
       "   1.0318054697910946,\n",
       "   1.0318155618508658,\n",
       "   1.009324775536855,\n",
       "   1.030169877409935,\n",
       "   1.0207442168394725,\n",
       "   1.0271315455436707,\n",
       "   1.0184095988670985,\n",
       "   1.06122403383255,\n",
       "   1.0198205453157425,\n",
       "   1.0205204703410466,\n",
       "   1.0094764437278112,\n",
       "   1.0206693683067958,\n",
       "   1.0017774667342503,\n",
       "   1.0256803411245345,\n",
       "   1.0341172804435095,\n",
       "   1.0220697861909867,\n",
       "   1.0119540995359422,\n",
       "   1.020190665324529,\n",
       "   1.0344882688919703,\n",
       "   1.005111286441485,\n",
       "   1.050649773478508,\n",
       "   1.0339922734101614,\n",
       "   1.042164526383082,\n",
       "   1.021306458711624,\n",
       "   1.0165406602621079,\n",
       "   1.0129109098513922,\n",
       "   1.048312164346377,\n",
       "   1.0098257009188334,\n",
       "   1.0121213106314342,\n",
       "   1.0341343559821448,\n",
       "   1.0300068499644597,\n",
       "   1.0182182582219441,\n",
       "   1.0257631001869838,\n",
       "   1.024888680577278,\n",
       "   1.0162987170616786,\n",
       "   1.0212844596306483,\n",
       "   1.0299896428982416,\n",
       "   1.0076777372757595,\n",
       "   1.0181020182371139,\n",
       "   1.0326851596434912,\n",
       "   1.013904168009758,\n",
       "   1.0162334471940995,\n",
       "   1.0377297514677049,\n",
       "   1.0181487456957499,\n",
       "   1.0156035810708999,\n",
       "   1.0137104294697443,\n",
       "   1.0230013134082159,\n",
       "   1.0133029079437257,\n",
       "   1.0222286359469095,\n",
       "   1.0030337206522624,\n",
       "   1.0063148140907288,\n",
       "   1.0275884489218394,\n",
       "   1.0543068075180053],\n",
       "  'val_loss_std': [0.09205461779074117,\n",
       "   0.10112325803857783,\n",
       "   0.1164816555206192,\n",
       "   0.11684442487868073,\n",
       "   0.12624354610210883,\n",
       "   0.12824916511328138,\n",
       "   0.13190054609615864,\n",
       "   0.13723190192099402,\n",
       "   0.13807000438458453,\n",
       "   0.139339988628521,\n",
       "   0.13426554246896522,\n",
       "   0.13898465266730373,\n",
       "   0.14230000650021354,\n",
       "   0.1437003945085759,\n",
       "   0.14093502381937267,\n",
       "   0.1445453384026066,\n",
       "   0.14431133382674588,\n",
       "   0.13963978126369894,\n",
       "   0.1445965086045647,\n",
       "   0.1473384673962332,\n",
       "   0.14427751914979267,\n",
       "   0.15651336387986003,\n",
       "   0.1504388733701974,\n",
       "   0.1377833643387174,\n",
       "   0.15208553578622375,\n",
       "   0.1511393792146939,\n",
       "   0.15462000489744357,\n",
       "   0.15079747962037007,\n",
       "   0.15380898367491055,\n",
       "   0.14545972001424556,\n",
       "   0.14390736502212356,\n",
       "   0.14739898371032523,\n",
       "   0.14194398915396272,\n",
       "   0.14588002424475452,\n",
       "   0.14871934461862624,\n",
       "   0.15301106322396665,\n",
       "   0.150860339033856,\n",
       "   0.16279383120706814,\n",
       "   0.1613931133569259,\n",
       "   0.1544442296543151,\n",
       "   0.1462521983935629,\n",
       "   0.15418683052514331,\n",
       "   0.1581760828815597,\n",
       "   0.1537545333982123,\n",
       "   0.15045548714138932,\n",
       "   0.14568185742774636,\n",
       "   0.15772383466341564,\n",
       "   0.14845824052159048,\n",
       "   0.1498361445585524,\n",
       "   0.1464824525521518,\n",
       "   0.15481009294325368,\n",
       "   0.1557191555224027,\n",
       "   0.15248729904574643,\n",
       "   0.16376856703765233,\n",
       "   0.15678532315604626,\n",
       "   0.15527713147984473,\n",
       "   0.15353407736482227,\n",
       "   0.16107553013494955,\n",
       "   0.15019259909839439,\n",
       "   0.16347435710670924,\n",
       "   0.15804517369472218,\n",
       "   0.1502543133017246,\n",
       "   0.15369445132971013,\n",
       "   0.15310228893681735,\n",
       "   0.14993041414150787,\n",
       "   0.14353553455377097,\n",
       "   0.15939827731580725,\n",
       "   0.15987908340345744,\n",
       "   0.15769095851692533,\n",
       "   0.15947950931045551,\n",
       "   0.15299200561337614,\n",
       "   0.15248077290781153,\n",
       "   0.15717625634433338,\n",
       "   0.16433322077473966,\n",
       "   0.15634289603947074,\n",
       "   0.1561236866621986,\n",
       "   0.15045094044689802,\n",
       "   0.1492959677872479,\n",
       "   0.16263876700244198,\n",
       "   0.14985152103048288,\n",
       "   0.15523837038001942,\n",
       "   0.14864309446757104,\n",
       "   0.14975653232640035,\n",
       "   0.15363722650206366,\n",
       "   0.14500426139129594,\n",
       "   0.1638719351944222,\n",
       "   0.15360695854519954,\n",
       "   0.15359483738455917,\n",
       "   0.16637769314754855,\n",
       "   0.15317978279037667,\n",
       "   0.15992324554765097,\n",
       "   0.15632972566148234,\n",
       "   0.15912426431886434,\n",
       "   0.15526225850688832,\n",
       "   0.15075307862951706,\n",
       "   0.1573381469391268,\n",
       "   0.15818645666927253,\n",
       "   0.14830270135498172,\n",
       "   0.16267633333612236],\n",
       "  'val_accuracy_mean': [0.40004444509744647,\n",
       "   0.4225333339969317,\n",
       "   0.4581111112733682,\n",
       "   0.48686666518449784,\n",
       "   0.47706666777531304,\n",
       "   0.4963777775565783,\n",
       "   0.5000444449981054,\n",
       "   0.5225999984145164,\n",
       "   0.5207111118237178,\n",
       "   0.527266666094462,\n",
       "   0.5304222198327383,\n",
       "   0.5323111128807068,\n",
       "   0.5304444437225659,\n",
       "   0.5424444445967674,\n",
       "   0.5477777771155039,\n",
       "   0.5516444446643194,\n",
       "   0.5323999987045924,\n",
       "   0.5623999993006389,\n",
       "   0.5551333330074946,\n",
       "   0.5654444433252017,\n",
       "   0.5665333320697149,\n",
       "   0.5747777782877286,\n",
       "   0.5751777780056,\n",
       "   0.5776888875166575,\n",
       "   0.5727777765194575,\n",
       "   0.5741333334644636,\n",
       "   0.5807111101349195,\n",
       "   0.5786222196618716,\n",
       "   0.5828222213188807,\n",
       "   0.5812888873616854,\n",
       "   0.5882222219308217,\n",
       "   0.5859555526574453,\n",
       "   0.585911110540231,\n",
       "   0.578488886654377,\n",
       "   0.5906666655341785,\n",
       "   0.5864444425702096,\n",
       "   0.5875555550058683,\n",
       "   0.5952666650215784,\n",
       "   0.5912444424629212,\n",
       "   0.5931111124157905,\n",
       "   0.5930666654308637,\n",
       "   0.5931777775287628,\n",
       "   0.5911999983588855,\n",
       "   0.591355554163456,\n",
       "   0.6047333331902822,\n",
       "   0.6045555541912715,\n",
       "   0.6015777760744094,\n",
       "   0.596177778840065,\n",
       "   0.6065777761737505,\n",
       "   0.597444442709287,\n",
       "   0.598822219868501,\n",
       "   0.600399999320507,\n",
       "   0.6049777765075366,\n",
       "   0.5903333340088527,\n",
       "   0.6065777761737505,\n",
       "   0.6009999984502792,\n",
       "   0.6096222213904063,\n",
       "   0.6034666643540064,\n",
       "   0.6111333331465721,\n",
       "   0.6009333326419195,\n",
       "   0.5967999984820683,\n",
       "   0.6005555545290311,\n",
       "   0.6077111103137334,\n",
       "   0.6022888872027398,\n",
       "   0.60113333294789,\n",
       "   0.6095999983946482,\n",
       "   0.5930888884266218,\n",
       "   0.5969555552800496,\n",
       "   0.5965333346525828,\n",
       "   0.6035333301623662,\n",
       "   0.6061777769525846,\n",
       "   0.6079777774214744,\n",
       "   0.5964222212632497,\n",
       "   0.6089333349466324,\n",
       "   0.6091333315769831,\n",
       "   0.5997111107905706,\n",
       "   0.6024888890981674,\n",
       "   0.6041333324710528,\n",
       "   0.6009111120303472,\n",
       "   0.6026888893047968,\n",
       "   0.6104888870318731,\n",
       "   0.6038666663567225,\n",
       "   0.6024888880054156,\n",
       "   0.6109333335359891,\n",
       "   0.6040888883670171,\n",
       "   0.6019555548826854,\n",
       "   0.6064222212632497,\n",
       "   0.6051555540164312,\n",
       "   0.6028666660189629,\n",
       "   0.6048444433013598,\n",
       "   0.6116666648785273,\n",
       "   0.6057333334287007,\n",
       "   0.605466666718324,\n",
       "   0.6083777764439583,\n",
       "   0.6010888877511025,\n",
       "   0.6102000002066295,\n",
       "   0.6114222208658854,\n",
       "   0.6065555544694264,\n",
       "   0.5980888879299164],\n",
       "  'val_accuracy_std': [0.05868179908627809,\n",
       "   0.05510485714643319,\n",
       "   0.06277167984344922,\n",
       "   0.06174107176794624,\n",
       "   0.0650582867996088,\n",
       "   0.06243982178396385,\n",
       "   0.06386790305381862,\n",
       "   0.06728311621093111,\n",
       "   0.06469138838995474,\n",
       "   0.06508407161029786,\n",
       "   0.06472711423172837,\n",
       "   0.06630674695412399,\n",
       "   0.06448849098113899,\n",
       "   0.06935647753425962,\n",
       "   0.06666407319079863,\n",
       "   0.06730004319728121,\n",
       "   0.06559823729987052,\n",
       "   0.06762546033436248,\n",
       "   0.06986243479584336,\n",
       "   0.06774343342027654,\n",
       "   0.0690655938373676,\n",
       "   0.06416288843149873,\n",
       "   0.06466327103140679,\n",
       "   0.06287429650511128,\n",
       "   0.06737723044081613,\n",
       "   0.0667862036693972,\n",
       "   0.071066569381169,\n",
       "   0.06587438984708674,\n",
       "   0.06891458743496932,\n",
       "   0.06418351805825863,\n",
       "   0.06417991640387938,\n",
       "   0.0668980269846182,\n",
       "   0.06234212174659084,\n",
       "   0.06813340442264552,\n",
       "   0.0659786716875323,\n",
       "   0.0658951657989604,\n",
       "   0.06462503610634722,\n",
       "   0.06873865646199548,\n",
       "   0.06804197728964713,\n",
       "   0.06782075038569553,\n",
       "   0.06432961374880992,\n",
       "   0.06639593536697386,\n",
       "   0.06719267945568227,\n",
       "   0.06473031996557653,\n",
       "   0.06569713795088973,\n",
       "   0.06292019283010349,\n",
       "   0.06504038203600078,\n",
       "   0.06616185242791073,\n",
       "   0.06592470186517314,\n",
       "   0.06400337432745269,\n",
       "   0.06280320432945151,\n",
       "   0.064700930124911,\n",
       "   0.06363174901732081,\n",
       "   0.06388676238835815,\n",
       "   0.06456457821275267,\n",
       "   0.06652902375333108,\n",
       "   0.06178821201799597,\n",
       "   0.06733760276706922,\n",
       "   0.06547188262972667,\n",
       "   0.0691901512208606,\n",
       "   0.0665987208787667,\n",
       "   0.06517772432585282,\n",
       "   0.06565187373077076,\n",
       "   0.06472006572094625,\n",
       "   0.06678470677825087,\n",
       "   0.06164163957479436,\n",
       "   0.06720956220565541,\n",
       "   0.06672268498490637,\n",
       "   0.0648815219575972,\n",
       "   0.06444605010081442,\n",
       "   0.06566400089159542,\n",
       "   0.06573729788530942,\n",
       "   0.06882570153425312,\n",
       "   0.06610129321431708,\n",
       "   0.06473871244250064,\n",
       "   0.06753044423178259,\n",
       "   0.06534261601371476,\n",
       "   0.06351981902850713,\n",
       "   0.0643870213035903,\n",
       "   0.06284866808422196,\n",
       "   0.06397613789675531,\n",
       "   0.06297686933089675,\n",
       "   0.06634387149727772,\n",
       "   0.06378798316558339,\n",
       "   0.05994707139859857,\n",
       "   0.06714736340778346,\n",
       "   0.06373393498820212,\n",
       "   0.06355701703417364,\n",
       "   0.06824306148040962,\n",
       "   0.06337844739629891,\n",
       "   0.06487623659809585,\n",
       "   0.06657562648520936,\n",
       "   0.06437653190840435,\n",
       "   0.06614600839751872,\n",
       "   0.06484054143281616,\n",
       "   0.062130960546699344,\n",
       "   0.06514410953449053,\n",
       "   0.06320445120708451,\n",
       "   0.0639969284160578],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6682222226262092,\n",
       " 'best_val_iter': 14000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 28,\n",
       " 'train_loss_mean': 0.3552659073024988,\n",
       " 'train_loss_std': 0.11188082095213106,\n",
       " 'train_accuracy_mean': 0.8689599990844726,\n",
       " 'train_accuracy_std': 0.05070948533209209,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.9532056687275569,\n",
       " 'val_loss_std': 0.16697529376818385,\n",
       " 'val_accuracy_mean': 0.6373111097017924,\n",
       " 'val_accuracy_std': 0.06381013172349882,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-0.2480, -0.4015,  0.0014],\n",
       "                         [-0.2312, -0.1655,  0.1531],\n",
       "                         [ 0.1413,  0.4547,  0.2247]],\n",
       "               \n",
       "                        [[ 0.0493, -0.0765,  0.2573],\n",
       "                         [-0.1286, -0.0250,  0.2375],\n",
       "                         [-0.2020, -0.0183, -0.0575]],\n",
       "               \n",
       "                        [[ 0.3931,  0.1681, -0.1345],\n",
       "                         [ 0.2876,  0.1437, -0.2763],\n",
       "                         [-0.1173, -0.1289, -0.3287]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.3116,  0.4870,  0.2534],\n",
       "                         [ 0.2432, -0.1407, -0.0977],\n",
       "                         [-0.4792, -0.4236, -0.2052]],\n",
       "               \n",
       "                        [[-0.2142, -0.1096, -0.1671],\n",
       "                         [ 0.2083, -0.1069, -0.1527],\n",
       "                         [ 0.1361,  0.0792,  0.2875]],\n",
       "               \n",
       "                        [[-0.2053, -0.0647, -0.1116],\n",
       "                         [-0.0310,  0.0801,  0.1403],\n",
       "                         [ 0.0084,  0.1216,  0.0787]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0437, -0.3315,  0.2521],\n",
       "                         [ 0.1009,  0.0149, -0.2589],\n",
       "                         [ 0.0329,  0.3464, -0.2381]],\n",
       "               \n",
       "                        [[-0.0556, -0.3781,  0.4143],\n",
       "                         [ 0.1141,  0.0238, -0.1336],\n",
       "                         [-0.2396,  0.2522, -0.0435]],\n",
       "               \n",
       "                        [[ 0.1396, -0.3227,  0.3076],\n",
       "                         [ 0.1571,  0.0654, -0.2792],\n",
       "                         [-0.1862,  0.3246, -0.1105]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1351,  0.0300, -0.1223],\n",
       "                         [ 0.1270,  0.0048, -0.1334],\n",
       "                         [ 0.0810, -0.0188, -0.1408]],\n",
       "               \n",
       "                        [[ 0.2396, -0.0822, -0.3059],\n",
       "                         [ 0.4441, -0.0444, -0.3420],\n",
       "                         [ 0.2833,  0.0272, -0.3057]],\n",
       "               \n",
       "                        [[ 0.4209,  0.0246, -0.1738],\n",
       "                         [ 0.3504, -0.0090, -0.4486],\n",
       "                         [ 0.1675,  0.0007, -0.2938]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.2755,  0.0557, -0.1066],\n",
       "                         [ 0.2694,  0.0838, -0.0686],\n",
       "                         [-0.3570,  0.3844,  0.2124]],\n",
       "               \n",
       "                        [[-0.2199, -0.1073,  0.1281],\n",
       "                         [ 0.3428, -0.3713, -0.2265],\n",
       "                         [-0.2334,  0.1225,  0.0139]],\n",
       "               \n",
       "                        [[-0.1144,  0.2898,  0.2389],\n",
       "                         [ 0.4023, -0.1239,  0.0823],\n",
       "                         [-0.0536, -0.0270, -0.0893]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0725,  0.1539,  0.2607],\n",
       "                         [-0.0546,  0.3192,  0.3256],\n",
       "                         [-0.1568, -0.0160, -0.0163]],\n",
       "               \n",
       "                        [[ 0.0523, -0.1319,  0.1332],\n",
       "                         [-0.0675, -0.1848,  0.0010],\n",
       "                         [-0.4852, -0.6518, -0.3679]],\n",
       "               \n",
       "                        [[-0.0099, -0.1169,  0.1932],\n",
       "                         [ 0.1625,  0.0150,  0.3123],\n",
       "                         [ 0.0098, -0.2516, -0.1048]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.5208e-02,  2.5390e-02,  3.0018e-03,  4.4591e-03, -2.9137e-03,\n",
       "                        4.0675e-03,  7.9458e-04, -2.5815e-04,  4.9559e-03, -6.6129e-04,\n",
       "                       -5.4219e-04,  5.1219e-03,  1.0490e-02, -2.6039e-03,  7.3144e-03,\n",
       "                        1.3565e-02, -1.1802e-02,  6.4440e-03, -1.9754e-04,  1.4577e-03,\n",
       "                       -9.9297e-03, -1.3110e-03,  7.1923e-04,  1.2678e-03, -2.2003e-03,\n",
       "                        1.9842e-03, -1.6205e-03,  8.0651e-03, -3.1640e-03, -5.3812e-03,\n",
       "                        4.0815e-03, -1.4013e-03,  5.3016e-03,  2.1504e-02, -2.9504e-03,\n",
       "                        3.1361e-03,  6.3633e-04,  3.5619e-03, -3.3044e-03, -6.8186e-03,\n",
       "                       -3.2047e-04,  7.1727e-03, -2.1244e-02, -6.7985e-03, -2.5285e-03,\n",
       "                        2.8363e-02, -4.6667e-03,  2.3479e-03,  1.6716e-03,  2.9250e-02,\n",
       "                       -4.1845e-03, -6.1779e-03,  9.1810e-03, -2.2454e-03,  1.4015e-02,\n",
       "                        4.7800e-03, -6.7125e-03, -2.9676e-02, -4.2823e-03,  1.5072e-02,\n",
       "                       -2.1694e-05,  2.5195e-02, -4.5476e-03, -5.0114e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 8.0872e-01,  2.5826e-02,  1.2239e+00, -1.0701e-01,  1.2125e-01,\n",
       "                        1.1130e-01, -5.3401e-01,  5.9129e-01, -7.1830e-01, -7.9132e-01,\n",
       "                        2.9149e+00, -5.7074e-02,  8.5595e-02, -1.1273e+00,  3.7595e-01,\n",
       "                       -1.1730e-01, -3.7074e-02, -3.3385e-01,  4.1393e-02, -2.6988e-02,\n",
       "                        8.5710e-01, -4.7896e-01, -4.6541e-01, -6.2972e-01, -6.3148e-01,\n",
       "                       -6.8662e-01, -6.1019e-01,  6.1170e-01, -1.0183e+00, -8.1659e-02,\n",
       "                        2.2809e-01, -4.1276e-01,  7.8192e-01, -5.0978e-02,  1.6534e-01,\n",
       "                       -7.8645e-01, -2.9839e-01,  4.1982e-01, -5.4669e-01, -5.4246e-01,\n",
       "                       -1.9608e-01, -3.7881e-02, -2.7093e-01, -1.2673e-02, -3.5418e-01,\n",
       "                       -1.8115e-01, -5.5975e-01, -5.2892e-01, -8.6472e-02, -7.8257e-02,\n",
       "                       -6.4245e-01, -4.2533e-02,  1.2028e-01, -5.7961e-01, -1.6516e-03,\n",
       "                       -7.2728e-01, -8.3224e-01, -4.8898e-02, -5.6684e-01,  1.0662e-01,\n",
       "                       -6.3893e-01, -1.3155e-01, -2.9047e-01, -3.5173e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.1498, 1.1812, 0.8218, 1.0698, 0.5014, 0.6273, 0.7548, 0.9297, 0.5291,\n",
       "                       0.8114, 0.4525, 1.2003, 1.0357, 0.7697, 0.9369, 1.0944, 1.1046, 0.6966,\n",
       "                       1.4008, 0.5814, 0.8688, 0.4863, 0.6664, 0.7144, 0.5125, 0.7753, 0.6545,\n",
       "                       0.5573, 0.8309, 0.5479, 0.5523, 0.5066, 1.0102, 1.4536, 0.3886, 0.8612,\n",
       "                       0.5718, 0.8246, 0.7692, 0.6788, 0.6182, 1.0388, 1.1315, 1.3241, 0.5688,\n",
       "                       1.1215, 0.7452, 0.6321, 0.5195, 0.9082, 0.6999, 0.7712, 1.0227, 0.9327,\n",
       "                       0.7435, 0.7165, 1.1134, 1.4649, 0.7982, 0.8505, 0.5951, 1.3962, 0.5888,\n",
       "                       0.7667], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-4.1312e-01, -1.6227e-04,  3.4149e-01],\n",
       "                         [-2.7282e-01,  4.6796e-02,  2.4729e-02],\n",
       "                         [ 1.1285e-01, -8.7938e-03, -1.3949e-01]],\n",
       "               \n",
       "                        [[ 7.0703e-02,  2.7661e-01,  1.8934e-01],\n",
       "                         [ 7.5093e-02, -1.0068e-01,  5.5375e-01],\n",
       "                         [-5.5755e-01, -1.3451e-01,  9.0573e-02]],\n",
       "               \n",
       "                        [[ 2.7900e-01, -1.9277e-02, -1.9884e-01],\n",
       "                         [-1.1705e-02, -8.3467e-03, -5.1389e-02],\n",
       "                         [ 2.0653e-01,  1.1718e-02,  7.6235e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.2917e-03, -1.7905e-01,  7.4060e-03],\n",
       "                         [ 2.6916e-01,  1.7755e-01, -3.1700e-01],\n",
       "                         [ 2.6055e-01,  1.5571e-01, -9.0200e-02]],\n",
       "               \n",
       "                        [[-1.8256e-02, -3.0389e-02,  1.2654e-01],\n",
       "                         [ 2.2712e-01,  6.9651e-02,  3.3226e-01],\n",
       "                         [ 1.0604e-02,  9.9532e-02,  1.0151e-02]],\n",
       "               \n",
       "                        [[-1.7045e-01,  5.6753e-02,  1.6290e-01],\n",
       "                         [-5.5549e-02, -1.4126e-01,  2.7278e-01],\n",
       "                         [-3.6538e-03,  6.0281e-02,  2.7530e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.1861e-01, -3.1472e-01,  4.1519e-01],\n",
       "                         [ 3.4928e-01,  3.4819e-01,  3.8559e-01],\n",
       "                         [-6.3390e-02,  8.4232e-02, -5.4491e-02]],\n",
       "               \n",
       "                        [[ 1.2718e-01, -1.3559e-01, -4.7782e-02],\n",
       "                         [ 4.1307e-02,  1.7878e-01,  1.2362e-01],\n",
       "                         [ 1.0239e-01, -9.0365e-03,  4.9777e-03]],\n",
       "               \n",
       "                        [[-3.3369e-01, -3.7403e-01,  3.4861e-01],\n",
       "                         [ 2.1765e-01, -1.3867e-02,  3.8202e-01],\n",
       "                         [-1.6896e-01,  2.3497e-01, -9.0130e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5692e-01, -3.4343e-01,  1.9818e-01],\n",
       "                         [ 3.1419e-01, -1.5508e-02,  2.5334e-01],\n",
       "                         [ 1.6515e-01,  2.5922e-01,  6.5351e-02]],\n",
       "               \n",
       "                        [[ 1.0458e-01,  3.0878e-02, -6.3882e-02],\n",
       "                         [ 1.8227e-01,  7.6744e-02, -1.4133e-01],\n",
       "                         [ 3.3930e-02, -4.1072e-02, -8.5017e-02]],\n",
       "               \n",
       "                        [[-6.1071e-02, -4.2596e-01, -2.4957e-01],\n",
       "                         [ 3.2059e-02,  2.9744e-02, -1.2323e-01],\n",
       "                         [-1.1508e-01, -5.1141e-02, -1.8648e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.4992e-01, -1.6823e-01,  8.6237e-02],\n",
       "                         [ 9.1967e-02,  5.9074e-02,  4.1728e-01],\n",
       "                         [-1.0201e-01,  1.4281e-01,  6.1781e-01]],\n",
       "               \n",
       "                        [[ 4.0766e-01, -2.1267e-01, -1.5046e-01],\n",
       "                         [ 3.6136e-01, -1.8711e-01, -1.6294e-01],\n",
       "                         [-8.1158e-02, -7.2800e-02, -2.7036e-02]],\n",
       "               \n",
       "                        [[-1.6823e-01,  1.3958e-02, -3.6508e-01],\n",
       "                         [-9.4534e-02,  7.0387e-03,  1.2322e-02],\n",
       "                         [-2.4264e-01, -3.9740e-01,  1.0883e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.6410e-02,  1.0301e-01, -1.1451e-02],\n",
       "                         [-9.3354e-02,  2.5274e-01,  1.5763e-01],\n",
       "                         [-3.1825e-02,  3.6326e-01,  2.6193e-01]],\n",
       "               \n",
       "                        [[-5.0392e-02, -3.2163e-01, -4.2966e-01],\n",
       "                         [ 5.7021e-02, -1.1543e-01, -3.7738e-01],\n",
       "                         [ 5.9241e-03, -8.7481e-02,  7.7213e-02]],\n",
       "               \n",
       "                        [[-7.3269e-02, -1.1449e-01, -9.1360e-02],\n",
       "                         [ 1.6103e-01, -7.3530e-02,  1.6078e-01],\n",
       "                         [-1.2673e-01, -8.8461e-02,  7.4887e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.5065e-01,  4.7147e-02, -8.0424e-02],\n",
       "                         [ 2.4274e-01,  4.5222e-01,  4.2784e-01],\n",
       "                         [-2.3557e-01, -3.4108e-01, -1.1743e-01]],\n",
       "               \n",
       "                        [[ 1.7934e-01,  1.7717e-01, -1.6228e-01],\n",
       "                         [-1.0346e-01, -9.9568e-03,  7.8267e-02],\n",
       "                         [-3.0269e-02,  1.7913e-01,  1.1449e-01]],\n",
       "               \n",
       "                        [[-3.3596e-01, -2.2693e-01, -3.2233e-01],\n",
       "                         [ 1.7807e-01, -1.9699e-01,  2.4574e-01],\n",
       "                         [-1.3650e-01,  9.6379e-02,  3.7812e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.1751e-01, -2.3717e-01,  2.1112e-01],\n",
       "                         [-3.0091e-01, -5.8008e-01,  2.7802e-01],\n",
       "                         [-5.9415e-01, -6.1847e-01,  1.8349e-01]],\n",
       "               \n",
       "                        [[-3.9316e-01, -2.0028e-01, -2.2816e-01],\n",
       "                         [ 1.2015e-02, -1.8270e-02, -9.4287e-02],\n",
       "                         [-1.3077e-01, -1.1800e-01,  4.8534e-02]],\n",
       "               \n",
       "                        [[-1.8961e-01, -1.4458e-01, -1.5945e-01],\n",
       "                         [ 2.4488e-01,  1.2875e-01,  1.4808e-01],\n",
       "                         [ 1.0853e-01,  8.6955e-02,  1.8913e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.2424e-02,  2.9699e-02,  4.5690e-02],\n",
       "                         [-8.6672e-04,  2.1960e-01, -2.7214e-01],\n",
       "                         [-4.6134e-02,  2.4270e-01, -1.3231e-01]],\n",
       "               \n",
       "                        [[-4.3817e-01, -3.1063e-01,  4.6949e-02],\n",
       "                         [-2.7266e-01,  4.6096e-01,  5.1677e-01],\n",
       "                         [-9.2784e-02,  4.4825e-01, -1.9910e-02]],\n",
       "               \n",
       "                        [[ 1.4898e-02, -1.1330e-01,  1.4584e-01],\n",
       "                         [ 2.0769e-01,  2.8786e-01, -2.8111e-01],\n",
       "                         [ 3.3062e-01, -9.0687e-02, -1.9024e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.6986e-01, -1.0008e-01, -7.0395e-02],\n",
       "                         [-2.0178e-02, -2.0115e-01, -2.3709e-01],\n",
       "                         [-1.1529e-01,  5.6939e-02, -1.2280e-01]],\n",
       "               \n",
       "                        [[-1.5446e-01, -2.4197e-02,  7.3777e-02],\n",
       "                         [-2.4365e-01,  5.5023e-03, -5.8357e-02],\n",
       "                         [-2.2152e-01,  2.7428e-01,  7.5736e-02]],\n",
       "               \n",
       "                        [[ 3.2301e-01,  4.9260e-01,  2.1876e-01],\n",
       "                         [ 3.9994e-01,  2.9845e-01,  4.9643e-01],\n",
       "                         [ 3.1532e-01,  4.5981e-02, -9.0076e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.7460e-01, -5.7188e-02,  6.0190e-01],\n",
       "                         [-2.6395e-01,  1.3636e-01,  2.7728e-01],\n",
       "                         [-4.6194e-02,  1.7979e-01,  5.6984e-02]],\n",
       "               \n",
       "                        [[ 4.0716e-01, -5.9650e-01, -8.3434e-02],\n",
       "                         [ 3.9790e-01, -3.5902e-01, -4.1521e-02],\n",
       "                         [ 3.2628e-01,  1.7219e-01, -3.5921e-01]],\n",
       "               \n",
       "                        [[ 1.7571e-01,  1.7559e-01,  1.0606e-01],\n",
       "                         [-1.5128e-01,  1.1447e-01,  7.3714e-02],\n",
       "                         [-2.0796e-02, -4.0582e-02,  6.2108e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.8890e-02,  1.6680e-02, -3.0398e-01],\n",
       "                         [ 1.7464e-02, -5.2477e-02, -3.8481e-01],\n",
       "                         [ 7.6514e-03, -6.2485e-02, -2.2321e-01]],\n",
       "               \n",
       "                        [[-1.7990e-01,  2.4814e-02,  3.4500e-01],\n",
       "                         [-3.2221e-01, -1.7785e-01, -4.5222e-02],\n",
       "                         [-1.6598e-01,  1.2593e-01, -1.6283e-01]],\n",
       "               \n",
       "                        [[-3.2473e-01, -1.4978e-01,  6.6387e-02],\n",
       "                         [ 1.0651e-01,  3.9147e-02,  1.6382e-01],\n",
       "                         [ 1.0310e-01,  3.5614e-01,  3.3206e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 8.3946e-04,  2.1957e-07,  1.1680e-01,  2.0517e-04,  6.0640e-05,\n",
       "                        2.2141e-02,  1.7357e-03,  5.7493e-07,  1.6405e-04, -2.8985e-04,\n",
       "                       -8.1576e-02, -2.5883e-03, -1.8408e-04,  3.4794e-05,  2.4704e-02,\n",
       "                        4.9052e-07, -2.0010e-02,  6.3993e-07, -5.6833e-03,  1.0538e-01,\n",
       "                        2.2632e-04, -1.0772e-01, -7.1650e-07,  6.3195e-05,  1.1582e-04,\n",
       "                       -2.0300e-03,  9.1178e-03,  9.5440e-04, -2.6446e-05, -8.8203e-04,\n",
       "                        5.1651e-04,  4.6882e-07, -1.2266e-04,  2.1099e-07, -9.1963e-02,\n",
       "                       -7.9286e-04,  9.1024e-02,  1.7741e-03, -2.3530e-07, -2.6480e-05,\n",
       "                        1.4779e-06,  2.7982e-04, -5.6503e-08, -8.0382e-07,  2.7161e-07,\n",
       "                       -2.3603e-07,  1.2290e-05, -8.9107e-05,  2.9923e-06,  4.4382e-04,\n",
       "                       -9.6389e-08,  2.2106e-02,  4.7107e-02, -8.8201e-04,  1.1469e-03,\n",
       "                        1.2126e-03, -7.6101e-07, -3.6058e-07,  6.7052e-03,  1.4530e-05,\n",
       "                       -5.2367e-10, -1.4045e-07, -1.7687e-02, -9.7846e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.6055, -0.6915, -0.6770, -0.4774, -0.6282, -0.6305, -0.5012, -0.5488,\n",
       "                       -0.6268, -0.2561, -0.7700, -0.4652, -0.6963, -0.6007, -0.6108, -0.5112,\n",
       "                       -0.4781, -0.4754, -0.5327, -0.3073, -0.3028, -0.6266, -0.4079, -0.6617,\n",
       "                       -0.5344, -0.4081, -0.5497, -0.3602, -0.3539, -0.6528, -0.5251, -0.5619,\n",
       "                       -0.5266, -0.6200, -0.5625, -0.5409, -0.5085, -0.4758, -0.4150, -0.6193,\n",
       "                       -0.6741, -0.2140, -0.5757, -0.5481, -0.5537, -0.4012, -0.3273, -0.5482,\n",
       "                       -0.5033, -0.3426, -0.3942, -0.5103, -0.4979, -0.4703, -0.4016, -0.1690,\n",
       "                       -0.4717, -0.5947, -0.6240, -0.4320, -0.6565, -0.6030, -0.7339, -0.3132],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.8806, 0.8063, 0.8719, 0.8357, 1.0389, 0.8047, 2.3151, 1.0354, 0.9910,\n",
       "                       0.8140, 1.0708, 1.0462, 0.8540, 0.8289, 1.1455, 1.0259, 0.8599, 0.7321,\n",
       "                       1.0163, 0.8306, 0.6740, 0.8763, 0.9151, 1.1967, 0.7201, 0.8903, 0.6978,\n",
       "                       0.7693, 0.5200, 0.9728, 0.8608, 0.7569, 0.8091, 0.6830, 0.9390, 0.8720,\n",
       "                       0.9813, 0.9509, 0.9292, 0.9856, 0.7588, 0.6817, 0.9465, 0.7923, 0.5713,\n",
       "                       0.9223, 0.9319, 0.9571, 1.0952, 0.7023, 0.7844, 0.8601, 0.9491, 0.8667,\n",
       "                       0.6732, 0.8439, 0.9650, 0.8608, 0.9666, 0.7852, 0.9246, 1.0141, 0.6330,\n",
       "                       0.8805], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[ 0.0293,  0.0202, -0.2089],\n",
       "                         [ 0.2703,  0.1298,  0.1432],\n",
       "                         [ 0.1551,  0.1138,  0.1775]],\n",
       "               \n",
       "                        [[-0.2334,  0.0632,  0.2549],\n",
       "                         [-0.2902,  0.0324, -0.1055],\n",
       "                         [-0.2416,  0.0756,  0.0254]],\n",
       "               \n",
       "                        [[-0.5350, -0.1650,  0.2494],\n",
       "                         [-0.1651, -0.1873,  0.0443],\n",
       "                         [-0.3499,  0.0968, -0.0916]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0138,  0.0708, -0.2529],\n",
       "                         [ 0.0858, -0.1280,  0.3195],\n",
       "                         [-0.2200, -0.1656,  0.2772]],\n",
       "               \n",
       "                        [[-0.1289,  0.1380,  0.0103],\n",
       "                         [-0.1484, -0.0465, -0.2708],\n",
       "                         [ 0.1239,  0.0019, -0.4340]],\n",
       "               \n",
       "                        [[-0.0010,  0.0354,  0.0090],\n",
       "                         [ 0.0540, -0.1659,  0.0626],\n",
       "                         [-0.1344,  0.1282,  0.2653]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0035,  0.1724,  0.3277],\n",
       "                         [-0.3252, -0.2051, -0.1299],\n",
       "                         [ 0.0945, -0.0098, -0.0422]],\n",
       "               \n",
       "                        [[-0.0042,  0.1585,  0.1007],\n",
       "                         [ 0.0242, -0.2068, -0.4225],\n",
       "                         [-0.0060, -0.8780, -0.2091]],\n",
       "               \n",
       "                        [[-0.5184, -0.4833, -0.5374],\n",
       "                         [-0.0651,  0.0292, -0.1154],\n",
       "                         [-0.5395,  0.0945,  0.0413]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.2891, -0.0908, -0.2147],\n",
       "                         [ 0.7691,  0.4875,  0.2277],\n",
       "                         [-0.0603, -0.0732, -0.0652]],\n",
       "               \n",
       "                        [[ 0.0106,  0.0218, -0.1000],\n",
       "                         [-0.1751, -0.1757,  0.1251],\n",
       "                         [-0.1052, -0.1156, -0.2452]],\n",
       "               \n",
       "                        [[ 0.5834,  0.0433,  0.2026],\n",
       "                         [ 0.2627,  0.1097, -0.3780],\n",
       "                         [ 0.1550, -0.2117, -0.2655]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.2862, -0.0910,  0.1750],\n",
       "                         [-0.0713, -0.2095,  0.1320],\n",
       "                         [ 0.2999,  0.0164,  0.4075]],\n",
       "               \n",
       "                        [[-0.2801, -0.4956,  0.0835],\n",
       "                         [ 0.3225, -0.3413, -0.0416],\n",
       "                         [ 0.2398, -0.0393, -0.1745]],\n",
       "               \n",
       "                        [[-0.3432,  0.0930, -0.0441],\n",
       "                         [-0.3411,  0.1800, -0.0563],\n",
       "                         [-0.1996, -0.0956,  0.1358]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0610, -0.5752,  0.0910],\n",
       "                         [ 0.3007, -0.1427,  0.5950],\n",
       "                         [ 0.6356,  0.4850,  0.4682]],\n",
       "               \n",
       "                        [[ 0.1228,  0.0025,  0.0575],\n",
       "                         [ 0.1039, -0.3849, -0.0541],\n",
       "                         [ 0.1398, -0.1747,  0.0908]],\n",
       "               \n",
       "                        [[-0.1602, -0.4428,  0.0932],\n",
       "                         [-0.0654, -0.0180,  0.2337],\n",
       "                         [ 0.1641,  0.0747,  0.6197]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.2420,  0.0244, -0.0919],\n",
       "                         [ 0.1542, -0.3867, -0.1447],\n",
       "                         [ 0.3646, -0.0167,  0.2486]],\n",
       "               \n",
       "                        [[ 0.4339,  0.1363,  0.2711],\n",
       "                         [ 0.0937, -0.7207, -0.2676],\n",
       "                         [ 0.1388,  0.3066,  0.0208]],\n",
       "               \n",
       "                        [[ 0.0706, -0.3865,  0.1560],\n",
       "                         [ 0.0801, -0.2461,  0.0425],\n",
       "                         [ 0.0364,  0.2323, -0.1042]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0801,  0.2931, -0.3949],\n",
       "                         [ 0.2251,  0.1216,  0.1971],\n",
       "                         [ 0.6196,  0.1732, -0.6708]],\n",
       "               \n",
       "                        [[-0.2675,  0.1265, -0.1134],\n",
       "                         [-0.2754,  0.0215, -0.1751],\n",
       "                         [-0.1676, -0.1214, -0.1655]],\n",
       "               \n",
       "                        [[-0.3325,  0.3404, -0.1830],\n",
       "                         [-0.0887, -0.0221, -0.2632],\n",
       "                         [-0.1530, -0.1690,  0.3367]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.1754,  0.0860, -0.3271],\n",
       "                         [-0.5400,  0.3389, -0.3477],\n",
       "                         [-0.6966, -0.2006, -0.0396]],\n",
       "               \n",
       "                        [[-0.0479,  0.1520, -0.0949],\n",
       "                         [ 0.1086,  0.0112, -0.3057],\n",
       "                         [ 0.0081,  0.0472,  0.1206]],\n",
       "               \n",
       "                        [[-0.5223, -0.5432,  0.0561],\n",
       "                         [-0.1210,  0.0216,  0.1901],\n",
       "                         [-0.1737, -0.2144, -0.1969]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0877, -0.0887, -0.2056],\n",
       "                         [ 0.0897, -0.0877, -0.1655],\n",
       "                         [ 0.2484,  0.1256,  0.1416]],\n",
       "               \n",
       "                        [[ 0.0635,  0.0742, -0.2207],\n",
       "                         [ 0.2093,  0.2069,  0.3509],\n",
       "                         [ 0.1927, -0.1008,  0.2833]],\n",
       "               \n",
       "                        [[-0.2218, -0.1303,  0.1885],\n",
       "                         [-0.2288,  0.1588,  0.2164],\n",
       "                         [-0.4227,  0.3029, -0.3153]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.4826, -0.1190, -0.3913],\n",
       "                         [-0.0479, -0.5462, -0.8011],\n",
       "                         [ 0.0999,  0.2228, -0.0523]],\n",
       "               \n",
       "                        [[-0.1725, -0.2090, -0.3677],\n",
       "                         [ 0.2159, -0.1996,  0.1702],\n",
       "                         [ 0.0941,  0.2919,  0.1278]],\n",
       "               \n",
       "                        [[ 0.0977, -0.0570, -0.1297],\n",
       "                         [ 0.0119, -0.3086, -0.6344],\n",
       "                         [-0.1449,  0.0366, -0.0149]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0652, -0.1558,  0.0617],\n",
       "                         [-0.1388, -0.0211,  0.1335],\n",
       "                         [-0.3578, -0.5395, -0.2666]],\n",
       "               \n",
       "                        [[ 0.1172,  0.1629, -0.0618],\n",
       "                         [-0.1939, -0.0199, -0.4162],\n",
       "                         [ 0.1782, -0.1845, -0.4947]],\n",
       "               \n",
       "                        [[ 0.2719,  0.4247, -0.0297],\n",
       "                         [ 0.0898,  0.2643, -0.4921],\n",
       "                         [-0.5011, -0.1873, -0.4014]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-6.8359e-04, -1.2729e-07, -3.9194e-08,  2.1041e-02,  2.8296e-02,\n",
       "                       -5.1110e-03, -1.0751e-06, -8.4118e-04,  4.4485e-04, -9.3722e-04,\n",
       "                        1.0010e-06,  2.7897e-04,  3.3267e-03, -2.5312e-07,  4.5552e-05,\n",
       "                       -1.8997e-04, -3.3249e-07,  2.2820e-02,  1.0542e-03, -3.1453e-02,\n",
       "                       -4.5750e-07,  1.1882e-02, -6.7375e-04, -1.7488e-06,  2.6364e-04,\n",
       "                        1.0105e-03, -1.2150e-06,  3.0385e-03, -1.0151e-03, -1.0925e-04,\n",
       "                       -2.1605e-06, -5.9022e-04, -4.0256e-04, -7.7848e-07, -2.3387e-02,\n",
       "                        8.7584e-04,  2.8271e-02,  1.1301e-03,  8.5861e-04,  2.6547e-06,\n",
       "                        3.4517e-03,  4.6462e-05, -2.9216e-02, -4.3997e-04, -3.3492e-07,\n",
       "                       -1.3912e-02,  9.4622e-07, -2.0972e-03, -6.5266e-06,  1.7638e-02,\n",
       "                       -9.4917e-06, -1.6230e-03, -3.5956e-07, -3.0157e-07, -5.0374e-04,\n",
       "                       -1.0613e-03, -6.9548e-05, -1.2844e-06, -4.6184e-06,  2.7330e-02,\n",
       "                        6.9338e-05, -1.0443e-07,  1.2881e-01, -5.7935e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.8344, -0.8144, -0.7412, -1.0507, -0.7844, -0.5525, -0.6862, -1.0505,\n",
       "                       -0.6986, -0.5469, -0.6094, -0.7939, -0.6307, -0.4779, -0.9206, -0.9028,\n",
       "                       -0.6207, -0.8258, -0.6456, -0.7169, -0.6839, -1.4032, -0.6040, -0.6868,\n",
       "                       -0.8061, -0.4387, -0.5660, -0.8220, -0.4953, -0.5383, -0.5935, -0.9333,\n",
       "                       -0.4512, -0.4936, -0.9524, -0.6048, -0.5859, -1.4659, -0.5854, -0.5569,\n",
       "                       -0.7219, -0.8884, -0.7906, -0.8391, -0.5877, -0.9876, -1.0592, -0.7290,\n",
       "                       -1.0113, -0.6870, -0.4038, -0.7048, -0.7637, -0.7767, -0.5388, -0.6816,\n",
       "                       -0.7309, -0.6666, -0.3682, -0.5092, -0.3153, -0.5867, -0.7741, -0.9918],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.8512, 0.9119, 0.6848, 0.9596, 0.8933, 0.5754, 0.8779, 0.9186, 0.5152,\n",
       "                       0.7092, 0.6319, 0.9775, 0.7759, 0.6694, 0.8938, 0.9525, 0.7944, 0.7831,\n",
       "                       0.7362, 0.7650, 0.8194, 1.1823, 0.8457, 0.9413, 0.7724, 0.7131, 0.6917,\n",
       "                       0.8837, 0.6799, 0.7607, 0.6679, 1.1073, 0.7492, 0.6311, 0.8731, 0.9611,\n",
       "                       0.6677, 1.2616, 0.6331, 0.6495, 0.9334, 0.9178, 0.9251, 0.7697, 0.8864,\n",
       "                       0.7987, 1.9401, 0.7002, 0.9865, 0.7275, 0.5773, 0.7267, 0.7392, 0.7266,\n",
       "                       0.6494, 0.7844, 0.8921, 0.8147, 0.8713, 0.8947, 0.7246, 0.6482, 0.9462,\n",
       "                       0.9071], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-8.7552e-02,  3.6811e-02,  1.0680e-01],\n",
       "                         [-2.1167e-01, -1.1866e-01, -9.5680e-02],\n",
       "                         [-1.0814e-01, -1.4503e-01,  8.5851e-02]],\n",
       "               \n",
       "                        [[ 1.6228e-01, -2.6700e-02,  2.0427e-02],\n",
       "                         [-1.7327e-01, -7.3424e-02,  6.7961e-02],\n",
       "                         [-3.5029e-01, -8.9401e-02, -1.5019e-01]],\n",
       "               \n",
       "                        [[ 1.8555e-01,  1.9061e-01, -3.2346e-02],\n",
       "                         [ 1.1673e-01, -2.0460e-02,  2.2612e-01],\n",
       "                         [ 1.2261e-02, -3.8869e-02, -1.2678e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0088e-01, -8.4761e-02,  1.9062e-02],\n",
       "                         [-1.9444e-01, -3.6870e-02, -1.2929e-01],\n",
       "                         [-9.4600e-03,  1.2700e-01, -2.5486e-02]],\n",
       "               \n",
       "                        [[ 2.0866e-01, -1.6317e-01, -1.6008e-02],\n",
       "                         [-1.2659e-01, -6.0073e-01, -3.3631e-01],\n",
       "                         [ 1.6573e-01, -4.4183e-01, -4.1603e-02]],\n",
       "               \n",
       "                        [[-3.1271e-02,  2.0477e-01,  5.1222e-02],\n",
       "                         [-8.7513e-02,  1.1851e-01,  1.1708e-01],\n",
       "                         [ 4.1252e-02,  1.2924e-01, -6.1048e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.4503e-02, -1.0228e-02,  8.7460e-02],\n",
       "                         [ 1.1532e-01, -6.4924e-02,  1.1366e-01],\n",
       "                         [ 6.4974e-02,  1.6366e-01,  1.3749e-02]],\n",
       "               \n",
       "                        [[-1.1784e-01, -2.2914e-01,  1.4545e-01],\n",
       "                         [-5.7581e-02,  6.0533e-02,  6.7308e-03],\n",
       "                         [-7.8829e-02, -1.8696e-02,  1.0146e-01]],\n",
       "               \n",
       "                        [[ 6.6641e-02, -5.9701e-02, -9.8120e-02],\n",
       "                         [ 4.4077e-02,  2.9778e-02, -1.3222e-02],\n",
       "                         [ 1.4897e-01,  2.9066e-02,  5.4808e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.2293e-02,  4.1223e-02, -1.8854e-01],\n",
       "                         [-2.1913e-02, -6.5991e-03, -6.4842e-02],\n",
       "                         [-2.1595e-01,  1.5484e-01,  1.0032e-01]],\n",
       "               \n",
       "                        [[-4.6857e-01, -9.0480e-02, -2.8663e-02],\n",
       "                         [-3.5118e-01,  1.5181e-02,  1.2960e-01],\n",
       "                         [-3.6663e-01,  5.1299e-03, -2.4004e-01]],\n",
       "               \n",
       "                        [[ 6.8407e-02, -5.4910e-02,  7.7420e-03],\n",
       "                         [-1.7025e-02, -1.5229e-01, -1.0877e-01],\n",
       "                         [-4.9169e-02, -9.0574e-02,  8.3049e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.3926e-01, -2.4775e-02, -8.9090e-02],\n",
       "                         [-1.4159e-01, -1.1241e-01, -1.0575e-01],\n",
       "                         [-1.5336e-01, -7.1448e-02,  1.7972e-02]],\n",
       "               \n",
       "                        [[ 2.8294e-02,  3.1177e-02, -5.8147e-02],\n",
       "                         [ 1.7637e-01,  2.6780e-02, -3.7790e-02],\n",
       "                         [ 3.7604e-02, -5.3999e-02, -2.1041e-02]],\n",
       "               \n",
       "                        [[ 5.2606e-02,  8.8243e-02,  5.4595e-02],\n",
       "                         [-2.3525e-03, -3.5199e-02,  1.5066e-02],\n",
       "                         [ 1.8567e-01,  1.0040e-01,  1.4200e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.2891e-01, -1.1475e-02, -1.9876e-01],\n",
       "                         [ 2.8312e-02, -4.9082e-02,  3.4412e-03],\n",
       "                         [ 5.6332e-02, -4.8585e-02, -1.5256e-02]],\n",
       "               \n",
       "                        [[ 1.1331e-01, -1.2464e-01,  1.5755e-01],\n",
       "                         [-2.0749e-01, -3.3079e-01, -2.2370e-01],\n",
       "                         [-8.5103e-02, -2.4210e-01, -1.8489e-01]],\n",
       "               \n",
       "                        [[ 1.1883e-01, -9.8887e-02, -2.0212e-02],\n",
       "                         [-1.6425e-02,  1.4799e-01, -4.7146e-02],\n",
       "                         [-3.7712e-02,  1.2866e-01,  7.6608e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.1921e-02, -1.0809e-01, -1.0977e-01],\n",
       "                         [-2.1153e-01, -9.1810e-03,  4.4138e-05],\n",
       "                         [-6.6231e-02, -1.8295e-01, -1.7007e-01]],\n",
       "               \n",
       "                        [[ 1.2970e-01,  6.8886e-02,  9.4518e-02],\n",
       "                         [ 5.8465e-02,  6.5637e-02,  2.2563e-03],\n",
       "                         [ 9.3079e-02, -2.5285e-02,  1.3603e-02]],\n",
       "               \n",
       "                        [[ 1.6936e-01,  2.8188e-01,  2.4319e-02],\n",
       "                         [ 2.2095e-02, -1.9907e-01, -2.1199e-01],\n",
       "                         [ 7.5583e-02, -3.5509e-02, -4.1352e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.8961e-02, -1.5039e-01, -1.9954e-01],\n",
       "                         [ 1.6779e-01,  2.2395e-02,  7.9977e-02],\n",
       "                         [-1.1588e-01, -2.1363e-02, -1.0905e-01]],\n",
       "               \n",
       "                        [[-1.0939e-01, -2.5495e-01, -2.9316e-01],\n",
       "                         [ 1.2936e-01, -1.0913e-01, -5.2845e-02],\n",
       "                         [-1.0139e-01, -4.4939e-01, -6.2750e-02]],\n",
       "               \n",
       "                        [[ 7.2664e-02,  1.1334e-02,  2.3803e-02],\n",
       "                         [ 2.8046e-02, -9.9811e-03, -5.1437e-02],\n",
       "                         [-5.8573e-02,  1.7868e-01, -5.1317e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.7244e-01, -1.3719e-01, -1.8414e-01],\n",
       "                         [-7.6278e-02, -1.1060e-01, -3.3203e-01],\n",
       "                         [ 1.6588e-01, -1.7144e-02, -1.2294e-02]],\n",
       "               \n",
       "                        [[-1.9968e-01, -2.3376e-01,  1.3382e-01],\n",
       "                         [ 4.2853e-02, -4.4808e-02, -1.1190e-01],\n",
       "                         [ 6.1613e-02, -2.5792e-02,  9.3901e-02]],\n",
       "               \n",
       "                        [[ 2.2107e-01,  7.2173e-02, -7.4735e-02],\n",
       "                         [-2.5611e-01,  7.5172e-02, -6.1458e-02],\n",
       "                         [-9.1648e-02, -2.6604e-01,  2.1997e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.1053e-02, -3.0245e-01, -1.2360e-01],\n",
       "                         [-2.3272e-01, -2.3561e-01, -2.0235e-01],\n",
       "                         [-1.5596e-01,  1.4241e-01, -1.1050e-01]],\n",
       "               \n",
       "                        [[ 1.6283e-02, -3.3806e-01, -2.0639e-01],\n",
       "                         [-1.3749e-01, -3.9942e-01, -1.8252e-02],\n",
       "                         [-2.9007e-01, -3.3615e-01, -2.8155e-01]],\n",
       "               \n",
       "                        [[-9.6216e-02, -4.9089e-02,  1.7011e-01],\n",
       "                         [ 3.0193e-01,  3.5524e-01,  1.1164e-01],\n",
       "                         [-5.1861e-02,  3.7790e-02, -6.6191e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.6755e-02, -1.5038e-01,  5.7528e-02],\n",
       "                         [-6.7016e-02,  5.2072e-01,  6.2419e-01],\n",
       "                         [-1.6548e-01,  9.4394e-02, -9.6652e-02]],\n",
       "               \n",
       "                        [[-3.1437e-01, -1.0235e-01, -1.4318e-01],\n",
       "                         [-3.3240e-01, -2.2377e-01,  2.8114e-02],\n",
       "                         [-6.1355e-01,  1.5560e-02, -7.4760e-02]],\n",
       "               \n",
       "                        [[ 1.6042e-01, -2.1368e-02, -2.9838e-01],\n",
       "                         [ 2.1281e-01, -1.4688e-01, -1.8545e-01],\n",
       "                         [-7.8951e-02, -4.6773e-01, -5.3284e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7595e-02,  2.0438e-01, -1.0577e-01],\n",
       "                         [-2.1460e-02,  5.3867e-02, -1.4065e-01],\n",
       "                         [-1.7475e-02,  7.1682e-03, -3.8578e-01]],\n",
       "               \n",
       "                        [[ 2.2349e-01, -1.9747e-01,  1.0218e-01],\n",
       "                         [-1.2101e-01, -1.0764e-01, -4.3538e-01],\n",
       "                         [ 9.3700e-02,  3.2672e-01, -9.2328e-02]],\n",
       "               \n",
       "                        [[-3.2435e-01, -4.3322e-01,  7.1314e-02],\n",
       "                         [-3.1392e-02, -2.7405e-02,  3.8437e-02],\n",
       "                         [-4.1010e-01, -2.7582e-01, -1.3464e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-1.1670e-03, -1.5722e-06, -5.9141e-08,  3.6126e-04,  1.6372e-06,\n",
       "                        9.0940e-03, -1.3443e-06, -1.9332e-09,  6.8183e-05,  1.0969e-06,\n",
       "                       -1.5022e-01,  2.0103e-06, -4.8615e-05, -5.1902e-08,  1.3165e-02,\n",
       "                       -4.4272e-05, -2.1144e-06, -2.0571e-04,  5.5367e-08,  2.8232e-02,\n",
       "                        1.5321e-03, -9.8460e-04, -1.8462e-06, -3.2149e-02,  6.6975e-06,\n",
       "                       -1.4172e-02, -5.7390e-07, -4.1034e-07,  4.4389e-03,  1.3945e-06,\n",
       "                        5.2448e-09,  1.4594e-07, -2.8601e-03, -3.0972e-04, -3.8025e-04,\n",
       "                       -1.1957e-08,  1.7676e-06, -8.8101e-05,  5.0049e-06,  3.1802e-04,\n",
       "                        1.3175e-07,  8.3416e-03, -7.9959e-04,  8.7841e-03, -3.5057e-07,\n",
       "                       -9.7874e-04,  1.3208e-08,  2.1812e-07, -8.8771e-03, -8.6390e-06,\n",
       "                       -5.7108e-08, -1.5421e-05,  2.9728e-03, -1.7931e-06,  7.9910e-04,\n",
       "                        1.6687e-03, -2.4286e-03, -7.0909e-02,  5.2742e-05, -2.7238e-06,\n",
       "                        1.1843e-09, -2.9221e-06,  3.9329e-07,  1.9569e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.6293, -0.2814, -0.7356, -0.6098, -0.3068, -0.4773, -0.5338, -0.3653,\n",
       "                       -0.5842, -0.5291, -0.2636, -0.3483, -0.5375, -0.4258, -0.6835, -0.3832,\n",
       "                       -0.7116, -0.6957, -0.7409, -0.3935, -0.4871, -0.2365, -0.2799, -0.6074,\n",
       "                       -0.5205, -0.2747, -0.4727, -0.2671, -0.7476, -0.4752, -0.4705, -0.3923,\n",
       "                       -0.5153, -0.4661, -0.3879, -0.4682, -0.6328, -0.6541, -0.5709, -0.5174,\n",
       "                       -0.2983, -0.5252, -0.3353, -0.4772, -0.4578, -0.3797, -0.7308, -0.3598,\n",
       "                       -0.2493, -0.4731, -0.4701, -0.5702, -0.5798, -0.5348, -0.5272, -0.7258,\n",
       "                       -0.4604, -0.4938, -0.2418, -0.4716, -0.2309, -0.4518, -0.6988, -0.6112],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.3040, 0.1846, 0.3476, 0.2983, 0.1595, 0.2943, 0.6585, 0.1734, 0.6513,\n",
       "                       0.6060, 0.3761, 0.1817, 0.6462, 0.2412, 0.3611, 0.4996, 0.3893, 0.4081,\n",
       "                       0.3958, 0.2477, 0.2207, 0.3596, 0.4358, 0.3513, 0.6368, 0.1483, 0.2928,\n",
       "                       0.4169, 0.3909, 0.5288, 0.2167, 0.5293, 0.3174, 0.5888, 0.4876, 0.2747,\n",
       "                       0.6656, 0.6525, 0.2249, 0.2137, 0.1633, 0.2494, 0.2110, 0.5546, 0.4395,\n",
       "                       0.4572, 0.2814, 0.2590, 0.1640, 0.6131, 0.2282, 0.6961, 0.2891, 0.6819,\n",
       "                       0.2640, 0.6684, 0.5680, 0.2786, 0.3352, 0.6231, 0.1636, 0.2511, 0.3929,\n",
       "                       0.3081], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[ 0.0125,  0.0647, -0.0042,  ...,  0.4857, -0.0441, -0.1969],\n",
       "                       [ 0.0174,  0.0430,  0.0744,  ..., -0.3114,  0.3788, -0.2694],\n",
       "                       [-0.2165, -0.1488,  0.0334,  ...,  0.1455, -0.4969,  0.3936],\n",
       "                       [-0.0231,  0.0443,  0.0223,  ..., -0.3246,  0.1842,  0.0382],\n",
       "                       [ 0.0805,  0.0384, -0.0419,  ...,  0.1118,  0.1264,  0.1276]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0260, -0.0130,  0.0044, -0.0376, -0.0102], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[ 3.3833e-01,  8.8771e-01,  8.6549e-01, -7.6491e-01,  7.3035e-01,\n",
       "                        -6.6038e-01,  5.2220e-01, -9.3733e-01,  9.5451e-01, -7.3970e-01,\n",
       "                        -6.7803e-01, -9.4298e-01, -8.5123e-01, -5.5513e-01, -6.2843e-01,\n",
       "                        -6.5258e-01, -7.7122e-01, -9.1034e-01, -3.5988e-02, -6.0679e-01],\n",
       "                       [ 4.1813e-01,  1.2456e+00,  5.3856e-01, -8.4552e-01,  5.8237e-01,\n",
       "                        -6.2136e-01,  6.9102e-01, -7.6687e-01,  1.0543e+00, -8.3941e-01,\n",
       "                        -8.8050e-01, -7.0374e-01, -9.3261e-01, -7.8744e-01, -9.5139e-01,\n",
       "                        -5.6390e-01, -4.9545e-01, -5.3369e-01, -1.3907e-01, -6.4701e-01],\n",
       "                       [ 1.8961e+00, -7.8000e+00,  1.3320e+00, -9.0210e-01,  1.2917e+00,\n",
       "                        -9.9739e-01,  1.2193e+00, -1.0270e+00, -2.1397e+00, -1.1658e+00,\n",
       "                        -3.0621e-01, -1.1396e+00, -7.0713e-01, -8.3840e-01, -9.6536e-01,\n",
       "                        -9.5105e-01, -5.1095e-01, -7.5035e-01,  2.1333e-01, -9.6162e-01],\n",
       "                       [ 7.3769e-01, -7.3040e-01,  8.1725e-01, -8.2185e-01,  8.0715e-01,\n",
       "                        -9.8212e-01,  1.0459e+00, -6.6562e-01,  1.0775e+00, -9.2869e-01,\n",
       "                        -1.0308e+00, -6.7376e-01, -8.2138e-01, -6.7634e-01, -9.7064e-01,\n",
       "                        -6.1652e-01, -6.2366e-01, -8.3644e-01,  2.3702e-01, -7.0220e-01],\n",
       "                       [ 2.2831e-01, -4.8406e-02, -1.0402e-01, -1.4108e-01,  4.5409e-02,\n",
       "                        -1.9042e-01, -6.3408e-02, -1.0905e-02, -1.3051e-01, -5.7843e-02,\n",
       "                         6.4771e-02,  1.1908e-01, -7.5306e-02,  4.4921e-02,  7.2006e-02,\n",
       "                        -6.6588e-02, -5.1194e-03,  1.2932e-01, -9.7224e-02,  2.1447e-01],\n",
       "                       [ 5.6456e-01,  6.8431e-01,  7.1643e-01, -8.8177e-01,  6.5751e-01,\n",
       "                        -6.5220e-01,  8.8955e-01, -7.9292e-01,  8.9195e-01, -9.0459e-01,\n",
       "                        -6.7771e-01, -4.9321e-01, -4.6822e-01, -6.7608e-01, -5.4482e-01,\n",
       "                        -9.0494e-01, -5.3338e-01, -8.9476e-01, -5.5159e-02, -9.1161e-01],\n",
       "                       [ 8.2680e-01, -3.9536e-02,  1.0138e+00, -8.2754e-01,  9.5657e-01,\n",
       "                        -9.8866e-01,  1.0059e+00, -8.4353e-01,  1.1703e+00, -8.5385e-01,\n",
       "                        -1.1582e+00, -7.2158e-01, -9.1037e-01, -6.4839e-01, -1.0332e+00,\n",
       "                        -7.9487e-01, -6.8073e-01, -6.2000e-01, -1.4946e-01, -6.5745e-01],\n",
       "                       [ 1.4151e-02,  1.1464e-01, -2.1784e-01, -1.0303e-01,  8.9848e-02,\n",
       "                         1.4065e-01, -2.0173e-02,  1.5730e-01,  7.4684e-02,  2.0262e-02,\n",
       "                        -1.6737e-01, -2.1540e-01, -1.3253e-01,  1.9497e-01,  1.0675e-01,\n",
       "                        -1.4650e-01, -9.3088e-02,  4.1591e-02,  1.2628e-01,  9.2926e-02],\n",
       "                       [ 5.3437e-02,  1.5308e-01, -8.0583e-02,  1.0906e-01, -9.7261e-02,\n",
       "                         9.2546e-02, -1.0113e-01, -1.0790e-01,  8.4748e-02,  2.0927e-01,\n",
       "                        -1.3212e-01, -6.1718e-02,  1.1575e-01,  8.3344e-02,  1.3111e-01,\n",
       "                         7.0203e-02, -1.3955e-04, -1.3035e-01, -5.3587e-02,  1.2128e-01],\n",
       "                       [-1.6270e-02, -5.1831e-02,  6.0192e-02,  1.7096e-01, -1.5190e-01,\n",
       "                        -1.0585e-01, -2.8034e-01, -4.4407e-03, -2.3000e-01,  4.4649e-02,\n",
       "                        -2.1198e-01,  3.7109e-02, -1.5708e-01,  3.2853e-02, -1.9600e-01,\n",
       "                        -9.9206e-02, -7.5591e-02, -5.9037e-02,  8.7050e-02, -2.0938e-02],\n",
       "                       [-1.1421e-01, -8.2194e-02, -1.8477e-01, -7.2684e-02, -9.7852e-02,\n",
       "                        -7.7699e-02, -1.6057e-01,  1.1719e-01,  7.0833e-02,  2.1149e-01,\n",
       "                        -1.9130e-01,  1.6590e-01, -7.6193e-02, -2.1895e-01, -1.3051e-01,\n",
       "                         1.4515e-01, -2.2225e-01, -1.3287e-01, -1.9505e-01,  6.0146e-02],\n",
       "                       [ 4.0716e-02,  8.4330e-02,  1.6210e-02,  7.9479e-02,  5.6646e-03,\n",
       "                         1.9948e-03, -2.2540e-01, -2.8410e-02,  2.2695e-02, -1.1514e-01,\n",
       "                         1.2124e-02,  1.9939e-01, -4.8010e-02, -1.3395e-01, -6.2925e-02,\n",
       "                         6.0103e-02,  1.7674e-01, -1.4864e-01,  2.0221e-01,  1.6045e-01],\n",
       "                       [ 1.3351e+00, -2.5982e+00,  1.1579e+00, -6.5907e-01,  8.0409e-01,\n",
       "                        -9.4314e-01,  9.7808e-01, -5.9899e-01,  5.3862e-01, -9.1454e-01,\n",
       "                        -1.2192e+00, -6.1778e-01, -1.1962e+00, -1.0391e+00, -1.0040e+00,\n",
       "                        -6.4616e-01, -9.2847e-01, -8.7444e-01, -3.1824e-02, -8.4265e-01],\n",
       "                       [-4.4157e-02,  1.9882e-01,  1.2129e-01,  1.1919e-01, -4.2201e-02,\n",
       "                        -1.3709e-01, -2.7140e-01, -1.1121e-01,  2.0645e-02,  1.7319e-01,\n",
       "                         6.4815e-02, -2.0655e-02,  2.3722e-01,  2.5704e-01, -1.0241e-01,\n",
       "                         8.4315e-02, -1.6044e-02, -7.4925e-02, -2.8036e-02,  9.2777e-02],\n",
       "                       [ 1.1478e+00, -3.4983e+00,  1.2991e+00, -1.0875e+00,  1.1815e+00,\n",
       "                        -1.1604e+00,  1.1011e+00, -1.0271e+00, -2.1501e-01, -8.0624e-01,\n",
       "                        -6.6741e-01, -8.3730e-01, -9.0621e-01, -9.0232e-01, -1.0907e+00,\n",
       "                        -9.0030e-01, -8.8670e-01, -7.8111e-01, -7.5538e-02, -9.7161e-01],\n",
       "                       [ 8.1875e-01,  2.0353e-01,  8.8208e-01, -8.1355e-01,  9.2584e-01,\n",
       "                        -6.6729e-01,  9.2782e-01, -9.0990e-01,  1.8531e+00, -9.8833e-01,\n",
       "                        -1.4827e+00, -6.9052e-01, -1.1947e+00, -6.5766e-01, -1.1851e+00,\n",
       "                        -7.3468e-01, -1.0070e+00, -6.5472e-01, -3.4354e-01, -8.3962e-01],\n",
       "                       [ 2.7295e-02, -1.7381e-01, -3.8673e-02, -1.1522e-01, -2.2003e-01,\n",
       "                         4.8910e-02, -1.7161e-01,  2.1635e-01,  4.2218e-02,  1.0293e-01,\n",
       "                        -1.3665e-01, -1.4709e-01,  1.1683e-01,  1.9865e-01, -1.4591e-01,\n",
       "                         1.8253e-01, -1.6759e-01,  1.5809e-01,  9.5630e-02,  1.1975e-01],\n",
       "                       [-2.0022e-01, -1.3849e-01,  1.5971e-01,  2.6901e-02,  2.1492e-02,\n",
       "                         2.0345e-01,  2.5418e-02,  1.5173e-01,  1.3067e-02,  9.2048e-02,\n",
       "                         1.7415e-02, -9.1162e-02, -1.0179e-01,  2.1474e-01,  2.0460e-01,\n",
       "                         1.5405e-01, -8.7541e-02,  6.6936e-02, -4.2196e-02,  1.0167e-01],\n",
       "                       [ 9.0152e-01, -4.8259e-02,  6.6383e-01, -6.3503e-01,  9.8203e-01,\n",
       "                        -9.1877e-01,  8.7809e-01, -6.2150e-01,  1.2389e+00, -8.4995e-01,\n",
       "                        -8.6597e-01, -8.5047e-01, -5.9669e-01, -7.6777e-01, -9.7400e-01,\n",
       "                        -7.2145e-01, -4.7518e-01, -5.3281e-01, -2.6610e-01, -7.1278e-01],\n",
       "                       [ 1.6181e+00, -4.1956e+00,  1.2187e+00, -7.1896e-01,  1.1013e+00,\n",
       "                        -8.9069e-01,  1.1123e+00, -7.6135e-01, -4.0383e-01, -8.5376e-01,\n",
       "                        -9.6347e-01, -8.0548e-01, -9.9334e-01, -7.4475e-01, -9.3232e-01,\n",
       "                        -7.0425e-01, -6.9532e-01, -7.0312e-01,  4.4493e-02, -7.7345e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([ 0.9264,  0.4377,  1.0926,  0.8921,  0.1491,  0.5853,  1.0312, -0.0428,\n",
       "                       -0.0887,  0.0784,  0.1319, -0.1175,  1.1346,  0.0642,  0.9665,  0.9846,\n",
       "                        0.1344, -0.1548,  0.8458,  1.1834], device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 2.8000e-01,  2.6045e-01, -3.4633e-02, -1.2624e-01,  1.8617e-01,\n",
       "                         2.0702e-01,  3.0544e-02,  1.9846e-01, -1.9138e-01, -1.2190e-01,\n",
       "                        -9.2299e-02,  2.1520e-01, -1.9940e-01,  8.0625e-02,  1.8190e-02,\n",
       "                        -2.0128e-01, -3.6457e-02,  4.0899e-02,  5.0923e-02, -8.2056e-02],\n",
       "                       [ 4.1278e+00,  3.9785e+00,  4.0155e+00,  3.9918e+00,  1.7553e-02,\n",
       "                         3.7917e+00,  4.1470e+00, -1.7247e-01, -2.0607e-01,  1.3565e-01,\n",
       "                        -2.2143e-01, -9.8389e-02,  3.9547e+00,  1.7521e-02,  4.0799e+00,\n",
       "                         3.7670e+00, -1.3384e-01,  2.0973e-02,  4.0045e+00,  4.0325e+00],\n",
       "                       [-1.6860e-01, -6.2668e-01, -6.6421e-02, -5.8498e-02, -1.1433e-01,\n",
       "                        -1.1640e-01, -1.3925e-01,  6.5066e-02, -2.1395e-02,  1.1329e-01,\n",
       "                        -1.7174e-01, -1.2383e-01, -7.6255e-02, -5.0383e-02, -1.8553e-03,\n",
       "                        -6.6968e-01, -1.8833e-01,  7.9664e-02,  3.5491e-02,  1.5305e-02],\n",
       "                       [-3.0796e-01, -2.1104e-01, -3.1822e-01, -1.0812e-01, -2.2044e-01,\n",
       "                        -3.1752e-01,  7.9267e-02,  1.3946e-01,  2.1579e-01, -2.5962e-01,\n",
       "                         1.8362e-01,  1.1896e-01, -1.9150e-01,  2.1971e-01, -5.6457e-02,\n",
       "                         1.0602e-01, -2.1969e-01, -7.4806e-02,  1.2837e-02, -7.5956e-02],\n",
       "                       [ 3.3305e-01,  4.1222e-01, -2.6804e-01,  1.9959e-01, -1.5614e-01,\n",
       "                         2.8057e-01,  3.0441e-01,  4.6785e-02,  8.2634e-02, -1.6735e-01,\n",
       "                        -1.6921e-01,  3.2611e-03,  8.7909e-02, -2.8136e-02,  3.3423e-02,\n",
       "                         3.0767e-01,  2.2874e-02, -5.4503e-02,  2.1020e-01, -1.5670e-02],\n",
       "                       [-1.6565e-01,  1.4222e-01,  1.1644e-02, -1.5795e-01,  1.7973e-01,\n",
       "                        -1.9431e-01, -2.0489e-01, -9.7798e-02, -2.0040e-01,  9.0615e-02,\n",
       "                         1.9690e-01,  3.3199e-02, -3.2587e-01,  1.7870e-02, -1.0624e-01,\n",
       "                         1.1562e-01, -1.2698e-01, -1.4503e-02, -3.2913e-01, -8.6503e-02],\n",
       "                       [ 4.5550e-02,  1.3537e-01, -8.4378e-02,  2.7683e-01, -1.4967e-03,\n",
       "                         1.7942e-01, -7.8780e-02, -1.8449e-01, -1.6355e-01,  2.5890e-02,\n",
       "                        -1.5935e-01, -1.4922e-01,  1.7517e-01,  6.6919e-02, -1.8603e-01,\n",
       "                         3.5379e-01, -2.1866e-01,  6.9435e-02,  1.8007e-01,  5.5042e-02],\n",
       "                       [-3.2340e-01, -2.3148e-01, -2.5940e-01,  1.0274e-01,  2.0388e-01,\n",
       "                         4.9506e-02, -2.3727e-01,  9.0464e-02, -6.4546e-02,  1.1481e-01,\n",
       "                        -1.2816e-01, -3.7657e-02, -2.9615e-01,  1.5634e-01,  5.5150e-02,\n",
       "                        -1.6607e-01, -4.0011e-02, -1.3586e-01, -2.2513e-01, -2.6197e-01],\n",
       "                       [ 1.4045e+00,  1.3112e+00,  1.6676e+00,  1.4193e+00, -1.6595e-01,\n",
       "                         1.4225e+00,  1.5018e+00,  1.8778e-01, -9.0719e-02, -4.9063e-02,\n",
       "                         1.7291e-01, -2.0355e-01,  1.5881e+00, -1.7963e-01,  1.7195e+00,\n",
       "                         1.1361e+00, -1.9560e-01,  1.6629e-01,  1.3048e+00,  1.7249e+00],\n",
       "                       [-2.9060e-01, -2.7838e-01, -2.0724e-02, -3.7516e-01, -5.8513e-02,\n",
       "                        -2.8523e-02, -2.4453e-01,  1.0720e-01,  7.2799e-02,  1.3506e-01,\n",
       "                        -1.8500e-01, -1.6369e-03,  2.1994e-02, -1.7271e-01, -1.1435e-01,\n",
       "                        -5.1748e-01, -2.9007e-02, -2.0239e-01, -7.0718e-03, -1.3381e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([ 0.0176,  4.0668,  0.1129, -0.1661, -0.1211,  0.0911, -0.0247, -0.2284,\n",
       "                        1.3531, -0.0968], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3781575206518173,\n",
       "   1.1611951447725295,\n",
       "   1.056187203526497,\n",
       "   0.9990508477687836,\n",
       "   0.9445107780694961,\n",
       "   0.9054419370889664,\n",
       "   0.8790644562244415,\n",
       "   0.8413167077302933,\n",
       "   0.81417639452219,\n",
       "   0.7987049117684364,\n",
       "   0.7568034706711769,\n",
       "   0.7553931249976158,\n",
       "   0.7352268889546394,\n",
       "   0.7141392946243286,\n",
       "   0.7015226419568061,\n",
       "   0.6975334255099297,\n",
       "   0.6787453933358193,\n",
       "   0.6638011794686317,\n",
       "   0.6654316591620445,\n",
       "   0.6448889576792717,\n",
       "   0.6430743549466134,\n",
       "   0.6242989985942841,\n",
       "   0.6167051736116409,\n",
       "   0.6294681013822555,\n",
       "   0.6102043255567551,\n",
       "   0.5981985551714897,\n",
       "   0.5888729234337806,\n",
       "   0.5964434978663922,\n",
       "   0.5904967539906502,\n",
       "   0.600672294139862,\n",
       "   0.5776494436264038,\n",
       "   0.5809593687653541,\n",
       "   0.5711286078691482,\n",
       "   0.5615584508776664,\n",
       "   0.578576858997345,\n",
       "   0.568163976997137,\n",
       "   0.565933336943388,\n",
       "   0.5549551609158516,\n",
       "   0.5519718555212021,\n",
       "   0.5414940205216408,\n",
       "   0.5583405958414078,\n",
       "   0.5460640159249306,\n",
       "   0.5488808604180813,\n",
       "   0.5614741460680962,\n",
       "   0.5370699279308319,\n",
       "   0.536879869043827,\n",
       "   0.5372144963443279,\n",
       "   0.5263860493004322,\n",
       "   0.5190091091394424,\n",
       "   0.5188511198759079,\n",
       "   0.5256872927248478,\n",
       "   0.5178931756019592,\n",
       "   0.5216619141697884,\n",
       "   0.5079065907001495,\n",
       "   0.5138000902533532,\n",
       "   0.5081043878495693,\n",
       "   0.49955484056472776,\n",
       "   0.5006109924912453,\n",
       "   0.498062181442976,\n",
       "   0.49138763758540155,\n",
       "   0.4893003736436367,\n",
       "   0.48436816799640653,\n",
       "   0.4743478760123253,\n",
       "   0.47067977902293207,\n",
       "   0.46788975048065184,\n",
       "   0.4714181786775589,\n",
       "   0.46892396995425223,\n",
       "   0.44566944932937624,\n",
       "   0.44449051773548126,\n",
       "   0.4562981061935425,\n",
       "   0.4459070282280445,\n",
       "   0.4399103821516037,\n",
       "   0.4360775443017483,\n",
       "   0.43937514385581017,\n",
       "   0.42802400305867194,\n",
       "   0.41874895346164703,\n",
       "   0.43747435557842257,\n",
       "   0.4303979601562023,\n",
       "   0.4149820187985897,\n",
       "   0.4245844931602478,\n",
       "   0.41245942771434785,\n",
       "   0.41349110144376755,\n",
       "   0.40514938470721246,\n",
       "   0.4058704424202442,\n",
       "   0.39639371493458747,\n",
       "   0.3995545819103718,\n",
       "   0.39597068361938,\n",
       "   0.39101729637384414,\n",
       "   0.38916833540797235,\n",
       "   0.3758792640566826,\n",
       "   0.37994975519180296,\n",
       "   0.38192700895667075,\n",
       "   0.3681240090280771,\n",
       "   0.37608788326382636,\n",
       "   0.3758507915139198,\n",
       "   0.3810917066037655,\n",
       "   0.3719368876516819,\n",
       "   0.37706916868686674,\n",
       "   0.3614755728244782],\n",
       "  'train_loss_std': [0.17195482925519584,\n",
       "   0.13223729366369957,\n",
       "   0.14950848825598906,\n",
       "   0.14504249775565234,\n",
       "   0.13877819291558124,\n",
       "   0.13717357680103787,\n",
       "   0.1459925724355128,\n",
       "   0.13507790187804092,\n",
       "   0.1453570269857471,\n",
       "   0.13699036921940497,\n",
       "   0.13759596813460773,\n",
       "   0.14608254697776027,\n",
       "   0.1312434942373071,\n",
       "   0.14665778178786656,\n",
       "   0.13382383572572518,\n",
       "   0.1406016402095006,\n",
       "   0.13145894369593022,\n",
       "   0.13707907507362302,\n",
       "   0.14173814488247002,\n",
       "   0.14081128939866447,\n",
       "   0.13524968194978343,\n",
       "   0.13179537899985236,\n",
       "   0.13106831976768674,\n",
       "   0.13808705647582897,\n",
       "   0.12638450810267757,\n",
       "   0.13787174869361385,\n",
       "   0.12862927768976437,\n",
       "   0.13066440748461927,\n",
       "   0.12819501510391568,\n",
       "   0.13760246423828432,\n",
       "   0.12806710313364306,\n",
       "   0.13170781437586956,\n",
       "   0.12946591517186914,\n",
       "   0.1332784380526051,\n",
       "   0.1338900963941404,\n",
       "   0.1348949597288956,\n",
       "   0.12727191667338808,\n",
       "   0.13295163920637557,\n",
       "   0.1357023773423051,\n",
       "   0.13496526779899057,\n",
       "   0.1301204017984478,\n",
       "   0.13037300502541588,\n",
       "   0.13122620400299762,\n",
       "   0.1404703724847463,\n",
       "   0.13221642198140338,\n",
       "   0.1353715415425566,\n",
       "   0.13765311218081078,\n",
       "   0.1318184496868414,\n",
       "   0.1272067905908375,\n",
       "   0.12828392282053525,\n",
       "   0.13170597760195782,\n",
       "   0.12158583732282384,\n",
       "   0.13509715171754832,\n",
       "   0.1300208767051514,\n",
       "   0.1364943611875932,\n",
       "   0.13203740266192548,\n",
       "   0.12588211306566435,\n",
       "   0.1323271056238559,\n",
       "   0.12062400555235682,\n",
       "   0.1260491005624538,\n",
       "   0.12539693321434892,\n",
       "   0.12908345714278271,\n",
       "   0.12369417731285481,\n",
       "   0.12557780316390868,\n",
       "   0.12532185724358536,\n",
       "   0.1277469500370503,\n",
       "   0.12236699742928225,\n",
       "   0.12915361408344397,\n",
       "   0.12037068210721714,\n",
       "   0.13132903710044816,\n",
       "   0.11848033979916509,\n",
       "   0.11764833706018332,\n",
       "   0.11669415513364588,\n",
       "   0.12802524821068373,\n",
       "   0.12027261860408733,\n",
       "   0.11833977919038612,\n",
       "   0.13126721702170707,\n",
       "   0.12125036028363909,\n",
       "   0.11896260652493297,\n",
       "   0.12578020014461386,\n",
       "   0.11779718636816115,\n",
       "   0.12472666541655295,\n",
       "   0.12906930241554274,\n",
       "   0.11962349833767787,\n",
       "   0.12165914246904393,\n",
       "   0.11532201171165911,\n",
       "   0.12173895904694045,\n",
       "   0.12219216243763845,\n",
       "   0.12318708083072856,\n",
       "   0.11669544246816176,\n",
       "   0.11908365731283738,\n",
       "   0.12434672346919737,\n",
       "   0.11460833967418399,\n",
       "   0.12215267649404063,\n",
       "   0.12092024761265681,\n",
       "   0.12973930627761987,\n",
       "   0.11080579499394533,\n",
       "   0.12115096952837631,\n",
       "   0.11808964759702778],\n",
       "  'train_accuracy_mean': [0.4320133344531059,\n",
       "   0.5428666653037071,\n",
       "   0.5934133334755898,\n",
       "   0.6178399996161461,\n",
       "   0.6404533326029778,\n",
       "   0.6581733325719833,\n",
       "   0.6701466664075851,\n",
       "   0.683839998960495,\n",
       "   0.6952000001072883,\n",
       "   0.7013600003123284,\n",
       "   0.7195066660642624,\n",
       "   0.7183599997758865,\n",
       "   0.7276666669249534,\n",
       "   0.7350266684889794,\n",
       "   0.7422266675233841,\n",
       "   0.7421999996900559,\n",
       "   0.7501199995279312,\n",
       "   0.7555466657876968,\n",
       "   0.7561733318567276,\n",
       "   0.7663199994564056,\n",
       "   0.766173333644867,\n",
       "   0.773813333272934,\n",
       "   0.7749600002765655,\n",
       "   0.770693332195282,\n",
       "   0.7787066670656204,\n",
       "   0.7836933345794678,\n",
       "   0.78821333360672,\n",
       "   0.7848533327579499,\n",
       "   0.788386666059494,\n",
       "   0.7832133324146271,\n",
       "   0.789786665558815,\n",
       "   0.791386666059494,\n",
       "   0.794133333325386,\n",
       "   0.7989066662788391,\n",
       "   0.7920266666412353,\n",
       "   0.7974800000190735,\n",
       "   0.7981733330488205,\n",
       "   0.8023733339309692,\n",
       "   0.8017066663503647,\n",
       "   0.8069599989652634,\n",
       "   0.8000266669988633,\n",
       "   0.8035066667795181,\n",
       "   0.8040399992465973,\n",
       "   0.8002666668891907,\n",
       "   0.8101333338618278,\n",
       "   0.8086800000667572,\n",
       "   0.805973333477974,\n",
       "   0.8131333342790603,\n",
       "   0.8157333327531815,\n",
       "   0.8148533333539962,\n",
       "   0.81394666659832,\n",
       "   0.8168666661977768,\n",
       "   0.8148933322429657,\n",
       "   0.8187333335876464,\n",
       "   0.8178666659593582,\n",
       "   0.8208933333158493,\n",
       "   0.8233200001716614,\n",
       "   0.8207733329534531,\n",
       "   0.822213332772255,\n",
       "   0.8241600003242493,\n",
       "   0.8241600011587142,\n",
       "   0.8266666671037673,\n",
       "   0.8299733332395554,\n",
       "   0.83058666741848,\n",
       "   0.8313733333349228,\n",
       "   0.8283466665744782,\n",
       "   0.8290400000810623,\n",
       "   0.8395733331441879,\n",
       "   0.8389733341932297,\n",
       "   0.8336666669845582,\n",
       "   0.8374000010490418,\n",
       "   0.8381466678380967,\n",
       "   0.8410000009536743,\n",
       "   0.8398400008678436,\n",
       "   0.8427600001096726,\n",
       "   0.8447333341836929,\n",
       "   0.8405466660261154,\n",
       "   0.8422266656160354,\n",
       "   0.8475866663455963,\n",
       "   0.8448933343887329,\n",
       "   0.8498933342695236,\n",
       "   0.8486266685724259,\n",
       "   0.8527600001096726,\n",
       "   0.850946667432785,\n",
       "   0.8548400003910065,\n",
       "   0.8537333338260651,\n",
       "   0.8573333343267441,\n",
       "   0.8576000007390976,\n",
       "   0.8575200006961823,\n",
       "   0.860973335981369,\n",
       "   0.8594666675329209,\n",
       "   0.8605733336210251,\n",
       "   0.8661600025892258,\n",
       "   0.8611866675615311,\n",
       "   0.861946667432785,\n",
       "   0.8613733333349228,\n",
       "   0.8640400027036667,\n",
       "   0.859760002374649,\n",
       "   0.86742666721344],\n",
       "  'train_accuracy_std': [0.09535030032048077,\n",
       "   0.06738713807352383,\n",
       "   0.07550948615911658,\n",
       "   0.07054172206468874,\n",
       "   0.06566696306949392,\n",
       "   0.06771407887542703,\n",
       "   0.06855638997144936,\n",
       "   0.06482394027032817,\n",
       "   0.06684828698268354,\n",
       "   0.06348346543450561,\n",
       "   0.06309094793313422,\n",
       "   0.06711366329126287,\n",
       "   0.06265514002520217,\n",
       "   0.0674386915168817,\n",
       "   0.061200742522833786,\n",
       "   0.0632698272543969,\n",
       "   0.05950375229691209,\n",
       "   0.06144546839102614,\n",
       "   0.0647156251644936,\n",
       "   0.06424027615024594,\n",
       "   0.0613045313833376,\n",
       "   0.06168695752400696,\n",
       "   0.060168821597071556,\n",
       "   0.061369257986236186,\n",
       "   0.05670836198700681,\n",
       "   0.061016238473167,\n",
       "   0.05703105506803011,\n",
       "   0.0588323473407338,\n",
       "   0.05775174286866846,\n",
       "   0.06105650281942186,\n",
       "   0.05807465577355138,\n",
       "   0.05894281141675942,\n",
       "   0.05661472076105744,\n",
       "   0.06095430104615192,\n",
       "   0.058961978427936414,\n",
       "   0.06092732165436578,\n",
       "   0.05746300158854285,\n",
       "   0.058460723742394256,\n",
       "   0.05972472818278029,\n",
       "   0.0609969273315128,\n",
       "   0.05719206782981214,\n",
       "   0.058071727535995994,\n",
       "   0.05822056428286339,\n",
       "   0.06226481132656846,\n",
       "   0.05862331796853777,\n",
       "   0.05997732850002439,\n",
       "   0.06249664450482905,\n",
       "   0.059458707695381835,\n",
       "   0.05529331940573527,\n",
       "   0.058011500479822925,\n",
       "   0.06034108118842431,\n",
       "   0.05368761591744022,\n",
       "   0.05990631429500429,\n",
       "   0.05946049978906187,\n",
       "   0.05892579088899308,\n",
       "   0.05835068237808166,\n",
       "   0.05593090935450085,\n",
       "   0.06123707845241745,\n",
       "   0.056437095558810255,\n",
       "   0.05538977918986213,\n",
       "   0.056641807849532155,\n",
       "   0.05702085634286417,\n",
       "   0.05486548721380025,\n",
       "   0.05554187905700503,\n",
       "   0.05539497107784327,\n",
       "   0.05637670925732753,\n",
       "   0.05524783229209577,\n",
       "   0.055702346628785336,\n",
       "   0.05318742044853485,\n",
       "   0.05710944621469104,\n",
       "   0.05215443063833848,\n",
       "   0.05226969274652294,\n",
       "   0.0504977451895434,\n",
       "   0.056075911716339674,\n",
       "   0.05245044544542268,\n",
       "   0.054092062819394396,\n",
       "   0.0568019082839093,\n",
       "   0.05488997011615869,\n",
       "   0.051179405527381615,\n",
       "   0.05466961183730825,\n",
       "   0.05263786101517798,\n",
       "   0.05401545688817389,\n",
       "   0.05456030603639239,\n",
       "   0.05217996115676144,\n",
       "   0.05256062182035268,\n",
       "   0.04887917416490754,\n",
       "   0.05081557238952186,\n",
       "   0.05139926745931647,\n",
       "   0.053073374054126786,\n",
       "   0.04917121379854685,\n",
       "   0.05124650746544509,\n",
       "   0.0545537884338254,\n",
       "   0.05092138924117259,\n",
       "   0.05321750760623796,\n",
       "   0.051585847335083465,\n",
       "   0.05570221175335322,\n",
       "   0.0469622840362014,\n",
       "   0.0520763149274742,\n",
       "   0.05063552042055142],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.3604539958635966,\n",
       "   1.2295724600553513,\n",
       "   1.1810067367553712,\n",
       "   1.1317395025491714,\n",
       "   1.0776124288638433,\n",
       "   1.0602580841382345,\n",
       "   1.0385244045654933,\n",
       "   1.0209460651874542,\n",
       "   0.9941475431124369,\n",
       "   0.9695110875368118,\n",
       "   0.9719539719820023,\n",
       "   0.9605769169330597,\n",
       "   0.9372861528396607,\n",
       "   0.9347321339448293,\n",
       "   0.9247482222318649,\n",
       "   0.945079775651296,\n",
       "   0.931222904920578,\n",
       "   0.9304475843906402,\n",
       "   0.9197140369812647,\n",
       "   0.9073787923653921,\n",
       "   0.9250515516599019,\n",
       "   0.8947171835104625,\n",
       "   0.9133474042018255,\n",
       "   0.8993886013825735,\n",
       "   0.8822960789998372,\n",
       "   0.90286052942276,\n",
       "   0.8913906526565551,\n",
       "   0.8748053294420243,\n",
       "   0.8958319195111593,\n",
       "   0.8969345790147781,\n",
       "   0.8976290702819825,\n",
       "   0.9050362437963486,\n",
       "   0.8782473544279734,\n",
       "   0.8967856297890345,\n",
       "   0.9235518652200699,\n",
       "   0.9114776211977005,\n",
       "   0.8776137193044027,\n",
       "   0.8970136739810308,\n",
       "   0.912795315782229,\n",
       "   0.9151053573687872,\n",
       "   0.9237683232625326,\n",
       "   0.9152018729845682,\n",
       "   0.9230090169111888,\n",
       "   0.8993451700607935,\n",
       "   0.9334647061427435,\n",
       "   0.9443150812387466,\n",
       "   0.9071224890152614,\n",
       "   0.8934851282835007,\n",
       "   0.915705546538035,\n",
       "   0.9119064319133758,\n",
       "   0.9028959925969442,\n",
       "   0.9046054500341415,\n",
       "   0.8896105515956879,\n",
       "   0.8911086418231329,\n",
       "   0.9037761076291402,\n",
       "   0.8997078682978948,\n",
       "   0.9104154775540034,\n",
       "   0.8968644746144613,\n",
       "   0.9079792527357737,\n",
       "   0.9017426294088363,\n",
       "   0.9024906549851099,\n",
       "   0.9163866412639617,\n",
       "   0.9298530761400858,\n",
       "   0.90790596028169,\n",
       "   0.9212246781587601,\n",
       "   0.895367820262909,\n",
       "   0.9041713819901148,\n",
       "   0.90622054596742,\n",
       "   0.9235953255494436,\n",
       "   0.9105389591058095,\n",
       "   0.9199749153852462,\n",
       "   0.9145842953523,\n",
       "   0.9335558911164602,\n",
       "   0.9004518419504166,\n",
       "   0.9243521593014399,\n",
       "   0.9383720032374064,\n",
       "   0.9360361878077189,\n",
       "   0.9215087070067723,\n",
       "   0.9277839509646097,\n",
       "   0.9244435673952103,\n",
       "   0.9201968417565027,\n",
       "   0.9199221871296565,\n",
       "   0.9202064857880274,\n",
       "   0.9410806312163671,\n",
       "   0.9392612999677659,\n",
       "   0.9168351123730342,\n",
       "   0.9480422782897949,\n",
       "   0.9513612119356791,\n",
       "   0.9275642279783884,\n",
       "   0.9558312946557999,\n",
       "   0.9477969525257747,\n",
       "   0.9757225700219472,\n",
       "   0.9719017247358958,\n",
       "   0.9629479922850926,\n",
       "   0.969860658844312,\n",
       "   0.9309313044945399,\n",
       "   0.9875974591573079,\n",
       "   0.9627461894353231,\n",
       "   0.9435005400578181],\n",
       "  'val_loss_std': [0.10037834070537976,\n",
       "   0.11977470663470424,\n",
       "   0.12184162748715079,\n",
       "   0.13128766276376533,\n",
       "   0.13672499736026777,\n",
       "   0.12806804213206455,\n",
       "   0.13315740311205856,\n",
       "   0.14190396170939068,\n",
       "   0.12930027788460452,\n",
       "   0.13658816595633025,\n",
       "   0.13752357381928615,\n",
       "   0.13088533423147486,\n",
       "   0.134793844903776,\n",
       "   0.1332764972875475,\n",
       "   0.13370764229910426,\n",
       "   0.13455362288115838,\n",
       "   0.13737467847686843,\n",
       "   0.12766140850308216,\n",
       "   0.12876664604916582,\n",
       "   0.13170334598787672,\n",
       "   0.13001579245529504,\n",
       "   0.1283115071208267,\n",
       "   0.12913901696778574,\n",
       "   0.129864303183502,\n",
       "   0.13010716326200464,\n",
       "   0.12060277118874799,\n",
       "   0.13221162930500294,\n",
       "   0.13019371838457727,\n",
       "   0.12937525569495573,\n",
       "   0.12904540397179842,\n",
       "   0.1298073794879692,\n",
       "   0.13228117476101459,\n",
       "   0.1263910154519477,\n",
       "   0.12447729867785845,\n",
       "   0.12911728544538495,\n",
       "   0.13672258277026675,\n",
       "   0.1283064661409082,\n",
       "   0.12373308945644018,\n",
       "   0.13340023015439137,\n",
       "   0.12646054419700528,\n",
       "   0.1308789311833651,\n",
       "   0.12665527151142247,\n",
       "   0.13108781909706274,\n",
       "   0.12787891482142932,\n",
       "   0.12371007543909683,\n",
       "   0.14325841386726326,\n",
       "   0.13608533131793746,\n",
       "   0.12962561456723637,\n",
       "   0.12730673620055458,\n",
       "   0.13811535436483413,\n",
       "   0.12070525277445937,\n",
       "   0.1252258851238631,\n",
       "   0.13260719055506234,\n",
       "   0.1266778146896411,\n",
       "   0.12330884368192545,\n",
       "   0.12607032793995507,\n",
       "   0.13479166251539815,\n",
       "   0.132886621798555,\n",
       "   0.13627788455558287,\n",
       "   0.13260500261119243,\n",
       "   0.13700466344255657,\n",
       "   0.13773384918310702,\n",
       "   0.13210280099032737,\n",
       "   0.13636082742547337,\n",
       "   0.1413824524810519,\n",
       "   0.14248738118718018,\n",
       "   0.14522921730227875,\n",
       "   0.13569724938899125,\n",
       "   0.13721978005164684,\n",
       "   0.1465416726368507,\n",
       "   0.14497599264168173,\n",
       "   0.1476056145278463,\n",
       "   0.15241538427806972,\n",
       "   0.1488989500621364,\n",
       "   0.1428005888357521,\n",
       "   0.14886417644899821,\n",
       "   0.15379214000469088,\n",
       "   0.15535280332348592,\n",
       "   0.14703877044913358,\n",
       "   0.13601508678105986,\n",
       "   0.14648878212889238,\n",
       "   0.144832129476734,\n",
       "   0.1484490643337639,\n",
       "   0.1727961743811147,\n",
       "   0.15288956615573523,\n",
       "   0.1591385828312991,\n",
       "   0.15741024714573343,\n",
       "   0.1446739007666138,\n",
       "   0.15836454537845415,\n",
       "   0.1641389237484448,\n",
       "   0.16011670698369002,\n",
       "   0.15675296392844007,\n",
       "   0.16671334554936137,\n",
       "   0.16173752403766076,\n",
       "   0.16708144275452408,\n",
       "   0.15640992845943508,\n",
       "   0.15612984154908832,\n",
       "   0.16711919381477475,\n",
       "   0.16417822456630743],\n",
       "  'val_accuracy_mean': [0.44148888895908994,\n",
       "   0.5113999987641971,\n",
       "   0.5351111100117365,\n",
       "   0.5526888880133629,\n",
       "   0.580199999610583,\n",
       "   0.5868888876835505,\n",
       "   0.5933777755498886,\n",
       "   0.6035333321491877,\n",
       "   0.6155999997258186,\n",
       "   0.6258666660388311,\n",
       "   0.6239111096660296,\n",
       "   0.6311777772506079,\n",
       "   0.6357999994357427,\n",
       "   0.6404222218195598,\n",
       "   0.6461999990542729,\n",
       "   0.6357999990383784,\n",
       "   0.6430888897180558,\n",
       "   0.6393777788678805,\n",
       "   0.6462888859709104,\n",
       "   0.6517777758836746,\n",
       "   0.645066664814949,\n",
       "   0.6566444446643194,\n",
       "   0.6470888874928157,\n",
       "   0.6528222212195396,\n",
       "   0.6613555546601614,\n",
       "   0.6523111099004746,\n",
       "   0.6564666662613551,\n",
       "   0.6682222226262092,\n",
       "   0.6554444432258606,\n",
       "   0.6564666655659676,\n",
       "   0.6562000009417533,\n",
       "   0.6539333323637645,\n",
       "   0.6638000000516574,\n",
       "   0.6566444445649783,\n",
       "   0.6453999998172124,\n",
       "   0.6500222219030062,\n",
       "   0.6637555558482806,\n",
       "   0.6558666652441025,\n",
       "   0.6515777773658434,\n",
       "   0.6462222219506899,\n",
       "   0.6414444416761398,\n",
       "   0.6471333330869675,\n",
       "   0.6428666667143503,\n",
       "   0.6534666673342386,\n",
       "   0.6348888888955116,\n",
       "   0.6406888893246651,\n",
       "   0.6495999977986018,\n",
       "   0.6561777763565382,\n",
       "   0.6443333328763644,\n",
       "   0.650533335407575,\n",
       "   0.6500888887047768,\n",
       "   0.6494000001748403,\n",
       "   0.6580444438258807,\n",
       "   0.656777776479721,\n",
       "   0.6497555554906527,\n",
       "   0.6497555552919706,\n",
       "   0.6486666651566824,\n",
       "   0.6521777773896853,\n",
       "   0.6483555549383163,\n",
       "   0.6488444416721662,\n",
       "   0.6496444436907768,\n",
       "   0.6461777794361114,\n",
       "   0.63668888737758,\n",
       "   0.6458888879418373,\n",
       "   0.6422666652997335,\n",
       "   0.6536222207546234,\n",
       "   0.6508222218354544,\n",
       "   0.6467555559674899,\n",
       "   0.6398444453875224,\n",
       "   0.6462222215533256,\n",
       "   0.644888888100783,\n",
       "   0.6477999989191691,\n",
       "   0.641622221271197,\n",
       "   0.6532444428404173,\n",
       "   0.6415555547674497,\n",
       "   0.6355111107230187,\n",
       "   0.641488889058431,\n",
       "   0.6461999993522962,\n",
       "   0.6366000001629194,\n",
       "   0.6375333324074746,\n",
       "   0.6421777799725532,\n",
       "   0.6413999987641971,\n",
       "   0.6449777761101723,\n",
       "   0.6475333333015442,\n",
       "   0.6363333330551784,\n",
       "   0.6474444436033567,\n",
       "   0.6373555529117584,\n",
       "   0.6297777769962947,\n",
       "   0.6435111112395923,\n",
       "   0.638222222328186,\n",
       "   0.6323111111919085,\n",
       "   0.6290222202738126,\n",
       "   0.6374000003933906,\n",
       "   0.6324222209056218,\n",
       "   0.6293333333730697,\n",
       "   0.6418666660785675,\n",
       "   0.6247555546959241,\n",
       "   0.6349111094077429,\n",
       "   0.6412222208579381],\n",
       "  'val_accuracy_std': [0.05516310584021097,\n",
       "   0.062222740783300014,\n",
       "   0.06044362640592046,\n",
       "   0.06422898509849118,\n",
       "   0.06156325231949481,\n",
       "   0.06396951489267358,\n",
       "   0.06335787423807511,\n",
       "   0.06525474896865693,\n",
       "   0.06358117692219567,\n",
       "   0.06446093323676573,\n",
       "   0.06422039354086166,\n",
       "   0.06306919990372445,\n",
       "   0.06381214336100582,\n",
       "   0.06617350581299629,\n",
       "   0.06254542732697359,\n",
       "   0.0631798482924692,\n",
       "   0.0602668110892077,\n",
       "   0.06071840055988702,\n",
       "   0.063376637825541,\n",
       "   0.06133051440245502,\n",
       "   0.06495436555660075,\n",
       "   0.06261108311238289,\n",
       "   0.0632640518381525,\n",
       "   0.0625971687195545,\n",
       "   0.06365950727336793,\n",
       "   0.06277879822460206,\n",
       "   0.0628492971170289,\n",
       "   0.062443654879068514,\n",
       "   0.0632255412647561,\n",
       "   0.06166664351506224,\n",
       "   0.0612916900087434,\n",
       "   0.061317112307860994,\n",
       "   0.060373564850455363,\n",
       "   0.061390067513755045,\n",
       "   0.06463821754246606,\n",
       "   0.06231461172590854,\n",
       "   0.06195075312943577,\n",
       "   0.060756564971487015,\n",
       "   0.06034492915447629,\n",
       "   0.05994400706076412,\n",
       "   0.06404040118607529,\n",
       "   0.06255963790329026,\n",
       "   0.06337370696993418,\n",
       "   0.06167763420969228,\n",
       "   0.062377189550117965,\n",
       "   0.06250309206178832,\n",
       "   0.06378194518592348,\n",
       "   0.06029235969120016,\n",
       "   0.06353331192084341,\n",
       "   0.062119456810005184,\n",
       "   0.061298234340752475,\n",
       "   0.0621403070288746,\n",
       "   0.06132872775102989,\n",
       "   0.06410745241163902,\n",
       "   0.061440241055105774,\n",
       "   0.060868525717002214,\n",
       "   0.06036555469336161,\n",
       "   0.06152023314865968,\n",
       "   0.06163061625203172,\n",
       "   0.06328768756491926,\n",
       "   0.0624513422807729,\n",
       "   0.06245162667437933,\n",
       "   0.06069840101385462,\n",
       "   0.06096862336439414,\n",
       "   0.06244087243669194,\n",
       "   0.0630278469504204,\n",
       "   0.059182624045617145,\n",
       "   0.06157193631005244,\n",
       "   0.06318794951937026,\n",
       "   0.06499990695021346,\n",
       "   0.06112848218413441,\n",
       "   0.06382421532178541,\n",
       "   0.06472647495340995,\n",
       "   0.062484306782117686,\n",
       "   0.06135708130596122,\n",
       "   0.06360757928831422,\n",
       "   0.06227541705235981,\n",
       "   0.06304323150038796,\n",
       "   0.06569398303984514,\n",
       "   0.06198438400646222,\n",
       "   0.0611930620635446,\n",
       "   0.06172792269491632,\n",
       "   0.06386182750599162,\n",
       "   0.06644176559768565,\n",
       "   0.06622995818268906,\n",
       "   0.06546349527524721,\n",
       "   0.0647911717413525,\n",
       "   0.06462503447282493,\n",
       "   0.06389202489825638,\n",
       "   0.06471437857300913,\n",
       "   0.06650084644071387,\n",
       "   0.06367341125925265,\n",
       "   0.06409873155453785,\n",
       "   0.06480443187393072,\n",
       "   0.06703454160264968,\n",
       "   0.06303259770452244,\n",
       "   0.06362546386829204,\n",
       "   0.06559606730017811,\n",
       "   0.06498651315245119],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb0c68",
   "metadata": {},
   "source": [
    "### 1.1 MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     #print(key)\n",
    "#     if value.requires_grad:\n",
    "#         print(key)\n",
    "#         print(value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a599c8",
   "metadata": {},
   "source": [
    "### 1.2 Arbiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ebc67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = arbiter_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = arbiter_system.state['best_epoch']\n",
    "\n",
    "state = arbiter_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "arbiter_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "569eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0531d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a86b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "\n",
    "x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "\n",
    "\n",
    "x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task = next(zip(x_support_set,y_support_set,x_target_set, y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdeb442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "647183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbiter_x_support_set, arbiter_x_target_set, arbiter_y_support_set, arbiter_y_target_set, seed = train_sample\n",
    "\n",
    "arbiter_x_support_set = torch.Tensor(arbiter_x_support_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_x_target_set = torch.Tensor(arbiter_x_target_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_y_support_set = torch.Tensor(arbiter_y_support_set).long().to(device=arbiter_system.model.device)\n",
    "arbiter_y_target_set = torch.Tensor(arbiter_y_target_set).long().to(device=arbiter_system.model.device)\n",
    "\n",
    "\n",
    "arbiter_x_support_set_task, arbiter_y_support_set_task, arbiter_x_target_set_task, arbiter_y_target_set_task = next(zip(arbiter_x_support_set,arbiter_y_support_set,arbiter_x_target_set, arbiter_y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce1c0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd4d6e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\MAML\\meta_neural_network_architectures.py:993: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "names_weights_copy = arbiter_system.model.get_inner_loop_parameter_dict(arbiter_system.model.classifier.named_parameters())\n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = arbiter_x_target_set_task.shape\n",
    "\n",
    "arbiter_x_support_set_task = arbiter_x_support_set_task.view(-1, c, h, w)\n",
    "arbiter_y_support_set_task = arbiter_y_support_set_task.view(-1)\n",
    "arbiter_x_target_set_task = arbiter_x_target_set_task.view(-1, c, h, w)\n",
    "arbiter_y_target_set_task = arbiter_y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = arbiter_system.model.net_forward(\n",
    "            x=arbiter_x_support_set_task,\n",
    "            y=arbiter_y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "    \n",
    "    if arbiter_system.model.args.arbiter:\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                retain_graph=True)\n",
    "\n",
    "        names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "\n",
    "        for key, weight in names_weights_copy.items():\n",
    "            weight_norm = torch.norm(weight, p=2)\n",
    "            per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "        for key, grad in names_grads_copy.items():\n",
    "            gradient_l2norm = torch.norm(grad, p=2)\n",
    "            per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "        per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                    per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        generated_gradient_rate = arbiter_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "            g += 1\n",
    "\n",
    "    names_weights_copy,names_grads_copy = arbiter_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                      support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_arbiter.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=arbiter_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in arbiter_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d16650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "y_support_set_task = y_support_set_task.view(-1)\n",
    "x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "y_target_set_task = y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "            x=x_support_set_task,\n",
    "            y=y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "\n",
    "\n",
    "    names_weights_copy,names_grads_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                   support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_maml.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=maml_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575454f0",
   "metadata": {},
   "source": [
    "## landscape 함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aec9618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape==  torch.Size([25, 3, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "tensor([ 1.5249e-03,  2.5939e-03,  9.9636e-04, -2.2071e-03,  3.3533e-04,\n",
      "        -5.8379e-04, -2.8012e-03, -1.7969e-03,  1.1807e-03,  2.2465e-03,\n",
      "         1.9939e-03,  3.0633e-04,  2.9469e-03, -1.6753e-03,  9.6097e-04,\n",
      "         3.2720e-04, -1.8252e-04,  3.5936e-03,  2.9323e-03, -1.2352e-04,\n",
      "        -3.5142e-03,  2.7021e-03, -1.3823e-03, -4.8046e-03, -1.2189e-03,\n",
      "        -1.0183e-03, -2.0403e-03,  1.9734e-03,  6.8108e-04,  2.3642e-03,\n",
      "         1.0462e-03, -3.8390e-03, -1.0657e-03, -3.9016e-04, -1.1164e-03,\n",
      "        -3.4185e-03,  3.7633e-04,  2.3697e-04,  5.6810e-04,  3.2179e-03,\n",
      "        -5.9915e-03, -1.7371e-04, -2.0750e-05,  3.6499e-04,  1.9583e-03,\n",
      "         1.2003e-03, -3.5386e-03, -1.4886e-03, -1.8121e-03,  7.1004e-04,\n",
      "        -8.3975e-04,  4.4664e-04,  5.0234e-04,  3.9900e-04, -4.4899e-04,\n",
      "        -1.8726e-03, -1.0480e-03,  2.6443e-03,  2.9035e-04,  8.8210e-04,\n",
      "         8.9815e-04, -1.0449e-04,  6.5409e-03,  1.6693e-03], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-3.2742e-11,  1.0914e-11,  7.2760e-11,  2.5466e-11,  8.7311e-11,\n",
      "        -3.6380e-12, -2.3283e-10, -5.8208e-11,  1.2369e-10, -1.4552e-11,\n",
      "        -7.2760e-12,  3.0923e-11,  4.3656e-11,  0.0000e+00, -5.8208e-11,\n",
      "         8.7311e-11,  2.3283e-10,  1.4552e-11, -2.3647e-11,  5.8208e-11,\n",
      "        -2.9104e-11,  2.0373e-10, -2.9104e-11,  1.3097e-10, -1.7462e-10,\n",
      "        -7.2760e-12,  5.0932e-11, -1.4552e-11, -1.4552e-11,  1.4552e-11,\n",
      "         1.8190e-11,  8.7311e-11,  6.5484e-11, -1.4552e-11,  5.8208e-11,\n",
      "        -7.2760e-11, -2.6375e-11,  2.9104e-11,  0.0000e+00, -3.6380e-12,\n",
      "        -1.0914e-11,  1.1642e-10,  5.4570e-11, -1.4552e-11,  2.1828e-11,\n",
      "         5.8208e-11,  1.7462e-10,  1.1642e-10, -1.4552e-11, -5.4570e-11,\n",
      "        -5.8208e-11,  4.3656e-11,  1.7280e-11, -3.4561e-11,  0.0000e+00,\n",
      "         1.3097e-10, -2.9104e-11, -5.8208e-11,  7.2760e-11, -1.3097e-10,\n",
      "        -9.0949e-11,  7.2760e-11,  0.0000e+00,  7.2760e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 2.5495e-03, -7.4685e-04,  1.6504e-03,  1.6548e-03,  3.1915e-03,\n",
      "         1.4340e-03, -2.5195e-03, -4.9171e-03,  4.1357e-03,  6.1917e-03,\n",
      "         8.1100e-04, -2.4995e-05, -6.9167e-04,  2.2759e-03, -3.1627e-03,\n",
      "        -1.3497e-03,  2.5692e-03, -3.8254e-03,  2.9106e-03,  2.2195e-03,\n",
      "         5.8007e-04,  1.5191e-03,  2.0182e-03, -5.2815e-03, -3.5953e-03,\n",
      "         4.7154e-03, -1.6890e-03, -3.0604e-03, -1.1070e-03,  3.4207e-03,\n",
      "        -2.3478e-03,  8.2306e-03, -2.8878e-03, -9.4841e-04,  1.3996e-03,\n",
      "         1.2891e-03, -1.6564e-03, -3.7399e-03,  7.8731e-04, -2.1945e-03,\n",
      "         3.6922e-04,  3.7780e-04, -1.2624e-03,  3.8149e-04, -3.8230e-03,\n",
      "         1.4445e-03, -2.2510e-03,  5.0234e-03,  7.3342e-04,  1.8208e-04,\n",
      "        -2.9444e-03, -2.6559e-03, -4.6968e-03, -3.1347e-03,  2.9351e-03,\n",
      "        -5.4668e-04, -5.8196e-03,  1.5787e-03,  3.0035e-03, -3.2358e-03,\n",
      "        -4.5180e-04,  6.0430e-03,  3.8977e-04,  6.1926e-03], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 2.8706e-03, -1.3409e-03,  1.3389e-03,  1.0792e-03,  1.8823e-03,\n",
      "         7.5671e-04, -3.0310e-03, -1.1359e-03,  3.3419e-03,  7.8482e-03,\n",
      "         1.9939e-03,  4.8795e-04,  2.8729e-04,  3.9020e-03, -1.8003e-03,\n",
      "        -3.0382e-03,  3.9832e-03, -4.8636e-03,  1.5877e-03,  9.2879e-04,\n",
      "         2.0329e-03,  3.4291e-03,  5.8840e-04, -5.0405e-03, -1.7886e-03,\n",
      "         3.0346e-03, -2.2163e-03, -6.1876e-03, -3.1668e-03,  2.2278e-03,\n",
      "        -5.0006e-03,  1.0629e-02, -2.3620e-03, -1.6070e-03,  1.8616e-03,\n",
      "         1.6805e-03, -1.9542e-03, -3.7496e-03,  1.2396e-03, -2.0971e-04,\n",
      "         2.6520e-04, -4.7673e-04, -1.6719e-03,  3.0631e-03, -5.3471e-03,\n",
      "         6.0525e-04, -1.6088e-03,  5.5461e-03, -4.7702e-04, -3.8383e-03,\n",
      "        -3.3967e-04,  9.2484e-07, -3.8924e-03, -3.7811e-03,  3.8037e-03,\n",
      "        -2.3791e-03, -6.7394e-03,  1.1413e-03,  3.5446e-03, -3.1297e-03,\n",
      "         1.1275e-04,  4.5666e-03,  4.8030e-04,  9.0615e-03], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 1.2445e-04,  1.3019e-04,  1.5163e-04],\n",
      "          [ 1.1389e-04,  1.2545e-04,  1.2914e-04],\n",
      "          [ 7.5542e-05,  6.9480e-05,  6.4035e-05]],\n",
      "\n",
      "         [[ 1.2830e-04,  4.5067e-05, -3.2206e-05],\n",
      "          [ 1.4098e-04,  6.8066e-05, -2.4441e-05],\n",
      "          [ 5.0319e-05,  1.1038e-04,  1.0447e-04]],\n",
      "\n",
      "         [[-1.6078e-04, -2.1116e-04, -5.3694e-05],\n",
      "          [-5.6631e-05, -1.6724e-04,  3.7872e-06],\n",
      "          [ 5.4323e-05, -9.0126e-05, -4.1851e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8989e-04,  1.2661e-04,  6.3887e-05],\n",
      "          [ 1.4500e-04,  1.5936e-04, -6.4875e-05],\n",
      "          [ 2.3779e-05,  1.2491e-04, -8.0693e-05]],\n",
      "\n",
      "         [[ 1.9954e-04,  2.7515e-04,  3.3269e-04],\n",
      "          [ 1.8078e-04,  2.5690e-04,  3.3234e-04],\n",
      "          [ 1.8995e-04,  2.4418e-04,  2.2560e-04]],\n",
      "\n",
      "         [[ 1.2189e-04,  9.5333e-07, -9.3030e-06],\n",
      "          [ 4.7892e-05,  2.9543e-04, -2.4665e-04],\n",
      "          [-2.5054e-05,  1.3061e-05,  2.8247e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.9955e-04, -1.9333e-04, -1.0087e-05],\n",
      "          [-2.1258e-04, -2.1311e-04, -4.8347e-06],\n",
      "          [-1.8720e-04, -1.9110e-04, -1.7069e-05]],\n",
      "\n",
      "         [[-1.2776e-05,  1.3752e-04,  3.1271e-04],\n",
      "          [ 3.4877e-05,  1.1657e-04,  1.8185e-04],\n",
      "          [ 9.2713e-05,  1.6296e-04,  2.8730e-04]],\n",
      "\n",
      "         [[-1.7511e-04, -6.1443e-05, -1.6338e-04],\n",
      "          [-1.3119e-04, -1.1861e-04, -8.7951e-05],\n",
      "          [-7.6191e-05, -3.3983e-05, -4.4705e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9054e-04, -1.6948e-04,  7.9543e-05],\n",
      "          [-2.1007e-04, -1.6443e-04, -9.8203e-05],\n",
      "          [-2.4593e-04, -2.2801e-04, -1.2116e-04]],\n",
      "\n",
      "         [[-6.4558e-05, -4.8517e-05,  3.3561e-05],\n",
      "          [-3.0443e-05, -7.9102e-05,  3.9357e-05],\n",
      "          [-1.1555e-04, -1.1799e-04,  4.7123e-05]],\n",
      "\n",
      "         [[ 2.3002e-04, -6.0127e-05,  1.7612e-04],\n",
      "          [ 1.2975e-04,  1.5102e-04,  8.2815e-05],\n",
      "          [ 5.9511e-05,  2.6821e-04,  2.1707e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.1524e-04, -1.4330e-04, -1.2882e-04],\n",
      "          [-9.3972e-05, -1.2971e-04, -1.6386e-04],\n",
      "          [-1.2216e-04, -6.2224e-05, -7.6167e-05]],\n",
      "\n",
      "         [[ 2.7812e-04,  1.1126e-04,  4.3281e-04],\n",
      "          [ 1.0042e-04,  1.3822e-05,  1.0464e-04],\n",
      "          [-1.4065e-04, -1.2466e-05,  1.3581e-04]],\n",
      "\n",
      "         [[ 1.6701e-04,  2.3863e-04,  1.9412e-04],\n",
      "          [ 2.7399e-04,  3.0667e-04,  2.7108e-04],\n",
      "          [ 1.7714e-04,  2.6857e-04,  2.1598e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.0429e-04, -5.8696e-04, -4.6669e-04],\n",
      "          [-4.0529e-04, -4.4709e-04, -4.4504e-04],\n",
      "          [-2.5977e-04, -3.1037e-04, -2.5647e-04]],\n",
      "\n",
      "         [[-6.5525e-05, -1.4142e-04, -8.3419e-05],\n",
      "          [-1.7921e-04, -1.2008e-04, -1.3112e-04],\n",
      "          [-1.6161e-04, -7.1637e-05,  1.3782e-05]],\n",
      "\n",
      "         [[ 1.6481e-04,  5.0678e-04,  3.7654e-04],\n",
      "          [ 3.0979e-04,  2.2714e-04, -1.6225e-04],\n",
      "          [ 2.2886e-05,  9.2640e-05, -1.9334e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7329e-04, -5.7186e-05, -1.0219e-04],\n",
      "          [-1.9018e-04, -2.6189e-05, -7.8802e-05],\n",
      "          [-1.5052e-04, -1.7356e-05, -1.2750e-04]],\n",
      "\n",
      "         [[-2.9320e-05, -5.0702e-05, -2.4451e-05],\n",
      "          [-1.8156e-04, -1.8648e-04, -1.6796e-04],\n",
      "          [-5.8468e-05, -1.0699e-04, -1.6458e-04]],\n",
      "\n",
      "         [[-2.0618e-04, -1.6032e-04, -1.7593e-04],\n",
      "          [-3.6947e-04, -3.5815e-04, -2.5589e-04],\n",
      "          [-2.5371e-04, -3.2670e-04, -3.6505e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.4082e-05,  2.6456e-04,  6.8425e-05],\n",
      "          [ 8.6236e-06,  2.1970e-04,  1.9745e-04],\n",
      "          [-3.1050e-04, -2.3226e-04,  1.0577e-04]],\n",
      "\n",
      "         [[-5.6070e-05, -6.1158e-05,  3.9831e-05],\n",
      "          [-1.0524e-05, -8.8250e-05, -9.3699e-05],\n",
      "          [-2.8488e-05, -2.5440e-05, -1.1369e-04]],\n",
      "\n",
      "         [[ 1.3945e-05, -3.3136e-05,  6.9045e-05],\n",
      "          [ 1.0771e-04,  6.7023e-05, -3.7300e-05],\n",
      "          [ 1.8903e-05,  2.8258e-04,  1.6545e-04]]],\n",
      "\n",
      "\n",
      "        [[[-9.7379e-05, -2.0449e-04, -1.6060e-04],\n",
      "          [-8.3936e-05, -1.7806e-04, -1.5092e-04],\n",
      "          [-1.0093e-04, -2.1853e-04, -1.0201e-04]],\n",
      "\n",
      "         [[-1.6089e-04, -8.2181e-05,  2.1213e-05],\n",
      "          [-1.1251e-04, -2.1167e-04, -1.1103e-04],\n",
      "          [-1.2659e-04, -1.8025e-04, -6.6670e-05]],\n",
      "\n",
      "         [[ 2.1286e-05, -7.3451e-05, -5.3009e-05],\n",
      "          [-1.7104e-04, -2.4368e-04, -3.3982e-04],\n",
      "          [-8.4677e-05, -1.6215e-04, -2.8201e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7797e-05, -9.7387e-06,  2.2926e-05],\n",
      "          [ 2.2366e-04,  1.1323e-05, -4.5875e-05],\n",
      "          [ 3.1955e-04,  1.3594e-04,  3.2865e-05]],\n",
      "\n",
      "         [[-3.5285e-04, -2.9621e-04, -1.7231e-04],\n",
      "          [-3.1513e-04, -4.4777e-04, -2.6313e-04],\n",
      "          [-2.8265e-04, -3.5887e-04, -1.8589e-04]],\n",
      "\n",
      "         [[ 3.0187e-04, -1.1294e-04,  1.2772e-04],\n",
      "          [ 1.4052e-04,  2.9259e-06,  7.9382e-05],\n",
      "          [ 1.4435e-04, -5.7102e-05, -3.3662e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.1109e-04, -5.3976e-05,  2.7763e-05],\n",
      "          [-2.4031e-04,  3.9196e-05, -7.6865e-05],\n",
      "          [ 4.2739e-05,  1.1893e-04,  5.9059e-07]],\n",
      "\n",
      "         [[ 3.4392e-04,  3.0510e-04,  3.3232e-04],\n",
      "          [ 5.0591e-04,  3.3846e-04,  6.1293e-05],\n",
      "          [ 2.8014e-04,  4.4744e-04,  3.2749e-04]],\n",
      "\n",
      "         [[ 7.4701e-04,  5.7315e-04,  3.4536e-04],\n",
      "          [ 7.3186e-04,  4.5272e-04,  1.5783e-04],\n",
      "          [ 9.0919e-04,  3.9746e-04,  1.6447e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1759e-04, -3.8592e-04, -3.3203e-04],\n",
      "          [-3.5203e-04, -2.3659e-04, -4.0069e-04],\n",
      "          [-8.7195e-05, -9.8337e-05, -3.1214e-04]],\n",
      "\n",
      "         [[ 1.5668e-04, -9.4478e-06,  5.2693e-06],\n",
      "          [ 1.4293e-04,  5.6850e-05, -3.8014e-05],\n",
      "          [ 1.8948e-04,  1.1250e-04, -4.5962e-05]],\n",
      "\n",
      "         [[ 1.4461e-04,  5.1586e-04,  2.0151e-04],\n",
      "          [ 3.5661e-05,  3.2105e-04, -4.0990e-05],\n",
      "          [ 1.6643e-06,  2.1708e-04,  1.8213e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-2.1828e-10,  0.0000e+00,  1.6007e-10,  4.3656e-11,  2.3283e-10,\n",
      "        -8.7311e-11,  1.3097e-10,  1.6735e-10, -7.2760e-11,  8.7311e-11,\n",
      "        -8.3674e-11,  0.0000e+00,  2.0373e-10, -1.4552e-11, -5.8208e-11,\n",
      "         3.6380e-11,  2.1828e-10,  3.2742e-10,  2.3283e-10,  2.9104e-11,\n",
      "         1.3824e-10,  1.7462e-10, -2.9104e-11,  2.3283e-10, -1.1642e-10,\n",
      "         7.2760e-12, -9.4587e-11, -1.1642e-10,  2.3283e-10, -2.9104e-11,\n",
      "        -6.5484e-11,  2.9104e-10, -3.4925e-10, -3.2014e-10,  8.7311e-11,\n",
      "        -1.0914e-10,  2.9104e-11,  0.0000e+00,  1.0186e-10,  4.3656e-11,\n",
      "         4.3656e-11, -4.3656e-11, -2.6193e-10, -1.4552e-11, -1.4552e-10,\n",
      "        -1.4552e-10,  1.8917e-10,  2.8376e-10, -1.1642e-10,  6.5484e-11,\n",
      "        -1.7462e-10, -8.3674e-11, -4.6566e-10,  3.7835e-10,  5.0932e-11,\n",
      "        -5.8208e-11,  5.8208e-11,  2.0373e-10, -1.6007e-10, -5.8208e-11,\n",
      "         4.3656e-11, -1.8917e-10,  3.2014e-10,  4.0745e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-4.4576e-03,  1.5367e-03, -4.3512e-03,  1.9533e-03, -2.3225e-04,\n",
      "         1.0393e-03,  3.3741e-03, -5.3040e-03,  9.9886e-04, -9.7317e-04,\n",
      "         1.7550e-03,  1.4201e-03,  5.2176e-03,  2.5112e-03, -5.2799e-03,\n",
      "         5.3403e-04,  2.5511e-03,  2.0940e-03, -3.1284e-04,  1.6367e-03,\n",
      "        -8.9514e-05,  4.5419e-03, -3.3664e-03, -3.0009e-03, -7.3477e-04,\n",
      "         3.0407e-03, -1.5758e-03,  1.4785e-03, -1.4455e-03, -3.4701e-03,\n",
      "        -2.9305e-04,  7.0845e-03,  2.7227e-03, -2.4610e-03, -4.6942e-04,\n",
      "         4.1435e-03,  1.5641e-03, -8.6099e-04,  1.2502e-03, -3.9921e-03,\n",
      "         2.0733e-03, -2.1790e-03, -2.9217e-05,  6.1773e-03, -1.8579e-03,\n",
      "        -3.8066e-03,  6.6392e-03,  4.7467e-03,  5.0783e-03,  1.1561e-03,\n",
      "        -1.3370e-03,  1.0754e-04, -6.6527e-03, -1.1290e-03,  1.1331e-02,\n",
      "        -7.6820e-04, -5.0500e-03,  3.2941e-03, -4.7973e-03, -1.1950e-03,\n",
      "        -5.5891e-03, -1.5152e-03,  9.9487e-04, -4.9901e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-0.0054,  0.0033, -0.0041,  0.0026, -0.0014, -0.0004,  0.0051, -0.0117,\n",
      "         0.0005, -0.0028,  0.0032,  0.0022,  0.0085,  0.0048, -0.0104,  0.0023,\n",
      "         0.0037,  0.0024,  0.0006,  0.0026, -0.0013,  0.0041, -0.0030, -0.0053,\n",
      "         0.0013,  0.0024, -0.0026, -0.0002, -0.0033, -0.0037, -0.0006,  0.0093,\n",
      "         0.0029, -0.0018, -0.0005,  0.0067,  0.0028, -0.0016,  0.0022, -0.0060,\n",
      "         0.0033, -0.0054,  0.0026,  0.0146, -0.0011, -0.0070,  0.0113,  0.0054,\n",
      "         0.0075,  0.0009, -0.0032, -0.0010, -0.0112, -0.0024,  0.0079, -0.0016,\n",
      "        -0.0073,  0.0040, -0.0041, -0.0006, -0.0084, -0.0014, -0.0014,  0.0012],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([[ 0.0020, -0.0041, -0.0063,  ...,  0.0003,  0.0027,  0.0021],\n",
      "        [ 0.0056,  0.0114,  0.0095,  ...,  0.0022,  0.0038,  0.0012],\n",
      "        [ 0.0126,  0.0161,  0.0042,  ...,  0.0039,  0.0155,  0.0045],\n",
      "        [-0.0182, -0.0238, -0.0075,  ..., -0.0077, -0.0249, -0.0058],\n",
      "        [-0.0019,  0.0005,  0.0002,  ...,  0.0013,  0.0028, -0.0020]],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([ 0.0009,  0.0006,  0.0019, -0.0038,  0.0003], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.7462e-10,  1.7553e-10,  4.3656e-11, -1.0914e-10,  8.7311e-11,\n",
      "         2.9104e-11,  1.8190e-12,  9.0949e-12, -1.6371e-11,  1.0914e-11,\n",
      "         6.9122e-11,  1.0186e-10,  5.8208e-11, -3.6380e-12, -4.3656e-11,\n",
      "         1.0914e-11,  1.4552e-11, -1.0914e-11,  8.0036e-11,  2.9559e-12,\n",
      "        -8.0036e-11, -9.0949e-13,  0.0000e+00, -9.0949e-13, -2.7285e-12,\n",
      "        -5.0022e-12, -2.7285e-12,  5.8208e-11, -5.4570e-12, -9.0949e-12,\n",
      "        -6.3665e-12,  9.0949e-13,  5.1841e-11, -3.4925e-10,  1.0004e-11,\n",
      "        -1.3642e-12, -6.3665e-12,  5.4570e-11, -1.8190e-12,  4.0018e-11,\n",
      "        -9.0949e-13,  1.4552e-10,  1.0186e-10, -2.5466e-11, -3.7517e-12,\n",
      "         5.8208e-11,  1.4552e-11, -3.1832e-12, -7.2760e-12,  3.6380e-11,\n",
      "         7.9581e-12, -4.0018e-11,  4.3656e-11, -3.6380e-12,  2.8194e-11,\n",
      "        -8.1855e-12,  3.6380e-11,  2.0373e-10, -9.0949e-13, -8.7311e-11,\n",
      "        -2.5011e-12,  2.5466e-11,  2.9104e-11,  2.0236e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.1757e-04,  2.9079e-05,  1.4998e-04,  6.3411e-05,  1.2973e-04,\n",
      "        -5.8227e-05,  1.1739e-04,  1.6825e-04, -1.9815e-05,  8.7793e-05,\n",
      "         1.6934e-04,  4.8374e-05,  3.0295e-05, -4.9496e-05,  5.0331e-05,\n",
      "         4.5530e-05,  8.8362e-05,  7.2769e-05,  8.9265e-05,  7.8438e-05,\n",
      "         9.7053e-05, -5.3566e-05, -8.2723e-05, -6.9496e-06,  6.9512e-05,\n",
      "         7.9193e-05,  9.4484e-05,  1.6185e-05,  3.6833e-05,  7.3092e-05,\n",
      "         9.5672e-05, -5.0793e-05,  1.3398e-04, -7.6820e-05,  7.4738e-05,\n",
      "        -6.9251e-05,  5.6813e-05,  1.0665e-04, -5.8641e-05, -4.5312e-06,\n",
      "         8.2789e-06, -3.6594e-06,  2.5836e-05, -3.3305e-05, -5.2377e-05,\n",
      "         9.6216e-05,  8.6049e-05,  1.4369e-05,  4.0929e-05,  6.8450e-05,\n",
      "         1.0108e-04, -6.6463e-05,  6.4943e-05,  2.9856e-05, -1.6719e-05,\n",
      "         7.9143e-05, -5.5826e-05,  7.3683e-05, -5.1903e-05,  8.1510e-05,\n",
      "        -6.0528e-05,  4.7309e-05,  3.8679e-05,  1.1322e-05], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-2.7664e-05, -4.8975e-06, -2.0080e-05],\n",
      "          [-1.2068e-05,  7.4652e-06, -1.4494e-05],\n",
      "          [-1.5609e-05,  3.6220e-06, -2.2530e-05]],\n",
      "\n",
      "         [[-2.7875e-05, -1.4821e-05, -1.2014e-05],\n",
      "          [-1.8643e-05, -1.6331e-05, -1.6124e-05],\n",
      "          [-1.9170e-05, -1.3232e-05, -1.0521e-05]],\n",
      "\n",
      "         [[-2.2622e-05, -3.1214e-06,  5.2141e-06],\n",
      "          [-1.9989e-05, -7.6798e-06, -2.8165e-06],\n",
      "          [-2.8161e-05, -5.0689e-06, -8.2902e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.8003e-06,  1.2001e-05, -7.2121e-07],\n",
      "          [-1.2176e-06,  6.8231e-06,  6.8985e-06],\n",
      "          [-1.0957e-05,  9.2779e-06,  4.6018e-06]],\n",
      "\n",
      "         [[-1.6054e-05, -1.3256e-05, -1.9751e-05],\n",
      "          [-1.2237e-05, -8.3704e-06, -1.2894e-05],\n",
      "          [-1.4115e-05, -6.5011e-06, -9.8726e-06]],\n",
      "\n",
      "         [[-2.3440e-05, -2.6169e-05, -2.2140e-05],\n",
      "          [-1.3956e-05, -1.8689e-05, -1.9281e-05],\n",
      "          [-1.1786e-05, -1.3437e-05, -1.7415e-05]]],\n",
      "\n",
      "\n",
      "        [[[-7.5420e-06, -4.7035e-06, -6.1295e-06],\n",
      "          [-2.3056e-05, -7.6702e-06,  3.3792e-07],\n",
      "          [-2.1373e-05, -1.5589e-05, -2.2572e-05]],\n",
      "\n",
      "         [[ 1.6486e-06,  3.6138e-06,  2.9120e-06],\n",
      "          [-1.1669e-06, -9.2738e-07, -3.4399e-06],\n",
      "          [-1.4784e-05, -1.1030e-05, -1.0530e-05]],\n",
      "\n",
      "         [[-6.0057e-06, -1.2225e-05, -3.6766e-06],\n",
      "          [-2.0257e-05, -3.8672e-06, -6.1836e-06],\n",
      "          [-2.6531e-05, -1.2048e-05, -1.4274e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0141e-05, -1.3710e-05, -3.8849e-07],\n",
      "          [-6.9314e-06, -2.8412e-06,  4.8894e-07],\n",
      "          [-1.3181e-05,  3.4950e-06,  4.0144e-06]],\n",
      "\n",
      "         [[-1.5982e-05, -9.6709e-06, -8.4714e-06],\n",
      "          [-1.8335e-05, -1.4085e-05, -1.4038e-05],\n",
      "          [-1.7541e-05, -1.3953e-05, -1.3763e-05]],\n",
      "\n",
      "         [[-3.8139e-07, -1.5097e-06, -2.5605e-06],\n",
      "          [ 2.8990e-06, -1.3181e-06, -1.3239e-06],\n",
      "          [-5.8250e-06, -3.4674e-06, -2.5510e-06]]],\n",
      "\n",
      "\n",
      "        [[[-5.3209e-07,  2.2557e-06,  8.9007e-06],\n",
      "          [ 3.3692e-06,  1.0677e-05,  1.2550e-05],\n",
      "          [ 5.2232e-06,  4.0774e-06,  1.7114e-06]],\n",
      "\n",
      "         [[-9.9119e-07, -5.2302e-06, -5.1048e-06],\n",
      "          [ 2.6666e-06, -2.2542e-06, -6.1850e-06],\n",
      "          [ 1.3588e-06, -3.2432e-06, -1.4931e-06]],\n",
      "\n",
      "         [[-8.6750e-06, -1.1653e-05, -9.9958e-06],\n",
      "          [-9.5915e-06, -1.0271e-05, -1.1593e-05],\n",
      "          [-8.5144e-06, -8.4464e-06, -1.1061e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.2725e-06, -1.2492e-05, -9.3990e-06],\n",
      "          [-8.3360e-06, -1.3124e-05, -7.2886e-06],\n",
      "          [-8.7784e-06, -1.2489e-05, -4.6850e-06]],\n",
      "\n",
      "         [[ 8.8772e-06,  8.2573e-06,  7.5613e-06],\n",
      "          [ 1.0278e-05,  1.1136e-05,  1.1602e-05],\n",
      "          [ 9.9509e-06,  1.0702e-05,  1.0300e-05]],\n",
      "\n",
      "         [[ 5.8492e-06,  3.2341e-06,  1.6604e-06],\n",
      "          [ 4.0248e-06,  1.3229e-06,  3.4393e-06],\n",
      "          [ 6.3901e-06,  3.2506e-06,  6.0326e-06]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1673e-05,  2.1812e-06, -2.3025e-05],\n",
      "          [-1.4391e-05,  1.1913e-05, -9.7620e-06],\n",
      "          [-3.3886e-06,  2.0427e-05, -3.3154e-06]],\n",
      "\n",
      "         [[-1.3119e-05,  2.9825e-06, -9.0714e-06],\n",
      "          [ 1.1767e-05,  3.1694e-05,  2.4762e-05],\n",
      "          [-2.2962e-06,  1.7639e-05,  7.9246e-06]],\n",
      "\n",
      "         [[-2.6564e-05, -6.2290e-06, -2.8354e-05],\n",
      "          [-1.5049e-05,  1.0811e-05, -1.0858e-05],\n",
      "          [-6.5019e-06,  1.4265e-05,  1.7811e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1732e-05,  2.7441e-05, -4.2726e-05],\n",
      "          [-5.6075e-06,  3.7905e-05, -3.6521e-05],\n",
      "          [-5.5066e-06,  3.9801e-05, -2.9114e-05]],\n",
      "\n",
      "         [[-2.1743e-05, -1.8313e-05, -1.7585e-05],\n",
      "          [-1.4598e-05, -1.1599e-05, -1.0002e-05],\n",
      "          [-1.0786e-05, -8.5640e-06, -5.8968e-06]],\n",
      "\n",
      "         [[-8.9085e-06, -3.0587e-06, -6.4535e-06],\n",
      "          [-5.7785e-06,  6.3388e-07,  3.6560e-07],\n",
      "          [-5.2363e-06,  1.5901e-06,  1.0120e-06]]],\n",
      "\n",
      "\n",
      "        [[[ 3.7467e-05,  3.2153e-05,  4.4849e-05],\n",
      "          [ 3.8979e-05,  6.4126e-06,  2.4945e-05],\n",
      "          [ 2.8378e-05,  1.3139e-05,  8.9252e-06]],\n",
      "\n",
      "         [[ 1.2613e-05,  9.6487e-06,  3.9026e-05],\n",
      "          [ 8.4345e-06,  9.4035e-06,  1.3641e-05],\n",
      "          [ 3.3019e-05,  2.9956e-05,  2.3787e-05]],\n",
      "\n",
      "         [[ 5.3753e-05,  5.2050e-05,  4.6079e-05],\n",
      "          [ 5.1211e-05,  5.7518e-05,  2.1046e-05],\n",
      "          [ 5.0214e-05,  6.6598e-05,  3.2544e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.8143e-05,  6.3762e-05,  5.4493e-05],\n",
      "          [ 2.9023e-05,  9.6183e-05,  4.2675e-05],\n",
      "          [ 5.4705e-06,  1.0371e-04,  5.1942e-05]],\n",
      "\n",
      "         [[ 2.0819e-05,  1.0336e-05,  6.5029e-06],\n",
      "          [ 1.9776e-05,  8.8390e-06, -1.5325e-06],\n",
      "          [ 1.0614e-05,  3.1243e-06, -1.1113e-06]],\n",
      "\n",
      "         [[ 7.5725e-06,  1.2004e-05,  1.8199e-05],\n",
      "          [ 6.7719e-06,  5.2474e-06,  6.1960e-06],\n",
      "          [ 6.1047e-06,  8.8677e-06,  1.0259e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.7156e-06, -4.4258e-06,  1.8479e-06],\n",
      "          [-1.5401e-05, -1.3553e-05,  1.5509e-05],\n",
      "          [-7.6327e-06, -6.5845e-06,  1.0619e-05]],\n",
      "\n",
      "         [[-4.3120e-05, -2.9502e-05, -1.6782e-05],\n",
      "          [-3.0504e-05, -1.9682e-05, -1.9835e-05],\n",
      "          [-2.6719e-05, -2.0235e-05, -2.0222e-05]],\n",
      "\n",
      "         [[-3.0786e-05,  2.5580e-05,  6.2754e-06],\n",
      "          [-3.2784e-05, -1.9967e-06,  1.6230e-05],\n",
      "          [-3.8147e-05, -4.2454e-06,  8.5911e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7564e-05,  3.6031e-05,  3.4167e-07],\n",
      "          [-5.4754e-05,  4.7959e-05,  3.9988e-07],\n",
      "          [-4.3392e-05,  4.5915e-05, -1.0535e-05]],\n",
      "\n",
      "         [[-4.4044e-06,  4.0393e-06,  8.1793e-06],\n",
      "          [-1.9348e-06, -1.8026e-06,  1.1928e-05],\n",
      "          [ 6.8326e-07,  6.5201e-06,  1.1213e-05]],\n",
      "\n",
      "         [[-1.9027e-05, -1.5316e-05, -1.1487e-05],\n",
      "          [-1.7891e-05, -1.3271e-05, -1.0630e-05],\n",
      "          [-2.0507e-05, -1.3940e-05, -8.5675e-06]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\autograd\\__init__.py:199: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\engine.cpp:1064.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.3234e-05, -1.6537e-04,  2.1394e-05, -2.7195e-04, -5.3385e-05,\n",
      "         1.4124e-04,  1.7165e-04,  5.2831e-04,  4.8400e-05, -2.8307e-05,\n",
      "         1.0893e-04, -3.5548e-05, -1.3021e-04,  1.9612e-04, -3.2714e-05,\n",
      "         1.2653e-04, -1.6857e-04,  5.5910e-05, -1.2382e-04,  4.4672e-05,\n",
      "         2.6159e-04,  6.4523e-05, -2.8308e-04, -2.8123e-04, -5.6220e-05,\n",
      "        -5.0554e-06, -4.4277e-05,  5.8170e-05,  2.5390e-04, -2.6828e-04,\n",
      "        -1.3316e-04, -1.4892e-04, -1.8894e-04,  4.7638e-05, -7.7088e-05,\n",
      "        -4.0302e-05,  1.8341e-04, -1.2855e-05, -1.1027e-04, -1.2956e-04,\n",
      "         4.9580e-05, -3.1995e-04, -2.0830e-05, -2.2187e-04,  9.3053e-05,\n",
      "         2.2175e-04,  1.0776e-04,  2.8328e-05,  1.6775e-04, -1.2522e-04,\n",
      "         1.3997e-05,  1.5810e-04, -5.9695e-06, -1.6364e-04,  1.4273e-04,\n",
      "         2.0940e-04,  2.7751e-04, -4.0691e-05,  1.4500e-04,  1.7132e-05,\n",
      "        -1.2797e-04,  2.9938e-05,  5.0914e-06,  4.1263e-05], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-2.6708e-06,  2.4783e-06,  8.1934e-06],\n",
      "          [ 1.0832e-05,  1.6827e-05,  1.8862e-05],\n",
      "          [ 2.1915e-06,  3.5440e-06,  1.3417e-06]],\n",
      "\n",
      "         [[-7.0468e-06, -1.0563e-05, -9.8543e-06],\n",
      "          [ 1.3244e-06, -1.7808e-05, -9.7661e-06],\n",
      "          [ 9.1169e-06, -9.8217e-06, -6.2669e-06]],\n",
      "\n",
      "         [[ 2.3570e-07, -5.8917e-07, -3.7840e-06],\n",
      "          [ 2.2492e-05,  2.4376e-05,  3.3963e-05],\n",
      "          [ 4.7838e-06,  5.4818e-06,  5.7955e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8679e-06, -3.8728e-06, -5.9497e-06],\n",
      "          [ 1.8479e-05,  2.3770e-05,  1.7686e-05],\n",
      "          [ 5.0877e-06,  6.2257e-06,  3.7287e-07]],\n",
      "\n",
      "         [[-3.5993e-06, -7.8568e-07,  1.3413e-06],\n",
      "          [ 6.6570e-06, -9.7844e-06,  6.9845e-06],\n",
      "          [-6.7591e-06, -5.7238e-06, -2.8244e-06]],\n",
      "\n",
      "         [[-1.2391e-05, -9.3204e-06, -2.4783e-05],\n",
      "          [-1.5632e-05, -2.9196e-06, -2.1305e-06],\n",
      "          [ 3.7425e-06,  2.3365e-06,  7.7406e-06]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2528e-06, -1.0252e-05, -5.0284e-06],\n",
      "          [ 8.4330e-06, -1.0706e-05, -2.4439e-05],\n",
      "          [-4.2798e-06, -3.8441e-06, -5.5531e-06]],\n",
      "\n",
      "         [[-1.9771e-05,  2.0035e-05,  1.3485e-05],\n",
      "          [-2.5093e-05,  2.1282e-05,  1.6007e-05],\n",
      "          [-1.8426e-05,  3.7728e-06,  1.8200e-06]],\n",
      "\n",
      "         [[-4.3679e-06,  6.1826e-08,  4.0892e-06],\n",
      "          [ 1.7532e-06,  1.6993e-06,  6.8635e-06],\n",
      "          [ 2.3169e-06,  2.0526e-06,  8.3594e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.3895e-06,  1.5901e-06, -3.4632e-07],\n",
      "          [ 9.7716e-07,  4.6355e-06,  4.6513e-07],\n",
      "          [-2.3786e-06, -2.7909e-06, -1.2133e-05]],\n",
      "\n",
      "         [[-2.1054e-05, -3.8422e-06,  3.0841e-06],\n",
      "          [-1.3895e-05,  2.3892e-06, -9.4110e-06],\n",
      "          [ 7.8829e-06,  2.3071e-05,  1.3114e-06]],\n",
      "\n",
      "         [[-1.1640e-05, -1.3305e-05, -2.8062e-07],\n",
      "          [-1.6588e-05, -1.4203e-05, -5.7827e-06],\n",
      "          [-2.7256e-05, -2.2787e-05, -2.8857e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8029e-07, -5.7766e-06, -1.9654e-05],\n",
      "          [ 2.5947e-06,  6.2831e-07, -9.2919e-06],\n",
      "          [-8.2570e-06, -3.2490e-06,  6.1867e-06]],\n",
      "\n",
      "         [[-1.9497e-06,  5.1220e-06,  5.8176e-06],\n",
      "          [ 9.4870e-07, -5.1066e-06, -2.8289e-06],\n",
      "          [ 8.8276e-06,  4.1530e-07,  3.3434e-06]],\n",
      "\n",
      "         [[ 4.0974e-07,  9.8410e-07,  1.4177e-06],\n",
      "          [ 2.1889e-06, -4.5218e-06, -6.3379e-06],\n",
      "          [-3.0290e-06, -8.9039e-06, -2.5518e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.1455e-06, -2.2892e-06, -2.1876e-06],\n",
      "          [ 2.2287e-06,  2.3234e-06,  4.2868e-06],\n",
      "          [-2.1398e-06,  2.8701e-06,  2.1875e-06]],\n",
      "\n",
      "         [[-2.4018e-05, -2.4449e-05, -3.0733e-05],\n",
      "          [-8.6665e-06, -6.5242e-06, -1.6296e-05],\n",
      "          [-5.0976e-06, -1.3194e-05, -2.1595e-05]],\n",
      "\n",
      "         [[-1.9661e-05, -2.4121e-05, -2.9181e-05],\n",
      "          [-6.2914e-06, -2.0104e-05, -3.9522e-05],\n",
      "          [-1.2229e-05, -1.2045e-05, -2.0275e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.5223e-06, -5.3953e-07,  8.2814e-06],\n",
      "          [-5.9057e-06, -2.8017e-06,  2.3973e-06],\n",
      "          [-3.2204e-06, -4.2545e-06, -1.9374e-06]],\n",
      "\n",
      "         [[-4.2512e-06, -1.2695e-05,  1.0933e-05],\n",
      "          [ 2.4423e-06, -8.4574e-06,  4.4046e-06],\n",
      "          [-3.8363e-06, -1.6977e-06,  9.1876e-07]],\n",
      "\n",
      "         [[-3.4401e-06,  8.3878e-07,  8.3058e-07],\n",
      "          [-3.8205e-06, -7.1364e-07,  2.2094e-06],\n",
      "          [ 7.0360e-07, -1.0355e-06,  1.3024e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.2316e-06,  5.9104e-06,  1.7856e-06],\n",
      "          [ 3.4168e-06,  9.6607e-06,  1.4987e-06],\n",
      "          [ 4.8303e-06,  5.7184e-06, -3.5918e-06]],\n",
      "\n",
      "         [[-1.4521e-07, -5.1438e-07,  5.0244e-06],\n",
      "          [ 2.9006e-06, -4.4860e-06, -7.3887e-06],\n",
      "          [ 1.0693e-06,  1.3734e-06, -1.3744e-05]],\n",
      "\n",
      "         [[-1.4546e-05, -1.3365e-05,  1.2335e-05],\n",
      "          [-2.0234e-05, -7.3967e-06,  6.6737e-06],\n",
      "          [-8.4441e-06,  1.9658e-07, -1.3516e-06]]],\n",
      "\n",
      "\n",
      "        [[[-8.4192e-06,  8.7086e-06, -4.9172e-07],\n",
      "          [ 2.4525e-06,  3.0062e-05,  2.4165e-05],\n",
      "          [-6.4275e-06,  1.3577e-05,  8.2588e-06]],\n",
      "\n",
      "         [[-2.5409e-06,  3.1462e-05,  6.8720e-06],\n",
      "          [-2.2506e-06,  3.9929e-05, -5.2627e-06],\n",
      "          [ 1.1349e-05,  5.4254e-05,  4.0440e-06]],\n",
      "\n",
      "         [[ 5.7363e-06,  2.0647e-05,  5.6487e-06],\n",
      "          [ 1.9120e-05,  4.5709e-05,  3.0298e-05],\n",
      "          [ 5.8444e-06,  2.7875e-05,  1.3606e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0008e-05, -2.2666e-06,  5.1890e-06],\n",
      "          [ 2.6085e-06,  1.6172e-05,  2.8692e-05],\n",
      "          [-9.7457e-06, -4.4295e-07,  1.5216e-05]],\n",
      "\n",
      "         [[ 7.8878e-06,  3.0247e-05,  1.1050e-05],\n",
      "          [ 1.0019e-05,  4.0713e-05,  2.0606e-05],\n",
      "          [ 5.1398e-06,  5.0335e-05,  2.7282e-05]],\n",
      "\n",
      "         [[-3.4279e-05,  8.1456e-07, -2.1238e-05],\n",
      "          [-4.1489e-05,  1.4555e-05,  7.0516e-07],\n",
      "          [-3.7316e-05,  1.0565e-05,  2.3174e-05]]],\n",
      "\n",
      "\n",
      "        [[[-7.2615e-06, -5.8963e-06, -4.2391e-06],\n",
      "          [-9.5771e-07, -5.5331e-06, -1.4044e-06],\n",
      "          [ 5.3728e-06, -5.1113e-06, -7.9485e-06]],\n",
      "\n",
      "         [[-6.3977e-06,  7.5538e-07, -3.5542e-06],\n",
      "          [-5.4882e-06,  4.9771e-06,  1.0684e-05],\n",
      "          [-8.2988e-06, -6.4474e-06, -3.5733e-06]],\n",
      "\n",
      "         [[-1.1178e-05, -4.1495e-06, -6.6059e-06],\n",
      "          [-3.3366e-06, -1.9281e-06, -4.7749e-06],\n",
      "          [-4.1895e-06, -8.1408e-07, -3.6193e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9350e-06, -1.6039e-06, -8.2636e-07],\n",
      "          [ 4.5816e-06,  4.6300e-07, -2.4096e-06],\n",
      "          [ 5.2567e-06,  1.0775e-06, -8.6689e-07]],\n",
      "\n",
      "         [[-2.0838e-05, -2.3991e-05, -2.3631e-05],\n",
      "          [-2.2059e-05, -9.7178e-06, -1.2772e-05],\n",
      "          [-2.3624e-05, -1.2275e-05, -1.2631e-05]],\n",
      "\n",
      "         [[-2.9321e-05,  8.2216e-06, -7.1157e-06],\n",
      "          [-2.0026e-05, -5.9072e-06, -4.4459e-07],\n",
      "          [-2.1531e-05, -1.9898e-05, -1.6859e-05]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-9.0949e-13, -4.5475e-12, -1.8190e-12,  2.9104e-11,  4.5475e-13,\n",
      "         1.1369e-11,  1.8190e-12, -5.4570e-12,  9.0949e-13, -8.1855e-12,\n",
      "        -8.1855e-12, -1.4552e-11,  0.0000e+00,  4.5475e-13, -1.8190e-12,\n",
      "        -5.4570e-12, -1.8190e-11, -2.7285e-12, -6.8212e-12, -4.5475e-12,\n",
      "        -6.3665e-12, -4.5475e-13,  8.6402e-12,  2.9104e-11, -4.0927e-11,\n",
      "         2.5466e-11,  3.6380e-12,  6.8212e-12, -2.9559e-12, -1.8190e-12,\n",
      "         8.1855e-12,  1.8190e-11,  7.7307e-12, -9.0949e-13,  4.5475e-12,\n",
      "        -5.4570e-12, -2.7285e-12, -1.8190e-12, -7.5033e-12,  0.0000e+00,\n",
      "        -8.1855e-12,  0.0000e+00, -3.6380e-12, -7.2760e-12,  9.0949e-13,\n",
      "         1.4097e-11,  2.7285e-11,  5.4570e-12,  7.2760e-12,  9.0949e-12,\n",
      "        -5.9117e-12,  3.1832e-12, -1.8190e-12, -4.5475e-12,  5.9117e-12,\n",
      "        -5.0022e-12, -6.3665e-12,  1.8190e-12,  5.4570e-12,  2.5466e-11,\n",
      "         6.3665e-12, -4.5475e-13, -7.2760e-12, -3.6380e-12], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[ 7.6719e-07,  1.4833e-06,  1.7056e-06,  ...,  8.0881e-06,\n",
      "         -6.1137e-06,  2.6773e-05],\n",
      "        [ 5.4668e-07,  1.3015e-07, -1.0540e-05,  ...,  9.7645e-06,\n",
      "          1.5534e-05,  1.9771e-05],\n",
      "        [-2.5567e-07,  5.6602e-07,  7.4981e-07,  ...,  3.1840e-05,\n",
      "          3.6271e-05,  4.8366e-05],\n",
      "        [ 1.3822e-06, -6.5062e-06, -8.2747e-07,  ..., -8.1516e-05,\n",
      "         -7.6851e-05, -1.8882e-04],\n",
      "        [-2.4405e-06,  4.3268e-06,  8.9119e-06,  ...,  3.1824e-05,\n",
      "          3.1161e-05,  9.3916e-05]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([-2.0004e-05, -1.3027e-04,  1.4687e-04, -4.1077e-04,  4.1418e-04],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = loss_landscape_join.landscape(maml_system.model.classifier, arbiter_system.model.classifier, args_arbiter)\n",
    "ls.show_2djoin(x_support_set_task, y_support_set_task, title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
