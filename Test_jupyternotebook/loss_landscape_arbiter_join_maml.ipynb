{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16249129",
   "metadata": {},
   "source": [
    "## [참고]\n",
    "### https://cocoa-t.tistory.com/entry/PyHessian-Loss-Landscape-%EC%8B%9C%EA%B0%81%ED%99%94-PyHessian-Neural-Networks-Through-the-Lens-of-the-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af476",
   "metadata": {},
   "source": [
    "# 0. Dataset 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7235fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset=\"mini_imagenet_full_size\"\n",
    "dataset=\"tiered_imagenet\"\n",
    "# dataset=\"CIFAR_FS\"\n",
    "# dataset=\"CUB\"\n",
    "\n",
    "# title = 'miniImageNet'\n",
    "title = 'tieredImageNet'\n",
    "# title = 'CIFAR-FS'\n",
    "# title = 'CUB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005193c",
   "metadata": {},
   "source": [
    "# 1. MAML 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_maml = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_JM\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_maml.im_shape = (2, 3, args_maml.image_height, args_maml.image_width)\n",
    "\n",
    "args_maml.use_cuda = torch.cuda.is_available()\n",
    "args_maml.seed = 104\n",
    "args_maml.reverse_channels=False\n",
    "args_maml.labels_as_int=False\n",
    "args_maml.reset_stored_filepaths=False\n",
    "args_maml.num_of_gpus=1\n",
    "\n",
    "args_maml.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9052a",
   "metadata": {},
   "source": [
    "## 2. Arbiter 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_arbiter = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 150,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_arbiter.im_shape = (2, 3, args_arbiter.image_height, args_arbiter.image_width)\n",
    "\n",
    "args_arbiter.use_cuda = torch.cuda.is_available()\n",
    "args_arbiter.seed = 104\n",
    "args_arbiter.reverse_channels=False\n",
    "args_arbiter.labels_as_int=False\n",
    "args_arbiter.reset_stored_filepaths=False\n",
    "args_arbiter.num_of_gpus=1\n",
    "\n",
    "args_arbiter.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f7d8",
   "metadata": {},
   "source": [
    "## 3. Model 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803156ee",
   "metadata": {},
   "source": [
    "### 3.1. MAML Model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f85286c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGReLUNormNetwork initialized\n",
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_JM\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 206209, 'train': 448695, 'val': 124261}\n",
      "train_seed 959531, val_seed: 959531, at start time\n",
      "0 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_maml = MAMLFewShotClassifier(args=args_maml, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_maml.image_height, args_maml.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model_maml, data=data, args=args_maml, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3acf",
   "metadata": {},
   "source": [
    "### 3.2.  Arbiter 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25651dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGReLUNormNetwork initialized\n",
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 206209, 'train': 448695, 'val': 124261}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_arbiter = MAMLFewShotClassifier(args=args_arbiter, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_arbiter.image_height, args_arbiter.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "arbiter_system = ExperimentBuilder(model=model_arbiter, data=data, args=args_arbiter, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a2ff6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.609755554497242,\n",
       " 'best_val_iter': 62500,\n",
       " 'current_iter': 75000,\n",
       " 'best_epoch': 125,\n",
       " 'train_loss_mean': 0.5830936436653137,\n",
       " 'train_loss_std': 0.13248762959620558,\n",
       " 'train_accuracy_mean': 0.7816533336639404,\n",
       " 'train_accuracy_std': 0.061554128982246006,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 1.0248335957527162,\n",
       " 'val_loss_std': 0.15533779380540755,\n",
       " 'val_accuracy_mean': 0.6047333323955536,\n",
       " 'val_accuracy_std': 0.06567684198868184,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-0.0409, -0.4467,  0.0429],\n",
       "                         [-0.2963, -0.1250, -0.0132],\n",
       "                         [-0.1106,  0.2660,  0.0018]],\n",
       "               \n",
       "                        [[ 0.2035, -0.1963,  0.3628],\n",
       "                         [-0.0770,  0.3089,  0.4387],\n",
       "                         [-0.1474,  0.3189,  0.1755]],\n",
       "               \n",
       "                        [[ 0.5152, -0.1190, -0.1241],\n",
       "                         [ 0.1760,  0.0768, -0.4136],\n",
       "                         [-0.1278, -0.0724, -0.4254]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0496,  0.6312, -0.0233],\n",
       "                         [ 0.1956, -0.0900,  0.0609],\n",
       "                         [-0.2298, -0.5546, -0.0384]],\n",
       "               \n",
       "                        [[-0.1876,  0.2887, -0.2277],\n",
       "                         [ 0.1476, -0.1774, -0.0852],\n",
       "                         [ 0.2142, -0.3373,  0.3006]],\n",
       "               \n",
       "                        [[-0.2607,  0.2268, -0.3460],\n",
       "                         [ 0.0560,  0.1103,  0.1956],\n",
       "                         [ 0.0893, -0.1852,  0.1546]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0748,  0.0476,  0.2461],\n",
       "                         [-0.2121, -0.1464,  0.1338],\n",
       "                         [ 0.0908,  0.0867, -0.0822]],\n",
       "               \n",
       "                        [[ 0.2339,  0.4846,  0.6200],\n",
       "                         [ 0.1067,  0.1562,  0.4239],\n",
       "                         [ 0.0059,  0.1925,  0.1062]],\n",
       "               \n",
       "                        [[ 0.1486,  0.2514,  0.4345],\n",
       "                         [ 0.3151,  0.4318,  0.3331],\n",
       "                         [ 0.1401,  0.4046, -0.0808]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.2117, -0.0682, -0.2444],\n",
       "                         [ 0.4216, -0.1547, -0.5258],\n",
       "                         [ 0.5607,  0.1941, -0.2801]],\n",
       "               \n",
       "                        [[-0.0915,  0.1077,  0.1366],\n",
       "                         [-0.2354,  0.0860,  0.1153],\n",
       "                         [-0.1601, -0.0385, -0.0107]],\n",
       "               \n",
       "                        [[-0.0812, -0.0576,  0.1726],\n",
       "                         [-0.2435,  0.1440,  0.4065],\n",
       "                         [-0.4243, -0.1394,  0.3312]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1254,  0.1059,  0.1173],\n",
       "                         [ 0.0554,  0.1419,  0.0558],\n",
       "                         [ 0.0138,  0.0996,  0.1134]],\n",
       "               \n",
       "                        [[ 0.0200, -0.1951, -0.4781],\n",
       "                         [-0.4310, -0.5708, -0.1947],\n",
       "                         [-0.3725, -0.2820, -0.1875]],\n",
       "               \n",
       "                        [[ 0.2555,  0.0543, -0.2713],\n",
       "                         [ 0.2611,  0.1576,  0.1757],\n",
       "                         [ 0.0562,  0.5708,  0.5290]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0290,  0.1489, -0.0417],\n",
       "                         [ 0.2613,  0.5926,  0.2504],\n",
       "                         [ 0.0091,  0.0454, -0.0223]],\n",
       "               \n",
       "                        [[-0.2341, -0.4374, -0.2404],\n",
       "                         [-0.3837, -0.6652, -0.4064],\n",
       "                         [-0.0541, -0.1243,  0.1028]],\n",
       "               \n",
       "                        [[ 0.2209,  0.3663,  0.3011],\n",
       "                         [ 0.0837,  0.0043,  0.1006],\n",
       "                         [ 0.1441,  0.0838, -0.0201]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([ 9.6650e-02,  9.0012e-02,  1.0153e-02,  1.3072e-02,  3.0882e-04,\n",
       "                        2.0923e-02,  1.1122e-01, -5.6767e-02,  2.9100e-02, -6.1414e-03,\n",
       "                        4.2690e-02, -3.8266e-02, -7.1119e-02,  4.1127e-03,  2.4325e-02,\n",
       "                       -9.6228e-03,  3.8698e-02,  7.5774e-03,  1.6726e-01,  1.5430e-02,\n",
       "                       -8.5455e-02, -1.8333e-01,  2.5994e-02, -1.7170e-01, -5.2944e-02,\n",
       "                       -4.8301e-02,  2.0229e-02,  1.6417e-01, -9.5401e-02,  5.8382e-02,\n",
       "                       -6.1296e-02,  1.3059e-02,  1.7859e-01, -6.5184e-02, -4.0300e-02,\n",
       "                        7.1912e-03,  1.7823e-01,  1.7506e-03,  1.9616e-02,  1.3052e-01,\n",
       "                       -1.0626e-01, -3.9649e-01, -1.5556e-01, -2.5635e-01,  3.1417e-01,\n",
       "                       -6.7512e-02,  6.3880e-02, -2.1920e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 6.1324e-01,  6.9091e-02, -9.2669e-01, -6.5256e-02, -1.0449e+00,\n",
       "                       -5.1087e-01, -5.4189e-01,  3.2142e-01, -5.8765e-01, -4.1698e-01,\n",
       "                       -1.3456e-01,  1.3041e+00,  1.4305e-01, -3.0954e-01,  1.7257e-02,\n",
       "                       -1.0122e-01, -6.4658e-01, -9.3308e-02,  1.0394e-01,  2.4457e-03,\n",
       "                       -3.3437e-01,  8.4199e-01, -7.8199e-01,  1.6571e-01, -6.8271e-01,\n",
       "                       -4.3834e-01,  4.3920e-01,  5.5236e-01, -2.8948e-01, -2.7206e-01,\n",
       "                       -6.6978e-01, -2.7035e-01,  5.0810e-01, -6.2210e-01, -1.3477e-01,\n",
       "                       -3.7271e-01,  9.5545e-01, -4.2554e-01, -2.6343e-01,  5.1485e-02,\n",
       "                       -1.0706e+00,  2.7841e+00, -8.0647e-01, -1.3359e-01,  3.7113e-01,\n",
       "                       -1.1396e-01,  3.4549e-01, -7.1616e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.7664, 1.0145, 0.6456, 0.7003, 1.0005, 0.5987, 1.0149, 1.3365, 0.7496,\n",
       "                       0.6985, 1.0529, 1.0448, 0.9460, 1.1433, 1.5522, 1.3730, 0.6531, 0.8381,\n",
       "                       1.0558, 0.6230, 0.6643, 0.7772, 0.8293, 0.9010, 0.8755, 0.7026, 1.1805,\n",
       "                       0.8757, 1.1107, 0.7663, 0.8612, 0.5319, 1.4689, 0.6769, 1.0167, 0.6895,\n",
       "                       0.9673, 0.7003, 0.5224, 0.6662, 1.2827, 0.9363, 0.9353, 1.6284, 0.9502,\n",
       "                       0.9283, 1.0427, 1.2381], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 1.0355e-01,  1.3591e-02, -1.6357e-01],\n",
       "                         [ 1.8003e-01,  1.7740e-01, -2.2552e-01],\n",
       "                         [ 2.4102e-01,  1.9049e-01,  1.5097e-02]],\n",
       "               \n",
       "                        [[ 4.5040e-01,  2.2925e-01,  2.4025e-01],\n",
       "                         [-4.6982e-01, -3.3014e-01,  5.9652e-01],\n",
       "                         [-2.5700e-01,  3.2935e-02,  4.1419e-01]],\n",
       "               \n",
       "                        [[ 2.2027e-01, -1.4794e-01, -5.8890e-01],\n",
       "                         [ 1.0851e-01, -3.2216e-01, -6.0179e-01],\n",
       "                         [ 1.4146e-01, -1.2020e-01, -4.8544e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.7611e-02, -5.0166e-01,  1.7844e-01],\n",
       "                         [-2.6575e-01, -6.6013e-01,  2.9132e-01],\n",
       "                         [-1.2133e-02, -5.8859e-02,  2.2711e-01]],\n",
       "               \n",
       "                        [[-2.6727e-01,  3.3210e-02,  2.8441e-01],\n",
       "                         [-6.2338e-02,  8.4224e-02,  3.1597e-01],\n",
       "                         [ 7.1377e-02,  3.3687e-01,  2.9377e-01]],\n",
       "               \n",
       "                        [[-4.3467e-01, -1.2870e-01,  8.2949e-02],\n",
       "                         [-4.6057e-01,  1.3044e-01,  4.8990e-01],\n",
       "                         [-2.2659e-01,  3.5803e-01,  3.5216e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.0202e-01, -4.9076e-01,  5.1206e-01],\n",
       "                         [-3.9465e-01, -3.7449e-02,  2.0182e-01],\n",
       "                         [-1.6296e-01,  2.9400e-01,  3.7678e-01]],\n",
       "               \n",
       "                        [[ 2.4573e-01,  2.7025e-02, -7.0021e-02],\n",
       "                         [ 2.5479e-01,  1.3986e-01, -2.9556e-02],\n",
       "                         [ 2.7119e-01, -1.8036e-01, -4.0758e-01]],\n",
       "               \n",
       "                        [[ 2.1736e-01,  2.7689e-01,  2.7145e-01],\n",
       "                         [ 3.4003e-01,  4.7908e-01,  1.4725e-01],\n",
       "                         [ 9.7582e-01,  8.9657e-01,  1.5954e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.5539e-01, -2.3376e-01, -1.9710e-01],\n",
       "                         [ 1.4038e-01, -4.6169e-01, -4.1987e-01],\n",
       "                         [ 2.2245e-01, -9.2985e-02,  1.5584e-01]],\n",
       "               \n",
       "                        [[-5.0134e-02, -5.9610e-01, -4.6238e-01],\n",
       "                         [-3.0361e-02,  1.5201e-01, -3.2343e-01],\n",
       "                         [ 5.7868e-01,  3.9043e-01,  3.5004e-01]],\n",
       "               \n",
       "                        [[-6.3743e-02, -4.2358e-01, -5.6998e-02],\n",
       "                         [ 1.9484e-01, -1.5678e-01,  5.0176e-02],\n",
       "                         [ 1.5885e-01,  2.2859e-01,  6.5600e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 9.9295e-02, -1.1507e-01,  8.9866e-02],\n",
       "                         [ 4.2335e-02,  1.8023e-01, -3.7862e-01],\n",
       "                         [ 1.7285e-01,  2.3915e-01, -2.2785e-01]],\n",
       "               \n",
       "                        [[-1.5751e-01,  1.4649e-01, -8.3445e-02],\n",
       "                         [ 3.0326e-01, -6.1076e-01,  2.2094e-01],\n",
       "                         [ 3.9057e-01, -1.4454e-02, -1.1489e-01]],\n",
       "               \n",
       "                        [[ 3.4721e-01, -1.1213e-01,  9.8206e-02],\n",
       "                         [ 5.0142e-01, -1.6775e-02, -1.6602e-03],\n",
       "                         [ 2.4909e-01,  1.0194e-01, -1.1857e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.9894e-01, -1.7770e-02,  2.5399e-01],\n",
       "                         [-1.6018e-01, -3.5263e-01,  4.7987e-02],\n",
       "                         [-2.0032e-01, -6.9469e-02, -9.9267e-02]],\n",
       "               \n",
       "                        [[-1.5619e-01, -6.2825e-01, -1.1486e-01],\n",
       "                         [ 6.1753e-02, -4.1169e-02, -1.1965e-01],\n",
       "                         [-2.0438e-02,  5.6363e-01,  9.4734e-02]],\n",
       "               \n",
       "                        [[-1.2496e-03, -9.4972e-02,  6.8935e-02],\n",
       "                         [-8.1317e-02, -1.2206e-01,  2.1920e-01],\n",
       "                         [-1.2840e-01,  2.1387e-02,  1.0233e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-5.7670e-01, -3.9662e-01, -1.1061e-01],\n",
       "                         [-1.8481e-01,  6.4905e-01,  7.8555e-02],\n",
       "                         [ 5.4240e-01,  1.0029e+00, -2.5323e-02]],\n",
       "               \n",
       "                        [[ 2.5130e-01,  4.0985e-02,  1.8773e-01],\n",
       "                         [-6.6784e-02,  1.4889e-03,  1.6503e-01],\n",
       "                         [-3.0215e-01,  3.2840e-01, -3.2285e-02]],\n",
       "               \n",
       "                        [[-6.1515e-02, -7.3349e-03,  3.0904e-01],\n",
       "                         [-3.2767e-01, -1.4268e-02, -1.7836e-01],\n",
       "                         [-2.0530e-01, -1.3628e-01, -2.7265e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.3443e-03,  1.6736e-01, -7.3255e-01],\n",
       "                         [-2.6284e-01, -2.1145e-02,  2.8312e-01],\n",
       "                         [ 2.8319e-01,  3.7257e-02,  4.7371e-01]],\n",
       "               \n",
       "                        [[ 1.5676e-01,  6.9225e-02,  4.1246e-02],\n",
       "                         [ 5.6233e-02,  9.0726e-02, -1.2716e-01],\n",
       "                         [-3.3404e-01, -1.6415e-01,  1.2229e-01]],\n",
       "               \n",
       "                        [[ 1.7375e-01,  1.1525e-01,  3.2228e-01],\n",
       "                         [-1.4588e-02, -6.6594e-02, -1.0925e-01],\n",
       "                         [-1.5959e-02, -1.1422e-01,  1.7720e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.7400e-01, -2.4398e-01, -1.4317e-01],\n",
       "                         [ 1.7216e-01, -2.2281e-01,  1.1228e-01],\n",
       "                         [ 1.0117e-01,  1.5216e-03,  1.3771e-01]],\n",
       "               \n",
       "                        [[-5.2008e-01, -5.6854e-01, -2.5960e-01],\n",
       "                         [-1.7391e-01, -5.4651e-02,  1.7905e-01],\n",
       "                         [-2.0365e-01,  7.1536e-02,  5.2312e-01]],\n",
       "               \n",
       "                        [[-4.8528e-01,  1.3428e-01, -2.5435e-01],\n",
       "                         [-4.6127e-01,  2.0775e-04, -8.5095e-02],\n",
       "                         [ 1.4762e-01,  1.4041e-01, -2.4085e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.7113e-01, -1.8128e-01, -6.3947e-02],\n",
       "                         [-9.9725e-02, -7.7055e-02,  2.7261e-03],\n",
       "                         [ 3.1629e-01,  1.2696e-01,  1.8958e-01]],\n",
       "               \n",
       "                        [[-2.3985e-01,  3.4028e-01,  1.0142e-01],\n",
       "                         [ 2.4547e-02,  3.5364e-01,  4.0262e-01],\n",
       "                         [-5.9031e-01, -1.8213e-01, -1.8852e-01]],\n",
       "               \n",
       "                        [[-1.7427e-01, -3.4044e-03, -5.4841e-01],\n",
       "                         [-7.4026e-02, -5.7879e-02,  3.6531e-02],\n",
       "                         [-2.5240e-01,  5.7121e-02, -2.4338e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.2067e-01, -2.2707e-01,  1.2869e-01],\n",
       "                         [ 1.0272e-01, -3.2546e-03,  1.1550e-01],\n",
       "                         [-6.6145e-02, -1.1896e-01, -6.2448e-02]],\n",
       "               \n",
       "                        [[-3.8120e-01, -2.2504e-01,  2.4456e-01],\n",
       "                         [ 3.2544e-01,  5.7452e-02,  4.9172e-01],\n",
       "                         [-3.5266e-01, -9.0978e-02,  4.7215e-01]],\n",
       "               \n",
       "                        [[-1.6901e-01, -6.1771e-02, -1.2542e-01],\n",
       "                         [ 5.8569e-02,  3.9067e-02,  9.7246e-03],\n",
       "                         [ 1.9677e-01,  2.2137e-01,  8.6855e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1623e-01,  4.9741e-02, -4.0720e-01],\n",
       "                         [ 2.2148e-01, -1.0157e-01, -1.2798e-01],\n",
       "                         [ 6.5055e-01, -1.0954e-01, -2.4333e-01]],\n",
       "               \n",
       "                        [[-2.4105e-01, -3.3171e-01, -4.2534e-02],\n",
       "                         [ 1.9024e-01, -2.4921e-01,  1.9452e-01],\n",
       "                         [ 5.2243e-03, -2.2437e-01,  2.7917e-02]],\n",
       "               \n",
       "                        [[ 2.3722e-01,  1.1292e-01,  7.0300e-02],\n",
       "                         [ 2.2697e-01,  1.8214e-01,  1.6504e-01],\n",
       "                         [ 3.4915e-02, -4.5112e-01,  8.4990e-04]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 0.0037,  0.0048,  0.0186, -0.0341,  0.0267, -0.0390,  0.0395,  0.0155,\n",
       "                       -0.0552, -0.0055, -0.0138, -0.0028, -0.0008, -0.0231, -0.0083, -0.0107,\n",
       "                       -0.0074,  0.0283,  0.0192, -0.0271,  0.0017, -0.0178,  0.0173,  0.0123,\n",
       "                       -0.0087,  0.1084, -0.0256,  0.0101,  0.0015,  0.0618, -0.0420,  0.0311,\n",
       "                        0.0593, -0.0143, -0.0292, -0.0089, -0.0032,  0.0288, -0.0028, -0.0079,\n",
       "                       -0.0041,  0.0070,  0.0319, -0.0142, -0.0007, -0.0008, -0.0207,  0.0041],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.2912, -0.6990, -0.4053, -0.3043, -0.2754, -0.5753, -0.6335, -0.8060,\n",
       "                       -0.1757, -0.7111, -0.4997, -0.4101, -0.4587, -0.5177, -0.5421, -0.2846,\n",
       "                       -0.6847, -0.2989, -0.6931, -0.4122, -0.6833, -0.8307, -0.3753, -0.6321,\n",
       "                       -0.6482, -0.6375, -0.6887, -0.3038, -0.5555, -0.7753, -0.4522, -0.3510,\n",
       "                       -0.6504, -0.2676, -0.5084, -0.6832, -0.4942, -0.9595, -0.5126, -0.3842,\n",
       "                       -0.2030, -0.7835, -0.4182, -0.6767, -0.2354, -0.1776, -0.3630, -0.3816],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.0198, 0.7616, 0.8562, 1.0324, 0.8874, 1.0720, 0.9482, 0.9479, 0.8845,\n",
       "                       1.1190, 0.8443, 0.8741, 0.7854, 1.0128, 1.1548, 0.6650, 1.1913, 0.8071,\n",
       "                       0.7515, 0.8632, 0.9713, 1.1035, 0.8957, 1.1331, 1.0700, 0.9691, 1.1660,\n",
       "                       1.0392, 1.0722, 1.0448, 1.0718, 0.8383, 0.9699, 0.9109, 0.8687, 1.1910,\n",
       "                       1.2653, 1.4808, 1.1349, 0.8185, 0.6630, 1.2552, 0.9236, 0.8525, 0.8826,\n",
       "                       0.7001, 0.9118, 1.0741], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[ 1.2004e-01,  4.0851e-01, -3.5228e-02],\n",
       "                         [-2.2771e-01, -5.1200e-02, -3.0558e-01],\n",
       "                         [-4.4398e-01, -1.4086e-01, -4.6412e-01]],\n",
       "               \n",
       "                        [[-1.5944e-01, -1.4459e-01, -8.8094e-02],\n",
       "                         [ 1.6384e-01, -5.5606e-03, -2.2797e-03],\n",
       "                         [-1.8782e-01, -4.1813e-01,  1.6389e-01]],\n",
       "               \n",
       "                        [[-4.1787e-01,  1.0584e-01, -3.5482e-01],\n",
       "                         [-2.0860e-01,  6.7204e-02,  5.6091e-02],\n",
       "                         [-5.8915e-02, -1.0117e-01,  1.6993e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.9183e-01, -1.0713e-01, -1.4654e-01],\n",
       "                         [ 7.7998e-02,  3.2350e-02, -3.1407e-03],\n",
       "                         [-3.8220e-01, -2.9949e-01,  2.4981e-03]],\n",
       "               \n",
       "                        [[-1.9483e-01,  1.9961e-02, -1.3599e-01],\n",
       "                         [ 1.6372e-01, -1.2816e-01,  1.4505e-01],\n",
       "                         [ 1.5455e-01,  3.0229e-01,  3.2884e-01]],\n",
       "               \n",
       "                        [[-3.4345e-01,  1.9271e-01, -3.6299e-02],\n",
       "                         [ 5.8030e-02,  4.0586e-01,  3.6157e-01],\n",
       "                         [-2.7372e-01,  3.6639e-02, -7.0017e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.2653e-01,  1.9884e-01, -4.5558e-01],\n",
       "                         [ 6.2553e-01,  2.0882e-01,  2.1480e-02],\n",
       "                         [ 1.8259e-01,  6.7432e-03, -1.1113e-01]],\n",
       "               \n",
       "                        [[-4.1320e-02, -8.5259e-02,  5.2199e-01],\n",
       "                         [-3.8717e-01, -9.6044e-02,  1.5279e-01],\n",
       "                         [ 5.7129e-01, -1.3030e-01, -3.7003e-01]],\n",
       "               \n",
       "                        [[-5.3686e-01, -4.1088e-01,  5.2905e-02],\n",
       "                         [-2.1293e-01,  1.6316e-01, -3.0777e-01],\n",
       "                         [-2.1384e-02,  1.7847e-01, -8.8146e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.0909e-02, -1.0252e-02, -3.0361e-01],\n",
       "                         [-2.8304e-01, -1.6919e-01, -1.7151e-01],\n",
       "                         [ 1.5448e-01, -6.9155e-02,  6.0459e-02]],\n",
       "               \n",
       "                        [[ 4.3089e-02, -5.4953e-01, -1.3262e-01],\n",
       "                         [-1.0824e-01,  1.4627e-01, -2.8082e-01],\n",
       "                         [ 1.7774e-01, -4.9432e-01, -3.7279e-01]],\n",
       "               \n",
       "                        [[-5.7746e-01, -3.5537e-01, -3.5627e-02],\n",
       "                         [-2.2196e-01, -8.5418e-02,  2.3273e-01],\n",
       "                         [-4.5820e-01, -4.4884e-01, -9.4067e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.8122e-02, -1.6459e-01, -2.1786e-01],\n",
       "                         [ 7.6647e-02,  5.4925e-01,  1.0796e-01],\n",
       "                         [-3.2193e-01,  1.5904e-01,  1.8183e-01]],\n",
       "               \n",
       "                        [[-7.0968e-02, -4.1744e-02,  2.3600e-01],\n",
       "                         [-1.7725e-01, -7.1547e-02,  1.4507e-01],\n",
       "                         [-4.2601e-01, -1.4408e-01,  6.6951e-02]],\n",
       "               \n",
       "                        [[ 7.6530e-02, -3.7878e-01, -2.0353e-01],\n",
       "                         [ 2.1217e-01,  1.7959e-01, -2.7092e-01],\n",
       "                         [-1.2193e-01,  1.0760e-01,  9.1126e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0475e-01, -2.8725e-01, -2.1425e-01],\n",
       "                         [-3.3452e-01, -5.1146e-01, -4.0861e-01],\n",
       "                         [ 1.3038e-02, -3.9179e-01, -3.7821e-01]],\n",
       "               \n",
       "                        [[ 2.0446e-01,  3.3433e-01,  3.2846e-02],\n",
       "                         [-3.2957e-01, -2.1335e-02, -1.9772e-02],\n",
       "                         [ 1.0576e-02,  3.4318e-01,  1.3595e-01]],\n",
       "               \n",
       "                        [[-4.9383e-01, -6.1754e-01, -2.1598e-01],\n",
       "                         [-2.9644e-01, -1.4774e-01, -1.1922e-01],\n",
       "                         [-8.7301e-01, -9.1379e-01, -5.0611e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.5452e-01,  7.8663e-01,  1.6966e-01],\n",
       "                         [ 2.0564e-01, -1.4083e-01, -1.0981e-01],\n",
       "                         [-1.7999e-01,  1.6964e-01, -3.8575e-01]],\n",
       "               \n",
       "                        [[-6.2577e-01, -5.4186e-01, -3.9336e-01],\n",
       "                         [-1.8360e-01,  2.2368e-01, -9.5944e-02],\n",
       "                         [ 1.2203e-01, -2.9925e-01, -4.5744e-01]],\n",
       "               \n",
       "                        [[ 2.7973e-01, -3.8171e-01, -3.5123e-01],\n",
       "                         [ 6.8530e-02,  2.2045e-01, -2.5743e-01],\n",
       "                         [ 3.8536e-01,  1.9976e-01,  1.0712e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.3482e-01, -6.2770e-02, -2.1515e-01],\n",
       "                         [-2.4981e-02,  2.1251e-02, -2.2884e-01],\n",
       "                         [ 4.9366e-02,  1.1638e-01,  3.1931e-02]],\n",
       "               \n",
       "                        [[-2.8866e-01, -1.2804e-02, -1.0991e-01],\n",
       "                         [ 1.8774e-01, -1.6139e-01, -4.4366e-01],\n",
       "                         [ 5.6140e-01,  6.4977e-02, -2.9914e-01]],\n",
       "               \n",
       "                        [[-1.6744e-01,  2.9720e-02, -8.3686e-02],\n",
       "                         [-2.7986e-01, -6.5045e-02, -1.6399e-01],\n",
       "                         [ 1.3565e-01,  6.2265e-02,  4.2455e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.0213e-01, -8.6038e-04, -1.3842e-01],\n",
       "                         [-2.0799e-01, -7.3823e-02,  8.9548e-02],\n",
       "                         [-2.6274e-01, -8.5267e-03,  1.0395e-01]],\n",
       "               \n",
       "                        [[-3.3975e-01, -1.8396e-01,  2.7345e-01],\n",
       "                         [-1.7517e-01,  5.4790e-01,  3.6965e-01],\n",
       "                         [-7.2966e-01, -1.8157e-02, -2.1441e-01]],\n",
       "               \n",
       "                        [[-6.2428e-01,  3.4692e-01,  1.7409e-01],\n",
       "                         [-2.3516e-01, -2.5093e-01,  4.5401e-02],\n",
       "                         [-1.7820e-01,  1.0160e-01,  1.4610e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.7490e-01, -3.4811e-01,  9.7943e-03],\n",
       "                         [-1.4634e-01, -2.3638e-01, -8.0588e-02],\n",
       "                         [-5.8745e-02, -5.7661e-02,  1.1794e-01]],\n",
       "               \n",
       "                        [[ 5.3300e-01, -3.6981e-01,  3.8205e-01],\n",
       "                         [ 7.3641e-02, -4.4698e-01, -3.9802e-01],\n",
       "                         [-1.6171e-02,  5.7106e-02,  5.8184e-03]],\n",
       "               \n",
       "                        [[-9.8860e-03, -3.2067e-01, -3.3434e-01],\n",
       "                         [-8.1425e-02,  4.0137e-01, -1.2337e-01],\n",
       "                         [-2.6587e-01, -3.9847e-01, -2.6775e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.5167e-01, -1.5427e-01, -6.5007e-02],\n",
       "                         [ 4.4342e-01,  3.2306e-01,  2.5981e-01],\n",
       "                         [ 2.8026e-01,  2.0856e-01, -1.1250e-01]],\n",
       "               \n",
       "                        [[-3.8516e-01, -9.0474e-02, -3.1699e-01],\n",
       "                         [ 2.9447e-01, -2.2285e-01, -1.7832e-01],\n",
       "                         [ 1.4876e-01,  1.1338e-01,  2.0053e-01]],\n",
       "               \n",
       "                        [[-1.5007e-01,  3.4457e-02, -6.8072e-02],\n",
       "                         [ 1.6757e-02,  5.6635e-02,  1.1384e-01],\n",
       "                         [ 4.9099e-01,  3.6529e-01,  2.5677e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.7670e-02, -5.5345e-02,  1.3000e-01],\n",
       "                         [-1.5170e-01, -2.0561e-01,  7.1264e-02],\n",
       "                         [-4.2104e-01, -1.8712e-01, -8.0762e-03]],\n",
       "               \n",
       "                        [[ 2.4010e-01,  6.5173e-02, -1.0713e-01],\n",
       "                         [ 8.0903e-03,  4.6448e-02, -1.2487e-01],\n",
       "                         [ 2.3772e-02, -2.3135e-02,  8.3383e-02]],\n",
       "               \n",
       "                        [[ 7.0674e-02,  1.0406e-02,  3.6516e-01],\n",
       "                         [-6.8340e-01,  5.8037e-02,  2.2842e-01],\n",
       "                         [-9.6291e-03,  7.8841e-02, -8.4268e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 0.1671, -0.0030, -0.0032, -0.0381,  0.0999,  0.0343, -0.1504,  0.0250,\n",
       "                       -0.0073,  0.0010, -0.0307, -0.0752, -0.0796, -0.0110,  0.0189, -0.0773,\n",
       "                        0.0137, -0.0566, -0.1002,  0.0105, -0.0633,  0.0848,  0.0121, -0.0892,\n",
       "                       -0.0279,  0.0079,  0.0922,  0.0600,  0.0321, -0.0006,  0.0137,  0.0416,\n",
       "                       -0.0226,  0.0985, -0.0395, -0.0003,  0.0639, -0.0122, -0.0489,  0.0465,\n",
       "                        0.0877,  0.0178,  0.0303, -0.0012, -0.0463, -0.0270, -0.0272,  0.0140],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.6191, -0.7229, -0.8064, -0.8732, -0.6730, -0.5606, -1.0298, -0.7592,\n",
       "                       -0.6126, -1.0615, -0.7488, -0.7244, -1.1252, -0.5559, -0.6213, -1.0725,\n",
       "                       -0.5873, -0.2536, -0.6278, -0.7103, -0.5687, -1.4164, -0.8862, -0.9114,\n",
       "                       -0.5265, -0.5860, -0.7373, -0.8555, -0.7887, -0.5869, -0.8168, -0.6249,\n",
       "                       -0.8641, -0.5489, -0.8672, -0.8050, -0.9887, -0.5186, -0.8487, -0.8852,\n",
       "                       -0.0414, -1.7984, -0.4610, -0.5082, -0.5578, -0.6294, -0.5998, -0.6823],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.9224, 0.8281, 0.7907, 1.0554, 0.7856, 0.8227, 0.9669, 0.7513, 0.7446,\n",
       "                       1.0020, 0.7530, 0.8299, 1.0517, 0.8648, 0.9043, 1.0083, 0.7044, 0.6301,\n",
       "                       0.6443, 1.0413, 0.8488, 0.8929, 1.0388, 0.8047, 0.7001, 0.7482, 1.0016,\n",
       "                       0.8495, 0.8712, 0.7042, 0.8560, 0.7684, 0.8712, 0.7099, 1.0163, 0.8958,\n",
       "                       1.0610, 0.9432, 0.9919, 0.9217, 0.6264, 0.9360, 0.8162, 0.6168, 0.8332,\n",
       "                       0.8975, 0.8273, 0.7335], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[ 7.0180e-02,  2.7000e-01,  1.8323e-01],\n",
       "                         [ 1.6390e-02,  2.8716e-01, -1.0347e-01],\n",
       "                         [-6.2804e-02,  7.4131e-03, -3.0621e-02]],\n",
       "               \n",
       "                        [[-7.1541e-02,  5.1496e-02,  1.3358e-01],\n",
       "                         [-1.2925e-01, -1.8145e-01, -3.3815e-01],\n",
       "                         [-1.3310e-01,  6.0728e-02, -3.7539e-02]],\n",
       "               \n",
       "                        [[-1.0652e-01,  2.6314e-02,  4.4114e-02],\n",
       "                         [-2.6664e-01, -7.1479e-01, -3.0949e-01],\n",
       "                         [-6.6172e-01, -1.8068e-01, -3.8446e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.6030e-01, -9.3052e-02, -1.3260e-01],\n",
       "                         [-2.2746e-01, -5.5780e-01, -5.8761e-01],\n",
       "                         [ 1.5707e-01,  3.3468e-02,  1.0019e-01]],\n",
       "               \n",
       "                        [[-4.9585e-01, -5.1636e-01, -8.1412e-01],\n",
       "                         [-3.8143e-01, -5.6174e-01, -8.8449e-01],\n",
       "                         [ 2.9056e-01,  7.1015e-02, -1.5375e-01]],\n",
       "               \n",
       "                        [[-1.5706e-01, -5.8765e-02,  5.1607e-02],\n",
       "                         [ 1.1672e-01, -2.8044e-02,  1.5399e-01],\n",
       "                         [-2.6334e-01,  3.0257e-02, -5.8239e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.8803e-02,  1.5246e-01,  1.7451e-01],\n",
       "                         [ 1.5451e-01,  2.8382e-01,  2.0903e-01],\n",
       "                         [ 3.9006e-01,  1.5260e-01,  2.7191e-01]],\n",
       "               \n",
       "                        [[-1.3297e-01, -1.6220e-02,  2.4398e-02],\n",
       "                         [ 1.4462e-01,  2.8841e-01,  8.0358e-02],\n",
       "                         [ 2.1239e-01, -2.2199e-02,  4.1839e-02]],\n",
       "               \n",
       "                        [[-2.7659e-01, -7.0144e-02, -1.1167e-01],\n",
       "                         [-1.1727e-01,  1.1973e-01, -4.4264e-02],\n",
       "                         [-3.8534e-04,  2.0010e-02,  5.7067e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.9303e-01,  4.0098e-01, -2.9678e-02],\n",
       "                         [-5.7488e-02,  6.4347e-02,  6.8074e-02],\n",
       "                         [ 4.3796e-02,  2.5248e-01,  7.1436e-02]],\n",
       "               \n",
       "                        [[ 2.2236e-01,  1.7502e-01,  1.6974e-01],\n",
       "                         [ 2.0466e-01,  1.8989e-01, -3.9463e-02],\n",
       "                         [ 2.0907e-01,  1.8013e-01, -2.6980e-02]],\n",
       "               \n",
       "                        [[-4.5906e-01,  2.1322e-01, -3.3689e-01],\n",
       "                         [-3.2912e-01,  2.9893e-01, -7.8785e-02],\n",
       "                         [ 3.0645e-02,  2.4248e-01,  7.8810e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.3115e-01, -1.5896e-01, -3.0404e-01],\n",
       "                         [-2.4997e-01, -2.4863e-01, -2.0629e-01],\n",
       "                         [-4.2779e-01, -2.9505e-01, -2.9721e-01]],\n",
       "               \n",
       "                        [[-4.1337e-01, -2.5323e-01, -2.0970e-01],\n",
       "                         [-2.3454e-01, -1.5815e-01, -2.8739e-01],\n",
       "                         [-3.7686e-01, -4.4242e-01, -1.7113e-01]],\n",
       "               \n",
       "                        [[-8.1843e-01, -7.5481e-01, -7.3458e-01],\n",
       "                         [-3.9257e-01, -2.8366e-01, -5.8617e-01],\n",
       "                         [-4.1107e-01, -2.2498e-01, -3.0492e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.8575e-02,  6.7633e-02, -9.4618e-02],\n",
       "                         [ 1.4164e-01,  1.1956e-01,  1.3405e-01],\n",
       "                         [-2.7319e-02,  5.2118e-02,  4.0791e-02]],\n",
       "               \n",
       "                        [[ 3.4362e-01,  3.0055e-01,  3.1885e-01],\n",
       "                         [ 3.4609e-01,  3.1468e-01,  1.9385e-01],\n",
       "                         [ 1.7592e-01,  1.4997e-01,  3.2748e-01]],\n",
       "               \n",
       "                        [[ 1.0496e-01,  3.3379e-02, -1.4672e-02],\n",
       "                         [-1.0145e-02,  7.9706e-02, -1.3348e-02],\n",
       "                         [-1.1411e-01,  3.1885e-02,  7.4834e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.8234e-01,  4.6440e-01,  4.0157e-01],\n",
       "                         [-8.7184e-03,  2.6032e-01,  3.0230e-01],\n",
       "                         [-2.1540e-01,  1.8813e-01,  1.2541e-01]],\n",
       "               \n",
       "                        [[-4.7989e-02, -4.0668e-02,  4.9685e-02],\n",
       "                         [-1.5668e-01,  2.8895e-01,  2.0592e-01],\n",
       "                         [-6.4507e-02,  2.3924e-01,  2.8930e-01]],\n",
       "               \n",
       "                        [[-3.8065e-01, -3.5528e-02, -2.9805e-01],\n",
       "                         [-1.7600e-01,  1.5898e-01, -1.7291e-01],\n",
       "                         [ 3.2534e-02,  1.9634e-01,  7.4522e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.9435e-01,  5.8204e-01,  2.3458e-01],\n",
       "                         [ 2.0667e-01,  2.0780e-01,  1.7733e-01],\n",
       "                         [-1.1438e-01, -2.6832e-02, -9.2321e-02]],\n",
       "               \n",
       "                        [[ 1.4857e-01,  3.7735e-01,  2.1693e-01],\n",
       "                         [ 1.2917e-01,  4.5950e-01,  9.7459e-02],\n",
       "                         [-2.2278e-02,  2.1271e-01,  7.4191e-02]],\n",
       "               \n",
       "                        [[-3.7609e-01, -1.3948e-01, -2.4781e-01],\n",
       "                         [-1.1976e-01, -6.0486e-01, -2.5795e-01],\n",
       "                         [-7.3246e-02, -1.0748e-01, -1.7823e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.7071e-02, -1.4659e-03, -1.9347e-01],\n",
       "                         [-1.9994e-01,  1.4423e-02,  1.0108e-01],\n",
       "                         [-4.9294e-02, -1.2477e-01, -3.6730e-01]],\n",
       "               \n",
       "                        [[ 3.5138e-01,  1.1564e-01, -3.4675e-01],\n",
       "                         [ 7.5812e-02, -1.2700e-02,  2.1742e-02],\n",
       "                         [ 4.0919e-02,  1.4552e-01, -3.4752e-01]],\n",
       "               \n",
       "                        [[ 1.4398e-01,  2.3680e-01, -6.3658e-02],\n",
       "                         [ 1.1651e-01,  1.9692e-01,  4.5407e-02],\n",
       "                         [ 2.9850e-01,  1.5109e-01,  3.2067e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.1577e-01, -2.0464e-01, -2.5542e-01],\n",
       "                         [-2.0597e-01, -7.8463e-02, -6.8024e-02],\n",
       "                         [-1.5553e-01, -1.2154e-01, -5.0893e-02]],\n",
       "               \n",
       "                        [[-1.8335e-01, -1.9135e-01, -3.9085e-01],\n",
       "                         [-4.7573e-01, -1.5821e-01, -3.1142e-01],\n",
       "                         [-3.2262e-01, -2.2103e-01, -4.1951e-01]],\n",
       "               \n",
       "                        [[ 3.5711e-01,  8.2878e-02,  3.4204e-01],\n",
       "                         [ 8.3935e-02,  7.9530e-02, -7.7235e-02],\n",
       "                         [-1.0013e-03,  1.4844e-01,  5.1121e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.7175e-01, -5.4080e-02, -2.2285e-01],\n",
       "                         [-1.3407e-01,  5.5269e-02, -1.0042e-01],\n",
       "                         [-6.0057e-02,  5.0796e-02, -1.3134e-01]],\n",
       "               \n",
       "                        [[-7.0956e-02,  1.5044e-01, -1.6627e-01],\n",
       "                         [-9.1977e-02,  1.1774e-01,  1.5036e-01],\n",
       "                         [-4.4493e-02, -1.6944e-01, -2.2152e-02]],\n",
       "               \n",
       "                        [[-9.4598e-01, -4.6096e-01, -1.0299e+00],\n",
       "                         [-9.3137e-01, -4.9324e-01, -2.3809e-01],\n",
       "                         [-8.5150e-01, -5.6957e-01, -3.3598e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5359e-01,  2.1436e-01,  2.1671e-01],\n",
       "                         [ 1.2367e-01,  2.4131e-01,  2.6480e-01],\n",
       "                         [-1.3481e-01,  5.2556e-02, -6.6304e-02]],\n",
       "               \n",
       "                        [[ 2.2290e-02,  6.4812e-02, -2.3682e-01],\n",
       "                         [-1.2581e-02, -1.7537e-02, -1.8130e-01],\n",
       "                         [-2.3959e-01, -8.1952e-02, -4.0843e-01]],\n",
       "               \n",
       "                        [[-2.3908e-01,  1.3986e-02,  2.9458e-02],\n",
       "                         [-4.4865e-01,  4.3078e-02,  3.2844e-01],\n",
       "                         [-1.3157e-01,  8.3499e-02, -5.7973e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-1.2111e-02,  1.4685e-02, -1.0587e-02,  6.1181e-02, -2.5747e-01,\n",
       "                       -3.6746e-01,  1.3218e-01, -9.5611e-03, -4.6007e-02,  2.8934e-02,\n",
       "                        5.9993e-04,  8.2192e-02,  1.3086e-02, -1.4107e-01,  5.4800e-02,\n",
       "                       -1.5211e-01, -2.2409e-01, -1.0518e-01, -1.0568e+00, -1.0073e-01,\n",
       "                       -2.7349e-01, -1.0988e-02,  1.5010e-02,  1.3158e-02,  6.3624e-03,\n",
       "                        2.8862e-01, -5.7318e-04,  1.4455e-01, -2.2568e-01,  5.9705e-02,\n",
       "                        3.7753e-02, -6.0357e-02,  5.2415e-02,  2.5887e-02, -4.7867e-03,\n",
       "                        3.0845e-02, -8.0888e-02,  1.3677e-01,  3.1605e-02,  9.3848e-03,\n",
       "                       -6.2489e-03,  2.6487e-02, -2.2928e-03,  7.6000e-02, -9.1850e-02,\n",
       "                        5.2918e-02, -1.5119e-01,  1.2626e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-1.2649, -1.1706, -2.5274, -1.6205, -1.3579, -2.9569, -1.3854, -2.1666,\n",
       "                       -2.0616, -1.4774, -1.9326, -2.0404, -0.8291, -3.0140, -1.4160, -1.6425,\n",
       "                       -2.4410, -1.3823, -2.9893, -1.4126, -2.2611, -1.1903, -0.9532, -1.2922,\n",
       "                       -1.0163, -2.3195, -1.1974, -1.5205, -1.3959, -1.7480, -0.9987, -1.4532,\n",
       "                       -1.6255, -1.1787, -2.1063, -1.1137, -1.2675, -1.5726, -0.9032, -0.7787,\n",
       "                       -2.2889, -1.2099, -1.3226, -1.7675, -1.8687, -1.1024, -2.2655, -1.8144],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.3852, 0.4157, 4.4411, 0.4978, 3.5611, 4.8632, 4.0644, 1.0993, 3.8462,\n",
       "                       0.4046, 0.7568, 4.0252, 0.2922, 4.2466, 3.9569, 3.4869, 4.6018, 4.1888,\n",
       "                       5.6731, 3.5759, 4.0254, 0.3652, 0.4632, 0.4843, 0.2324, 0.9148, 0.4497,\n",
       "                       3.8224, 3.5282, 0.6945, 0.3061, 3.7796, 3.8039, 0.3880, 0.8340, 0.3565,\n",
       "                       4.6833, 0.4378, 0.2824, 0.2666, 1.0716, 0.4185, 0.4278, 0.4537, 4.0891,\n",
       "                       0.3584, 3.7694, 0.6210], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[ 0.1919,  0.1602, -0.0820,  ...,  0.0615,  0.1100, -0.0550],\n",
       "                       [ 1.7461,  1.4713,  1.4611,  ...,  0.4582,  0.3388,  0.0727],\n",
       "                       [-0.6489, -0.6005, -0.4829,  ..., -0.2998, -0.0724,  0.0565],\n",
       "                       [-0.7426, -0.4667, -0.6575,  ..., -0.1392, -0.1346, -0.0427],\n",
       "                       [-0.8972, -0.7585, -0.6198,  ..., -0.2690, -0.3482, -0.0554]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([ 0.3201,  0.3947, -0.2482, -0.2279, -0.2249], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3926170508861542,\n",
       "   1.3010607197284698,\n",
       "   1.2345221453905106,\n",
       "   1.2001424884796144,\n",
       "   1.1686772278547286,\n",
       "   1.1295522274971008,\n",
       "   1.1108175331354142,\n",
       "   1.1185452698469163,\n",
       "   1.1177888790369033,\n",
       "   1.0926730536222458,\n",
       "   1.0467171201705932,\n",
       "   1.043311931014061,\n",
       "   1.0272930032014846,\n",
       "   1.0005080678462983,\n",
       "   0.9980165719985962,\n",
       "   0.9813516043424606,\n",
       "   0.9666589261293411,\n",
       "   0.9613648065328598,\n",
       "   0.9457783190011978,\n",
       "   0.930355311870575,\n",
       "   0.924197390794754,\n",
       "   0.8933096603155136,\n",
       "   0.896969137787819,\n",
       "   0.8999821889400482,\n",
       "   0.8843387161493301,\n",
       "   0.879183464050293,\n",
       "   0.8615117793083191,\n",
       "   0.8568120962977409,\n",
       "   0.8532249338626862,\n",
       "   0.8620581514835358,\n",
       "   0.8387096236944198,\n",
       "   0.8416438456773758,\n",
       "   0.8261049611568451,\n",
       "   0.8059040637016296,\n",
       "   0.8317589925527573,\n",
       "   0.8008215594887733,\n",
       "   0.8042342989444733,\n",
       "   0.7961013267636299,\n",
       "   0.7913425167798996,\n",
       "   0.7744184668660163,\n",
       "   0.7897835193276406,\n",
       "   0.7797613232731819,\n",
       "   0.7669421337842941,\n",
       "   0.7681149886250496,\n",
       "   0.7531083436012268,\n",
       "   0.7538032372593879,\n",
       "   0.7504551619291305,\n",
       "   0.738890763759613,\n",
       "   0.7362096289992333,\n",
       "   0.7419838643670083,\n",
       "   0.7440319802761078,\n",
       "   0.7208403154015541,\n",
       "   0.7314037905931473,\n",
       "   0.7238497809171677,\n",
       "   0.7298847239017486,\n",
       "   0.7260459195971489,\n",
       "   0.7149192021489144,\n",
       "   0.7180596362352372,\n",
       "   0.7137412533164025,\n",
       "   0.7140538129210472,\n",
       "   0.7106206297278405,\n",
       "   0.7074845098853111,\n",
       "   0.6907189017534257,\n",
       "   0.6904461938142776,\n",
       "   0.7059491257667542,\n",
       "   0.704595358967781,\n",
       "   0.7013618202805519,\n",
       "   0.6788725346922875,\n",
       "   0.6858946844935417,\n",
       "   0.6948507321476937,\n",
       "   0.6764821914434433,\n",
       "   0.6822874434590339,\n",
       "   0.6883578748106957,\n",
       "   0.6807307059168816,\n",
       "   0.676978440284729,\n",
       "   0.6706757701039314,\n",
       "   0.6835994419455528,\n",
       "   0.6786686539649963,\n",
       "   0.6660057445764541,\n",
       "   0.6766564584970475,\n",
       "   0.6649804428815842,\n",
       "   0.6675675492882729,\n",
       "   0.6688668528199195,\n",
       "   0.662778748691082,\n",
       "   0.654959679543972,\n",
       "   0.6607732901573181,\n",
       "   0.6578817547559738,\n",
       "   0.6569326740503311,\n",
       "   0.6495456776618957,\n",
       "   0.6389931970238686,\n",
       "   0.6454221054911613,\n",
       "   0.6565416570901871,\n",
       "   0.6313881105780601,\n",
       "   0.6451759369373321,\n",
       "   0.6457859637141228,\n",
       "   0.6513195328712463,\n",
       "   0.6414093055129051,\n",
       "   0.6386672241687774,\n",
       "   0.6312013285160065,\n",
       "   0.6253918450474739,\n",
       "   0.6387978113293648,\n",
       "   0.6326787423491478,\n",
       "   0.6264600097537041,\n",
       "   0.6246019793152809,\n",
       "   0.6312365233898163,\n",
       "   0.6347365726232529,\n",
       "   0.6302184299826622,\n",
       "   0.6265192975997925,\n",
       "   0.6228293669819832,\n",
       "   0.6227528101801872,\n",
       "   0.6160336861610413,\n",
       "   0.6263818098902703,\n",
       "   0.6274429650306702,\n",
       "   0.6155200707316398,\n",
       "   0.6173333336114883,\n",
       "   0.6202688345313072,\n",
       "   0.6192937748432159,\n",
       "   0.6116949747204781,\n",
       "   0.6123555713295936,\n",
       "   0.618682853102684,\n",
       "   0.6223476822376252,\n",
       "   0.6120990746617317,\n",
       "   0.6028409512042999,\n",
       "   0.6076440044641495,\n",
       "   0.6079717314243317,\n",
       "   0.6069483634829521,\n",
       "   0.6022118409872055,\n",
       "   0.6014715539813041,\n",
       "   0.6018922481238842,\n",
       "   0.6000142448842526,\n",
       "   0.5988579663038254,\n",
       "   0.6023223468661308,\n",
       "   0.5948709986805916,\n",
       "   0.6104994596838951,\n",
       "   0.5737385996580124,\n",
       "   0.5922974192202092,\n",
       "   0.5878487656712532,\n",
       "   0.5919873539209366,\n",
       "   0.5880447199046612,\n",
       "   0.5806403536796569,\n",
       "   0.5853483033776283,\n",
       "   0.5927135399580001,\n",
       "   0.5808477174639702,\n",
       "   0.5897521151304245,\n",
       "   0.5881019215583801,\n",
       "   0.5874773001074791,\n",
       "   0.5891482857465744,\n",
       "   0.5933485289812088,\n",
       "   0.5765847603082657],\n",
       "  'train_loss_std': [0.13093112477469845,\n",
       "   0.11739018119945344,\n",
       "   0.13048834185827973,\n",
       "   0.12635838751202555,\n",
       "   0.1270206975838066,\n",
       "   0.1264197664257354,\n",
       "   0.13206035004875794,\n",
       "   0.14280508553456397,\n",
       "   0.13868439352783204,\n",
       "   0.13387753188739238,\n",
       "   0.1358557642307108,\n",
       "   0.1404842697051786,\n",
       "   0.12955813152120396,\n",
       "   0.14056842055037302,\n",
       "   0.1362390405511453,\n",
       "   0.14752042682584546,\n",
       "   0.12683315467415432,\n",
       "   0.13756805193255453,\n",
       "   0.14318407893316087,\n",
       "   0.14385882535568176,\n",
       "   0.14000558354397405,\n",
       "   0.12759784829870555,\n",
       "   0.13621100633174307,\n",
       "   0.1410714060630192,\n",
       "   0.13776232132279073,\n",
       "   0.1534560933448049,\n",
       "   0.1343985533526886,\n",
       "   0.13824527380875073,\n",
       "   0.1421471158773288,\n",
       "   0.14691909876779055,\n",
       "   0.135912039314847,\n",
       "   0.14863772467021988,\n",
       "   0.14140812047112905,\n",
       "   0.1390856298952819,\n",
       "   0.14966018096485195,\n",
       "   0.14833828567749338,\n",
       "   0.14132002175856034,\n",
       "   0.14791146756991067,\n",
       "   0.1520901586307603,\n",
       "   0.1442756166036444,\n",
       "   0.14306681370505242,\n",
       "   0.14557185154203603,\n",
       "   0.14557640732367272,\n",
       "   0.15284270196947292,\n",
       "   0.14357752637095397,\n",
       "   0.14385157171415133,\n",
       "   0.1516026993000309,\n",
       "   0.14448965578254064,\n",
       "   0.14521135043823324,\n",
       "   0.1441174158506002,\n",
       "   0.14698315556348893,\n",
       "   0.1416088623623454,\n",
       "   0.14745944889894586,\n",
       "   0.14284736575807602,\n",
       "   0.15576548460761963,\n",
       "   0.145333062525076,\n",
       "   0.14217253297671317,\n",
       "   0.15289555578640476,\n",
       "   0.14432105003343168,\n",
       "   0.14406341146220247,\n",
       "   0.14722731653536805,\n",
       "   0.14812721519849317,\n",
       "   0.14380303430823013,\n",
       "   0.13904549173373634,\n",
       "   0.1436670674549665,\n",
       "   0.14322949855809858,\n",
       "   0.1407160510491455,\n",
       "   0.146333327364791,\n",
       "   0.14175292475678086,\n",
       "   0.15091883586418778,\n",
       "   0.1356020520792575,\n",
       "   0.14293006772347427,\n",
       "   0.1424533436494002,\n",
       "   0.14195989371400514,\n",
       "   0.1450528061183657,\n",
       "   0.14156128073757074,\n",
       "   0.1490231375979186,\n",
       "   0.14526416261828,\n",
       "   0.13732562863702746,\n",
       "   0.14832656815698014,\n",
       "   0.14175701027706777,\n",
       "   0.14121479022796168,\n",
       "   0.15010956329813105,\n",
       "   0.1367841873272306,\n",
       "   0.14888649823349148,\n",
       "   0.13575665067883533,\n",
       "   0.13642660243669294,\n",
       "   0.1469619997706896,\n",
       "   0.14414551240317466,\n",
       "   0.14454968561666567,\n",
       "   0.14424314105682118,\n",
       "   0.1496186764415889,\n",
       "   0.14015029643291504,\n",
       "   0.1439950921779864,\n",
       "   0.14887735342277808,\n",
       "   0.1551721344627039,\n",
       "   0.13394708293607327,\n",
       "   0.14343887137230787,\n",
       "   0.13630927463184786,\n",
       "   0.13273126646506292,\n",
       "   0.14449922326759898,\n",
       "   0.1391858966317336,\n",
       "   0.1450560827210317,\n",
       "   0.13673277887842503,\n",
       "   0.1439908495769786,\n",
       "   0.1434187727671569,\n",
       "   0.13846023870118027,\n",
       "   0.1380742155528634,\n",
       "   0.12796903800161552,\n",
       "   0.14410306049996796,\n",
       "   0.13993392716488393,\n",
       "   0.13992205780595007,\n",
       "   0.1430462146429495,\n",
       "   0.139480679090095,\n",
       "   0.13903049500799133,\n",
       "   0.14467430587297095,\n",
       "   0.14108614527591531,\n",
       "   0.13387404617114557,\n",
       "   0.13541941775875074,\n",
       "   0.14167355897787862,\n",
       "   0.14710990199348195,\n",
       "   0.13969034268268007,\n",
       "   0.14094060251184254,\n",
       "   0.13831343895309758,\n",
       "   0.13705221911889573,\n",
       "   0.1313597097312315,\n",
       "   0.13934170363014692,\n",
       "   0.13550842344642533,\n",
       "   0.13841981417002616,\n",
       "   0.13742500713654685,\n",
       "   0.1333037011276847,\n",
       "   0.13674748107833057,\n",
       "   0.13766019139061628,\n",
       "   0.14503304320512167,\n",
       "   0.12384266331970638,\n",
       "   0.1406565047871049,\n",
       "   0.135788449788968,\n",
       "   0.14135590961394667,\n",
       "   0.1365852903335332,\n",
       "   0.14871149050098012,\n",
       "   0.13059652379377631,\n",
       "   0.14195214155135544,\n",
       "   0.13982179698514483,\n",
       "   0.14273348715474884,\n",
       "   0.139190622166576,\n",
       "   0.1450812807427978,\n",
       "   0.1338178354330871,\n",
       "   0.13740256942180523,\n",
       "   0.1427849616448718],\n",
       "  'train_accuracy_mean': [0.4244400005042553,\n",
       "   0.46848000103235243,\n",
       "   0.5054799994826317,\n",
       "   0.5208666670322418,\n",
       "   0.5343466663360595,\n",
       "   0.555239999473095,\n",
       "   0.5648666661381722,\n",
       "   0.5610799989700317,\n",
       "   0.5568799991011619,\n",
       "   0.569133332490921,\n",
       "   0.5905199996829033,\n",
       "   0.5905066661834717,\n",
       "   0.5985066658258438,\n",
       "   0.6109333319664001,\n",
       "   0.6133733326792717,\n",
       "   0.6199599989652633,\n",
       "   0.625826667368412,\n",
       "   0.6298400005102157,\n",
       "   0.6350933341383934,\n",
       "   0.6420533330440521,\n",
       "   0.6457066658735275,\n",
       "   0.6553333327770233,\n",
       "   0.6551599999070168,\n",
       "   0.6536000006198883,\n",
       "   0.660826665520668,\n",
       "   0.6633733325004577,\n",
       "   0.6722666659355163,\n",
       "   0.6722399993538857,\n",
       "   0.6737333329916,\n",
       "   0.6698799995779992,\n",
       "   0.6800666663646698,\n",
       "   0.678919999063015,\n",
       "   0.685093332529068,\n",
       "   0.6932399993538857,\n",
       "   0.6798666656017304,\n",
       "   0.6971333327293396,\n",
       "   0.6929599997997284,\n",
       "   0.6998266650438308,\n",
       "   0.6994399993419648,\n",
       "   0.7062133342623711,\n",
       "   0.7007599997520447,\n",
       "   0.7063600012660026,\n",
       "   0.7109200004339218,\n",
       "   0.7094666664600372,\n",
       "   0.7141866672039032,\n",
       "   0.7127066671848297,\n",
       "   0.7153333324790001,\n",
       "   0.718386667251587,\n",
       "   0.7215600017905235,\n",
       "   0.7196000003814698,\n",
       "   0.7175199993848801,\n",
       "   0.7289066662788392,\n",
       "   0.7260000001192093,\n",
       "   0.7278133324384689,\n",
       "   0.7245333323478699,\n",
       "   0.7258666659593582,\n",
       "   0.7297733331918717,\n",
       "   0.7291066660881043,\n",
       "   0.7314000003933907,\n",
       "   0.7303599989414216,\n",
       "   0.7321066663265229,\n",
       "   0.7322266652584076,\n",
       "   0.7392266677618027,\n",
       "   0.7380000001192093,\n",
       "   0.734079999089241,\n",
       "   0.7336399993896484,\n",
       "   0.7361066665053367,\n",
       "   0.7446533316373825,\n",
       "   0.7419066665172577,\n",
       "   0.7371466668844223,\n",
       "   0.7433466674089432,\n",
       "   0.7412133321762086,\n",
       "   0.7416133320331574,\n",
       "   0.743013333439827,\n",
       "   0.7473733322620392,\n",
       "   0.7463066664934158,\n",
       "   0.7415066652297974,\n",
       "   0.741093332529068,\n",
       "   0.748866666674614,\n",
       "   0.7435466661453247,\n",
       "   0.7479200001955032,\n",
       "   0.7492266656160355,\n",
       "   0.7477600003480911,\n",
       "   0.7490399985313415,\n",
       "   0.7530533324480057,\n",
       "   0.7512933319807052,\n",
       "   0.7524799996614456,\n",
       "   0.7542666670084,\n",
       "   0.7561199991703034,\n",
       "   0.7598133322000503,\n",
       "   0.7590266679525375,\n",
       "   0.7515199996232986,\n",
       "   0.763293332695961,\n",
       "   0.7567999993562698,\n",
       "   0.7548933335542679,\n",
       "   0.7533333330750466,\n",
       "   0.7583733326196671,\n",
       "   0.7577600008249283,\n",
       "   0.7617733336687088,\n",
       "   0.7640400009155274,\n",
       "   0.759346667766571,\n",
       "   0.7621199996471405,\n",
       "   0.764786667585373,\n",
       "   0.7649333341121674,\n",
       "   0.7606666668653488,\n",
       "   0.7603333328962326,\n",
       "   0.7619733328819275,\n",
       "   0.7637466663122177,\n",
       "   0.7654000000357628,\n",
       "   0.7671066671609879,\n",
       "   0.7681199990510941,\n",
       "   0.7654533336162567,\n",
       "   0.7632666670084,\n",
       "   0.7684266656637192,\n",
       "   0.767693333029747,\n",
       "   0.7680666663646698,\n",
       "   0.766399999499321,\n",
       "   0.7693866665363311,\n",
       "   0.7690000002384185,\n",
       "   0.7676533331871033,\n",
       "   0.7668266661167145,\n",
       "   0.7693200008869171,\n",
       "   0.7742799986600876,\n",
       "   0.7725066667795182,\n",
       "   0.7714533325433731,\n",
       "   0.77137333381176,\n",
       "   0.7731466666460037,\n",
       "   0.7731466662883758,\n",
       "   0.7736266654729843,\n",
       "   0.7751599987745285,\n",
       "   0.7752666668891907,\n",
       "   0.7724933342933655,\n",
       "   0.7760933322906494,\n",
       "   0.7711466661691666,\n",
       "   0.7821999986171723,\n",
       "   0.7790000001192093,\n",
       "   0.779133332490921,\n",
       "   0.7788533322811126,\n",
       "   0.7798933329582214,\n",
       "   0.7825466669797897,\n",
       "   0.7796266673803329,\n",
       "   0.7756533324718475,\n",
       "   0.7831999999284744,\n",
       "   0.7766133344173431,\n",
       "   0.7792399988174439,\n",
       "   0.7785733324289322,\n",
       "   0.7788133326768875,\n",
       "   0.7775199989080429,\n",
       "   0.7836666666269302],\n",
       "  'train_accuracy_std': [0.06791185253550996,\n",
       "   0.06539385426352445,\n",
       "   0.07211913365860437,\n",
       "   0.06882799890240783,\n",
       "   0.07143136429539296,\n",
       "   0.06911253492301592,\n",
       "   0.06843037008117424,\n",
       "   0.07355595106193653,\n",
       "   0.0712394951867877,\n",
       "   0.06794805135290088,\n",
       "   0.0696147870864723,\n",
       "   0.07290914504890576,\n",
       "   0.06708595521889703,\n",
       "   0.0722681419035931,\n",
       "   0.06996696344529098,\n",
       "   0.07292186599376259,\n",
       "   0.0674389032384637,\n",
       "   0.068413586950689,\n",
       "   0.06976095876883333,\n",
       "   0.07384462279739587,\n",
       "   0.06964138739333652,\n",
       "   0.06582333204667083,\n",
       "   0.0680375470597207,\n",
       "   0.07101123525576057,\n",
       "   0.06868563866064817,\n",
       "   0.07445982285890693,\n",
       "   0.06706759604855618,\n",
       "   0.06885382457186952,\n",
       "   0.0658686910882383,\n",
       "   0.07000497664112275,\n",
       "   0.06771768242099986,\n",
       "   0.07309773028399418,\n",
       "   0.06794517669990582,\n",
       "   0.06723154235479196,\n",
       "   0.07311227381848937,\n",
       "   0.07122830204517505,\n",
       "   0.06673225625383754,\n",
       "   0.06860039711422933,\n",
       "   0.07208912619703499,\n",
       "   0.0709000470480985,\n",
       "   0.06765172387256196,\n",
       "   0.0694984844865113,\n",
       "   0.06641116291555572,\n",
       "   0.06976901647523608,\n",
       "   0.06591698948754729,\n",
       "   0.067617606832999,\n",
       "   0.07106366413210291,\n",
       "   0.06801811360276899,\n",
       "   0.06800890264397087,\n",
       "   0.06682394695962868,\n",
       "   0.06838245883402037,\n",
       "   0.06454872498090755,\n",
       "   0.06964098460460225,\n",
       "   0.06546191988322995,\n",
       "   0.07079661091979052,\n",
       "   0.06726716159387286,\n",
       "   0.06506640762169015,\n",
       "   0.06968374505965323,\n",
       "   0.0674414642931126,\n",
       "   0.0666890239656638,\n",
       "   0.06721181911637748,\n",
       "   0.06760619481543051,\n",
       "   0.06683397175770563,\n",
       "   0.0635588960061606,\n",
       "   0.06618742839756156,\n",
       "   0.06554248273648959,\n",
       "   0.06376029381143822,\n",
       "   0.0671617609578104,\n",
       "   0.06648649080216726,\n",
       "   0.06875166702201983,\n",
       "   0.06439254438840723,\n",
       "   0.0657581009388295,\n",
       "   0.06424516825738265,\n",
       "   0.06571848787989554,\n",
       "   0.06578576836113693,\n",
       "   0.06412871935877555,\n",
       "   0.06791200952313065,\n",
       "   0.06808788505582201,\n",
       "   0.0613206866445243,\n",
       "   0.06706447128567676,\n",
       "   0.06485150905851084,\n",
       "   0.06562538243831653,\n",
       "   0.06738977871833773,\n",
       "   0.06477765921226489,\n",
       "   0.06685315143206101,\n",
       "   0.060434120224322804,\n",
       "   0.06171749830740384,\n",
       "   0.06563075294325611,\n",
       "   0.06548393317530027,\n",
       "   0.06662105540639637,\n",
       "   0.06366742991279487,\n",
       "   0.06823554536373411,\n",
       "   0.06236593258489329,\n",
       "   0.06609945317711048,\n",
       "   0.0656127501315794,\n",
       "   0.06999936460653383,\n",
       "   0.061221805653803515,\n",
       "   0.06544721264283461,\n",
       "   0.06101338247487146,\n",
       "   0.061357517818876654,\n",
       "   0.06501842291187168,\n",
       "   0.06191925707888255,\n",
       "   0.06499999059096084,\n",
       "   0.0628735592730696,\n",
       "   0.0664148585913032,\n",
       "   0.06510692172356887,\n",
       "   0.06395428428821272,\n",
       "   0.06478345550890847,\n",
       "   0.05915625233070482,\n",
       "   0.06319692108816698,\n",
       "   0.0641285074917724,\n",
       "   0.06288362435856705,\n",
       "   0.06519505797489097,\n",
       "   0.0628685762256978,\n",
       "   0.06446300648184146,\n",
       "   0.06400343718979884,\n",
       "   0.06453746556764418,\n",
       "   0.060812839355243516,\n",
       "   0.06260262643647156,\n",
       "   0.0634539520877287,\n",
       "   0.06895261677165171,\n",
       "   0.06453649965844106,\n",
       "   0.06513774757914773,\n",
       "   0.060957042452822366,\n",
       "   0.061395070072812606,\n",
       "   0.05968642687588772,\n",
       "   0.061438755806975234,\n",
       "   0.06255707249398469,\n",
       "   0.06139347232986529,\n",
       "   0.0608381903087652,\n",
       "   0.062061582945331484,\n",
       "   0.0614079696817349,\n",
       "   0.06198874543295747,\n",
       "   0.06541369833082869,\n",
       "   0.05846978408971251,\n",
       "   0.06252590484697625,\n",
       "   0.05944973491261932,\n",
       "   0.06276815112737445,\n",
       "   0.06124767270159584,\n",
       "   0.06694478048847541,\n",
       "   0.05997957612154158,\n",
       "   0.06371146616426734,\n",
       "   0.061060843630310044,\n",
       "   0.06416157908954183,\n",
       "   0.06208703703580123,\n",
       "   0.06598188539381651,\n",
       "   0.05999344829066271,\n",
       "   0.061548036624724974,\n",
       "   0.06300881778708635],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.4126862080891927,\n",
       "   1.36410302122434,\n",
       "   1.3032175076007844,\n",
       "   1.3346502526601156,\n",
       "   1.2648283584912619,\n",
       "   1.2426117489735284,\n",
       "   1.2272531908750535,\n",
       "   1.2715359749396642,\n",
       "   1.2340878198544185,\n",
       "   1.1955910112460455,\n",
       "   1.1892612719535827,\n",
       "   1.1696053868532181,\n",
       "   1.151397960384687,\n",
       "   1.1590449794133504,\n",
       "   1.1552731547753017,\n",
       "   1.144583241144816,\n",
       "   1.157933100859324,\n",
       "   1.126733717918396,\n",
       "   1.143941417535146,\n",
       "   1.1383232581615448,\n",
       "   1.144604618549347,\n",
       "   1.1080067056417464,\n",
       "   1.1036954816182454,\n",
       "   1.10605668703715,\n",
       "   1.1054812504847844,\n",
       "   1.0959363146622976,\n",
       "   1.1040097218751908,\n",
       "   1.0957043506701787,\n",
       "   1.0847328639030456,\n",
       "   1.089960390528043,\n",
       "   1.0750224574406941,\n",
       "   1.080700960556666,\n",
       "   1.0802415722608567,\n",
       "   1.065570437113444,\n",
       "   1.0497316473722458,\n",
       "   1.0802438702185948,\n",
       "   1.0786317412058513,\n",
       "   1.0430425236622491,\n",
       "   1.079861937959989,\n",
       "   1.0471709163983662,\n",
       "   1.0651027548313141,\n",
       "   1.0518202797571818,\n",
       "   1.0405995094776153,\n",
       "   1.0526628375053406,\n",
       "   1.0586674221356709,\n",
       "   1.0418524702390035,\n",
       "   1.049041781425476,\n",
       "   1.050646510720253,\n",
       "   1.039780881802241,\n",
       "   1.0362301448980966,\n",
       "   1.0484811011950175,\n",
       "   1.043492046991984,\n",
       "   1.0480613176027933,\n",
       "   1.0384671284755072,\n",
       "   1.037234220902125,\n",
       "   1.030691418250402,\n",
       "   1.029623421827952,\n",
       "   1.0467426721254984,\n",
       "   1.0384310191869737,\n",
       "   1.0438303673267364,\n",
       "   1.0447979835669199,\n",
       "   1.0474048300584158,\n",
       "   1.034619049032529,\n",
       "   1.0315294277668,\n",
       "   1.022634115020434,\n",
       "   1.0085485402743022,\n",
       "   1.0381146347522736,\n",
       "   1.038284341096878,\n",
       "   1.0209457057714462,\n",
       "   1.0326985341310502,\n",
       "   1.0247101563215255,\n",
       "   1.0244468919436136,\n",
       "   1.028713817000389,\n",
       "   1.0121683011452356,\n",
       "   1.0309433126449585,\n",
       "   1.034206495086352,\n",
       "   1.0143029928207397,\n",
       "   1.0403799893458685,\n",
       "   1.026076990365982,\n",
       "   1.0454289239645005,\n",
       "   1.013445159594218,\n",
       "   1.0152680444717408,\n",
       "   1.0227266506354014,\n",
       "   1.0244837651650112,\n",
       "   1.0252376993497212,\n",
       "   1.0421523354450861,\n",
       "   1.0330570568641027,\n",
       "   1.039835439324379,\n",
       "   1.041808614730835,\n",
       "   1.036710177063942,\n",
       "   1.043648176987966,\n",
       "   1.0266064381599427,\n",
       "   1.0190908112128576,\n",
       "   1.0407692537705104,\n",
       "   1.0377411490678787,\n",
       "   1.0168001451094946,\n",
       "   1.0232407303651174,\n",
       "   1.0177416815360387,\n",
       "   1.016655152440071,\n",
       "   1.0263865013917288,\n",
       "   1.0368680268526078,\n",
       "   1.061092988650004,\n",
       "   1.0513432196776071,\n",
       "   1.0169496502478916,\n",
       "   1.0254684388637543,\n",
       "   1.023001880645752,\n",
       "   1.0243823007742563,\n",
       "   1.0257030989726383,\n",
       "   1.024768728017807,\n",
       "   1.0484053919712701,\n",
       "   1.0379176300764084,\n",
       "   1.0192293006181716,\n",
       "   1.0341051222880682,\n",
       "   1.0370608915885289,\n",
       "   1.0279586903254192,\n",
       "   1.0352991755803427,\n",
       "   1.0273067937294642,\n",
       "   1.0283440653483074,\n",
       "   1.027023114959399,\n",
       "   1.0362734756867091,\n",
       "   1.0119092190265655,\n",
       "   1.0136164665222167,\n",
       "   1.0134441941976546,\n",
       "   1.030579135020574,\n",
       "   1.005862692395846,\n",
       "   1.041543767650922,\n",
       "   1.038139246503512,\n",
       "   1.0296111293633778,\n",
       "   1.020626846353213,\n",
       "   1.0235966392358145,\n",
       "   1.021104893485705,\n",
       "   1.0122524346907933,\n",
       "   1.032179864446322,\n",
       "   1.05186986943086,\n",
       "   1.0369841227928798,\n",
       "   1.0267882736523946,\n",
       "   1.0385332411527635,\n",
       "   1.0290460004409154,\n",
       "   1.0249956967433294,\n",
       "   1.027591982881228,\n",
       "   1.0329858150084814,\n",
       "   1.0258656154076258,\n",
       "   1.0378028651078541,\n",
       "   1.022735144495964,\n",
       "   1.025453585187594,\n",
       "   1.0379447893301645,\n",
       "   1.023435923655828,\n",
       "   1.0333597326278687,\n",
       "   1.0395336949825287],\n",
       "  'val_loss_std': [0.09404332937981227,\n",
       "   0.10301122091741616,\n",
       "   0.11075428673284343,\n",
       "   0.12019021796620413,\n",
       "   0.11921297198643649,\n",
       "   0.11675614310811885,\n",
       "   0.12167520787958368,\n",
       "   0.11542284513394588,\n",
       "   0.12599289332213057,\n",
       "   0.12714337736358933,\n",
       "   0.12208213344537226,\n",
       "   0.1279969926673849,\n",
       "   0.12538978601823605,\n",
       "   0.13701264048158857,\n",
       "   0.1348404485657853,\n",
       "   0.13060419068293655,\n",
       "   0.13173530111104953,\n",
       "   0.13284270980925325,\n",
       "   0.13363677588714593,\n",
       "   0.13386941653362439,\n",
       "   0.1461256674440342,\n",
       "   0.1373580625194547,\n",
       "   0.14023314185965927,\n",
       "   0.13380871382300716,\n",
       "   0.13948100542658645,\n",
       "   0.13641479714144658,\n",
       "   0.1373777945578902,\n",
       "   0.14324008741735117,\n",
       "   0.14684133648658315,\n",
       "   0.14782324037283248,\n",
       "   0.14094242474026433,\n",
       "   0.1439913645061699,\n",
       "   0.14468557059317821,\n",
       "   0.14188667949666,\n",
       "   0.14534881017172105,\n",
       "   0.14804816927401834,\n",
       "   0.1468681895434907,\n",
       "   0.13756362385507187,\n",
       "   0.1520683405591535,\n",
       "   0.14605182969326067,\n",
       "   0.14704217282688906,\n",
       "   0.14992336487393795,\n",
       "   0.1458523369334576,\n",
       "   0.1517520025593666,\n",
       "   0.14931824304760538,\n",
       "   0.14389215955203552,\n",
       "   0.15774717811921907,\n",
       "   0.15171952171704942,\n",
       "   0.15440644712503446,\n",
       "   0.15534984391063794,\n",
       "   0.1509305862359921,\n",
       "   0.1610232509252337,\n",
       "   0.15595372005573532,\n",
       "   0.15395917151144328,\n",
       "   0.15785992581578015,\n",
       "   0.15340014666097873,\n",
       "   0.1547773900668262,\n",
       "   0.15862447220276651,\n",
       "   0.15556504206078772,\n",
       "   0.15565057136182658,\n",
       "   0.16095420426469212,\n",
       "   0.15423084078008625,\n",
       "   0.15704233508777612,\n",
       "   0.14782257621133252,\n",
       "   0.14876960857798713,\n",
       "   0.15046868504217079,\n",
       "   0.15303848170791323,\n",
       "   0.14589494431670744,\n",
       "   0.15283499721620075,\n",
       "   0.1483018584376941,\n",
       "   0.15188047948536004,\n",
       "   0.15960675840465738,\n",
       "   0.14812208368000146,\n",
       "   0.1508996078124708,\n",
       "   0.1522968784195828,\n",
       "   0.15212552291202724,\n",
       "   0.14799469363439555,\n",
       "   0.15652618435317794,\n",
       "   0.14978874582553026,\n",
       "   0.15514960914619566,\n",
       "   0.1495473319688164,\n",
       "   0.14777404838554598,\n",
       "   0.1523856610354038,\n",
       "   0.15678782764775637,\n",
       "   0.1508444527787447,\n",
       "   0.1517811815616136,\n",
       "   0.15850801469371095,\n",
       "   0.15739866923874574,\n",
       "   0.16243859502728272,\n",
       "   0.15387885557346606,\n",
       "   0.16477728536059533,\n",
       "   0.15609381285591375,\n",
       "   0.15097088023529426,\n",
       "   0.15647401171207245,\n",
       "   0.155721979421038,\n",
       "   0.15235245889144328,\n",
       "   0.15460345074023668,\n",
       "   0.1508374967664189,\n",
       "   0.15475861739848265,\n",
       "   0.14866172834242522,\n",
       "   0.15093058922478206,\n",
       "   0.16043213877422471,\n",
       "   0.16496673139913037,\n",
       "   0.14876250030905738,\n",
       "   0.15090284264623657,\n",
       "   0.15079097109494163,\n",
       "   0.15502858935567487,\n",
       "   0.15945815749647663,\n",
       "   0.1574940838931024,\n",
       "   0.14651328863522506,\n",
       "   0.15998276486467877,\n",
       "   0.1541199892965902,\n",
       "   0.1540655313208876,\n",
       "   0.15952465270947813,\n",
       "   0.158697725060113,\n",
       "   0.15129290849274654,\n",
       "   0.1574124973046826,\n",
       "   0.15725321581282145,\n",
       "   0.1580509150375843,\n",
       "   0.15475198107757948,\n",
       "   0.15667832530456108,\n",
       "   0.15231166428500928,\n",
       "   0.1525315559570475,\n",
       "   0.15474361837712372,\n",
       "   0.14618100794819852,\n",
       "   0.15274473738319128,\n",
       "   0.15060576247893886,\n",
       "   0.1502471068627393,\n",
       "   0.147655695374756,\n",
       "   0.1582704660161576,\n",
       "   0.15646439852906383,\n",
       "   0.15774168180209597,\n",
       "   0.15521262110662096,\n",
       "   0.1659304055333176,\n",
       "   0.15910004535004382,\n",
       "   0.14845718172279726,\n",
       "   0.15717750043836165,\n",
       "   0.15657897478740818,\n",
       "   0.15488647005189168,\n",
       "   0.15309177675883068,\n",
       "   0.1575753401677046,\n",
       "   0.14367483586967328,\n",
       "   0.14639785039729067,\n",
       "   0.15089632489287533,\n",
       "   0.1572749701813865,\n",
       "   0.1550446412614563,\n",
       "   0.1563936859423769,\n",
       "   0.1577236755230788,\n",
       "   0.14955360367937204],\n",
       "  'val_accuracy_mean': [0.40851111203432083,\n",
       "   0.43886666695276894,\n",
       "   0.4662666670481364,\n",
       "   0.45417777766784034,\n",
       "   0.4861999999483426,\n",
       "   0.49408888955911,\n",
       "   0.5000000008940697,\n",
       "   0.479244444668293,\n",
       "   0.4960000001390775,\n",
       "   0.5133555566271146,\n",
       "   0.5179555543263753,\n",
       "   0.5286444435516994,\n",
       "   0.5339555543661118,\n",
       "   0.5351111112038295,\n",
       "   0.5350444443027178,\n",
       "   0.5387777757644653,\n",
       "   0.5318444446722667,\n",
       "   0.546422220269839,\n",
       "   0.5446444429953893,\n",
       "   0.543177776436011,\n",
       "   0.5448444417119026,\n",
       "   0.5596666657924652,\n",
       "   0.5593555545806885,\n",
       "   0.5572444444894791,\n",
       "   0.5558000002304713,\n",
       "   0.5595777773857117,\n",
       "   0.5566222208738327,\n",
       "   0.5616000003616015,\n",
       "   0.5684888898332914,\n",
       "   0.5637777763605117,\n",
       "   0.5711999992529552,\n",
       "   0.5713333329558372,\n",
       "   0.5701555544137955,\n",
       "   0.5770222201943398,\n",
       "   0.5813777767618498,\n",
       "   0.5723555540045102,\n",
       "   0.5703555543224017,\n",
       "   0.5887333319584529,\n",
       "   0.574933333893617,\n",
       "   0.5876444435119629,\n",
       "   0.5817999978860219,\n",
       "   0.5835999988516172,\n",
       "   0.5883555544416109,\n",
       "   0.5841555533806483,\n",
       "   0.5873555564880371,\n",
       "   0.5917999981840452,\n",
       "   0.59131111095349,\n",
       "   0.5896888881921768,\n",
       "   0.5926888898015022,\n",
       "   0.5954222225149473,\n",
       "   0.5895555554827054,\n",
       "   0.5923999983072281,\n",
       "   0.5907555537422499,\n",
       "   0.594155553082625,\n",
       "   0.5946888878941536,\n",
       "   0.597666666607062,\n",
       "   0.5953555559118588,\n",
       "   0.5923111093044281,\n",
       "   0.5979555556178093,\n",
       "   0.590288888613383,\n",
       "   0.5943333328763644,\n",
       "   0.5869111123681069,\n",
       "   0.5953333329161008,\n",
       "   0.5949111093084017,\n",
       "   0.598155554831028,\n",
       "   0.6080666654308637,\n",
       "   0.5943777779738109,\n",
       "   0.5942666656772295,\n",
       "   0.6007777760426204,\n",
       "   0.5964666657646497,\n",
       "   0.5996222207943599,\n",
       "   0.5996222217877706,\n",
       "   0.5975333328048388,\n",
       "   0.6067333314816157,\n",
       "   0.5978444437185924,\n",
       "   0.5972888872027398,\n",
       "   0.6018888859947522,\n",
       "   0.5945111110806465,\n",
       "   0.6011111096541086,\n",
       "   0.5962888883550962,\n",
       "   0.6018444446722666,\n",
       "   0.6035999987522761,\n",
       "   0.6013555534680685,\n",
       "   0.6035555544495582,\n",
       "   0.6019333319862684,\n",
       "   0.5988666650652885,\n",
       "   0.5956444446245829,\n",
       "   0.5945777763923009,\n",
       "   0.5936888891458512,\n",
       "   0.5955333308378855,\n",
       "   0.5949777788917223,\n",
       "   0.6016444439689318,\n",
       "   0.6017777784665426,\n",
       "   0.5931333324313164,\n",
       "   0.5976666653156281,\n",
       "   0.6034888886411984,\n",
       "   0.604888888100783,\n",
       "   0.6047111096978187,\n",
       "   0.6056666647394499,\n",
       "   0.6002666652202606,\n",
       "   0.5974888878067335,\n",
       "   0.5887555556495985,\n",
       "   0.5937111097574234,\n",
       "   0.6027333320180575,\n",
       "   0.6023333307107289,\n",
       "   0.6008444441358248,\n",
       "   0.5987111098567645,\n",
       "   0.6023333317041397,\n",
       "   0.605066666106383,\n",
       "   0.5924444430073103,\n",
       "   0.6010222228368124,\n",
       "   0.6051333324114482,\n",
       "   0.6033777770400047,\n",
       "   0.597533331712087,\n",
       "   0.6029111110170682,\n",
       "   0.6000666649142901,\n",
       "   0.6017999994754791,\n",
       "   0.6032888886332511,\n",
       "   0.601599998474121,\n",
       "   0.5960222206513087,\n",
       "   0.6082222220301629,\n",
       "   0.6064666658639908,\n",
       "   0.6071333327889442,\n",
       "   0.5995555560787519,\n",
       "   0.609755554497242,\n",
       "   0.5934888881444931,\n",
       "   0.5968222210804621,\n",
       "   0.5985777762532234,\n",
       "   0.6048666661977768,\n",
       "   0.6024888870120049,\n",
       "   0.6051111114025116,\n",
       "   0.6076666661103567,\n",
       "   0.6016222218672435,\n",
       "   0.5969555555780729,\n",
       "   0.5994888885815939,\n",
       "   0.6019777766863506,\n",
       "   0.59582222143809,\n",
       "   0.603755555152893,\n",
       "   0.6053999999165535,\n",
       "   0.6026666664083798,\n",
       "   0.5985777777433395,\n",
       "   0.5988000011444092,\n",
       "   0.5963555527726809,\n",
       "   0.6029777763287226,\n",
       "   0.6012222211559614,\n",
       "   0.596711110273997,\n",
       "   0.6056666652361552,\n",
       "   0.6051777767141661,\n",
       "   0.5984444427490234],\n",
       "  'val_accuracy_std': [0.0547506619807204,\n",
       "   0.057986370773348075,\n",
       "   0.06489298343236759,\n",
       "   0.0639897986990493,\n",
       "   0.06235089432458126,\n",
       "   0.06292399427184149,\n",
       "   0.06378435835573458,\n",
       "   0.06486241421489884,\n",
       "   0.06301322450676122,\n",
       "   0.06737512099579347,\n",
       "   0.06266168528071847,\n",
       "   0.06519554230817139,\n",
       "   0.06726551668632637,\n",
       "   0.0674165216460187,\n",
       "   0.06767157684238961,\n",
       "   0.06609187275359209,\n",
       "   0.06820018145809588,\n",
       "   0.06803767463261717,\n",
       "   0.06828415030900137,\n",
       "   0.06488375476570102,\n",
       "   0.06367694398455365,\n",
       "   0.06768363215304954,\n",
       "   0.06810253245847753,\n",
       "   0.06636682124441612,\n",
       "   0.06591794004643395,\n",
       "   0.06753851572011774,\n",
       "   0.06518462305497529,\n",
       "   0.06920149879567047,\n",
       "   0.06816275424370317,\n",
       "   0.06892794548125399,\n",
       "   0.06947771221671092,\n",
       "   0.06705332319817528,\n",
       "   0.06841576067830467,\n",
       "   0.06693517858821423,\n",
       "   0.06525526638886135,\n",
       "   0.06723264879177049,\n",
       "   0.06699490963762098,\n",
       "   0.0666824080625415,\n",
       "   0.07023001454295892,\n",
       "   0.06491195917977621,\n",
       "   0.06577304359177427,\n",
       "   0.07018971068539696,\n",
       "   0.0695815506075465,\n",
       "   0.06930387903970786,\n",
       "   0.06703652571672022,\n",
       "   0.06492745746427797,\n",
       "   0.06628043311220067,\n",
       "   0.06913898583492711,\n",
       "   0.0677820001515349,\n",
       "   0.06900836913585948,\n",
       "   0.06721735601937712,\n",
       "   0.0672773378198891,\n",
       "   0.06586984402855031,\n",
       "   0.06599712483124195,\n",
       "   0.06938309882918146,\n",
       "   0.06783886088437323,\n",
       "   0.06790341344271189,\n",
       "   0.06737066736453017,\n",
       "   0.06358638033958114,\n",
       "   0.06722481412530237,\n",
       "   0.06947874753671075,\n",
       "   0.06918638055144471,\n",
       "   0.06845923403613685,\n",
       "   0.06272124918896711,\n",
       "   0.06551453092014513,\n",
       "   0.06371669989995676,\n",
       "   0.06738022167757085,\n",
       "   0.06517681933411892,\n",
       "   0.06686295231106049,\n",
       "   0.0651820574196081,\n",
       "   0.06486228962124921,\n",
       "   0.06621182348964555,\n",
       "   0.06587747371098188,\n",
       "   0.06503442801383995,\n",
       "   0.065328386043074,\n",
       "   0.06397266516476079,\n",
       "   0.06519590572283374,\n",
       "   0.06457110512413786,\n",
       "   0.06426065813476792,\n",
       "   0.062184953924493684,\n",
       "   0.06474798821080827,\n",
       "   0.0645835888512123,\n",
       "   0.06382684611905093,\n",
       "   0.06433554029408306,\n",
       "   0.06336931158119018,\n",
       "   0.06590715064373313,\n",
       "   0.06772706240535589,\n",
       "   0.06600734566656521,\n",
       "   0.06575729310381204,\n",
       "   0.06277334930349397,\n",
       "   0.066079353873276,\n",
       "   0.06552550734604798,\n",
       "   0.06508645732686398,\n",
       "   0.06790704954941917,\n",
       "   0.06475738230657724,\n",
       "   0.06450361783655036,\n",
       "   0.06598166462813111,\n",
       "   0.06436491742376695,\n",
       "   0.06290292278567493,\n",
       "   0.06212236522006426,\n",
       "   0.06185339523841293,\n",
       "   0.0635196630212442,\n",
       "   0.06674222025249363,\n",
       "   0.06366878433637783,\n",
       "   0.0628322285754059,\n",
       "   0.06575733876580124,\n",
       "   0.06621937364412181,\n",
       "   0.06675577385993361,\n",
       "   0.06631989810820704,\n",
       "   0.06135345897337402,\n",
       "   0.06451522514595161,\n",
       "   0.06475838689452323,\n",
       "   0.06108038130737237,\n",
       "   0.06751233514412178,\n",
       "   0.06693036465665582,\n",
       "   0.06432207114971199,\n",
       "   0.06282383411183996,\n",
       "   0.06488164276996754,\n",
       "   0.06510700080436727,\n",
       "   0.06353557888822167,\n",
       "   0.06580967940051163,\n",
       "   0.06428262721104525,\n",
       "   0.06359539826403017,\n",
       "   0.060421565864037664,\n",
       "   0.06383612997387783,\n",
       "   0.0640911810975877,\n",
       "   0.06369003331801129,\n",
       "   0.06266471295366795,\n",
       "   0.06274360582822791,\n",
       "   0.06488073042815233,\n",
       "   0.06407827057214144,\n",
       "   0.06461080209124097,\n",
       "   0.06397364850824855,\n",
       "   0.06446869712241245,\n",
       "   0.06459129318094145,\n",
       "   0.06186077168970778,\n",
       "   0.06611122606641337,\n",
       "   0.06267824585637208,\n",
       "   0.06283213429039215,\n",
       "   0.06628334330443623,\n",
       "   0.06499270021650204,\n",
       "   0.062356538383957376,\n",
       "   0.06481237530172515,\n",
       "   0.06254142050015078,\n",
       "   0.06520272276658323,\n",
       "   0.06651870941453801,\n",
       "   0.0643373631171685,\n",
       "   0.06268089218278014,\n",
       "   0.06333645037650147],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6830666663249334,\n",
       " 'best_val_iter': 49500,\n",
       " 'current_iter': 75000,\n",
       " 'best_epoch': 99,\n",
       " 'train_loss_mean': 0.3095538062006235,\n",
       " 'train_loss_std': 0.10350616538914063,\n",
       " 'train_accuracy_mean': 0.8880266679525375,\n",
       " 'train_accuracy_std': 0.043581027606914846,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.8918851272265116,\n",
       " 'val_loss_std': 0.16786081625014862,\n",
       " 'val_accuracy_mean': 0.674844443599383,\n",
       " 'val_accuracy_std': 0.05915026309581101,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0187, -0.0774,  0.0538],\n",
       "                         [-0.0283, -0.0311, -0.0033],\n",
       "                         [-0.0194,  0.1007, -0.0321]],\n",
       "               \n",
       "                        [[ 0.0613, -0.0807,  0.0713],\n",
       "                         [-0.0286,  0.0119,  0.0533],\n",
       "                         [-0.0654,  0.0343, -0.0387]],\n",
       "               \n",
       "                        [[ 0.0608, -0.0221, -0.0123],\n",
       "                         [ 0.0283,  0.0654, -0.0531],\n",
       "                         [-0.0675,  0.0344, -0.0407]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0358,  0.1153,  0.0423],\n",
       "                         [ 0.0527, -0.0572,  0.0056],\n",
       "                         [-0.0805, -0.0695, -0.0477]],\n",
       "               \n",
       "                        [[-0.0337,  0.0238, -0.0098],\n",
       "                         [ 0.0441, -0.0691, -0.0570],\n",
       "                         [ 0.0675, -0.0412,  0.0857]],\n",
       "               \n",
       "                        [[-0.0611,  0.0651, -0.0768],\n",
       "                         [-0.0160,  0.0502,  0.0748],\n",
       "                         [-0.0130, -0.0209, -0.0138]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0140, -0.0168,  0.0453],\n",
       "                         [-0.0133, -0.0419, -0.0380],\n",
       "                         [ 0.0761,  0.0594, -0.0528]],\n",
       "               \n",
       "                        [[-0.0613, -0.0171,  0.0453],\n",
       "                         [ 0.0891, -0.0047,  0.0099],\n",
       "                         [-0.0676, -0.0377,  0.0442]],\n",
       "               \n",
       "                        [[ 0.0323, -0.0691,  0.0337],\n",
       "                         [ 0.0748,  0.0163, -0.0643],\n",
       "                         [-0.0830,  0.0315,  0.0205]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0255, -0.0966, -0.0481],\n",
       "                         [-0.0091, -0.0651,  0.0950],\n",
       "                         [-0.0086,  0.0758, -0.0160]],\n",
       "               \n",
       "                        [[ 0.0391, -0.0042,  0.0548],\n",
       "                         [ 0.0204, -0.0324, -0.0361],\n",
       "                         [ 0.0060,  0.0420, -0.0124]],\n",
       "               \n",
       "                        [[ 0.0136, -0.0873, -0.0365],\n",
       "                         [-0.0556,  0.0171,  0.0540],\n",
       "                         [-0.0247,  0.0391,  0.0630]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0240, -0.0163, -0.0523],\n",
       "                         [ 0.0012,  0.0659, -0.0632],\n",
       "                         [ 0.0595,  0.0960, -0.0572]],\n",
       "               \n",
       "                        [[ 0.0368,  0.0109,  0.0611],\n",
       "                         [ 0.0108, -0.0410, -0.0210],\n",
       "                         [-0.0302, -0.0765,  0.0402]],\n",
       "               \n",
       "                        [[-0.0079,  0.0090, -0.0038],\n",
       "                         [ 0.0017, -0.0225,  0.0946],\n",
       "                         [-0.0622, -0.0254,  0.0181]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0861,  0.0197, -0.0231],\n",
       "                         [-0.0528,  0.0311,  0.0661],\n",
       "                         [ 0.0620,  0.0759,  0.0976]],\n",
       "               \n",
       "                        [[-0.0728, -0.0765,  0.0390],\n",
       "                         [ 0.0493, -0.0581, -0.0083],\n",
       "                         [-0.0037,  0.0313,  0.0212]],\n",
       "               \n",
       "                        [[-0.0036, -0.0539,  0.0443],\n",
       "                         [-0.0087, -0.0273,  0.0034],\n",
       "                         [-0.0243, -0.0326, -0.0043]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([ 1.4238e-05, -4.4478e-04, -6.0116e-05, -4.3803e-04,  3.6526e-04,\n",
       "                       -6.4164e-05,  1.6253e-04,  4.5672e-04, -5.2776e-04,  2.7196e-04,\n",
       "                        1.4221e-04,  1.0351e-03, -3.5400e-04, -7.9882e-05,  3.0815e-04,\n",
       "                       -3.3925e-04, -8.6905e-04,  1.4922e-04, -1.4027e-04,  1.0681e-04,\n",
       "                        6.2918e-04,  6.9962e-05,  7.5708e-04, -2.2402e-04,  1.0354e-03,\n",
       "                       -5.9469e-05, -2.0325e-04,  7.2481e-04,  9.1283e-05, -2.6908e-04,\n",
       "                       -6.3177e-04, -2.8542e-05, -1.8221e-04,  4.0845e-04, -4.8595e-05,\n",
       "                        4.3824e-05,  2.0706e-04,  5.1482e-04, -2.7153e-04,  2.1139e-04,\n",
       "                        3.2754e-04,  1.2842e-03, -2.9722e-04,  2.6587e-04,  3.1353e-04,\n",
       "                       -4.1072e-04, -5.9115e-05,  1.3727e-04, -6.5142e-05, -8.0838e-05,\n",
       "                        1.6540e-04,  1.8807e-04,  1.4793e-04, -4.2005e-05,  6.2232e-05,\n",
       "                       -5.4256e-05,  7.6140e-05, -4.0285e-04,  4.0816e-05, -2.3275e-04,\n",
       "                        7.0165e-05,  3.8396e-04,  2.6189e-04, -2.6779e-04,  3.0945e-04,\n",
       "                        3.7133e-04,  3.1991e-05,  4.0369e-04, -3.8504e-04,  1.9080e-04,\n",
       "                        7.3609e-04,  5.7260e-04,  2.3384e-04, -8.8130e-05, -1.2851e-03,\n",
       "                       -3.5138e-04,  4.1874e-04,  4.0361e-04, -9.5891e-05, -4.7029e-04,\n",
       "                       -1.3563e-04, -1.5394e-04,  4.7103e-04,  1.4700e-04,  3.5831e-04,\n",
       "                       -1.8871e-04,  1.6914e-04,  5.0700e-04,  6.1192e-05, -1.0942e-03,\n",
       "                        3.1633e-04,  3.1873e-04,  3.7636e-04,  3.4634e-04, -1.2589e-04,\n",
       "                        2.4011e-04, -1.5688e-04, -2.3391e-04,  2.6830e-04,  1.4461e-04,\n",
       "                       -1.1644e-05,  1.1864e-03,  2.2924e-04, -2.4787e-04, -7.2765e-05,\n",
       "                        1.4883e-04,  1.1379e-04,  1.7178e-04, -8.0286e-05, -1.0111e-03,\n",
       "                       -3.5040e-04,  7.8296e-04, -7.7673e-04,  1.2717e-03,  2.1959e-04,\n",
       "                       -1.3714e-04, -3.7135e-04,  3.4097e-04,  9.9214e-05, -4.4798e-04,\n",
       "                       -2.4445e-05,  8.4707e-05, -1.1372e-03,  2.9444e-04,  4.8274e-05,\n",
       "                       -1.0023e-04,  5.7931e-04,  1.7703e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.2137, -0.0739,  0.2703, -0.2811, -0.1241,  0.0650, -0.3227, -0.2452,\n",
       "                       -0.0099, -0.2449, -0.2550, -0.2123,  0.1032, -0.1017, -0.0090, -0.0965,\n",
       "                        0.3110,  0.2337,  0.0301, -0.2140, -0.1524, -0.1034, -0.1506, -0.1258,\n",
       "                        0.0763, -0.2714, -0.0860, -0.0392, -0.0280,  0.0687, -0.1174, -0.1248,\n",
       "                       -0.0774, -0.3157, -0.0567, -0.3031, -0.2026, -0.1492, -0.1923, -0.2210,\n",
       "                       -0.1987, -0.0733, -0.2735, -0.1715,  0.1801, -0.2440, -0.1538, -0.2946,\n",
       "                       -0.1979, -0.2307,  0.0128, -0.1120,  0.0870,  0.0880, -0.0718, -0.0926,\n",
       "                       -0.0644, -0.0413, -0.1990,  0.3544, -0.3121,  0.0923,  0.0614, -0.3758,\n",
       "                       -0.0322, -0.2410, -0.1619, -0.2185, -0.0978,  0.0062,  0.0318, -0.2244,\n",
       "                       -0.2521, -0.2390,  0.2466, -0.0578,  0.0233, -0.0517, -0.0957, -0.1819,\n",
       "                        0.1354, -0.1976, -0.1335, -0.3170, -0.0357,  0.5878,  0.0266,  0.0162,\n",
       "                       -0.2261, -0.0481,  0.0994, -0.1417,  0.1569, -0.0081, -0.0560, -0.2318,\n",
       "                       -0.0168,  0.1158, -0.0306, -0.1613, -0.3535, -0.1017,  0.2875,  0.0194,\n",
       "                       -0.2404, -0.1992, -0.2335, -0.2934,  0.0272, -0.0325,  0.0956, -0.0855,\n",
       "                       -0.0543, -0.0080, -0.1701, -0.0801, -0.1751,  0.3041, -0.1968,  0.1002,\n",
       "                       -0.2327, -0.0436, -0.2173, -0.1617,  0.2901, -0.1488, -0.2339, -0.0120],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.0832, 1.1988, 1.1196, 0.9462, 0.8702, 0.9330, 0.8405, 0.8892, 0.9754,\n",
       "                       0.8409, 0.8362, 0.9256, 1.0839, 0.9449, 1.1024, 1.0781, 1.1393, 0.9998,\n",
       "                       0.9932, 0.8605, 0.9934, 0.8316, 0.8565, 0.9799, 0.8993, 0.8353, 0.9304,\n",
       "                       0.9007, 0.9706, 0.9465, 1.1253, 0.8913, 1.1193, 1.2352, 0.7988, 0.8896,\n",
       "                       0.9648, 0.9429, 0.8774, 0.9301, 0.9549, 1.0625, 0.8071, 1.0064, 1.0837,\n",
       "                       0.8686, 0.8734, 0.8493, 1.0899, 0.9250, 1.0171, 0.9834, 1.1224, 1.0816,\n",
       "                       1.0241, 0.8895, 0.9158, 1.1479, 0.9490, 1.0742, 0.9116, 0.9931, 0.9251,\n",
       "                       0.8395, 0.9287, 0.9049, 0.8513, 0.9362, 1.0871, 0.9421, 0.9728, 0.9959,\n",
       "                       0.8444, 0.8789, 1.1257, 0.8801, 0.9100, 1.0801, 1.0851, 0.9141, 0.9765,\n",
       "                       0.9065, 0.9191, 0.7375, 1.1445, 1.1256, 0.9468, 1.0322, 0.8924, 0.9494,\n",
       "                       0.9817, 0.8825, 1.0055, 1.0330, 1.0876, 0.9907, 1.1974, 0.9100, 0.9402,\n",
       "                       0.8332, 0.8654, 1.0943, 1.1396, 0.8930, 0.9469, 0.9816, 0.8580, 0.7331,\n",
       "                       1.1171, 0.9403, 1.0453, 0.9779, 0.9225, 1.1777, 0.9236, 1.0930, 0.8208,\n",
       "                       1.0439, 0.9545, 0.9936, 0.8761, 0.8682, 1.2120, 1.0894, 1.0802, 0.9027,\n",
       "                       1.1223, 0.8790], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-0.0187, -0.0093, -0.0077],\n",
       "                         [-0.0457, -0.0328, -0.0263],\n",
       "                         [-0.0291, -0.0165, -0.0291]],\n",
       "               \n",
       "                        [[-0.0088,  0.0463, -0.0837],\n",
       "                         [-0.0870, -0.0650, -0.0369],\n",
       "                         [-0.0665,  0.0940,  0.0089]],\n",
       "               \n",
       "                        [[ 0.0217, -0.0122,  0.0137],\n",
       "                         [-0.0779, -0.0221,  0.0171],\n",
       "                         [ 0.0036,  0.0402, -0.0335]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0048, -0.0224, -0.0348],\n",
       "                         [-0.0401, -0.0487, -0.0269],\n",
       "                         [ 0.0054,  0.0258, -0.0129]],\n",
       "               \n",
       "                        [[ 0.0582,  0.0799, -0.0280],\n",
       "                         [ 0.0399,  0.0418, -0.0217],\n",
       "                         [ 0.0501, -0.0485,  0.0256]],\n",
       "               \n",
       "                        [[-0.0463,  0.0528, -0.0142],\n",
       "                         [-0.0035,  0.0391,  0.0194],\n",
       "                         [-0.0424, -0.0213,  0.0393]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0122, -0.0116,  0.0380],\n",
       "                         [ 0.0015, -0.0087, -0.0715],\n",
       "                         [-0.0044, -0.0364, -0.0133]],\n",
       "               \n",
       "                        [[ 0.0818,  0.0612,  0.0409],\n",
       "                         [-0.0136, -0.0041,  0.0558],\n",
       "                         [-0.0069, -0.0341,  0.0100]],\n",
       "               \n",
       "                        [[ 0.0857,  0.0197, -0.0112],\n",
       "                         [-0.0292,  0.0005, -0.0135],\n",
       "                         [-0.0651, -0.0169, -0.0527]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0345, -0.0019,  0.0599],\n",
       "                         [-0.0209, -0.0278, -0.0208],\n",
       "                         [ 0.0203,  0.0148, -0.0052]],\n",
       "               \n",
       "                        [[ 0.0102, -0.0407, -0.0676],\n",
       "                         [ 0.0132,  0.0362, -0.0088],\n",
       "                         [ 0.0536, -0.0438,  0.0127]],\n",
       "               \n",
       "                        [[ 0.0272, -0.0002,  0.1031],\n",
       "                         [ 0.0248, -0.0411,  0.0034],\n",
       "                         [-0.0583, -0.0494, -0.0219]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0971, -0.0288, -0.0035],\n",
       "                         [ 0.0284,  0.0108, -0.0595],\n",
       "                         [ 0.0301,  0.0036, -0.0377]],\n",
       "               \n",
       "                        [[ 0.0273,  0.0400, -0.0514],\n",
       "                         [ 0.0322, -0.0079, -0.0088],\n",
       "                         [ 0.0266,  0.0273, -0.0522]],\n",
       "               \n",
       "                        [[-0.0357,  0.0110, -0.0509],\n",
       "                         [ 0.0215, -0.0119, -0.0210],\n",
       "                         [-0.0062,  0.0511,  0.0830]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0208,  0.0472,  0.0245],\n",
       "                         [ 0.0132, -0.0237, -0.0341],\n",
       "                         [ 0.0172, -0.0218, -0.0534]],\n",
       "               \n",
       "                        [[ 0.0281,  0.0557,  0.0854],\n",
       "                         [-0.0277,  0.0130,  0.0433],\n",
       "                         [-0.0148,  0.0615,  0.0727]],\n",
       "               \n",
       "                        [[-0.0486, -0.0783, -0.0346],\n",
       "                         [-0.0389, -0.0050,  0.0004],\n",
       "                         [-0.0028, -0.0156, -0.0054]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0380,  0.0207,  0.0129],\n",
       "                         [ 0.0613,  0.0298,  0.0876],\n",
       "                         [ 0.0179, -0.0054, -0.0218]],\n",
       "               \n",
       "                        [[-0.0030,  0.0256,  0.0497],\n",
       "                         [-0.0618, -0.0257, -0.0430],\n",
       "                         [-0.1002, -0.0354, -0.0596]],\n",
       "               \n",
       "                        [[ 0.0303,  0.0577,  0.0365],\n",
       "                         [ 0.0054, -0.0333,  0.0675],\n",
       "                         [-0.0495, -0.0492,  0.0288]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0060,  0.0092,  0.0301],\n",
       "                         [ 0.0426, -0.0481, -0.0053],\n",
       "                         [-0.0113, -0.0301, -0.0345]],\n",
       "               \n",
       "                        [[ 0.0002,  0.0146,  0.0206],\n",
       "                         [ 0.0316,  0.0206, -0.0016],\n",
       "                         [-0.0344,  0.0389,  0.0584]],\n",
       "               \n",
       "                        [[-0.0342, -0.0536, -0.0243],\n",
       "                         [-0.0300, -0.0186, -0.0221],\n",
       "                         [-0.0317, -0.0036, -0.0583]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0578,  0.0004,  0.0453],\n",
       "                         [-0.0089, -0.0440,  0.0173],\n",
       "                         [-0.0439,  0.0100,  0.0388]],\n",
       "               \n",
       "                        [[-0.0271,  0.0488,  0.0920],\n",
       "                         [ 0.0081, -0.0620, -0.0061],\n",
       "                         [ 0.0035, -0.0411, -0.0232]],\n",
       "               \n",
       "                        [[-0.0351,  0.0383,  0.0987],\n",
       "                         [ 0.0144, -0.0030, -0.0414],\n",
       "                         [-0.0873, -0.0086,  0.0335]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0369,  0.0395,  0.0826],\n",
       "                         [ 0.0076,  0.0717, -0.0165],\n",
       "                         [-0.0208,  0.0120, -0.0259]],\n",
       "               \n",
       "                        [[ 0.0084, -0.0269, -0.0042],\n",
       "                         [-0.0223, -0.1458, -0.1065],\n",
       "                         [ 0.0009, -0.0567, -0.0484]],\n",
       "               \n",
       "                        [[-0.0714, -0.0880, -0.0890],\n",
       "                         [-0.0701, -0.0700, -0.1258],\n",
       "                         [ 0.0216, -0.0852, -0.0753]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0755, -0.0536, -0.0317],\n",
       "                         [-0.0050,  0.0173, -0.0314],\n",
       "                         [-0.0601,  0.0342,  0.0386]],\n",
       "               \n",
       "                        [[-0.0500,  0.0104, -0.0318],\n",
       "                         [ 0.0521, -0.0459,  0.0063],\n",
       "                         [-0.1155, -0.0326,  0.0352]],\n",
       "               \n",
       "                        [[-0.0198, -0.0025, -0.0047],\n",
       "                         [-0.0423, -0.0162, -0.0335],\n",
       "                         [-0.0078, -0.0148,  0.0287]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0202,  0.0326, -0.0139],\n",
       "                         [ 0.0475,  0.0790,  0.0041],\n",
       "                         [-0.0225,  0.0145,  0.0054]],\n",
       "               \n",
       "                        [[-0.0247,  0.0005, -0.0657],\n",
       "                         [ 0.0281,  0.0903,  0.0160],\n",
       "                         [-0.0308,  0.0143, -0.0113]],\n",
       "               \n",
       "                        [[-0.0324,  0.0202, -0.0202],\n",
       "                         [-0.0182,  0.0273,  0.0339],\n",
       "                         [ 0.0155, -0.0210, -0.0393]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 5.7178e-05,  5.4299e-03,  1.2462e-02, -3.8505e-02, -9.4791e-05,\n",
       "                       -2.2006e-06, -8.3002e-03,  7.3085e-03,  6.9288e-04,  4.2830e-06,\n",
       "                       -4.2396e-05, -6.1437e-07, -1.6076e-02, -1.5353e-05, -7.1608e-03,\n",
       "                        2.2091e-02, -2.1123e-04,  5.9594e-03, -1.5167e-07, -9.6857e-07,\n",
       "                       -2.2210e-07, -1.5760e-06,  6.1685e-03,  2.5375e-07, -3.7148e-02,\n",
       "                       -3.1553e-05,  1.0800e-05, -2.1461e-04,  5.3959e-03,  8.8803e-07,\n",
       "                       -3.1898e-05,  3.7288e-05,  9.9633e-04, -7.2832e-03,  5.2888e-06,\n",
       "                       -4.1546e-06, -1.6728e-03,  1.8617e-06,  6.9827e-03, -2.4328e-02,\n",
       "                       -1.8130e-02,  3.0765e-02, -7.4219e-03, -2.9240e-02,  8.9736e-06,\n",
       "                       -2.5685e-02, -1.3289e-02, -7.2717e-04,  6.6258e-05, -2.5749e-02,\n",
       "                        5.6267e-05, -6.2420e-03,  5.4734e-07, -1.5771e-02,  2.7269e-03,\n",
       "                       -1.4432e-02,  3.3291e-02, -8.1161e-03,  8.5593e-03, -1.0325e-03,\n",
       "                        2.0624e-03, -5.8880e-05,  5.2892e-03, -6.5251e-04, -1.9590e-03,\n",
       "                        2.0032e-06,  1.6710e-06,  5.3519e-06,  4.5839e-07,  3.8123e-08,\n",
       "                        8.1304e-06, -2.5951e-06,  9.7952e-05, -3.3802e-06, -2.5898e-02,\n",
       "                        3.0568e-05,  1.6175e-02,  9.6420e-06,  2.4758e-02,  4.1412e-03,\n",
       "                        3.7150e-02,  8.7743e-06, -1.8360e-06, -2.9906e-03, -1.1865e-02,\n",
       "                        2.3456e-03, -2.6641e-02,  7.0907e-03,  2.6484e-05, -6.9515e-04,\n",
       "                        6.4974e-03,  1.4716e-06, -4.3197e-03, -7.1082e-03,  3.4019e-06,\n",
       "                        1.6235e-07, -1.6684e-03,  8.3209e-03,  1.9914e-02,  1.1856e-06,\n",
       "                       -8.9693e-07, -8.2266e-06, -1.8091e-06,  2.4365e-02, -1.2722e-02,\n",
       "                        2.1026e-05, -1.6686e-05, -1.2077e-02, -7.0923e-03,  7.2051e-03,\n",
       "                        7.1461e-03,  7.4243e-03, -2.0429e-02,  6.2037e-03, -1.1138e-06,\n",
       "                       -1.4836e-02,  9.7714e-03, -6.1631e-06, -1.1701e-03, -9.4662e-06,\n",
       "                       -2.0993e-02,  1.4859e-02,  5.7944e-03, -1.5848e-02,  1.1721e-02,\n",
       "                       -2.7776e-02, -1.7310e-02, -1.5559e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.2874, -0.1405, -0.3985, -0.1820, -0.3265, -0.2929, -0.0514, -0.3258,\n",
       "                       -0.2032, -0.2223, -0.4568, -0.2407, -0.2737, -0.1333, -0.1857, -0.2389,\n",
       "                       -0.2011, -0.0379, -0.1931, -0.4106, -0.3120, -0.2874, -0.1714, -0.1396,\n",
       "                       -0.0196, -0.0877, -0.3508, -0.2681, -0.2282, -0.2539, -0.1941,  0.0421,\n",
       "                       -0.1975, -0.1718, -0.2214, -0.3593, -0.1185, -0.1577, -0.3116, -0.4044,\n",
       "                       -0.3150, -0.1171, -0.2011, -0.2011, -0.2347, -0.3852, -0.2438, -0.2470,\n",
       "                       -0.3094, -0.1473, -0.2918, -0.2990, -0.2644, -0.4436, -0.2640, -0.2099,\n",
       "                        0.0209, -0.2244, -0.3144, -0.2937, -0.2818, -0.4153, -0.2504, -0.2239,\n",
       "                       -0.3080, -0.1367, -0.2227, -0.2473, -0.0318, -0.2456, -0.2006, -0.3266,\n",
       "                       -0.2489, -0.2170, -0.3220, -0.1397, -0.3181, -0.1888, -0.1985, -0.0895,\n",
       "                       -0.2981, -0.2063, -0.2057, -0.1390, -0.2431, -0.2685, -0.2345, -0.2406,\n",
       "                       -0.1713, -0.2728, -0.2722, -0.3768, -0.1096, -0.1698, -0.1754, -0.1675,\n",
       "                       -0.1926, -0.0667, -0.3980, -0.2165, -0.2163, -0.1463, -0.1705, -0.1439,\n",
       "                       -0.1229, -0.2190, -0.2433,  0.0395, -0.2496, -0.2082, -0.2967, -0.2530,\n",
       "                       -0.2850, -0.2580, -0.2470, -0.2656, -0.1123, -0.2698, -0.0026, -0.1973,\n",
       "                       -0.2747, -0.1816, -0.1178, -0.1465, -0.2615, -0.3578, -0.2755, -0.3059],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9524, 0.9734, 0.9343, 0.9316, 1.0987, 1.0354, 1.0280, 1.1503, 0.9671,\n",
       "                       0.9680, 0.9539, 1.0884, 0.9267, 0.9839, 1.0441, 1.0150, 1.0190, 0.9349,\n",
       "                       1.0162, 0.8706, 0.9917, 0.8612, 1.0447, 1.0323, 1.0418, 1.0487, 0.8840,\n",
       "                       1.0035, 0.8894, 0.8530, 0.9065, 1.0160, 1.0100, 0.9588, 0.8340, 1.0373,\n",
       "                       1.0306, 1.0027, 0.8872, 0.8202, 1.0099, 0.9803, 1.0411, 0.8788, 0.9841,\n",
       "                       0.9636, 0.9426, 0.9574, 1.0319, 1.0021, 0.9059, 0.9675, 0.9632, 0.9640,\n",
       "                       0.9798, 0.8718, 0.9612, 0.8944, 0.9825, 0.9348, 0.9551, 1.0277, 1.0426,\n",
       "                       0.9883, 0.9485, 1.0241, 0.9763, 0.8979, 1.0032, 1.0348, 0.9282, 0.8569,\n",
       "                       0.8887, 0.9901, 1.0287, 1.0146, 1.0713, 1.0107, 1.0172, 1.0319, 1.0063,\n",
       "                       0.9212, 0.9840, 0.9899, 0.9303, 0.9610, 0.9225, 1.0216, 0.9831, 0.9663,\n",
       "                       1.0782, 0.9571, 1.0088, 0.9250, 1.0150, 0.9963, 0.9649, 1.0013, 1.0137,\n",
       "                       0.9642, 0.9461, 1.0227, 0.9561, 0.9767, 1.2044, 0.9423, 1.0002, 1.0417,\n",
       "                       0.9994, 0.9947, 0.9982, 0.9873, 0.9766, 1.0798, 0.9655, 1.0126, 1.0380,\n",
       "                       1.0426, 0.9799, 1.0170, 0.9051, 1.0429, 1.0326, 0.9623, 0.8949, 0.9283,\n",
       "                       0.9285, 1.0811], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-3.3450e-02,  3.4974e-02, -4.3271e-02],\n",
       "                         [-6.1254e-02, -2.1036e-02, -3.2305e-02],\n",
       "                         [-4.0856e-02, -1.5401e-02, -2.0185e-02]],\n",
       "               \n",
       "                        [[ 1.4035e-02, -1.4260e-02,  9.1193e-03],\n",
       "                         [-2.5656e-02,  3.3091e-02, -5.1078e-02],\n",
       "                         [-5.6376e-02,  7.2344e-02, -4.8334e-02]],\n",
       "               \n",
       "                        [[-2.6294e-03, -3.0731e-02, -3.9807e-02],\n",
       "                         [ 1.2556e-02,  3.5268e-02,  2.5117e-02],\n",
       "                         [ 1.6693e-02,  3.9184e-02, -4.9743e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.9052e-02, -1.1424e-02, -2.1010e-02],\n",
       "                         [ 4.4572e-02,  4.2004e-02,  2.7880e-02],\n",
       "                         [ 2.9193e-02, -3.5237e-02, -4.1887e-02]],\n",
       "               \n",
       "                        [[-5.3178e-02, -9.3157e-02, -1.5602e-03],\n",
       "                         [ 3.6024e-02, -3.1877e-02, -3.3699e-02],\n",
       "                         [ 3.9434e-02,  3.7392e-03,  2.6185e-02]],\n",
       "               \n",
       "                        [[-3.2886e-02,  3.1436e-02,  4.2597e-02],\n",
       "                         [ 3.1496e-05,  1.9412e-03,  2.8980e-02],\n",
       "                         [ 3.3565e-02,  3.5059e-02,  5.6761e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.4380e-02,  4.8757e-02,  1.5016e-02],\n",
       "                         [-4.1642e-02, -5.2212e-03, -2.0832e-02],\n",
       "                         [-6.7380e-02, -2.6924e-02,  3.8751e-02]],\n",
       "               \n",
       "                        [[-1.3584e-02,  5.2896e-02,  3.5286e-03],\n",
       "                         [ 5.0405e-02,  1.0174e-01,  3.0061e-02],\n",
       "                         [-3.8621e-02,  1.7733e-02, -1.7543e-02]],\n",
       "               \n",
       "                        [[-8.4086e-02,  2.4781e-02, -2.7912e-02],\n",
       "                         [-6.6276e-02,  1.5021e-02, -3.0652e-02],\n",
       "                         [-3.0980e-02,  6.7531e-03, -1.3224e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.7750e-02,  1.5371e-02, -3.0936e-02],\n",
       "                         [-3.2895e-02,  5.0101e-02,  4.9406e-03],\n",
       "                         [-1.5760e-03,  8.0844e-02, -1.9241e-02]],\n",
       "               \n",
       "                        [[ 1.2925e-02, -3.4582e-02,  3.5487e-02],\n",
       "                         [-6.7894e-02, -4.7608e-02,  9.8384e-05],\n",
       "                         [-4.0323e-02, -7.3979e-02, -4.9113e-02]],\n",
       "               \n",
       "                        [[ 6.5026e-02, -1.3419e-02, -9.2444e-02],\n",
       "                         [-7.1905e-03,  5.8218e-02, -1.1573e-02],\n",
       "                         [-4.2938e-02, -2.9889e-02, -9.9551e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.2992e-03, -2.2778e-02, -8.7260e-02],\n",
       "                         [-1.0299e-02,  2.8567e-03,  1.4970e-02],\n",
       "                         [-1.3424e-03,  3.6510e-03, -2.2827e-02]],\n",
       "               \n",
       "                        [[-1.2357e-02,  6.5183e-02,  6.5558e-02],\n",
       "                         [-3.3882e-02,  3.4676e-02,  6.4399e-02],\n",
       "                         [-6.1777e-02,  3.1395e-02, -4.3014e-02]],\n",
       "               \n",
       "                        [[-6.3576e-03, -6.8975e-02, -5.7495e-03],\n",
       "                         [-7.0674e-02, -4.4107e-03, -4.9310e-02],\n",
       "                         [-6.0046e-02, -2.9698e-02, -1.0215e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.8517e-02, -3.4336e-02, -4.9666e-02],\n",
       "                         [-5.9624e-02, -8.3675e-02, -4.7875e-02],\n",
       "                         [-7.7360e-02, -7.0540e-02,  1.0686e-02]],\n",
       "               \n",
       "                        [[ 2.5802e-03, -2.3390e-03,  3.9795e-02],\n",
       "                         [ 4.8919e-02, -1.8628e-02, -6.6609e-02],\n",
       "                         [-2.2084e-02, -5.6890e-02, -5.9191e-03]],\n",
       "               \n",
       "                        [[-1.8620e-04,  3.8911e-02,  4.9277e-02],\n",
       "                         [ 4.6142e-02,  9.1802e-02,  5.3490e-02],\n",
       "                         [-2.9320e-02,  2.6841e-02,  3.0845e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-4.6246e-02, -1.8669e-02, -5.7250e-02],\n",
       "                         [ 1.0447e-02, -3.9038e-02, -5.3725e-02],\n",
       "                         [ 4.2854e-02,  4.9041e-02, -1.6618e-02]],\n",
       "               \n",
       "                        [[-1.0057e-01, -7.1651e-02, -1.2741e-01],\n",
       "                         [-3.3930e-02, -1.6120e-02, -5.8627e-02],\n",
       "                         [ 1.6861e-02,  1.5057e-02, -5.4989e-02]],\n",
       "               \n",
       "                        [[-3.0304e-02,  9.9240e-03, -6.9069e-02],\n",
       "                         [ 4.3606e-03, -3.6256e-02, -2.0975e-02],\n",
       "                         [ 2.9903e-02, -6.9400e-03, -4.5620e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.1277e-02,  4.9520e-02,  3.6264e-02],\n",
       "                         [ 3.7528e-02,  1.1871e-02,  7.2222e-02],\n",
       "                         [ 3.8166e-02, -2.8697e-02, -5.2302e-03]],\n",
       "               \n",
       "                        [[ 2.0585e-02, -3.0872e-02,  3.5484e-02],\n",
       "                         [-3.4297e-02, -2.3005e-02, -4.5813e-02],\n",
       "                         [-4.9808e-02,  3.6720e-02, -8.5597e-02]],\n",
       "               \n",
       "                        [[ 6.4947e-02,  1.6535e-02,  1.1727e-02],\n",
       "                         [ 3.1817e-02,  1.7985e-02, -3.3101e-02],\n",
       "                         [ 6.2776e-02,  1.1804e-01,  4.6207e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.2001e-02,  1.6419e-02, -4.7302e-02],\n",
       "                         [-3.2954e-02, -4.7322e-02, -1.7651e-02],\n",
       "                         [-1.0547e-03,  1.9960e-02, -4.1738e-02]],\n",
       "               \n",
       "                        [[-3.8143e-02, -3.4207e-02,  2.6695e-02],\n",
       "                         [-2.4007e-02, -4.2688e-02,  2.7353e-03],\n",
       "                         [-2.1339e-02,  2.4383e-02, -1.0088e-02]],\n",
       "               \n",
       "                        [[ 7.1725e-03,  1.0686e-02, -1.6415e-02],\n",
       "                         [ 2.8011e-02,  5.9000e-02,  4.2356e-02],\n",
       "                         [-3.1816e-02, -3.3386e-02,  1.0129e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.0573e-02, -5.9218e-02,  3.3286e-03],\n",
       "                         [-1.4706e-02,  3.9396e-02, -2.9936e-02],\n",
       "                         [-4.9003e-02, -1.5258e-02, -1.5734e-02]],\n",
       "               \n",
       "                        [[-2.5624e-02, -2.5596e-04, -2.7650e-02],\n",
       "                         [ 2.0257e-03, -8.0217e-02,  2.1182e-02],\n",
       "                         [ 4.1904e-02,  1.4841e-02, -2.8846e-02]],\n",
       "               \n",
       "                        [[ 5.4881e-02,  8.1984e-02, -2.9506e-02],\n",
       "                         [-3.3438e-03,  7.2477e-02, -3.5344e-02],\n",
       "                         [-8.5299e-03,  3.7473e-02,  3.8407e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4815e-02, -2.6972e-03, -1.2346e-02],\n",
       "                         [-5.7544e-02, -3.0743e-02,  8.1464e-02],\n",
       "                         [-3.5099e-02, -3.4605e-02,  3.6735e-02]],\n",
       "               \n",
       "                        [[ 2.2609e-02,  5.1983e-02,  1.4371e-03],\n",
       "                         [ 2.1496e-02, -1.3150e-02, -8.8070e-02],\n",
       "                         [ 5.7430e-03, -5.1240e-02, -8.2214e-03]],\n",
       "               \n",
       "                        [[-7.7999e-02, -4.0306e-02,  3.1089e-02],\n",
       "                         [-8.6493e-02,  6.3072e-02, -1.1660e-02],\n",
       "                         [-4.4584e-02, -1.1616e-04,  3.0042e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.8771e-02,  6.3697e-02, -1.2499e-02],\n",
       "                         [ 7.5787e-02,  2.9947e-02, -1.1418e-02],\n",
       "                         [-5.0952e-02,  3.8301e-03,  1.7848e-02]],\n",
       "               \n",
       "                        [[ 6.6315e-02,  7.9504e-02,  7.2816e-02],\n",
       "                         [ 8.0956e-04,  4.0859e-02,  2.7047e-03],\n",
       "                         [-5.7265e-03,  4.0063e-03, -6.1880e-02]],\n",
       "               \n",
       "                        [[-1.7554e-02,  7.5568e-02,  5.2876e-02],\n",
       "                         [ 4.4501e-03,  5.4262e-02, -6.0417e-03],\n",
       "                         [ 9.6254e-02,  2.7372e-02, -2.0260e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-2.8316e-04,  9.2417e-05,  4.1929e-04,  4.7528e-04,  6.3164e-04,\n",
       "                       -7.7474e-04, -1.1019e-04,  1.2047e-04, -1.2923e-05, -5.6241e-04,\n",
       "                       -3.3657e-03,  8.1044e-04, -4.7147e-04, -1.3969e-04,  1.9640e-06,\n",
       "                       -6.3067e-04, -1.0722e-03, -4.6611e-04,  5.6271e-04, -4.0940e-05,\n",
       "                       -3.9328e-05, -1.1614e-04, -3.0717e-03,  7.1140e-04, -3.2071e-04,\n",
       "                        4.4273e-04,  4.0927e-04, -9.5186e-05,  1.6726e-04,  7.1939e-04,\n",
       "                        2.4242e-04, -2.9129e-04, -7.0618e-04, -8.0081e-04,  1.1164e-04,\n",
       "                       -8.4681e-05,  1.3069e-03,  8.3174e-04,  6.6022e-06, -2.6046e-04,\n",
       "                        4.3484e-04, -1.4578e-04, -6.2554e-04, -1.9882e-04,  1.3071e-04,\n",
       "                       -2.8044e-05, -7.0419e-04,  3.3350e-05, -1.1847e-04,  4.0030e-04,\n",
       "                       -9.9668e-04, -3.6515e-04, -4.2002e-04,  3.1678e-04, -9.3812e-04,\n",
       "                        2.7635e-04,  7.4666e-04,  5.1500e-04, -5.7312e-05,  2.8715e-04,\n",
       "                       -3.4007e-04, -1.4483e-04,  4.1604e-04, -3.3307e-04,  9.3537e-04,\n",
       "                        4.4165e-04, -4.5955e-05, -1.3150e-05, -1.6037e-04, -2.1966e-04,\n",
       "                       -6.1773e-04,  7.8195e-04, -1.0219e-03, -5.3524e-04,  6.5356e-04,\n",
       "                       -3.1525e-04,  7.3128e-04, -1.7965e-04,  1.2831e-04, -7.0858e-04,\n",
       "                       -2.5682e-04, -3.8255e-04, -2.2163e-03, -4.5730e-04,  2.7833e-04,\n",
       "                        1.5706e-04, -7.8330e-05, -2.5857e-04,  9.5895e-05,  9.9820e-05,\n",
       "                       -1.8566e-04,  4.9185e-04,  1.4274e-03, -5.2201e-04, -8.6112e-05,\n",
       "                       -7.7474e-04, -4.7126e-04,  1.4448e-04, -7.3736e-05,  4.5571e-04,\n",
       "                        2.7408e-04, -8.2242e-04, -8.3232e-06,  1.9891e-04,  6.4504e-05,\n",
       "                       -3.0453e-05,  9.3314e-04, -9.6061e-05,  2.3460e-05,  4.4154e-04,\n",
       "                        8.0685e-04, -3.6102e-04,  4.9263e-04, -1.0057e-04, -8.4459e-04,\n",
       "                       -3.5422e-04,  2.3062e-04,  6.7029e-04, -3.2508e-04, -3.4675e-04,\n",
       "                       -6.2084e-04,  1.3381e-04,  3.3908e-04, -1.6440e-04, -5.0065e-04,\n",
       "                        3.1470e-04, -4.3148e-04, -8.5855e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.4610, -0.5046, -0.4297, -0.3376, -0.4602, -0.3360, -0.4191, -0.6102,\n",
       "                       -0.4482, -0.3306, -0.3496, -0.5002, -0.3982, -0.3907, -0.3740, -0.5092,\n",
       "                       -0.4717, -0.3176, -0.3521, -0.3229, -0.3485, -0.5390, -0.2809, -0.4148,\n",
       "                       -0.4424, -0.3791, -0.3599, -0.5204, -0.4391, -0.3743, -0.4319, -0.5085,\n",
       "                       -0.3719, -0.3607, -0.5827, -0.4762, -0.4190, -0.4384, -0.3098, -0.5192,\n",
       "                       -0.4573, -0.3409, -0.4209, -0.3650, -0.3931, -0.3895, -0.4676, -0.4645,\n",
       "                       -0.4273, -0.5889, -0.2982, -0.3992, -0.4250, -0.4192, -0.3736, -0.4321,\n",
       "                       -0.4321, -0.3916, -0.3830, -0.4082, -0.4168, -0.3332, -0.2841, -0.3489,\n",
       "                       -0.3908, -0.3658, -0.3916, -0.3396, -0.3391, -0.3283, -0.4103, -0.3534,\n",
       "                       -0.4409, -0.3877, -0.3222, -0.3832, -0.5254, -0.3270, -0.3734, -0.3783,\n",
       "                       -0.3444, -0.4281, -0.3028, -0.4551, -0.4055, -0.3008, -0.4523, -0.3444,\n",
       "                       -0.3699, -0.3661, -0.4376, -0.3221, -0.3529, -0.5773, -0.2904, -0.4661,\n",
       "                       -0.4638, -0.3658, -0.5641, -0.3546, -0.3297, -0.3084, -0.3451, -0.3866,\n",
       "                       -0.3935, -0.3537, -0.3813, -0.3869, -0.3711, -0.3866, -0.3365, -0.4021,\n",
       "                       -0.4074, -0.2629, -0.3565, -0.4380, -0.4857, -0.3627, -0.3869, -0.3605,\n",
       "                       -0.3527, -0.3321, -0.3885, -0.3358, -0.3873, -0.3840, -0.3426, -0.3049],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([1.0182, 0.9962, 1.0078, 0.8506, 0.9262, 0.8523, 0.9391, 1.1096, 0.9501,\n",
       "                       0.8668, 0.9796, 1.0244, 0.9693, 1.0209, 0.9920, 1.0875, 1.0613, 0.7908,\n",
       "                       0.8965, 0.8299, 0.8649, 0.7369, 1.1288, 0.8288, 0.8668, 0.8475, 0.8856,\n",
       "                       1.0264, 0.9157, 0.8968, 0.9539, 1.0240, 0.9989, 1.0174, 1.1410, 0.8998,\n",
       "                       0.9140, 0.9985, 0.7590, 1.0445, 0.9802, 0.9777, 0.9403, 0.8538, 0.8641,\n",
       "                       0.9185, 0.8273, 0.9531, 0.9837, 1.0528, 0.9385, 0.9886, 0.9764, 1.0277,\n",
       "                       0.8303, 0.8183, 0.8699, 0.9006, 0.8608, 0.9006, 0.8699, 0.8080, 0.8649,\n",
       "                       0.7977, 0.9410, 0.9171, 0.8336, 0.8554, 0.7874, 0.8653, 1.0606, 0.9052,\n",
       "                       0.9885, 0.8369, 0.8585, 0.9763, 1.0048, 0.8491, 0.8808, 0.9400, 0.9043,\n",
       "                       0.9434, 1.0614, 1.0564, 0.8917, 0.8909, 0.6614, 0.8238, 1.0558, 0.7685,\n",
       "                       0.7542, 0.8933, 0.9358, 1.1636, 0.8403, 0.9882, 0.9794, 0.7628, 1.1063,\n",
       "                       1.0054, 0.8170, 0.8389, 0.8860, 0.9012, 0.9462, 1.0071, 0.9497, 0.9281,\n",
       "                       0.8401, 0.7834, 0.9331, 0.9296, 0.9430, 0.8533, 1.0777, 0.9449, 0.9900,\n",
       "                       0.8547, 0.9459, 0.8284, 0.8563, 0.9392, 0.8737, 0.8756, 0.9592, 0.9032,\n",
       "                       0.8890, 0.8707], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-1.8694e-02, -2.4483e-04,  6.0933e-04],\n",
       "                         [ 3.2579e-02,  4.9116e-02, -2.6536e-02],\n",
       "                         [ 6.1449e-03,  1.8249e-02,  3.4655e-02]],\n",
       "               \n",
       "                        [[ 5.8310e-02, -8.5767e-03,  1.4964e-02],\n",
       "                         [ 1.7528e-02, -3.3210e-02, -1.1570e-02],\n",
       "                         [ 5.5065e-03, -1.9485e-02, -6.5369e-02]],\n",
       "               \n",
       "                        [[ 3.6619e-02, -3.5309e-02,  4.7112e-02],\n",
       "                         [-5.4654e-02, -5.9616e-02,  3.4309e-02],\n",
       "                         [-4.5261e-02,  1.6816e-02,  5.8036e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.5955e-02,  2.7236e-02, -5.1518e-03],\n",
       "                         [ 6.4764e-02, -1.4941e-02,  7.2098e-03],\n",
       "                         [ 1.0983e-02,  6.4798e-02, -2.8869e-02]],\n",
       "               \n",
       "                        [[-3.8317e-02, -2.4951e-02, -2.0550e-02],\n",
       "                         [-3.8966e-03, -6.3437e-03, -1.5059e-02],\n",
       "                         [-1.1240e-02, -2.3870e-02, -1.3455e-02]],\n",
       "               \n",
       "                        [[-1.3174e-02, -3.5279e-02,  2.5549e-02],\n",
       "                         [-5.8744e-02, -1.7841e-02, -1.3953e-02],\n",
       "                         [ 1.3665e-02, -3.7422e-02, -2.9939e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.3741e-02, -2.4110e-02, -7.7147e-02],\n",
       "                         [-2.0263e-03, -5.5409e-02, -3.5891e-02],\n",
       "                         [-2.3089e-02, -4.1805e-02, -5.4971e-02]],\n",
       "               \n",
       "                        [[ 4.8525e-03,  2.7969e-02,  4.9042e-03],\n",
       "                         [-7.2932e-02,  1.2441e-03, -9.6800e-02],\n",
       "                         [-1.0440e-01,  3.2619e-02, -4.4203e-02]],\n",
       "               \n",
       "                        [[-2.2077e-02,  4.0887e-02,  2.7354e-02],\n",
       "                         [ 7.2628e-02,  6.3490e-02,  5.4264e-02],\n",
       "                         [ 3.3197e-02,  3.1694e-02,  6.0477e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.1610e-02, -3.8691e-02,  1.2466e-02],\n",
       "                         [-1.9584e-02, -4.8673e-02,  4.8133e-02],\n",
       "                         [ 5.0105e-02, -1.6440e-02, -4.4101e-02]],\n",
       "               \n",
       "                        [[ 8.4920e-02,  1.1393e-01,  5.4894e-02],\n",
       "                         [ 4.9950e-02,  6.3407e-02,  3.7567e-02],\n",
       "                         [ 1.3501e-01,  5.9836e-02,  5.1843e-02]],\n",
       "               \n",
       "                        [[ 4.8957e-02,  4.4837e-02, -4.9335e-02],\n",
       "                         [-3.6868e-02, -9.3771e-03, -4.8242e-02],\n",
       "                         [ 3.5366e-02,  3.6813e-02, -6.9778e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.8828e-02,  6.9160e-03,  8.8844e-03],\n",
       "                         [-6.2021e-02,  2.6692e-02,  1.2937e-02],\n",
       "                         [-2.0243e-02,  4.3049e-05, -3.8240e-02]],\n",
       "               \n",
       "                        [[ 1.0999e-02, -4.7814e-03,  2.5985e-02],\n",
       "                         [-5.5079e-02,  1.1369e-02, -2.2354e-02],\n",
       "                         [-7.2246e-02, -1.9009e-02, -2.7274e-02]],\n",
       "               \n",
       "                        [[ 5.7365e-02,  1.8759e-02,  8.7606e-03],\n",
       "                         [ 6.4082e-03,  5.3876e-02,  1.6150e-02],\n",
       "                         [ 4.2171e-02,  6.6910e-02, -2.3621e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.3876e-02,  6.3546e-02, -4.8112e-02],\n",
       "                         [ 9.2763e-03,  1.0816e-02,  7.3067e-03],\n",
       "                         [-7.7809e-02,  3.9198e-02,  2.9565e-02]],\n",
       "               \n",
       "                        [[ 6.4182e-02,  9.0909e-02,  7.8321e-02],\n",
       "                         [ 8.7435e-02,  7.3698e-02,  9.1933e-02],\n",
       "                         [-3.4183e-03,  4.0073e-02,  4.9450e-02]],\n",
       "               \n",
       "                        [[-1.9624e-02,  2.7981e-02,  4.2093e-02],\n",
       "                         [-9.7686e-03,  1.4421e-02,  1.3674e-02],\n",
       "                         [ 3.0911e-02,  3.5781e-02,  2.8966e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-5.0030e-02, -8.9260e-03, -2.4350e-02],\n",
       "                         [ 2.3805e-03, -1.6211e-03, -5.6431e-02],\n",
       "                         [-9.1689e-02, -8.0817e-02, -1.0248e-01]],\n",
       "               \n",
       "                        [[-1.8533e-02, -9.0979e-03,  4.3929e-03],\n",
       "                         [ 3.4159e-02,  1.0462e-02,  1.6642e-02],\n",
       "                         [-5.3592e-02, -4.9993e-02,  3.7131e-02]],\n",
       "               \n",
       "                        [[ 2.4240e-02,  5.2971e-02,  4.8573e-02],\n",
       "                         [ 6.0148e-02,  4.5567e-02, -5.0640e-03],\n",
       "                         [-4.8412e-02,  4.0328e-02, -3.1494e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.5488e-02,  3.6555e-02, -5.5399e-02],\n",
       "                         [ 2.4555e-02,  1.0129e-02, -3.9425e-02],\n",
       "                         [ 3.0607e-02, -3.6948e-02, -4.2313e-02]],\n",
       "               \n",
       "                        [[ 4.8113e-03,  9.7693e-02, -2.3073e-02],\n",
       "                         [ 2.4520e-02,  7.9191e-02,  3.7183e-02],\n",
       "                         [ 1.1549e-02, -3.3168e-02, -6.3129e-02]],\n",
       "               \n",
       "                        [[-7.1377e-02,  4.4538e-02, -1.1684e-02],\n",
       "                         [-1.6189e-02,  3.3947e-02,  4.4755e-02],\n",
       "                         [-5.5957e-02, -1.1146e-02, -5.2389e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.4933e-02, -7.9501e-03, -1.8014e-02],\n",
       "                         [-2.4404e-02, -1.6881e-02,  3.2212e-02],\n",
       "                         [ 2.3009e-02, -1.5058e-02, -2.1562e-02]],\n",
       "               \n",
       "                        [[ 2.1339e-02, -2.4287e-03,  1.5186e-02],\n",
       "                         [ 6.2641e-02,  9.5333e-02,  5.6168e-02],\n",
       "                         [ 4.7679e-02,  4.8884e-02, -1.3646e-03]],\n",
       "               \n",
       "                        [[ 2.2059e-03,  1.0484e-02, -3.1927e-02],\n",
       "                         [-2.9049e-02, -1.4063e-02, -4.6151e-02],\n",
       "                         [-6.5698e-03,  8.5133e-02, -4.3545e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.3301e-02, -5.1285e-02, -5.7043e-03],\n",
       "                         [-7.3297e-03, -7.5322e-02, -2.1633e-02],\n",
       "                         [-2.0961e-02,  1.4845e-03,  2.9727e-03]],\n",
       "               \n",
       "                        [[-9.4496e-02, -9.8252e-02,  1.6405e-02],\n",
       "                         [-1.5982e-01, -1.7375e-02, -1.6884e-02],\n",
       "                         [-7.3543e-02, -8.2879e-02,  5.2069e-02]],\n",
       "               \n",
       "                        [[ 3.3122e-03,  3.4377e-02, -1.2793e-02],\n",
       "                         [-4.1080e-02,  6.1744e-03, -6.0144e-02],\n",
       "                         [ 1.3667e-02,  5.0744e-02,  3.2287e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.5109e-02, -3.2255e-02,  3.7919e-02],\n",
       "                         [ 6.0291e-02,  4.6455e-02,  2.4972e-02],\n",
       "                         [-5.1008e-02,  9.8793e-03,  3.0467e-02]],\n",
       "               \n",
       "                        [[-8.2251e-03,  1.9240e-02,  2.8501e-02],\n",
       "                         [ 2.4942e-02,  3.5047e-03,  3.4148e-02],\n",
       "                         [-3.9065e-04,  5.7687e-02,  1.6956e-02]],\n",
       "               \n",
       "                        [[ 2.7015e-02, -2.0371e-02, -1.5745e-02],\n",
       "                         [-5.3851e-02, -3.3983e-02,  3.7506e-04],\n",
       "                         [ 5.4108e-03, -7.2030e-02,  3.6871e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.4516e-02, -2.0235e-02,  9.0817e-02],\n",
       "                         [ 3.6596e-02, -7.9565e-03,  3.4559e-02],\n",
       "                         [-2.9059e-02, -3.5269e-04, -3.3308e-02]],\n",
       "               \n",
       "                        [[ 1.5057e-02,  6.0290e-02,  5.9963e-02],\n",
       "                         [ 4.3951e-02,  3.9789e-02, -1.5480e-02],\n",
       "                         [-1.0087e-02,  5.4025e-02,  4.9306e-03]],\n",
       "               \n",
       "                        [[-6.7737e-03, -1.4213e-02, -4.0248e-02],\n",
       "                         [ 1.2259e-02,  2.5299e-02, -1.5473e-02],\n",
       "                         [ 2.0751e-02, -4.5265e-02,  2.6289e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 0.5817,  0.5810,  0.5722, -0.5747, -0.5791, -0.5748,  0.5724, -0.5778,\n",
       "                        0.5753, -0.5803, -0.5718,  0.5539, -0.5775,  0.5797,  0.5764, -0.5766,\n",
       "                       -0.5792, -0.5804, -0.5733,  0.5771, -0.5742,  0.5731,  0.5802, -0.5750,\n",
       "                        0.5753,  0.5762, -0.5782, -0.5777,  0.5720, -0.5781,  0.5758,  0.5729,\n",
       "                        0.5794, -0.5683,  0.5797,  0.5729, -0.5748,  0.5776, -0.5741,  0.5769,\n",
       "                       -0.5829, -0.5805, -0.5797,  0.5789,  0.5785, -0.5724,  0.5817, -0.5791,\n",
       "                       -0.5784,  0.5821, -0.5733,  0.5743,  0.5772, -0.5801,  0.5775, -0.5812,\n",
       "                        0.5821,  0.5752, -0.5765,  0.5768, -0.5759, -0.5736,  0.5746, -0.5765,\n",
       "                       -0.5824, -0.5783, -0.5720,  0.5800,  0.5725, -0.5738,  0.5756, -0.5808,\n",
       "                       -0.5767, -0.5782,  0.5720, -0.5750, -0.5774,  0.5761,  0.5766,  0.5767,\n",
       "                       -0.5786,  0.5787,  0.5743, -0.5770, -0.5742,  0.5723,  0.5766, -0.5715,\n",
       "                        0.5778,  0.5747, -0.5748,  0.5771,  0.5775, -0.5723,  0.5750,  0.5807,\n",
       "                        0.5745, -0.5754, -0.5808,  0.5714, -0.5722, -0.5808, -0.5715,  0.5771,\n",
       "                        0.5735,  0.5757, -0.5765,  0.5726, -0.5805,  0.5755, -0.5788, -0.5762,\n",
       "                       -0.5801, -0.5759,  0.5809, -0.5782, -0.5794,  0.5732, -0.5736, -0.5763,\n",
       "                        0.5736, -0.5815, -0.5773,  0.5802,  0.5796,  0.5787, -0.5740, -0.5814],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.1365, -0.3004, -0.3328, -0.1868, -0.2478, -0.3174, -0.2841, -0.2390,\n",
       "                       -0.3388, -0.2348, -0.1742, -0.3258, -0.1499, -0.1518, -0.3325, -0.1503,\n",
       "                       -0.1128, -0.2569, -0.2070, -0.1854, -0.2171, -0.2643, -0.2481, -0.2792,\n",
       "                       -0.2897, -0.2657, -0.2792, -0.0937, -0.2843, -0.1623, -0.2623, -0.2006,\n",
       "                       -0.0453, -0.2320, -0.2245, -0.2404, -0.3192, -0.2278, -0.1135, -0.2675,\n",
       "                       -0.1745, -0.2615, -0.2930, -0.3161, -0.2781, -0.2448, -0.3060, -0.2650,\n",
       "                       -0.1861, -0.1399, -0.0268, -0.2898, -0.2997, -0.3271, -0.2185, -0.3105,\n",
       "                       -0.3150, -0.1638, -0.2597, -0.1270, -0.2370, -0.2600, -0.1844, -0.2869,\n",
       "                       -0.2097, -0.2443, -0.3018, -0.2886, -0.0803, -0.2014, -0.0959, -0.2511,\n",
       "                       -0.2878, -0.0996, -0.2642, -0.3255, -0.2707, -0.2031, -0.2236, -0.2880,\n",
       "                       -0.2013, -0.2196, -0.1881, -0.2594, -0.2494, -0.2636, -0.1585, -0.2514,\n",
       "                       -0.0911, -0.2298, -0.1232, -0.2006, -0.1826, -0.2361, -0.3240, -0.3167,\n",
       "                       -0.3372, -0.1676, -0.3012, -0.1926, -0.2104, -0.0386, -0.2300, -0.3292,\n",
       "                       -0.2782, -0.1258, -0.2288, -0.1577, -0.2646, -0.2042, -0.3020, -0.0708,\n",
       "                       -0.3026, -0.2275, -0.2870, -0.2229, -0.2407, -0.0831, -0.2415, -0.2579,\n",
       "                       -0.2800, -0.1856, -0.2556, -0.2506, -0.1560, -0.3115, -0.0963, -0.0894],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.7135, 0.6246, 0.5321, 0.9549, 0.6986, 0.7806, 0.6041, 0.6070, 0.6882,\n",
       "                       0.6596, 0.7051, 0.7673, 0.7273, 0.9884, 0.7759, 0.9082, 0.8187, 0.8281,\n",
       "                       0.7062, 0.7759, 0.6721, 0.9895, 0.6175, 0.6390, 0.6352, 0.6086, 0.6567,\n",
       "                       0.8652, 0.8182, 0.7043, 1.0500, 0.6112, 0.7012, 0.9938, 0.6000, 0.8939,\n",
       "                       0.6003, 0.8212, 0.7616, 0.6183, 0.8414, 0.7218, 0.6432, 0.7757, 0.5315,\n",
       "                       0.9760, 0.7120, 0.7886, 0.7569, 0.6142, 0.8440, 0.9092, 0.6533, 0.5901,\n",
       "                       0.9765, 0.5364, 0.8127, 0.6889, 0.9937, 0.7785, 0.8468, 0.6017, 0.8552,\n",
       "                       0.6193, 0.6628, 0.6080, 0.7278, 0.6327, 0.7919, 0.9753, 0.6739, 0.5635,\n",
       "                       0.6301, 0.7831, 0.6520, 0.5608, 0.7225, 0.8010, 0.9787, 0.5759, 0.6510,\n",
       "                       1.0548, 0.9582, 0.6270, 0.5727, 0.7097, 0.8674, 0.6734, 0.7381, 0.8862,\n",
       "                       0.6862, 1.0242, 0.8824, 0.6743, 0.7067, 0.7032, 0.6362, 0.9486, 0.6954,\n",
       "                       0.6976, 0.8718, 0.7878, 0.7737, 0.8515, 0.6061, 0.7743, 0.7303, 0.7098,\n",
       "                       0.9879, 0.9606, 0.6002, 0.7458, 0.6924, 0.6490, 0.7555, 0.7515, 0.6654,\n",
       "                       0.7422, 0.9403, 0.7609, 0.9700, 0.6498, 0.6399, 0.6130, 0.7407, 0.5230,\n",
       "                       0.7820, 0.8304], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0176,  0.0174,  0.0006,  ..., -0.0044,  0.0053, -0.0104],\n",
       "                       [ 0.0072, -0.0148,  0.0231,  ..., -0.0072,  0.0089, -0.0128],\n",
       "                       [-0.0048, -0.0059, -0.0017,  ..., -0.0041,  0.0098, -0.0191],\n",
       "                       [-0.0237, -0.0141,  0.0056,  ..., -0.0021,  0.0002, -0.0086],\n",
       "                       [ 0.0090, -0.0049,  0.0107,  ..., -0.0172,  0.0016,  0.0061]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0055, -0.0007,  0.0015, -0.0046, -0.0025], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[ 1.9450e-01, -9.9795e-02,  5.8899e-02,  2.0421e-01, -6.8798e-02,\n",
       "                        -2.0007e-01, -1.7882e-01,  1.9901e-01,  7.3716e-02, -7.7455e-02,\n",
       "                        -1.7553e-01,  1.2926e-01, -2.1960e-01, -5.1855e-02,  1.6968e-01,\n",
       "                        -7.0568e-02, -1.7667e-01,  8.5367e-02, -1.8126e-01, -4.0493e-02],\n",
       "                       [ 5.2346e-01, -1.6529e-01,  3.4687e-01, -2.2298e-01,  4.1869e-01,\n",
       "                         4.1310e-02,  2.9931e-01,  4.4746e-01,  7.9774e-01, -5.2963e-02,\n",
       "                        -6.3618e-01, -1.2951e-02, -4.8660e-01,  3.5319e-02, -3.3291e-01,\n",
       "                        -4.8768e-02, -7.0401e-01, -3.1907e-01, -1.0651e+00, -3.5918e-01],\n",
       "                       [-1.6694e-01, -8.9527e-03, -4.4028e-02,  7.6108e-03,  1.1657e-01,\n",
       "                        -1.6403e-01, -6.1110e-02, -9.5870e-03,  1.4024e-01,  2.5067e-02,\n",
       "                         3.1832e-02,  1.9200e-01,  1.6992e-01,  1.4726e-01,  1.9761e-01,\n",
       "                        -1.4038e-01,  1.1315e-01,  1.8848e-01,  7.8475e-03, -1.8232e-01],\n",
       "                       [-3.3486e-01, -3.1478e-01,  1.9695e-01, -3.2975e-01,  8.6443e-02,\n",
       "                        -2.8814e-01,  2.9092e-01,  5.7258e-02, -6.7843e-03, -7.9162e-02,\n",
       "                         1.3197e-02, -2.9772e-01, -3.0350e-01, -2.7890e-01, -3.7246e-01,\n",
       "                        -3.9478e-01,  4.6622e-03, -1.2402e-01,  4.0368e-01, -2.2499e-01],\n",
       "                       [-8.2182e-01, -6.6639e-01,  3.2541e-01, -3.3423e-01,  2.2087e-01,\n",
       "                        -6.6342e-01,  3.2421e-01,  2.2937e-01, -6.8524e-01, -6.1384e-01,\n",
       "                         4.4107e-01, -3.7824e-01, -2.6293e-01, -4.1774e-01,  5.6607e-03,\n",
       "                        -4.7103e-01, -1.3577e-01, -3.5937e-01,  1.2264e+00, -5.2829e-01],\n",
       "                       [-1.8804e-01, -4.0074e-01,  3.9767e-01, -5.0878e-01,  6.4794e-01,\n",
       "                        -4.4584e-01,  6.9425e-01,  2.1882e-01,  2.5964e-01, -6.7297e-01,\n",
       "                         2.0502e-01, -4.1099e-01, -5.6557e-01, -5.8361e-01, -3.8876e-01,\n",
       "                        -4.7207e-01, -3.4853e-01, -6.0168e-01, -2.7203e-01, -4.4962e-01],\n",
       "                       [-4.0175e-01, -2.4770e-01,  3.8955e-01, -3.5436e-01,  5.1054e-01,\n",
       "                        -4.8842e-01,  5.0293e-01, -1.4356e-01, -1.3233e-01, -5.2031e-01,\n",
       "                         8.5051e-02, -1.3783e-01, -7.4531e-02, -3.6012e-01, -4.5801e-01,\n",
       "                        -4.2651e-01,  1.1843e-02, -3.9925e-01,  5.9731e-01, -3.3804e-01],\n",
       "                       [-2.1373e-01,  1.5093e-01, -1.8004e-01, -1.2638e-02,  1.6221e-01,\n",
       "                         1.9895e-01, -2.4298e-02, -2.0858e-02, -3.3278e-03, -1.5543e-01,\n",
       "                        -1.4810e-01, -1.7973e-01,  1.3173e-01, -4.3931e-02,  2.1647e-01,\n",
       "                         1.6515e-01, -2.1437e-01,  3.3630e-02, -1.7162e-01,  5.6127e-02],\n",
       "                       [-4.1406e-01, -3.8315e-01,  3.0809e-01, -4.8395e-01,  3.4835e-01,\n",
       "                        -5.2036e-01,  3.0212e-01, -8.8761e-02, -6.0186e-01, -4.8301e-01,\n",
       "                         4.2569e-01, -1.6205e-01, -2.6032e-01, -2.9095e-01,  1.7556e-01,\n",
       "                        -4.8376e-01,  8.4760e-02, -2.3611e-01,  1.1511e+00, -1.5831e-01],\n",
       "                       [-1.1362e-02, -2.0247e-01, -1.8257e-01, -6.8434e-02,  1.5786e-02,\n",
       "                        -6.1164e-02, -1.5247e-01,  6.1289e-02, -9.6104e-02,  2.0382e-01,\n",
       "                         4.3108e-02,  7.8391e-02, -1.7161e-01, -3.9588e-03, -1.0681e-01,\n",
       "                        -1.2430e-02,  1.1632e-02, -7.3115e-02, -4.3982e-02,  1.9425e-01],\n",
       "                       [-5.4940e-01, -3.4049e-01,  4.2890e-01, -5.8399e-02,  3.0928e-01,\n",
       "                        -2.6881e-01,  1.6818e-01,  4.2774e-02, -1.8537e-01, -2.6148e-01,\n",
       "                         2.6579e-01, -2.5009e-01,  2.9611e-02, -2.0857e-01,  1.6869e-01,\n",
       "                        -3.0467e-01,  3.0290e-01, -2.1991e-01,  5.8198e-01, -2.5371e-01],\n",
       "                       [ 8.5867e-02, -1.0679e-01, -7.1459e-02,  3.1513e-01,  9.3302e-02,\n",
       "                         1.0799e-01, -2.6852e-01,  1.9266e-01, -3.7224e-02,  1.0171e-03,\n",
       "                         1.2404e-01,  1.1830e-01,  2.4423e-01, -7.6670e-02,  2.3267e-01,\n",
       "                         1.7039e-02,  2.1786e-01,  3.4474e-02,  9.9998e-02,  8.0150e-02],\n",
       "                       [ 2.1736e-01, -1.2078e-01, -4.0625e-04,  1.3962e-01,  1.8630e-01,\n",
       "                         5.1323e-02, -1.9837e-01,  1.5539e-02,  1.1814e-01,  4.6512e-02,\n",
       "                         1.0298e-01,  2.3731e-01,  9.3286e-02,  1.1501e-01,  1.0868e-01,\n",
       "                         1.8514e-01, -9.7257e-02,  1.9191e-01,  1.5074e-01, -4.0412e-03],\n",
       "                       [-4.3744e-02, -6.9996e-02, -1.6303e-01,  2.1344e-01, -1.0878e-01,\n",
       "                         1.4868e-01,  3.5501e-02, -2.0548e-01,  1.4880e-01,  1.4516e-01,\n",
       "                         2.1064e-01,  6.0525e-02,  1.9195e-01,  1.8585e-01, -1.8688e-01,\n",
       "                         2.1175e-01,  7.7859e-02, -1.2162e-01, -4.6285e-02, -2.0865e-01],\n",
       "                       [-5.4143e-01, -4.2645e-01,  2.9691e-01, -1.6625e-01, -3.0313e-02,\n",
       "                        -4.7286e-01,  3.7695e-01,  1.6919e-02, -7.5424e-01, -3.1499e-01,\n",
       "                         2.8891e-01, -1.1133e-01,  5.3339e-02, -3.3524e-01, -4.7191e-02,\n",
       "                        -2.6940e-01,  2.2977e-01, -4.6436e-01,  8.6547e-01, -3.8427e-01],\n",
       "                       [ 6.2884e-02,  1.5268e-01, -9.8051e-02, -7.9451e-02,  5.4087e-02,\n",
       "                        -8.8575e-02, -2.0254e-01,  7.8244e-03, -2.1483e-01,  1.6323e-01,\n",
       "                         5.5022e-02,  1.6895e-01,  1.5126e-01, -4.8578e-04, -1.3752e-01,\n",
       "                         1.3657e-02,  8.7678e-02,  7.2777e-02, -2.0556e-01,  1.6001e-01],\n",
       "                       [-1.7184e-01,  1.5199e-01, -1.4167e-01,  4.3341e-02, -1.1432e-01,\n",
       "                         1.8257e-02,  1.5661e-01,  6.9146e-03,  1.4682e-01,  1.5712e-01,\n",
       "                        -1.8079e-01, -2.4925e-02,  9.8675e-02,  1.8632e-01,  7.6713e-02,\n",
       "                        -7.8911e-02, -9.5569e-02, -1.8848e-01,  2.3741e-02, -1.2041e-01],\n",
       "                       [-4.0071e-01, -6.1581e-01,  8.0090e-01, -6.7051e-01,  8.3600e-01,\n",
       "                        -5.6193e-01,  7.3714e-01,  4.9516e-01,  3.9324e-01, -5.0199e-01,\n",
       "                         2.4362e-01, -5.5344e-01, -8.2635e-01, -5.5234e-01, -9.9000e-01,\n",
       "                        -7.7811e-01, -8.6551e-01, -4.9253e-01, -2.8526e-01, -7.8908e-01],\n",
       "                       [-3.5561e-01, -3.8041e-01,  1.4092e-01, -5.5030e-01,  1.5341e-01,\n",
       "                        -4.5834e-01,  3.3869e-01,  1.4417e-01, -1.0766e-01, -2.7173e-01,\n",
       "                         1.2898e-01, -5.2948e-01, -6.2403e-02, -5.1174e-01, -3.6534e-01,\n",
       "                        -5.3992e-01, -2.8041e-01, -5.3359e-01,  3.8086e-01, -5.3192e-01],\n",
       "                       [-2.1612e-02, -2.2817e-01,  7.0758e-02, -2.2693e-01,  2.9300e-01,\n",
       "                        -3.0908e-01,  4.1390e-01,  5.5248e-01,  7.0786e-03, -2.5224e-01,\n",
       "                         6.8136e-02,  7.1134e-03, -5.2143e-01, -3.0412e-01, -3.8382e-01,\n",
       "                        -3.6731e-01, -3.8546e-01, -4.7853e-02, -8.0040e-01, -3.2707e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([-0.1579,  0.3453, -0.2288,  0.0537,  0.2526,  0.6160,  0.4153,  0.0139,\n",
       "                        0.2359,  0.1393,  0.2996,  0.1191,  0.1070, -0.1610,  0.2631,  0.1238,\n",
       "                        0.1505,  0.8275,  0.3886,  0.1625], device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 0.1263, -0.0982,  0.1589, -0.5313, -0.3307, -0.4198, -0.5725,  0.1538,\n",
       "                        -0.3467, -0.0184, -0.2735,  0.2020,  0.2049,  0.1252, -0.1665,  0.0328,\n",
       "                         0.0779, -0.5737, -0.1408, -0.3440],\n",
       "                       [ 0.0947,  0.1007, -0.1501,  0.3778,  0.3756,  0.4337,  0.2929, -0.1357,\n",
       "                         0.4287,  0.1764,  0.2034,  0.0089,  0.1094,  0.0969,  0.4444, -0.0959,\n",
       "                        -0.1545,  0.4571,  0.3251,  0.1207],\n",
       "                       [-0.1231, -0.3273, -0.1299, -0.3534, -0.4551, -0.1352, -0.5017, -0.1648,\n",
       "                        -0.3452, -0.0091, -0.2576,  0.0192,  0.0235,  0.0105, -0.4695,  0.0933,\n",
       "                         0.0714, -0.5384, -0.3885, -0.5036],\n",
       "                       [ 0.2137,  0.1136,  0.1479, -0.0579, -0.3187, -0.1340, -0.1073,  0.0262,\n",
       "                        -0.2543,  0.1709, -0.1988, -0.1264, -0.0042,  0.0076, -0.2939,  0.0615,\n",
       "                        -0.1830, -0.2882, -0.0917,  0.1008],\n",
       "                       [-0.1682,  0.2547, -0.1232,  0.1331, -0.1950,  0.1790,  0.2550,  0.1976,\n",
       "                        -0.1063, -0.1313,  0.0840,  0.1219, -0.1585, -0.0820, -0.0419,  0.0231,\n",
       "                         0.0909,  0.0605,  0.1998, -0.0896],\n",
       "                       [ 0.1320,  0.2417,  0.1746,  0.3841,  0.4092,  0.3226,  0.0886,  0.0265,\n",
       "                         0.1819, -0.1544,  0.0413,  0.1650,  0.0632,  0.0601,  0.2722, -0.0616,\n",
       "                         0.2075,  0.5421,  0.0936,  0.1955],\n",
       "                       [ 0.0186,  0.7471, -0.0562,  0.9325,  1.0555,  1.0350,  1.1093, -0.1340,\n",
       "                         0.7340,  0.0664,  0.7916, -0.0312,  0.0644, -0.1446,  0.7886,  0.0342,\n",
       "                         0.1900,  0.9407,  1.0865,  0.5031],\n",
       "                       [-0.0348, -0.1226, -0.2060, -0.0712,  0.0292, -0.1753, -0.1864,  0.2235,\n",
       "                         0.1089,  0.2147,  0.1014, -0.1460,  0.1234,  0.1064, -0.1826, -0.2108,\n",
       "                         0.0877, -0.3594, -0.2758, -0.0419],\n",
       "                       [-0.0676, -0.4029, -0.1883,  0.1660,  0.3572, -0.0874,  0.2286, -0.1765,\n",
       "                         0.2320,  0.1800,  0.2421, -0.1187, -0.1170, -0.0483,  0.2848, -0.1009,\n",
       "                         0.0885, -0.1841,  0.0485, -0.4397],\n",
       "                       [ 0.0187, -0.4748,  0.0170, -0.4280, -0.3555, -0.2162, -0.3611,  0.0621,\n",
       "                        -0.3962,  0.1226, -0.3849,  0.0459, -0.0111,  0.0952, -0.1456, -0.0094,\n",
       "                         0.2114, -0.6149, -0.1556, -0.3293]], device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.3690,  0.3630, -0.3466, -0.1015,  0.0176,  0.2323,  0.7596, -0.2032,\n",
       "                        0.1293, -0.0602], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.4890122148990632,\n",
       "   1.3599313499927521,\n",
       "   1.3122278995513916,\n",
       "   1.263568129181862,\n",
       "   1.2115357151031494,\n",
       "   1.1527762405872346,\n",
       "   1.1063622422218322,\n",
       "   1.0719169086217881,\n",
       "   1.0414993832111359,\n",
       "   1.0287322256565095,\n",
       "   0.9884507023096084,\n",
       "   0.978219517827034,\n",
       "   0.9670064885616303,\n",
       "   0.9357376105189323,\n",
       "   0.930062896490097,\n",
       "   0.9212201446294784,\n",
       "   0.8994493836164474,\n",
       "   0.892454871892929,\n",
       "   0.8805869992375374,\n",
       "   0.8662967274188995,\n",
       "   0.8599254228472709,\n",
       "   0.8318805048465728,\n",
       "   0.827732775747776,\n",
       "   0.8268952717781067,\n",
       "   0.819969786465168,\n",
       "   0.8045594525933266,\n",
       "   0.788696259200573,\n",
       "   0.7838237702846527,\n",
       "   0.7843875247240066,\n",
       "   0.7843775973916054,\n",
       "   0.7575415322184562,\n",
       "   0.757233423948288,\n",
       "   0.748283129453659,\n",
       "   0.7322137492895127,\n",
       "   0.7371691310405731,\n",
       "   0.7217823389172554,\n",
       "   0.7126781229376793,\n",
       "   0.7079500596523285,\n",
       "   0.7030578963160515,\n",
       "   0.6839392355680466,\n",
       "   0.701887010216713,\n",
       "   0.6828366332054138,\n",
       "   0.6750042840838433,\n",
       "   0.6822033582925796,\n",
       "   0.6627746062874794,\n",
       "   0.6611326861977577,\n",
       "   0.6562396683096886,\n",
       "   0.6494071682095528,\n",
       "   0.6354554601609707,\n",
       "   0.6445574988126754,\n",
       "   0.644352512896061,\n",
       "   0.6181837678551674,\n",
       "   0.6249247009754181,\n",
       "   0.612467559158802,\n",
       "   0.6243481777906418,\n",
       "   0.6179823871254921,\n",
       "   0.6041152629852294,\n",
       "   0.6014821066260337,\n",
       "   0.5967994345128537,\n",
       "   0.5962394275069237,\n",
       "   0.5921666466593742,\n",
       "   0.5865854976773262,\n",
       "   0.5698407548367977,\n",
       "   0.5630888887643815,\n",
       "   0.5743186659812928,\n",
       "   0.5793118818998336,\n",
       "   0.5640674892663956,\n",
       "   0.5455433739423752,\n",
       "   0.5497118222713471,\n",
       "   0.5563703108131886,\n",
       "   0.5390113534331322,\n",
       "   0.5401530054211616,\n",
       "   0.5469049251973629,\n",
       "   0.5430005521178246,\n",
       "   0.524293736577034,\n",
       "   0.5232412034273147,\n",
       "   0.5291962631046772,\n",
       "   0.5254160203337669,\n",
       "   0.5154229901134968,\n",
       "   0.5176381588280201,\n",
       "   0.5010969158411026,\n",
       "   0.5088311149775981,\n",
       "   0.5010652993619442,\n",
       "   0.5000353639125824,\n",
       "   0.48994715708494185,\n",
       "   0.49120724156498907,\n",
       "   0.48482105979323387,\n",
       "   0.4839111033976078,\n",
       "   0.4809921236038208,\n",
       "   0.473193500995636,\n",
       "   0.46491629067063334,\n",
       "   0.4803873443901539,\n",
       "   0.45502642896771434,\n",
       "   0.4577629123032093,\n",
       "   0.4620052195191383,\n",
       "   0.46800047445297244,\n",
       "   0.45711222222447395,\n",
       "   0.4576713859438896,\n",
       "   0.4422991942167282,\n",
       "   0.451129138559103,\n",
       "   0.43758512634038926,\n",
       "   0.43218092775344846,\n",
       "   0.4343853465616703,\n",
       "   0.43593542551994324,\n",
       "   0.4356130637973547,\n",
       "   0.4286271196603775,\n",
       "   0.4242150369286537,\n",
       "   0.4180742202103138,\n",
       "   0.4143091880232096,\n",
       "   0.4080926300883293,\n",
       "   0.4083968482017517,\n",
       "   0.40796515925228594,\n",
       "   0.3996075845509768,\n",
       "   0.40217464566230776,\n",
       "   0.39802789148688317,\n",
       "   0.39762489892542363,\n",
       "   0.3924752089083195,\n",
       "   0.3854433444738388,\n",
       "   0.3925152005851269,\n",
       "   0.3866725883632898,\n",
       "   0.3877907060533762,\n",
       "   0.3688792528808117,\n",
       "   0.37193668691813947,\n",
       "   0.36806272354722025,\n",
       "   0.3694519515633583,\n",
       "   0.3681644252240658,\n",
       "   0.3678797065913677,\n",
       "   0.3601397400647402,\n",
       "   0.3611739548742771,\n",
       "   0.3528351319879293,\n",
       "   0.3564950845837593,\n",
       "   0.34310044303536413,\n",
       "   0.35642847026884555,\n",
       "   0.3303900445103645,\n",
       "   0.3429559666365385,\n",
       "   0.33516078792512416,\n",
       "   0.33823150201141833,\n",
       "   0.3370885474979877,\n",
       "   0.3279214413166046,\n",
       "   0.3370945183336735,\n",
       "   0.3305105810016394,\n",
       "   0.3206436398178339,\n",
       "   0.3204374437481165,\n",
       "   0.32187760795652864,\n",
       "   0.41701636750996113,\n",
       "   0.32367992417514324,\n",
       "   0.31579931880533696,\n",
       "   0.3067500615864992],\n",
       "  'train_loss_std': [0.13965570780430467,\n",
       "   0.11567742340050627,\n",
       "   0.1307083343968383,\n",
       "   0.1209022255090067,\n",
       "   0.12992264394818415,\n",
       "   0.12892971029328573,\n",
       "   0.14069383397632548,\n",
       "   0.13068093557131427,\n",
       "   0.136348287450504,\n",
       "   0.1346788569990074,\n",
       "   0.13333724956060158,\n",
       "   0.14310942377009075,\n",
       "   0.13122117422967675,\n",
       "   0.14142547756331414,\n",
       "   0.13712962554962957,\n",
       "   0.143601113223894,\n",
       "   0.1313464651719406,\n",
       "   0.1363149916167216,\n",
       "   0.14469809208379503,\n",
       "   0.14926679049061703,\n",
       "   0.1406206292054327,\n",
       "   0.13254016320039752,\n",
       "   0.14334671566186466,\n",
       "   0.14740296365370906,\n",
       "   0.1384160277017543,\n",
       "   0.1483239528528262,\n",
       "   0.14262805834478942,\n",
       "   0.13758825143734923,\n",
       "   0.14093705697802708,\n",
       "   0.15311467291430716,\n",
       "   0.13767942455395646,\n",
       "   0.14317972123152795,\n",
       "   0.14186473069350122,\n",
       "   0.14281934639603053,\n",
       "   0.14508654883859382,\n",
       "   0.14924852651214243,\n",
       "   0.13628178402519625,\n",
       "   0.1450536077282867,\n",
       "   0.14566920877496703,\n",
       "   0.1460791119153492,\n",
       "   0.14115278239373535,\n",
       "   0.14124143006880327,\n",
       "   0.13910279571514889,\n",
       "   0.14628361880638419,\n",
       "   0.13843547567149062,\n",
       "   0.1431061662821319,\n",
       "   0.14868695924801673,\n",
       "   0.1368053248558164,\n",
       "   0.13961324171648046,\n",
       "   0.1407849400411612,\n",
       "   0.14028699136508865,\n",
       "   0.13514856529201838,\n",
       "   0.14070727467834693,\n",
       "   0.14548559076322748,\n",
       "   0.1493920610636503,\n",
       "   0.13793124003971893,\n",
       "   0.13499109071537418,\n",
       "   0.14280303882898188,\n",
       "   0.13679118591631348,\n",
       "   0.1382201756395091,\n",
       "   0.1438097958968032,\n",
       "   0.13985046938717818,\n",
       "   0.13862466902723303,\n",
       "   0.13561659895340325,\n",
       "   0.13816739096038885,\n",
       "   0.1373856742823443,\n",
       "   0.1387986000157669,\n",
       "   0.13896044247313902,\n",
       "   0.13107843256877671,\n",
       "   0.13837020918157217,\n",
       "   0.12538917546023925,\n",
       "   0.13041595847337506,\n",
       "   0.13357519673191046,\n",
       "   0.13722043960719235,\n",
       "   0.1269861542883161,\n",
       "   0.12734903624036303,\n",
       "   0.13322913146261842,\n",
       "   0.13229070699398324,\n",
       "   0.1287673559662748,\n",
       "   0.13256146274528394,\n",
       "   0.12944476126125357,\n",
       "   0.13037170482576382,\n",
       "   0.1333300740466993,\n",
       "   0.12841420338246776,\n",
       "   0.13471561867027299,\n",
       "   0.12498406396540533,\n",
       "   0.12616307776667857,\n",
       "   0.12740479192890483,\n",
       "   0.12809513175208617,\n",
       "   0.11972799637010385,\n",
       "   0.12873045190142615,\n",
       "   0.1310838206083332,\n",
       "   0.12195807738016637,\n",
       "   0.12672684380008373,\n",
       "   0.13148271311392523,\n",
       "   0.1439746376799185,\n",
       "   0.12023787763994637,\n",
       "   0.12548784067807506,\n",
       "   0.12551336954440642,\n",
       "   0.1288036239419169,\n",
       "   0.12065096009164462,\n",
       "   0.12633473778048077,\n",
       "   0.12046158284864392,\n",
       "   0.12669468229395195,\n",
       "   0.12727462771791873,\n",
       "   0.12626187615836781,\n",
       "   0.12015245910417044,\n",
       "   0.11925585217873665,\n",
       "   0.1220467905036821,\n",
       "   0.11551354656864976,\n",
       "   0.11572553593184581,\n",
       "   0.12109513001326117,\n",
       "   0.11888777936235903,\n",
       "   0.12093295854192053,\n",
       "   0.12005924509744073,\n",
       "   0.12205379213003742,\n",
       "   0.11484681932464438,\n",
       "   0.11185404436839311,\n",
       "   0.12097871605929462,\n",
       "   0.12669737415856216,\n",
       "   0.11670753941546567,\n",
       "   0.11092069432396368,\n",
       "   0.11915934843173046,\n",
       "   0.11578814776580858,\n",
       "   0.1157612139900117,\n",
       "   0.11883543341787733,\n",
       "   0.10961845526229816,\n",
       "   0.11327843939329625,\n",
       "   0.1107572350754641,\n",
       "   0.11240021339461863,\n",
       "   0.10925887671439734,\n",
       "   0.11660253661417291,\n",
       "   0.11650035988200562,\n",
       "   0.10918788200768595,\n",
       "   0.11830982467834414,\n",
       "   0.10926545976969312,\n",
       "   0.11459756726593943,\n",
       "   0.11550345650692517,\n",
       "   0.1168030593622015,\n",
       "   0.11085591379814173,\n",
       "   0.11386715470990273,\n",
       "   0.11400443079880201,\n",
       "   0.10885931886397207,\n",
       "   0.1100448681688623,\n",
       "   0.2083490647282109,\n",
       "   0.10942848718200174,\n",
       "   0.1045943009550011,\n",
       "   0.11087370395197647],\n",
       "  'train_accuracy_mean': [0.36681333342194555,\n",
       "   0.43917333346605303,\n",
       "   0.4653200007379055,\n",
       "   0.49058666807413104,\n",
       "   0.515306665956974,\n",
       "   0.5451600003242493,\n",
       "   0.5662799993753433,\n",
       "   0.5840266672372818,\n",
       "   0.5979066662192345,\n",
       "   0.600919998884201,\n",
       "   0.6208666654229165,\n",
       "   0.6247866669297218,\n",
       "   0.6290666660666466,\n",
       "   0.6425733333826065,\n",
       "   0.6459466673731804,\n",
       "   0.6514666672348977,\n",
       "   0.6600266669392586,\n",
       "   0.6615600004792214,\n",
       "   0.666733332157135,\n",
       "   0.6732266668081284,\n",
       "   0.673826667368412,\n",
       "   0.6852666662931443,\n",
       "   0.6870399997830391,\n",
       "   0.6867733324170112,\n",
       "   0.6904666677713395,\n",
       "   0.6993333337903023,\n",
       "   0.7035866669416427,\n",
       "   0.7052800005078316,\n",
       "   0.7058000005483628,\n",
       "   0.7071600001454353,\n",
       "   0.7151733322143554,\n",
       "   0.7178133336305619,\n",
       "   0.7212400007247924,\n",
       "   0.7251866670846939,\n",
       "   0.722839998960495,\n",
       "   0.7306800000667572,\n",
       "   0.7333200004100799,\n",
       "   0.7341733336448669,\n",
       "   0.7382666665315628,\n",
       "   0.7439466652870178,\n",
       "   0.7395066663026809,\n",
       "   0.7449866681694984,\n",
       "   0.7489866666793823,\n",
       "   0.747106667637825,\n",
       "   0.7523866659402847,\n",
       "   0.7535333330631256,\n",
       "   0.7551599994897843,\n",
       "   0.758640001296997,\n",
       "   0.7643600000143052,\n",
       "   0.7600400011539459,\n",
       "   0.7594266662597656,\n",
       "   0.7700399994850159,\n",
       "   0.767639998793602,\n",
       "   0.774200001835823,\n",
       "   0.7691333329677582,\n",
       "   0.7706533340215683,\n",
       "   0.774293332695961,\n",
       "   0.7782799997329712,\n",
       "   0.7811200007200241,\n",
       "   0.7798266663551331,\n",
       "   0.7793999991416931,\n",
       "   0.7820666660070419,\n",
       "   0.789506667137146,\n",
       "   0.7918799988031388,\n",
       "   0.7875466665029526,\n",
       "   0.7863066650629044,\n",
       "   0.7912400000095368,\n",
       "   0.7983466670513153,\n",
       "   0.7960000017881393,\n",
       "   0.7942266682386399,\n",
       "   0.8007066658735276,\n",
       "   0.8006400014162064,\n",
       "   0.7981066658496857,\n",
       "   0.7997866667509079,\n",
       "   0.8074666650295258,\n",
       "   0.8070000003576279,\n",
       "   0.8046533328294754,\n",
       "   0.8055466657876968,\n",
       "   0.809319999575615,\n",
       "   0.809386666059494,\n",
       "   0.8155866665840149,\n",
       "   0.8120800000429154,\n",
       "   0.8159066671133042,\n",
       "   0.8143866667747498,\n",
       "   0.820226667881012,\n",
       "   0.8190666666030884,\n",
       "   0.8223866657018661,\n",
       "   0.8221866672039032,\n",
       "   0.8229733328819275,\n",
       "   0.8264533320665359,\n",
       "   0.8281333339214325,\n",
       "   0.824413332939148,\n",
       "   0.8321199996471405,\n",
       "   0.832373334646225,\n",
       "   0.8304933333396911,\n",
       "   0.8272666674852371,\n",
       "   0.8329066668748856,\n",
       "   0.8316666668653488,\n",
       "   0.8389066690206528,\n",
       "   0.834573333621025,\n",
       "   0.8403199994564057,\n",
       "   0.8432799998521805,\n",
       "   0.8419066669940949,\n",
       "   0.840760001540184,\n",
       "   0.8412800019979477,\n",
       "   0.8435200008153916,\n",
       "   0.844093333363533,\n",
       "   0.8474799997806549,\n",
       "   0.8486933351755143,\n",
       "   0.8512666682004929,\n",
       "   0.8518933348655701,\n",
       "   0.850800000667572,\n",
       "   0.8535733340978623,\n",
       "   0.8532933350801468,\n",
       "   0.8531466677188874,\n",
       "   0.8543066675662995,\n",
       "   0.856506666302681,\n",
       "   0.8588933333158493,\n",
       "   0.8575200010538101,\n",
       "   0.8586933355331421,\n",
       "   0.8590000010728837,\n",
       "   0.8658800011873246,\n",
       "   0.865466668009758,\n",
       "   0.8664800020456314,\n",
       "   0.8656800004243851,\n",
       "   0.8676133358478546,\n",
       "   0.867066667675972,\n",
       "   0.867280001282692,\n",
       "   0.869106667637825,\n",
       "   0.8729866683483124,\n",
       "   0.8698800019025803,\n",
       "   0.8760400018692016,\n",
       "   0.8706266678571701,\n",
       "   0.8818666677474976,\n",
       "   0.8765600016117095,\n",
       "   0.8785600008964538,\n",
       "   0.878573335647583,\n",
       "   0.8785733349323273,\n",
       "   0.8817600002288818,\n",
       "   0.8779600019454956,\n",
       "   0.8805333354473114,\n",
       "   0.8839600015878677,\n",
       "   0.8837600017786026,\n",
       "   0.8834533354043961,\n",
       "   0.8479066681861878,\n",
       "   0.8851466680765152,\n",
       "   0.8860933341979981,\n",
       "   0.8897200005054474],\n",
       "  'train_accuracy_std': [0.07580721657157585,\n",
       "   0.06426909586188917,\n",
       "   0.07201148056149743,\n",
       "   0.06530960836994781,\n",
       "   0.07004137809795971,\n",
       "   0.06738510242585864,\n",
       "   0.07148539385388343,\n",
       "   0.06713541360230489,\n",
       "   0.06933570692366317,\n",
       "   0.06691253086913436,\n",
       "   0.0679742126088977,\n",
       "   0.07106881737887598,\n",
       "   0.06494250422962157,\n",
       "   0.0698102206833616,\n",
       "   0.06883033471085283,\n",
       "   0.06958339689877191,\n",
       "   0.06374811151219663,\n",
       "   0.06512730944959627,\n",
       "   0.06652180808949792,\n",
       "   0.07150065935397372,\n",
       "   0.06746473107821839,\n",
       "   0.06291224986212422,\n",
       "   0.06661760492339092,\n",
       "   0.06917409414692735,\n",
       "   0.06364243905515267,\n",
       "   0.06649644944843788,\n",
       "   0.0636366625518655,\n",
       "   0.0638542566563435,\n",
       "   0.06295575166329367,\n",
       "   0.0692013265475842,\n",
       "   0.06323284192293421,\n",
       "   0.06515909319714502,\n",
       "   0.06514783337148188,\n",
       "   0.06319800119787553,\n",
       "   0.06436718341424177,\n",
       "   0.0668755374501533,\n",
       "   0.06335858923090452,\n",
       "   0.06496977951201559,\n",
       "   0.06472519686835246,\n",
       "   0.06476265505287895,\n",
       "   0.062146967092915444,\n",
       "   0.0625376325244728,\n",
       "   0.06197450943958627,\n",
       "   0.06491760280173695,\n",
       "   0.06185927971806972,\n",
       "   0.06453977273556603,\n",
       "   0.06534113057886695,\n",
       "   0.062123312959140205,\n",
       "   0.062270479791495546,\n",
       "   0.06085683212479574,\n",
       "   0.061919878059578104,\n",
       "   0.05835598219616801,\n",
       "   0.06131328880238961,\n",
       "   0.0624581993625935,\n",
       "   0.06437704911483007,\n",
       "   0.06060744120757558,\n",
       "   0.05853745906870779,\n",
       "   0.06103821614498997,\n",
       "   0.05897938493627146,\n",
       "   0.060309323050880395,\n",
       "   0.06261057869337658,\n",
       "   0.061641941911005235,\n",
       "   0.059732932143653245,\n",
       "   0.05840642145465611,\n",
       "   0.0583150719931037,\n",
       "   0.06023475599790184,\n",
       "   0.06113569827968916,\n",
       "   0.06095335109884396,\n",
       "   0.05724333988579885,\n",
       "   0.06048545983795344,\n",
       "   0.054490270439679,\n",
       "   0.056305036619982436,\n",
       "   0.05780247861048189,\n",
       "   0.05999962086965057,\n",
       "   0.05472582154489966,\n",
       "   0.055931505234367754,\n",
       "   0.0591205156355211,\n",
       "   0.0575977542211388,\n",
       "   0.05559200589350398,\n",
       "   0.05560077481193892,\n",
       "   0.05523435203138135,\n",
       "   0.055952620767268015,\n",
       "   0.05558137450670555,\n",
       "   0.0570274930623804,\n",
       "   0.05695411281791558,\n",
       "   0.054399511687927664,\n",
       "   0.053400723893070864,\n",
       "   0.053183296518918156,\n",
       "   0.05458656198080758,\n",
       "   0.05118115011222718,\n",
       "   0.05504567698194322,\n",
       "   0.05577405101894646,\n",
       "   0.05132592203771232,\n",
       "   0.05405645868416138,\n",
       "   0.056055736828204056,\n",
       "   0.05968282978003172,\n",
       "   0.05075251804984887,\n",
       "   0.05264535453250857,\n",
       "   0.052789353663520514,\n",
       "   0.054176421944503166,\n",
       "   0.05065688512022816,\n",
       "   0.053309135601771036,\n",
       "   0.051145199592658785,\n",
       "   0.053483541338332795,\n",
       "   0.05219243816194358,\n",
       "   0.052979333367021084,\n",
       "   0.05131211899576178,\n",
       "   0.04948944673769883,\n",
       "   0.050696301409429045,\n",
       "   0.04949967442350085,\n",
       "   0.04815223523622713,\n",
       "   0.04901682365022524,\n",
       "   0.049193815265696925,\n",
       "   0.05003019909588836,\n",
       "   0.05159294639876711,\n",
       "   0.05078612587296802,\n",
       "   0.048715694271024006,\n",
       "   0.047854614420833344,\n",
       "   0.051251717636619135,\n",
       "   0.052166222170284986,\n",
       "   0.0487822841185791,\n",
       "   0.047922196349851884,\n",
       "   0.048155118594382984,\n",
       "   0.04832262982840405,\n",
       "   0.04539118861096655,\n",
       "   0.04870858293342483,\n",
       "   0.046226688196907145,\n",
       "   0.046580653318868084,\n",
       "   0.044528166260566056,\n",
       "   0.0458525640690356,\n",
       "   0.04572462441835191,\n",
       "   0.04735735023124891,\n",
       "   0.04796534220224343,\n",
       "   0.044969917059614374,\n",
       "   0.049149995893201674,\n",
       "   0.044594641635716165,\n",
       "   0.04637825366419303,\n",
       "   0.048605534599340076,\n",
       "   0.04686922895093636,\n",
       "   0.04690622605782226,\n",
       "   0.045995942943487224,\n",
       "   0.04704662703172908,\n",
       "   0.045118069732523164,\n",
       "   0.04561368132841507,\n",
       "   0.08160484927466846,\n",
       "   0.04470546497262253,\n",
       "   0.04397100269472288,\n",
       "   0.0450817709844791],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.4677701131502787,\n",
       "   1.4297467597325644,\n",
       "   1.391860957145691,\n",
       "   1.349475741783778,\n",
       "   1.302433622678121,\n",
       "   1.2668046967188518,\n",
       "   1.2162744404872259,\n",
       "   1.1889880720774333,\n",
       "   1.174274509747823,\n",
       "   1.1547126577297846,\n",
       "   1.1348595823844274,\n",
       "   1.123218083580335,\n",
       "   1.1131797536214192,\n",
       "   1.0928680376211801,\n",
       "   1.09029012521108,\n",
       "   1.0701727890968322,\n",
       "   1.0696283133824667,\n",
       "   1.0584121922651926,\n",
       "   1.0520950174331665,\n",
       "   1.0371566692988077,\n",
       "   1.0403027627865473,\n",
       "   1.017073194583257,\n",
       "   1.009309208393097,\n",
       "   1.036103335817655,\n",
       "   1.000248285929362,\n",
       "   0.9966659865776698,\n",
       "   0.9904166722297668,\n",
       "   0.982530819773674,\n",
       "   0.9697547435760498,\n",
       "   0.9600497390826543,\n",
       "   0.9507189162572225,\n",
       "   0.9613455033302307,\n",
       "   0.9552050121625264,\n",
       "   0.9579398328065872,\n",
       "   0.9406995568672816,\n",
       "   0.9476857086022695,\n",
       "   0.9405221817890803,\n",
       "   0.9422021188338597,\n",
       "   0.9350758578379949,\n",
       "   0.9307720384995143,\n",
       "   0.930591560403506,\n",
       "   0.9136267056067785,\n",
       "   0.9234247221549352,\n",
       "   0.9161131670077641,\n",
       "   0.9007128117481867,\n",
       "   0.9036211013793946,\n",
       "   0.9163809088865916,\n",
       "   0.8948895919322968,\n",
       "   0.9053746048609416,\n",
       "   0.8973648291826248,\n",
       "   0.8976358685890834,\n",
       "   0.88433853328228,\n",
       "   0.8947590206066768,\n",
       "   0.8932069637378057,\n",
       "   0.8994354073206584,\n",
       "   0.8850870277484258,\n",
       "   0.8836886823177338,\n",
       "   0.8693646852175395,\n",
       "   0.8708331779638926,\n",
       "   0.8708137766520182,\n",
       "   0.8854561634858449,\n",
       "   0.869923834403356,\n",
       "   0.8974177008867263,\n",
       "   0.88004780570666,\n",
       "   0.8604749097426733,\n",
       "   0.8569226988156636,\n",
       "   0.8727700171868006,\n",
       "   0.8758762276172638,\n",
       "   0.864939816792806,\n",
       "   0.8837002261479696,\n",
       "   0.852893951535225,\n",
       "   0.847739280462265,\n",
       "   0.8623308722178141,\n",
       "   0.8405650188525517,\n",
       "   0.8639176052808761,\n",
       "   0.8715188884735108,\n",
       "   0.8644208014011383,\n",
       "   0.8572435837984085,\n",
       "   0.8587938698132833,\n",
       "   0.8826759948333105,\n",
       "   0.8532906915744146,\n",
       "   0.8511113961537679,\n",
       "   0.8735847888390224,\n",
       "   0.8519174824158351,\n",
       "   0.8640016720692316,\n",
       "   0.8573652696609497,\n",
       "   0.8411618307232857,\n",
       "   0.8536817137400309,\n",
       "   0.8616619175672531,\n",
       "   0.8465925033887227,\n",
       "   0.8702378598848979,\n",
       "   0.8504563625653585,\n",
       "   0.847489848335584,\n",
       "   0.8519916741053263,\n",
       "   0.8497564820448558,\n",
       "   0.8657340755065283,\n",
       "   0.863470017115275,\n",
       "   0.8565456718206406,\n",
       "   0.8448140060901642,\n",
       "   0.8564735094706217,\n",
       "   0.8714937845865885,\n",
       "   0.8567773999770483,\n",
       "   0.9067918459574381,\n",
       "   0.8666608170668284,\n",
       "   0.8449226313829422,\n",
       "   0.8522586065530777,\n",
       "   0.865306496421496,\n",
       "   0.8636132943630218,\n",
       "   0.8634896256526311,\n",
       "   0.8696633865435918,\n",
       "   0.8737040450175603,\n",
       "   0.8645745303233464,\n",
       "   0.8561256700754165,\n",
       "   0.8594726622104645,\n",
       "   0.8630378345648447,\n",
       "   0.8691549531618754,\n",
       "   0.884721983273824,\n",
       "   0.8491676274935405,\n",
       "   0.853225687344869,\n",
       "   0.8770070787270864,\n",
       "   0.8607275972763697,\n",
       "   0.8472686911622683,\n",
       "   0.8674799640973408,\n",
       "   0.8528406616051992,\n",
       "   0.8668581467866897,\n",
       "   0.8674661813179652,\n",
       "   0.8789606640736262,\n",
       "   0.8743310344219207,\n",
       "   0.8940540945529938,\n",
       "   0.8545677345991135,\n",
       "   0.8862439815203349,\n",
       "   0.8633508531252543,\n",
       "   0.8576361199220022,\n",
       "   0.8894108362992604,\n",
       "   0.8535459001859029,\n",
       "   0.8879317822058995,\n",
       "   0.8504452804724375,\n",
       "   0.9194115958611171,\n",
       "   0.8955332352717718,\n",
       "   0.8848587056001027,\n",
       "   0.8821671342849732,\n",
       "   0.8889677266279856,\n",
       "   0.910123392144839,\n",
       "   0.8942683766285578,\n",
       "   0.900521009961764,\n",
       "   0.8943216935793559,\n",
       "   0.8763294514020284,\n",
       "   0.8981925435860951],\n",
       "  'val_loss_std': [0.08702158029127123,\n",
       "   0.09261053555663593,\n",
       "   0.10547903625850938,\n",
       "   0.1110603728053613,\n",
       "   0.1137522421335439,\n",
       "   0.11712714598028302,\n",
       "   0.12234011889655697,\n",
       "   0.12144739751234479,\n",
       "   0.1294313772193103,\n",
       "   0.12863507559837728,\n",
       "   0.12979496076721037,\n",
       "   0.13338648331686903,\n",
       "   0.13682077765549064,\n",
       "   0.1336752098896649,\n",
       "   0.1316836034866733,\n",
       "   0.13306175792848687,\n",
       "   0.13741560442646494,\n",
       "   0.13682651359295997,\n",
       "   0.1332379550668774,\n",
       "   0.13807123473702454,\n",
       "   0.1338286561221299,\n",
       "   0.13837348344011088,\n",
       "   0.13592386370227377,\n",
       "   0.1357539468295609,\n",
       "   0.13785397373065514,\n",
       "   0.13927932491965703,\n",
       "   0.1414613181840104,\n",
       "   0.14117973179809012,\n",
       "   0.14418630326413195,\n",
       "   0.13962154485597805,\n",
       "   0.14368901163763168,\n",
       "   0.13949118245465816,\n",
       "   0.13606068467298224,\n",
       "   0.13867295240620386,\n",
       "   0.14032078376380303,\n",
       "   0.1422904761217325,\n",
       "   0.1422287831948555,\n",
       "   0.14212126014491572,\n",
       "   0.14178614129305916,\n",
       "   0.14239956587682,\n",
       "   0.1409806581310671,\n",
       "   0.13838489916314803,\n",
       "   0.1426758194521505,\n",
       "   0.13798383915283566,\n",
       "   0.1400858827170174,\n",
       "   0.1458625160602982,\n",
       "   0.14345972517152483,\n",
       "   0.14027288198205817,\n",
       "   0.14337642372257994,\n",
       "   0.13878653133041996,\n",
       "   0.13771689724676653,\n",
       "   0.1419974929415396,\n",
       "   0.14487652345514035,\n",
       "   0.1434099870363119,\n",
       "   0.14150383864733224,\n",
       "   0.1395538142792724,\n",
       "   0.13324714664124698,\n",
       "   0.13923643705123975,\n",
       "   0.13535238559727747,\n",
       "   0.14494384491008394,\n",
       "   0.14567163828118848,\n",
       "   0.13736501163557577,\n",
       "   0.14884295435982553,\n",
       "   0.13965960119190224,\n",
       "   0.14402537245818872,\n",
       "   0.1442734511530091,\n",
       "   0.14462906735134845,\n",
       "   0.14747583302225015,\n",
       "   0.14267140927291388,\n",
       "   0.1474329146291908,\n",
       "   0.14416684022284196,\n",
       "   0.1392315008911683,\n",
       "   0.14163669507577092,\n",
       "   0.13791581050334487,\n",
       "   0.1443724380166178,\n",
       "   0.14298339787692288,\n",
       "   0.14520890246806759,\n",
       "   0.14094933880058752,\n",
       "   0.14202905521266845,\n",
       "   0.15000230729576208,\n",
       "   0.1387675925506738,\n",
       "   0.15061929974847957,\n",
       "   0.14390947924725156,\n",
       "   0.14715590348931176,\n",
       "   0.14011537061355947,\n",
       "   0.1486056138745116,\n",
       "   0.145992153306862,\n",
       "   0.1417062597186321,\n",
       "   0.14340784923905545,\n",
       "   0.14347369985570035,\n",
       "   0.15527193701062567,\n",
       "   0.1404317886892445,\n",
       "   0.14360184166584486,\n",
       "   0.1465676572274153,\n",
       "   0.1450280855699862,\n",
       "   0.15044926506981035,\n",
       "   0.14746083732617196,\n",
       "   0.15181519920742767,\n",
       "   0.14841536078192968,\n",
       "   0.15208912084117587,\n",
       "   0.15403228112716624,\n",
       "   0.15252499119130675,\n",
       "   0.15303806790703905,\n",
       "   0.15047329799633974,\n",
       "   0.14376782011270772,\n",
       "   0.1535490338078205,\n",
       "   0.1565839209879322,\n",
       "   0.15432112554277,\n",
       "   0.14793733908290296,\n",
       "   0.14857517429106154,\n",
       "   0.15676007287406374,\n",
       "   0.15595429400195693,\n",
       "   0.14697672450372257,\n",
       "   0.1526861780802271,\n",
       "   0.15168239690890417,\n",
       "   0.15367461143181999,\n",
       "   0.15559208255074425,\n",
       "   0.15231610476807678,\n",
       "   0.15182001066759232,\n",
       "   0.16018952045794815,\n",
       "   0.14680857464804964,\n",
       "   0.15011434195453882,\n",
       "   0.14571005671683224,\n",
       "   0.14527101216687102,\n",
       "   0.15904254220735103,\n",
       "   0.1465381295090142,\n",
       "   0.16492130209744985,\n",
       "   0.15306758079994429,\n",
       "   0.16067293154177953,\n",
       "   0.1498813392393514,\n",
       "   0.15900612712414386,\n",
       "   0.15397593072129984,\n",
       "   0.14973478090019351,\n",
       "   0.15640354971638204,\n",
       "   0.14322264534605064,\n",
       "   0.16060574353708684,\n",
       "   0.1509843362677458,\n",
       "   0.1511558966474686,\n",
       "   0.165843795917432,\n",
       "   0.15281948728883,\n",
       "   0.15642755137386366,\n",
       "   0.14330608442913223,\n",
       "   0.16635018213137798,\n",
       "   0.16743941764976938,\n",
       "   0.16782386765208654,\n",
       "   0.16207639471125146,\n",
       "   0.15430903131638019,\n",
       "   0.16042776044236684],\n",
       "  'val_accuracy_mean': [0.3804222233593464,\n",
       "   0.4032666672269503,\n",
       "   0.4256666669249535,\n",
       "   0.44733333379030227,\n",
       "   0.4739777772625287,\n",
       "   0.4860666670401891,\n",
       "   0.5103777781128883,\n",
       "   0.5251777788003286,\n",
       "   0.5323999985059102,\n",
       "   0.5410888888438543,\n",
       "   0.5504000008106231,\n",
       "   0.5557777762413025,\n",
       "   0.5599111088116964,\n",
       "   0.5700666642189026,\n",
       "   0.5715555553634961,\n",
       "   0.5813777774572373,\n",
       "   0.579377775490284,\n",
       "   0.5828000008066495,\n",
       "   0.5873111099998156,\n",
       "   0.5933555552363395,\n",
       "   0.5962666668494543,\n",
       "   0.6052888877193133,\n",
       "   0.609488888780276,\n",
       "   0.5940888891617457,\n",
       "   0.6129111110170682,\n",
       "   0.6129555544257164,\n",
       "   0.6158888872464497,\n",
       "   0.6201555549105009,\n",
       "   0.6244888888796171,\n",
       "   0.6290222218632698,\n",
       "   0.634088886876901,\n",
       "   0.6310222229361534,\n",
       "   0.6308666660388311,\n",
       "   0.6311999994516373,\n",
       "   0.6377777764201165,\n",
       "   0.6345999988913537,\n",
       "   0.6384222210446994,\n",
       "   0.6375333336989085,\n",
       "   0.6402444450060526,\n",
       "   0.6424666658043862,\n",
       "   0.642622220714887,\n",
       "   0.6493333328763644,\n",
       "   0.6448888885974884,\n",
       "   0.648155553539594,\n",
       "   0.6550444440046946,\n",
       "   0.6514444422721862,\n",
       "   0.6498222224911054,\n",
       "   0.656888888378938,\n",
       "   0.6514666666587193,\n",
       "   0.6543333327770233,\n",
       "   0.6540666650732359,\n",
       "   0.6619111114740371,\n",
       "   0.6592666655778885,\n",
       "   0.6584888890385627,\n",
       "   0.6554888892173767,\n",
       "   0.6603555560112,\n",
       "   0.6631999983390172,\n",
       "   0.6682888895273209,\n",
       "   0.6662000014384588,\n",
       "   0.6684222213427226,\n",
       "   0.6626000015934308,\n",
       "   0.6688222226500511,\n",
       "   0.6578666669130325,\n",
       "   0.6624888901909193,\n",
       "   0.6718666645884513,\n",
       "   0.6739111117521922,\n",
       "   0.6665333324670791,\n",
       "   0.6681777768333753,\n",
       "   0.671866666773955,\n",
       "   0.6656666666269302,\n",
       "   0.6747333325942357,\n",
       "   0.6763555552562078,\n",
       "   0.6728666659196217,\n",
       "   0.679355555176735,\n",
       "   0.6701111114025116,\n",
       "   0.6701999999086062,\n",
       "   0.6702666672070822,\n",
       "   0.6716444446643194,\n",
       "   0.6735555537541708,\n",
       "   0.6679333331187566,\n",
       "   0.6765333340565364,\n",
       "   0.6785999990502993,\n",
       "   0.669466667274634,\n",
       "   0.6766222204764684,\n",
       "   0.6719555564721426,\n",
       "   0.6750666680932045,\n",
       "   0.680422222216924,\n",
       "   0.6748888887961706,\n",
       "   0.6730444439252218,\n",
       "   0.6775999983151754,\n",
       "   0.6741777787605922,\n",
       "   0.6761111104488373,\n",
       "   0.6775777769088746,\n",
       "   0.6779999977350235,\n",
       "   0.6786666675408681,\n",
       "   0.6730888879299164,\n",
       "   0.674777778784434,\n",
       "   0.677066666285197,\n",
       "   0.6830666663249334,\n",
       "   0.6784444439411164,\n",
       "   0.6717555562655131,\n",
       "   0.6783333337306976,\n",
       "   0.6583333339293798,\n",
       "   0.6720444430907567,\n",
       "   0.6771111104885738,\n",
       "   0.6796888888875643,\n",
       "   0.6773111124833425,\n",
       "   0.6782888885339101,\n",
       "   0.6742222225666046,\n",
       "   0.6737111113468806,\n",
       "   0.6730000000198683,\n",
       "   0.6766444462537765,\n",
       "   0.6773777774969737,\n",
       "   0.6743333325783412,\n",
       "   0.6754888878266017,\n",
       "   0.6756888894240062,\n",
       "   0.6722444446881612,\n",
       "   0.6787333331505457,\n",
       "   0.6784666671355566,\n",
       "   0.672755556901296,\n",
       "   0.6720222210884095,\n",
       "   0.6789999995628992,\n",
       "   0.672911110719045,\n",
       "   0.6786000011364619,\n",
       "   0.6757111116250356,\n",
       "   0.6696000003814697,\n",
       "   0.674733334183693,\n",
       "   0.6737333328525226,\n",
       "   0.6702000008026758,\n",
       "   0.6762444452444712,\n",
       "   0.6708888887365659,\n",
       "   0.6776666661103566,\n",
       "   0.6797555563847224,\n",
       "   0.6686888895432155,\n",
       "   0.6781555531422298,\n",
       "   0.6683333354194959,\n",
       "   0.6803777780135473,\n",
       "   0.6502666667103767,\n",
       "   0.670866666038831,\n",
       "   0.6685555557409922,\n",
       "   0.6695777793725332,\n",
       "   0.6634888883431752,\n",
       "   0.6606888894240062,\n",
       "   0.6731555539369584,\n",
       "   0.6714888876676559,\n",
       "   0.667400000890096,\n",
       "   0.6676444435119628,\n",
       "   0.6646222194035848],\n",
       "  'val_accuracy_std': [0.052232807853414234,\n",
       "   0.05273971060511536,\n",
       "   0.056939538720173294,\n",
       "   0.058516221130003473,\n",
       "   0.05694207021118858,\n",
       "   0.06289387851931089,\n",
       "   0.06156963752777345,\n",
       "   0.06346540257420963,\n",
       "   0.06444304866549702,\n",
       "   0.06246183612767767,\n",
       "   0.06195807382501116,\n",
       "   0.06557061934465207,\n",
       "   0.06624080294108926,\n",
       "   0.06340109867950791,\n",
       "   0.06384085037221256,\n",
       "   0.06272950362470729,\n",
       "   0.06482499236844752,\n",
       "   0.06558351361629187,\n",
       "   0.06355189783934491,\n",
       "   0.06371347494091377,\n",
       "   0.0629237268704988,\n",
       "   0.0637090448933459,\n",
       "   0.06061076338087074,\n",
       "   0.062108801478983795,\n",
       "   0.06383752354985012,\n",
       "   0.0649102374552076,\n",
       "   0.06322085680969776,\n",
       "   0.064812936942755,\n",
       "   0.06356797313331992,\n",
       "   0.06175045165818181,\n",
       "   0.06269520253188196,\n",
       "   0.06198679177537979,\n",
       "   0.0644497294182347,\n",
       "   0.06115345689236958,\n",
       "   0.06308450096671313,\n",
       "   0.0680045428886169,\n",
       "   0.06391684029113544,\n",
       "   0.06407163645605247,\n",
       "   0.06387325116743453,\n",
       "   0.06379821140978814,\n",
       "   0.06367694862603193,\n",
       "   0.06373091741435737,\n",
       "   0.06396372564956408,\n",
       "   0.06483030537652815,\n",
       "   0.06085713457867812,\n",
       "   0.06437726533350123,\n",
       "   0.06247107661077161,\n",
       "   0.06393129137435374,\n",
       "   0.06306314455944699,\n",
       "   0.061867542424670956,\n",
       "   0.06329164746249183,\n",
       "   0.0618856139650494,\n",
       "   0.0620291110471066,\n",
       "   0.06196604536975289,\n",
       "   0.06411521839453284,\n",
       "   0.06002363348779493,\n",
       "   0.06217166438324185,\n",
       "   0.06222924959191527,\n",
       "   0.06049613581220165,\n",
       "   0.06267635536520509,\n",
       "   0.06297720857426056,\n",
       "   0.06370724661723015,\n",
       "   0.062322079865477234,\n",
       "   0.06208499310733638,\n",
       "   0.06109067962086788,\n",
       "   0.06280333507530612,\n",
       "   0.062479486631756787,\n",
       "   0.06157029990819353,\n",
       "   0.060547479439099244,\n",
       "   0.06077310547635139,\n",
       "   0.06298473680000301,\n",
       "   0.061016509209592805,\n",
       "   0.06035147869043885,\n",
       "   0.06056058669270306,\n",
       "   0.062095407957355846,\n",
       "   0.06199012033124209,\n",
       "   0.062016150590225055,\n",
       "   0.060399591691657685,\n",
       "   0.060745573249513546,\n",
       "   0.06232551307379143,\n",
       "   0.06225976678275706,\n",
       "   0.06359102150566211,\n",
       "   0.06019853652788939,\n",
       "   0.06330524138868697,\n",
       "   0.05995453504177254,\n",
       "   0.06595814724143838,\n",
       "   0.06096240164719374,\n",
       "   0.06216783323458769,\n",
       "   0.062223537471640576,\n",
       "   0.0636321830795996,\n",
       "   0.06163114317555816,\n",
       "   0.06222172820348577,\n",
       "   0.061988526521038964,\n",
       "   0.061971319911275385,\n",
       "   0.06111070573739991,\n",
       "   0.06291039689289084,\n",
       "   0.060681215818523336,\n",
       "   0.06041329379443427,\n",
       "   0.05828730901955466,\n",
       "   0.06072849825716914,\n",
       "   0.06043569987300581,\n",
       "   0.06257706478732804,\n",
       "   0.0619919362475011,\n",
       "   0.0641003711833958,\n",
       "   0.0599724231565168,\n",
       "   0.06380217835873735,\n",
       "   0.06016791178486079,\n",
       "   0.060008934582175114,\n",
       "   0.0592454640629,\n",
       "   0.05650045721422104,\n",
       "   0.06134329692200392,\n",
       "   0.05972653450681351,\n",
       "   0.059880854189743496,\n",
       "   0.06089973720501223,\n",
       "   0.06144149709785642,\n",
       "   0.05808875393117156,\n",
       "   0.05853393638016509,\n",
       "   0.05958395343673561,\n",
       "   0.05988276343451457,\n",
       "   0.060275596746594065,\n",
       "   0.060405294770623445,\n",
       "   0.06173569828794877,\n",
       "   0.05994293308596448,\n",
       "   0.05828381529709244,\n",
       "   0.06063227022300588,\n",
       "   0.05780929017996129,\n",
       "   0.06249357772232313,\n",
       "   0.05889465798106086,\n",
       "   0.05985257316933383,\n",
       "   0.061564537443879366,\n",
       "   0.06160046413450892,\n",
       "   0.06036463524259925,\n",
       "   0.06041167152915129,\n",
       "   0.05941118168334273,\n",
       "   0.058810475601280075,\n",
       "   0.06189387587495292,\n",
       "   0.057668319633166716,\n",
       "   0.05922026812796283,\n",
       "   0.06252220970062716,\n",
       "   0.060193414650306976,\n",
       "   0.0612146160277214,\n",
       "   0.05859444759397418,\n",
       "   0.062034393995966726,\n",
       "   0.05984645342978796,\n",
       "   0.06029807172766212,\n",
       "   0.06039790552260989,\n",
       "   0.06037113105629234,\n",
       "   0.060088560869246035],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb0c68",
   "metadata": {},
   "source": [
    "### 1.1 MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     #print(key)\n",
    "#     if value.requires_grad:\n",
    "#         print(key)\n",
    "#         print(value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a599c8",
   "metadata": {},
   "source": [
    "### 1.2 Arbiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ebc67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = arbiter_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = arbiter_system.state['best_epoch']\n",
    "\n",
    "state = arbiter_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "arbiter_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "569eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0531d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a86b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "\n",
    "x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "\n",
    "\n",
    "x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task = next(zip(x_support_set,y_support_set,x_target_set, y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cdeb442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "647183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbiter_x_support_set, arbiter_x_target_set, arbiter_y_support_set, arbiter_y_target_set, seed = train_sample\n",
    "\n",
    "arbiter_x_support_set = torch.Tensor(arbiter_x_support_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_x_target_set = torch.Tensor(arbiter_x_target_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_y_support_set = torch.Tensor(arbiter_y_support_set).long().to(device=arbiter_system.model.device)\n",
    "arbiter_y_target_set = torch.Tensor(arbiter_y_target_set).long().to(device=arbiter_system.model.device)\n",
    "\n",
    "\n",
    "arbiter_x_support_set_task, arbiter_y_support_set_task, arbiter_x_target_set_task, arbiter_y_target_set_task = next(zip(arbiter_x_support_set,arbiter_y_support_set,arbiter_x_target_set, arbiter_y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce1c0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fd4d6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = arbiter_system.model.get_inner_loop_parameter_dict(arbiter_system.model.classifier.named_parameters())\n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = arbiter_x_target_set_task.shape\n",
    "\n",
    "arbiter_x_support_set_task = arbiter_x_support_set_task.view(-1, c, h, w)\n",
    "arbiter_y_support_set_task = arbiter_y_support_set_task.view(-1)\n",
    "arbiter_x_target_set_task = arbiter_x_target_set_task.view(-1, c, h, w)\n",
    "arbiter_y_target_set_task = arbiter_y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds = arbiter_system.model.net_forward(\n",
    "            x=arbiter_x_support_set_task,\n",
    "            y=arbiter_y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "            soft_target=None\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "    \n",
    "    if arbiter_system.model.args.arbiter:\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                retain_graph=True)\n",
    "\n",
    "        names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "\n",
    "        for key, weight in names_weights_copy.items():\n",
    "            weight_norm = torch.norm(weight, p=2)\n",
    "            per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "        for key, grad in names_grads_copy.items():\n",
    "            gradient_l2norm = torch.norm(grad, p=2)\n",
    "            per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "        per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                    per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        generated_gradient_rate = arbiter_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "            g += 1\n",
    "\n",
    "    names_weights_copy = arbiter_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_arbiter.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=arbiter_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in arbiter_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d16650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "y_support_set_task = y_support_set_task.view(-1)\n",
    "x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "y_target_set_task = y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds = maml_system.model.net_forward(\n",
    "            x=x_support_set_task,\n",
    "            y=y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "            soft_target=None\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "\n",
    "\n",
    "    names_weights_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_maml.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=maml_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575454f0",
   "metadata": {},
   "source": [
    "## landscape 함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aec9618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = loss_landscape_join.landscape(maml_system.model.classifier, arbiter_system.model.classifier, args_arbiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8c578d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "tensor([-0.0030, -0.0046,  0.0077, -0.0017, -0.0127, -0.0049, -0.0015,  0.0048,\n",
      "        -0.0004,  0.0105, -0.0018,  0.0004,  0.0023,  0.0053,  0.0014,  0.0100,\n",
      "         0.0026,  0.0049,  0.0023, -0.0010, -0.0086, -0.0009, -0.0073,  0.0013,\n",
      "        -0.0013,  0.0064,  0.0021,  0.0027, -0.0043, -0.0033, -0.0129,  0.0154,\n",
      "         0.0006,  0.0108, -0.0048, -0.0087,  0.0045, -0.0112,  0.0044,  0.0101,\n",
      "         0.0015,  0.0004,  0.0092, -0.0045, -0.0066,  0.0052,  0.0002, -0.0054],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-8.3373e-04, -4.6284e-04, -4.7697e-05],\n",
      "          [-1.3657e-03, -7.8921e-04, -3.2446e-04],\n",
      "          [-1.0051e-03, -6.2322e-04, -1.2597e-04]],\n",
      "\n",
      "         [[ 1.1361e-04,  6.4409e-04,  4.7735e-04],\n",
      "          [-1.9052e-05,  8.5108e-04,  2.1031e-04],\n",
      "          [-6.2297e-04, -5.0136e-05,  1.2701e-03]],\n",
      "\n",
      "         [[-1.7019e-05, -1.6049e-05,  4.3474e-05],\n",
      "          [-1.7225e-05, -1.6459e-05,  4.6466e-05],\n",
      "          [-7.6598e-05, -7.2285e-05,  3.4657e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8462e-04, -2.2410e-04, -2.0553e-05],\n",
      "          [-1.8478e-04,  2.3123e-05, -1.5920e-04],\n",
      "          [-5.3610e-04,  2.7647e-04,  3.6196e-04]],\n",
      "\n",
      "         [[-3.0232e-04,  2.7117e-04,  7.6174e-04],\n",
      "          [-1.9860e-04,  3.8521e-04,  9.7665e-04],\n",
      "          [ 3.2074e-04,  4.6119e-04,  2.8960e-04]],\n",
      "\n",
      "         [[-1.1340e-04,  2.0793e-04,  3.2548e-04],\n",
      "          [-1.2541e-04,  1.9295e-04,  3.0071e-04],\n",
      "          [-1.7032e-05,  1.8511e-04,  2.4539e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1365e-03,  9.3769e-04,  1.8419e-03],\n",
      "          [ 1.6302e-03,  1.7867e-03,  2.2938e-03],\n",
      "          [ 1.5725e-03,  1.7246e-03,  1.9695e-03]],\n",
      "\n",
      "         [[ 7.1760e-04,  1.0158e-03, -6.7957e-04],\n",
      "          [ 1.5292e-03, -8.9140e-04, -3.5336e-05],\n",
      "          [ 2.3775e-03,  1.2405e-03,  1.0813e-03]],\n",
      "\n",
      "         [[ 1.1649e-04,  1.4814e-04,  1.5085e-04],\n",
      "          [ 5.9312e-05,  4.0092e-05,  9.3176e-05],\n",
      "          [ 6.2678e-05,  4.2082e-05,  7.1134e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3945e-03,  9.5225e-04,  9.4699e-04],\n",
      "          [ 1.1719e-03,  8.6761e-04,  8.3430e-04],\n",
      "          [ 1.4895e-03,  8.1885e-04,  5.7174e-04]],\n",
      "\n",
      "         [[ 1.2081e-04,  2.5292e-04, -2.8603e-04],\n",
      "          [ 1.1342e-04,  2.5980e-04, -9.9528e-04],\n",
      "          [ 7.8266e-04,  8.6066e-04, -2.4651e-04]],\n",
      "\n",
      "         [[ 1.5155e-04,  1.1394e-04, -1.0725e-04],\n",
      "          [ 1.5762e-04,  2.2130e-04,  4.5945e-05],\n",
      "          [ 2.4645e-04,  3.6742e-04,  3.5318e-04]]],\n",
      "\n",
      "\n",
      "        [[[-9.3270e-04, -5.5896e-04,  1.7068e-04],\n",
      "          [-1.3333e-03, -8.1117e-04, -1.2001e-05],\n",
      "          [-5.5446e-04,  3.2398e-04,  2.5767e-04]],\n",
      "\n",
      "         [[ 2.5445e-04,  8.3636e-04, -2.6683e-04],\n",
      "          [-4.6275e-04,  9.9069e-04,  5.4361e-04],\n",
      "          [ 9.5823e-04,  7.3603e-04, -5.6893e-04]],\n",
      "\n",
      "         [[-7.8450e-05, -7.2795e-05, -7.3720e-05],\n",
      "          [-9.0407e-05, -1.0469e-04, -4.3342e-05],\n",
      "          [-1.7780e-04, -1.3857e-04, -3.4970e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.5515e-04,  4.3649e-04,  1.9922e-04],\n",
      "          [ 5.2117e-05, -1.6810e-04, -6.8236e-05],\n",
      "          [-2.0910e-04, -4.9679e-04, -2.8524e-04]],\n",
      "\n",
      "         [[ 5.5948e-04,  3.9068e-04,  5.1869e-04],\n",
      "          [ 4.1844e-04,  4.9650e-04,  2.8918e-04],\n",
      "          [ 3.7066e-04,  8.0785e-04,  3.6030e-04]],\n",
      "\n",
      "         [[ 8.4875e-05,  9.9568e-05,  4.0010e-06],\n",
      "          [ 1.8043e-04,  1.3622e-04, -5.2335e-07],\n",
      "          [ 3.3648e-04,  1.7885e-04, -1.0304e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.1182e-03,  7.7163e-04, -3.8324e-05],\n",
      "          [ 1.2753e-03, -1.2935e-04,  2.2455e-04],\n",
      "          [ 1.0309e-03, -1.7734e-04, -2.6904e-04]],\n",
      "\n",
      "         [[ 4.0871e-04,  1.1009e-03,  6.6792e-04],\n",
      "          [ 4.5083e-05, -1.6800e-03, -1.3387e-03],\n",
      "          [-9.1382e-04,  2.6582e-04,  7.0863e-05]],\n",
      "\n",
      "         [[-2.1816e-04, -2.7321e-04, -8.9245e-05],\n",
      "          [-2.5479e-04, -2.9619e-04, -2.1105e-04],\n",
      "          [-1.3004e-04, -2.0371e-04, -2.7370e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4393e-03, -2.0938e-03, -1.2186e-03],\n",
      "          [-1.3543e-03, -2.3721e-03, -1.8161e-03],\n",
      "          [-8.3028e-04, -1.8330e-03, -1.9240e-03]],\n",
      "\n",
      "         [[-9.1986e-04, -1.1847e-03, -1.8844e-03],\n",
      "          [-1.0448e-03, -1.1398e-03, -1.8130e-03],\n",
      "          [-1.1997e-03, -1.2998e-03, -1.3527e-03]],\n",
      "\n",
      "         [[-6.3963e-05, -1.0222e-04, -1.4893e-04],\n",
      "          [-7.6792e-05, -4.0472e-04, -3.3782e-04],\n",
      "          [-1.8712e-04, -3.1780e-04, -2.1245e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.5941e-04,  4.0866e-04,  2.5363e-04],\n",
      "          [ 7.3900e-04,  1.3319e-03, -3.4492e-04],\n",
      "          [-1.7251e-04,  4.4831e-04, -2.2170e-04]],\n",
      "\n",
      "         [[-8.9309e-05,  1.1948e-03, -5.8600e-04],\n",
      "          [ 9.2437e-04,  3.0698e-04, -7.1518e-04],\n",
      "          [-1.7552e-03, -6.6572e-04, -1.0567e-03]],\n",
      "\n",
      "         [[ 3.6345e-05,  2.5006e-05,  4.2713e-05],\n",
      "          [ 3.9548e-05,  3.2703e-05,  5.1462e-05],\n",
      "          [ 3.2981e-05,  2.2339e-05,  4.5587e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.1517e-04,  3.7665e-04,  5.7546e-04],\n",
      "          [ 4.8736e-04,  4.2285e-04,  6.3453e-04],\n",
      "          [-2.7614e-04,  5.4268e-04,  5.2202e-04]],\n",
      "\n",
      "         [[-1.2328e-03, -6.7686e-04, -6.8953e-04],\n",
      "          [-8.2307e-04, -2.4274e-04,  2.4565e-05],\n",
      "          [-5.3012e-04, -3.5952e-04, -4.3528e-04]],\n",
      "\n",
      "         [[-2.8854e-04, -1.7040e-05,  4.3147e-05],\n",
      "          [-3.2513e-04, -1.6021e-04, -6.8448e-05],\n",
      "          [-3.0857e-04, -1.3199e-04, -7.4327e-05]]],\n",
      "\n",
      "\n",
      "        [[[-5.3519e-04, -6.1065e-04, -8.1318e-04],\n",
      "          [-3.5792e-04, -1.3550e-03, -6.8175e-04],\n",
      "          [-3.6931e-04, -2.8402e-04, -1.0556e-03]],\n",
      "\n",
      "         [[-4.1221e-04, -7.0133e-04, -2.0464e-03],\n",
      "          [-1.1281e-03,  5.7252e-04, -5.3961e-04],\n",
      "          [ 6.8451e-04, -5.8192e-04, -5.7671e-04]],\n",
      "\n",
      "         [[ 1.9197e-04,  2.2809e-04,  2.3737e-04],\n",
      "          [ 1.8373e-04,  1.9670e-04,  2.0263e-04],\n",
      "          [ 1.3136e-04,  1.2944e-04,  1.1721e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.4429e-04,  6.3558e-04,  3.3272e-04],\n",
      "          [ 5.4516e-04,  3.3256e-04,  5.1428e-04],\n",
      "          [ 9.6907e-04,  5.0789e-04, -2.7081e-05]],\n",
      "\n",
      "         [[ 2.2339e-04,  9.8566e-04,  9.3072e-04],\n",
      "          [ 2.6036e-04,  2.1122e-04,  3.7846e-04],\n",
      "          [ 1.3719e-03,  1.4264e-03,  1.1717e-03]],\n",
      "\n",
      "         [[-1.2933e-04, -7.1435e-05, -1.3059e-05],\n",
      "          [ 5.6239e-06, -2.6677e-05, -1.0785e-04],\n",
      "          [ 1.0948e-04, -1.1576e-06, -3.9574e-05]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-5.8208e-10,  2.9104e-10, -5.8208e-11, -4.0745e-10, -5.8208e-11,\n",
      "         3.2014e-10, -9.3132e-10,  1.1642e-10, -2.6193e-10, -5.8208e-11,\n",
      "         7.5670e-10, -1.7462e-10,  0.0000e+00,  3.4925e-10, -4.6566e-10,\n",
      "         1.7462e-10, -4.2201e-10,  2.9104e-11, -3.2014e-10,  0.0000e+00,\n",
      "        -1.3097e-10, -3.2014e-10,  3.4925e-10, -4.6566e-10,  3.7835e-10,\n",
      "         1.0477e-09,  0.0000e+00, -5.2387e-10, -1.0186e-10, -2.3283e-10,\n",
      "        -8.7311e-11, -1.1642e-10, -5.8208e-11, -1.1642e-10,  5.8208e-11,\n",
      "         5.8208e-11,  5.8208e-11,  5.8208e-10,  2.3283e-10,  3.4925e-10,\n",
      "         1.7462e-10,  4.6566e-10,  4.6566e-10,  0.0000e+00, -1.1642e-10,\n",
      "         4.0745e-10,  0.0000e+00, -1.1642e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-1.5993e-02, -2.5139e-03, -2.7997e-03,  1.6118e-02, -2.1426e-02,\n",
      "         1.2148e-02,  4.2585e-03, -6.2679e-03,  2.5433e-03,  5.3829e-03,\n",
      "        -8.9225e-03,  1.3174e-02,  1.0637e-02,  2.7809e-03,  1.3494e-02,\n",
      "        -8.8094e-03,  1.0334e-02, -2.8855e-03,  1.6492e-03,  2.2674e-03,\n",
      "        -3.3189e-05, -9.2980e-03, -2.3393e-03, -1.1447e-02,  1.9288e-03,\n",
      "        -1.4076e-04,  7.8670e-03,  4.8950e-03,  1.8222e-02, -4.0778e-03,\n",
      "         9.9507e-03,  1.3553e-02,  1.1093e-02,  8.8963e-03, -6.4641e-03,\n",
      "        -1.5982e-02, -7.3918e-03, -1.1900e-02, -2.2201e-03, -1.0176e-02,\n",
      "         1.6447e-02, -4.9808e-03, -4.6047e-03, -1.3185e-03, -1.2466e-02,\n",
      "         3.3173e-03, -7.0484e-04, -3.0956e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 1.7278e-03,  3.2873e-03, -1.8343e-04],\n",
      "          [ 7.1950e-04,  2.8136e-03,  7.3306e-04],\n",
      "          [ 1.8019e-03,  3.9591e-03,  1.7036e-03]],\n",
      "\n",
      "         [[-1.2437e-03,  3.3640e-04, -3.2629e-04],\n",
      "          [-3.6132e-03, -1.2764e-03,  1.3167e-04],\n",
      "          [-1.3646e-03, -2.8007e-04,  4.1657e-04]],\n",
      "\n",
      "         [[-2.3831e-05,  1.5300e-04, -8.8762e-04],\n",
      "          [-3.0321e-03, -1.6261e-06, -1.1704e-03],\n",
      "          [-3.1385e-03, -2.0800e-03, -2.7322e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5610e-03,  1.9196e-03,  3.1522e-04],\n",
      "          [-5.6999e-04,  9.0498e-04,  2.1226e-03],\n",
      "          [-5.6751e-04,  1.6174e-03, -6.3133e-04]],\n",
      "\n",
      "         [[-2.0924e-03,  1.2707e-03, -5.3413e-04],\n",
      "          [ 2.2268e-04,  1.3407e-03, -5.2946e-04],\n",
      "          [-1.0275e-04,  5.8993e-04,  1.2317e-03]],\n",
      "\n",
      "         [[ 2.8638e-04,  1.5579e-03,  1.6527e-03],\n",
      "          [ 2.7124e-04,  8.2651e-04,  1.4508e-03],\n",
      "          [-4.9743e-04,  6.0184e-04,  2.9593e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.8466e-04,  5.7336e-04,  3.8181e-04],\n",
      "          [-3.5999e-04,  1.7556e-03,  1.4326e-03],\n",
      "          [-9.7241e-04,  4.7947e-04,  6.2716e-04]],\n",
      "\n",
      "         [[ 5.5141e-04, -2.9503e-04,  8.1653e-04],\n",
      "          [ 1.7193e-03,  8.9447e-04,  1.1624e-03],\n",
      "          [ 1.9837e-03,  8.1513e-04,  6.7292e-04]],\n",
      "\n",
      "         [[ 6.0983e-04, -1.1697e-03,  1.8871e-03],\n",
      "          [-7.3595e-04,  2.0475e-03,  1.1454e-04],\n",
      "          [ 1.0737e-03, -5.1829e-05, -8.1277e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1376e-03,  1.4946e-03,  4.8270e-04],\n",
      "          [-2.4410e-04,  5.0353e-04,  5.6949e-04],\n",
      "          [ 1.7959e-03,  1.7283e-04, -3.4062e-04]],\n",
      "\n",
      "         [[-8.9346e-04,  5.8404e-04, -9.8572e-05],\n",
      "          [ 8.5846e-04, -1.1703e-04, -4.9658e-04],\n",
      "          [ 5.5739e-04,  1.2886e-04,  1.1934e-04]],\n",
      "\n",
      "         [[-5.0605e-04, -4.6130e-04, -9.0403e-04],\n",
      "          [ 1.9259e-04,  7.6962e-05, -1.2920e-03],\n",
      "          [-5.7450e-04, -3.0014e-04, -8.4627e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1118e-04, -3.4588e-04,  9.1102e-04],\n",
      "          [-7.0666e-04, -1.8886e-04,  9.2031e-04],\n",
      "          [-6.1147e-04, -4.3378e-04,  5.7616e-04]],\n",
      "\n",
      "         [[ 9.0785e-04,  9.3370e-04,  1.4476e-03],\n",
      "          [ 5.3101e-04,  1.5878e-03,  7.5979e-04],\n",
      "          [ 2.8668e-04, -5.2369e-04,  3.2681e-04]],\n",
      "\n",
      "         [[ 4.7624e-04,  8.0534e-05,  1.0549e-03],\n",
      "          [ 2.4974e-04,  9.3003e-05, -1.6977e-05],\n",
      "          [ 1.1218e-04, -7.3894e-05,  4.3001e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.5121e-03,  1.8996e-03,  1.9446e-03],\n",
      "          [ 4.8678e-04,  4.0625e-04,  1.4782e-03],\n",
      "          [ 1.0079e-03,  6.8119e-04,  9.9957e-04]],\n",
      "\n",
      "         [[ 6.8955e-04, -5.6070e-04, -3.0487e-05],\n",
      "          [ 1.8347e-04, -6.7829e-04,  1.2352e-04],\n",
      "          [ 1.2521e-03,  1.9348e-03,  1.9390e-03]],\n",
      "\n",
      "         [[-5.6235e-04, -1.5066e-04, -1.3287e-03],\n",
      "          [-1.2343e-03, -8.8985e-04, -1.1223e-03],\n",
      "          [-3.3245e-04, -5.3993e-04, -2.9725e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.6915e-03, -5.6286e-03, -3.4550e-03],\n",
      "          [-7.0208e-03, -5.0697e-03, -3.3144e-03],\n",
      "          [-4.4429e-03, -3.8421e-03, -1.8076e-03]],\n",
      "\n",
      "         [[ 2.2299e-03,  3.3354e-03,  2.7428e-03],\n",
      "          [ 1.7900e-04, -3.3613e-04,  2.2813e-03],\n",
      "          [ 8.8994e-04, -8.8203e-05,  1.3147e-03]],\n",
      "\n",
      "         [[ 4.9366e-03, -1.7483e-04, -6.9056e-05],\n",
      "          [ 2.9027e-03,  3.5787e-03, -2.9081e-04],\n",
      "          [ 7.8171e-04, -5.7625e-04, -1.4380e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0379e-03,  4.6540e-03,  1.8636e-03],\n",
      "          [-5.8676e-03, -3.1913e-03,  3.2171e-04],\n",
      "          [-9.7657e-04, -2.8066e-03, -5.0715e-04]],\n",
      "\n",
      "         [[-1.5934e-03, -3.6128e-04,  2.1188e-03],\n",
      "          [-1.6639e-03,  7.8501e-04,  1.7420e-03],\n",
      "          [-5.8711e-04,  1.2282e-03,  2.1788e-03]],\n",
      "\n",
      "         [[-1.7022e-03, -2.4917e-03, -1.4098e-03],\n",
      "          [-1.8727e-03, -2.3899e-03, -1.9613e-03],\n",
      "          [ 1.9728e-03, -4.0386e-04, -1.3746e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.5104e-03, -1.9307e-03, -2.6631e-03],\n",
      "          [-2.1410e-03, -1.2120e-03, -1.3240e-03],\n",
      "          [-2.5467e-03, -1.2712e-03, -2.2881e-03]],\n",
      "\n",
      "         [[ 1.0521e-03,  1.2846e-04, -6.1284e-04],\n",
      "          [-1.0109e-03, -2.3689e-03, -1.2112e-05],\n",
      "          [-1.0160e-03, -2.9885e-03, -5.2166e-04]],\n",
      "\n",
      "         [[ 1.1204e-03,  3.1676e-03,  8.7642e-04],\n",
      "          [ 1.1664e-03,  8.9061e-04,  2.9283e-03],\n",
      "          [-3.0419e-06, -1.0927e-03,  1.4487e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.7363e-04, -2.1059e-03, -2.6179e-03],\n",
      "          [-1.0910e-03, -3.4119e-03, -4.8684e-03],\n",
      "          [-2.8892e-03, -3.6898e-03, -4.7121e-03]],\n",
      "\n",
      "         [[-1.1151e-04,  1.9954e-03,  1.1441e-03],\n",
      "          [-5.7055e-04,  1.5724e-04,  1.5430e-03],\n",
      "          [ 3.4733e-04,  7.8315e-04,  1.9054e-03]],\n",
      "\n",
      "         [[-5.2782e-04, -2.7812e-04,  5.2807e-04],\n",
      "          [ 7.6608e-04,  1.0072e-03,  1.4999e-03],\n",
      "          [ 1.8311e-03,  2.4480e-03,  1.8355e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9715e-03,  9.5003e-04,  1.6506e-03],\n",
      "          [ 2.1506e-03,  1.2498e-03,  7.0952e-04],\n",
      "          [ 4.4681e-04,  9.6218e-04,  6.3863e-04]],\n",
      "\n",
      "         [[ 2.1984e-04,  4.5421e-04,  1.0483e-04],\n",
      "          [ 4.9156e-04,  1.1569e-04,  1.1399e-03],\n",
      "          [ 2.9504e-04,  4.1783e-04,  3.4752e-04]],\n",
      "\n",
      "         [[ 1.0671e-04, -1.6186e-04,  3.8774e-04],\n",
      "          [-2.0335e-03, -6.3868e-04,  9.5743e-04],\n",
      "          [-2.3930e-03, -2.6780e-03, -1.4826e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2269e-03, -7.8368e-04, -4.1098e-04],\n",
      "          [-8.0446e-04, -1.5372e-04,  2.9654e-04],\n",
      "          [-1.5750e-03, -2.1768e-03, -4.9709e-04]],\n",
      "\n",
      "         [[ 1.1502e-03,  1.5056e-04,  1.2943e-03],\n",
      "          [ 1.6998e-03,  1.5645e-03,  1.6119e-03],\n",
      "          [ 4.8851e-04,  1.1270e-03,  1.0485e-03]],\n",
      "\n",
      "         [[ 5.6952e-05, -1.1325e-04,  8.6305e-04],\n",
      "          [-5.3198e-05,  1.4960e-04,  4.9689e-04],\n",
      "          [ 1.7540e-04,  1.2822e-04,  5.4839e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-9.3132e-10, -2.3283e-10,  1.7462e-10,  9.3132e-10,  4.6566e-10,\n",
      "         4.9477e-10,  6.9849e-10, -8.1491e-10,  5.2387e-10, -9.3132e-10,\n",
      "        -5.8208e-10,  2.3283e-10,  1.1642e-09, -4.6566e-10, -1.1642e-10,\n",
      "         2.3283e-10,  4.6566e-10, -8.4401e-10,  2.7649e-10, -3.4925e-10,\n",
      "         9.0222e-10,  0.0000e+00,  1.1642e-10, -1.8626e-09,  5.8208e-11,\n",
      "         0.0000e+00, -9.3132e-10,  4.6566e-10, -4.0745e-10,  0.0000e+00,\n",
      "         4.6566e-10, -6.9849e-10,  3.7835e-10,  2.3283e-10,  1.1642e-09,\n",
      "        -3.4925e-10, -5.8208e-10,  9.0222e-10,  0.0000e+00,  2.3283e-10,\n",
      "         2.3283e-10,  3.7835e-10,  1.1642e-09,  0.0000e+00, -2.3283e-10,\n",
      "         4.6566e-10,  9.3132e-10,  1.1642e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 2.7204e-04,  3.4139e-04,  4.7787e-04],\n",
      "          [ 5.9673e-04,  5.4866e-04,  5.7286e-04],\n",
      "          [ 6.8164e-04,  6.6528e-04,  5.4871e-04]],\n",
      "\n",
      "         [[ 6.0977e-04,  7.2901e-04,  8.6259e-04],\n",
      "          [ 8.9813e-04,  8.4253e-04,  8.8125e-04],\n",
      "          [ 9.0706e-04,  8.7119e-04,  7.7817e-04]],\n",
      "\n",
      "         [[ 8.0375e-04,  8.6078e-04,  9.7717e-04],\n",
      "          [ 1.0422e-03,  9.7778e-04,  1.0064e-03],\n",
      "          [ 1.0363e-03,  1.0224e-03,  9.2504e-04]]],\n",
      "\n",
      "\n",
      "        [[[-9.8922e-05, -1.4950e-04, -1.7504e-04],\n",
      "          [-1.7410e-04, -2.0793e-04, -2.2023e-04],\n",
      "          [-1.8241e-04, -1.2318e-04, -9.8390e-05]],\n",
      "\n",
      "         [[-4.5665e-05, -1.3360e-04, -1.2391e-04],\n",
      "          [-1.1469e-04, -1.5037e-04, -1.3426e-04],\n",
      "          [-9.5578e-05, -9.8761e-06,  2.3519e-05]],\n",
      "\n",
      "         [[ 5.8759e-05, -2.9929e-05, -1.1122e-05],\n",
      "          [-2.7251e-06, -2.7935e-05, -7.9863e-06],\n",
      "          [-1.3069e-05,  9.2210e-05,  1.2214e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.5860e-04, -3.3485e-05,  2.3973e-04],\n",
      "          [-3.1940e-04, -1.2974e-04,  5.2598e-05],\n",
      "          [-3.7183e-04, -2.1664e-04, -1.7581e-04]],\n",
      "\n",
      "         [[-2.8060e-04, -2.1712e-04, -2.2401e-05],\n",
      "          [-4.7318e-04, -3.3258e-04, -2.4996e-04],\n",
      "          [-5.6827e-04, -4.7342e-04, -5.1067e-04]],\n",
      "\n",
      "         [[-3.9267e-04, -3.2427e-04, -1.7669e-04],\n",
      "          [-6.5310e-04, -4.6765e-04, -4.1691e-04],\n",
      "          [-7.2735e-04, -6.4032e-04, -6.9745e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.2693e-04,  1.2690e-04,  1.6766e-04],\n",
      "          [ 1.3868e-04,  1.2949e-04,  1.5570e-04],\n",
      "          [ 1.3902e-04,  1.4886e-04,  1.5071e-04]],\n",
      "\n",
      "         [[ 1.0304e-04,  1.0438e-04,  1.5121e-04],\n",
      "          [ 1.2488e-04,  1.2398e-04,  1.5564e-04],\n",
      "          [ 1.3939e-04,  1.5981e-04,  1.5772e-04]],\n",
      "\n",
      "         [[ 8.8706e-05,  9.5648e-05,  1.4076e-04],\n",
      "          [ 1.0770e-04,  1.1168e-04,  1.4392e-04],\n",
      "          [ 1.2526e-04,  1.3985e-04,  1.3603e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.7651e-04,  6.1687e-04,  9.4858e-04],\n",
      "          [ 8.1839e-04,  7.8999e-04,  8.4838e-04],\n",
      "          [ 9.4320e-04,  7.0662e-04,  5.3784e-04]],\n",
      "\n",
      "         [[ 7.6924e-04,  6.5290e-04,  9.3655e-04],\n",
      "          [ 8.4352e-04,  7.6180e-04,  7.8999e-04],\n",
      "          [ 1.0141e-03,  7.3850e-04,  5.3384e-04]],\n",
      "\n",
      "         [[ 7.9717e-04,  6.9935e-04,  9.7212e-04],\n",
      "          [ 8.4978e-04,  7.8914e-04,  8.1035e-04],\n",
      "          [ 1.0560e-03,  7.8450e-04,  5.5920e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.2715e-05, -1.8840e-05,  2.1706e-06],\n",
      "          [ 1.6812e-05,  2.3848e-05,  6.0636e-05],\n",
      "          [ 3.8978e-05,  5.9122e-05,  3.0730e-05]],\n",
      "\n",
      "         [[ 2.1753e-05,  3.4407e-05,  5.7999e-05],\n",
      "          [ 6.9967e-05,  7.3724e-05,  1.1985e-04],\n",
      "          [ 9.8102e-05,  1.1619e-04,  8.8485e-05]],\n",
      "\n",
      "         [[ 8.4813e-05,  9.7991e-05,  1.2513e-04],\n",
      "          [ 1.2415e-04,  1.3614e-04,  1.8566e-04],\n",
      "          [ 1.4984e-04,  1.7127e-04,  1.4490e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 4.0785e-05, -1.3081e-05,  2.1069e-05,  5.6626e-06, -9.1753e-06,\n",
      "        -1.2146e-06, -1.7823e-05, -1.8257e-05,  7.5994e-06, -6.5937e-06,\n",
      "         2.3750e-05,  2.1770e-05,  9.5907e-06, -1.6635e-05, -1.5709e-05,\n",
      "         2.3662e-05,  7.1872e-06,  1.9969e-05, -9.1763e-06,  6.3043e-06,\n",
      "        -9.3891e-06, -3.4240e-06, -3.8428e-05, -5.0638e-06,  4.8320e-06,\n",
      "         2.5840e-05,  2.1544e-05, -5.7340e-06, -1.0003e-05,  5.7738e-05,\n",
      "         1.0117e-05, -1.8772e-05,  1.8879e-05,  1.0248e-05, -1.3977e-05,\n",
      "         6.5872e-06,  8.5878e-06, -1.4068e-05, -1.5711e-05,  2.3472e-05,\n",
      "         3.7800e-05, -5.9730e-07, -2.6530e-05, -1.0172e-05,  1.3156e-05,\n",
      "         8.1521e-06, -1.6824e-05,  2.1665e-05,  1.1378e-05,  2.0931e-05,\n",
      "         2.2455e-05, -2.4243e-05,  1.4984e-05,  1.4548e-05, -7.2746e-06,\n",
      "         9.5236e-05,  5.9730e-06,  4.1769e-05,  1.7618e-05,  1.7525e-05,\n",
      "        -2.9210e-05,  2.8129e-06,  3.1083e-06, -7.4522e-06,  1.1353e-05,\n",
      "        -1.2430e-05,  4.8597e-05, -1.1904e-06,  4.2975e-05,  3.4340e-06,\n",
      "         3.1520e-05, -1.5271e-05,  4.6737e-06, -1.6028e-05, -5.5605e-06,\n",
      "         1.6794e-05, -1.1478e-05,  3.9255e-05,  1.7620e-05,  1.1827e-05,\n",
      "         7.5071e-06, -2.1706e-05,  1.6188e-06, -7.8938e-06,  4.4962e-05,\n",
      "         4.9816e-06, -5.3174e-07, -2.7361e-05,  7.9460e-06, -4.7086e-05,\n",
      "        -1.1583e-07, -5.0959e-06,  1.3425e-05,  2.5275e-05,  1.4858e-06,\n",
      "         3.3109e-05,  1.4351e-06,  6.3064e-06,  9.7050e-06,  2.8743e-05,\n",
      "         1.9130e-05,  1.4353e-05,  1.4483e-05,  1.2133e-05, -7.1920e-07,\n",
      "         7.6720e-08,  9.7513e-06,  3.3163e-05,  1.6544e-05, -5.3172e-06,\n",
      "         1.1614e-05,  9.4933e-06,  3.9053e-05,  2.4344e-05, -3.8080e-05,\n",
      "        -3.3119e-06,  1.7852e-06,  2.8904e-06,  1.3948e-05,  9.7594e-07,\n",
      "        -5.4160e-05,  2.5273e-05,  2.4741e-05,  1.9541e-05,  2.0470e-05,\n",
      "         6.4934e-06,  3.2663e-05, -9.3948e-06], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-8.8756e-06, -4.1232e-05,  6.5962e-05, -6.6940e-06, -1.1756e-05,\n",
      "         2.9963e-05, -7.0789e-06, -1.9402e-05,  4.2647e-05,  2.7821e-06,\n",
      "         1.0598e-06,  8.1048e-06,  2.5647e-05, -4.0020e-05, -1.7869e-05,\n",
      "         3.5300e-05, -2.3668e-05,  2.9191e-06, -3.0883e-05,  4.3654e-06,\n",
      "        -9.4490e-06, -1.9039e-05, -3.8387e-05,  1.1534e-05,  2.4642e-05,\n",
      "         8.5051e-06, -1.4386e-05, -1.6283e-05, -2.4076e-05,  3.9130e-05,\n",
      "         1.6297e-05, -3.4932e-05, -8.6533e-06,  3.1335e-05, -1.8709e-05,\n",
      "        -1.9402e-05, -4.6496e-06, -6.0302e-06, -4.0280e-06,  3.6864e-05,\n",
      "         3.2830e-05, -3.5036e-05, -3.4137e-05,  8.9580e-06, -7.8936e-06,\n",
      "        -5.6912e-06, -1.8483e-05,  2.7702e-05,  5.5305e-06,  2.5018e-05,\n",
      "        -1.3950e-05, -1.2745e-05, -2.7360e-05,  3.3473e-05, -3.4802e-06,\n",
      "         7.8388e-05,  4.6804e-06, -5.2515e-06,  2.4357e-05, -2.5255e-05,\n",
      "        -2.6403e-05,  2.0585e-05,  1.2806e-05, -2.1353e-05, -1.4711e-05,\n",
      "        -2.1282e-05,  2.8060e-05, -1.3805e-06,  2.6393e-05, -8.9064e-07,\n",
      "         8.7587e-06, -2.1763e-06, -1.0780e-05, -3.2443e-05, -3.1642e-05,\n",
      "        -2.4281e-05, -4.4166e-05,  5.6238e-05, -6.0896e-05,  1.9314e-05,\n",
      "        -1.4553e-05, -2.2219e-05,  1.4650e-05, -1.7094e-05, -6.5821e-05,\n",
      "        -7.6354e-06, -2.5023e-05, -2.4838e-05,  8.2387e-06, -1.2102e-05,\n",
      "        -1.0440e-05,  1.5739e-05, -2.5884e-06,  6.5870e-05, -2.8508e-05,\n",
      "         5.6604e-05, -2.4923e-06, -7.3253e-06,  4.5128e-07,  2.2261e-06,\n",
      "         1.4011e-05,  3.2905e-06,  3.7466e-05,  1.9555e-05,  6.4444e-06,\n",
      "         2.6648e-05,  1.1038e-05,  4.3304e-05,  3.7231e-05, -4.7516e-06,\n",
      "        -3.0091e-05,  9.0919e-06,  3.4638e-05, -1.2586e-06, -2.8687e-05,\n",
      "        -1.1542e-05,  1.2881e-05, -2.1851e-05,  1.3635e-05,  2.2663e-05,\n",
      "        -4.6567e-05,  2.8108e-05,  3.3723e-06,  8.9970e-06,  3.7899e-06,\n",
      "        -5.9925e-07,  1.6694e-05, -3.3581e-06], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 8.1855e-12, -7.2760e-12, -7.2760e-12,  4.3201e-12,  3.6380e-12,\n",
      "        -1.2506e-12, -8.6402e-12, -6.8212e-13, -4.5475e-13, -9.0949e-13,\n",
      "         4.5475e-12,  7.2760e-12,  4.5475e-12,  5.6843e-12,  7.7307e-12,\n",
      "         2.7285e-12, -1.3642e-12,  1.3642e-12, -5.0022e-12, -1.3642e-12,\n",
      "        -9.3223e-12,  3.5811e-12,  1.8190e-12,  2.2737e-12,  5.4570e-12,\n",
      "         9.5497e-12, -3.8654e-12,  5.4570e-12, -5.6843e-12,  4.5475e-12,\n",
      "         1.0459e-11, -3.6380e-12, -3.1832e-12,  8.1855e-12,  4.0927e-12,\n",
      "         2.7285e-12,  5.9117e-12, -9.0949e-12, -4.5475e-13, -3.1832e-12,\n",
      "         9.0949e-13,  9.0949e-12, -9.0949e-13,  2.9559e-12, -3.6380e-12,\n",
      "        -5.9117e-12,  2.2737e-12,  1.8190e-12,  1.7280e-11,  8.6402e-12,\n",
      "        -9.0949e-13, -1.1369e-12,  6.3665e-12, -2.7285e-12, -4.5475e-12,\n",
      "        -7.2760e-12,  9.3223e-12, -4.5475e-13, -4.7748e-12, -2.9559e-12,\n",
      "         1.5916e-12,  1.5916e-12,  7.7307e-12, -3.6380e-12,  9.0949e-13,\n",
      "         2.7285e-12, -8.6402e-12, -5.9117e-12, -2.3874e-12,  1.1369e-12,\n",
      "        -9.0949e-13,  1.8190e-12, -5.6843e-13, -1.5916e-12, -1.5916e-12,\n",
      "        -9.0949e-13, -1.7053e-12, -3.1832e-12, -5.9117e-12,  1.8190e-12,\n",
      "         3.6380e-12, -6.3665e-12, -2.7285e-12,  1.1255e-11,  4.0927e-12,\n",
      "         2.7285e-12,  5.9117e-12,  0.0000e+00,  7.9581e-12, -6.8212e-13,\n",
      "         9.0949e-13,  2.7285e-12,  6.1391e-12, -6.8212e-12,  8.6402e-12,\n",
      "         4.4338e-12,  3.6380e-12,  1.8190e-12, -6.5938e-12,  4.5475e-12,\n",
      "        -9.0949e-12, -5.9117e-12, -5.4570e-12,  6.3665e-12, -9.0949e-13,\n",
      "         4.5475e-12, -7.2760e-12, -6.3665e-12,  6.3665e-12, -4.5475e-12,\n",
      "         3.7517e-12, -4.5475e-13, -6.3665e-12, -3.6380e-12,  2.7285e-12,\n",
      "        -3.6380e-12,  1.0004e-11,  0.0000e+00, -1.8190e-12,  4.2064e-12,\n",
      "         4.7748e-12,  9.0949e-13,  5.0022e-12, -1.0914e-11,  3.6380e-12,\n",
      "         0.0000e+00,  4.8885e-12,  1.8190e-12], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 5.7018e-05, -6.4489e-05,  3.3595e-05,  9.2117e-05, -2.7547e-05,\n",
      "        -2.5950e-05,  9.7738e-06, -4.2464e-05,  8.1932e-07,  9.0073e-06,\n",
      "         3.2624e-05, -3.3947e-05,  7.4517e-06,  1.3625e-05, -1.1219e-05,\n",
      "        -4.1911e-05,  1.5334e-05, -3.0156e-05,  4.2396e-05, -2.9489e-05,\n",
      "         1.3692e-05, -5.7088e-06, -6.0818e-05,  4.2726e-06,  8.7147e-05,\n",
      "         6.4234e-05, -2.6585e-05,  1.3310e-05,  1.3021e-05,  4.2544e-05,\n",
      "         2.0599e-05, -1.2174e-05,  8.2269e-06,  1.8481e-05, -1.9778e-05,\n",
      "         3.6803e-05,  6.4295e-05, -5.9491e-05,  3.2252e-05,  9.6859e-06,\n",
      "        -6.5682e-05,  3.4608e-05,  3.8512e-06,  4.9729e-05,  7.9921e-05,\n",
      "        -3.3381e-05,  2.1400e-05,  1.1252e-06,  9.9094e-05,  3.2231e-05,\n",
      "         8.1996e-05, -5.2901e-07,  8.7001e-05, -4.2893e-05,  5.8537e-05,\n",
      "        -4.1350e-05,  4.4437e-05,  4.8022e-06,  3.3003e-05, -1.0162e-04,\n",
      "        -2.1998e-05, -3.1451e-05, -2.5292e-06,  5.0712e-05, -3.5429e-06,\n",
      "        -6.8748e-06, -7.8727e-06,  4.4390e-05, -6.1425e-06,  3.7645e-05,\n",
      "        -7.1584e-05, -3.6433e-05,  4.3380e-06,  5.4694e-05,  2.5666e-05,\n",
      "         6.0256e-06, -2.1067e-05, -4.9603e-06, -6.2737e-05, -5.1994e-06,\n",
      "        -8.2971e-05, -2.8738e-05,  5.1267e-05,  3.6093e-05, -3.1352e-05,\n",
      "        -1.3596e-05,  5.4928e-07, -1.1213e-04,  2.2005e-05, -3.8662e-05,\n",
      "         4.3417e-07,  8.5123e-06,  5.9341e-06, -3.3318e-05, -3.4807e-05,\n",
      "         6.9347e-06,  4.6505e-05, -1.5840e-05, -2.3539e-05, -4.1453e-06,\n",
      "         1.2649e-05, -6.4609e-06,  7.9961e-06, -4.1963e-05,  2.9562e-05,\n",
      "         3.0410e-06, -6.1675e-06, -2.4727e-05,  2.9194e-06,  4.7185e-05,\n",
      "        -1.4700e-06,  1.9634e-05,  2.1620e-06, -1.6892e-05,  1.4878e-05,\n",
      "         2.1131e-06,  4.7845e-06,  7.9018e-05,  2.0649e-05,  1.4027e-05,\n",
      "         1.7894e-05, -2.5738e-05, -3.3165e-05, -1.1943e-05,  1.6563e-05,\n",
      "         2.2945e-05, -2.4811e-06, -1.6699e-05], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.8869e-05, -2.9037e-05,  4.9293e-06,  8.9834e-05,  3.1833e-05,\n",
      "        -8.9088e-06, -3.3617e-05,  3.3780e-06,  1.2537e-05,  8.2462e-06,\n",
      "         6.5183e-05,  1.0427e-06,  3.2495e-06,  2.1753e-05, -1.0466e-05,\n",
      "        -3.4111e-05,  7.1543e-05, -6.6867e-05,  3.7055e-05, -5.6004e-05,\n",
      "         2.7408e-05,  1.6474e-05, -7.6184e-05, -1.6018e-05,  9.9564e-05,\n",
      "         3.5831e-05, -3.7294e-05,  8.0356e-06, -3.3525e-06,  1.9414e-05,\n",
      "        -2.4236e-05,  1.0733e-05,  3.1282e-06,  3.1850e-05,  9.0703e-06,\n",
      "         6.6194e-05,  8.2978e-05, -6.6366e-05, -1.0371e-05,  7.5276e-06,\n",
      "        -5.9954e-05,  4.6930e-05, -2.1181e-05,  7.1816e-05,  8.6084e-05,\n",
      "        -3.9766e-05,  3.2724e-05, -2.7505e-05,  1.0701e-04,  1.7524e-05,\n",
      "         7.6731e-05, -2.1560e-06,  7.5211e-05, -4.5394e-05,  5.3882e-05,\n",
      "        -4.7592e-05,  2.8759e-05, -1.0427e-05,  1.9126e-05, -5.9049e-05,\n",
      "        -1.4973e-05, -4.0121e-05,  1.8472e-05,  4.6528e-05, -6.8023e-05,\n",
      "         2.5517e-05, -1.8604e-05,  3.7804e-05, -4.5981e-05,  5.5317e-05,\n",
      "        -8.7747e-05, -3.4878e-05,  5.0090e-06,  7.7074e-05,  3.2633e-05,\n",
      "        -2.6325e-05, -2.7838e-05, -1.8235e-05, -8.3763e-05, -1.1118e-05,\n",
      "        -7.6945e-05, -3.8718e-05,  1.6174e-05,  1.5790e-05, -6.0471e-06,\n",
      "        -3.0252e-05, -3.9489e-05, -9.9501e-05,  1.8460e-05, -3.3873e-05,\n",
      "         4.6945e-06,  3.1362e-05,  3.4479e-05, -5.3486e-05, -6.3582e-05,\n",
      "         1.1945e-05,  7.9233e-05,  1.1110e-05, -7.1651e-06, -6.5015e-05,\n",
      "        -4.0157e-06, -5.7406e-05, -2.4210e-05, -2.5930e-05,  2.4224e-05,\n",
      "        -2.8672e-05, -1.6486e-05, -4.1672e-06, -1.8265e-05, -3.1994e-06,\n",
      "         6.1386e-06,  1.8042e-05, -4.0806e-06,  3.0556e-05,  2.3549e-06,\n",
      "        -2.7452e-05,  1.8216e-05,  1.0085e-04,  1.1965e-05,  2.8593e-05,\n",
      "         3.1949e-05, -8.5707e-05, -4.5724e-05, -4.2175e-05,  2.1093e-05,\n",
      "         5.1932e-06, -7.6294e-06, -9.4850e-06], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 2.2416e-06,  3.7634e-07, -1.6244e-06],\n",
      "          [-3.6173e-07, -4.1639e-06, -3.4293e-06],\n",
      "          [-5.1106e-07, -1.3892e-07,  3.3460e-06]],\n",
      "\n",
      "         [[ 3.4190e-06, -1.5593e-06, -1.6610e-06],\n",
      "          [ 2.3375e-06,  4.4321e-06,  2.6888e-06],\n",
      "          [ 5.0466e-07, -2.1146e-06, -3.9475e-06]],\n",
      "\n",
      "         [[ 1.8713e-06, -2.1876e-06, -2.0745e-07],\n",
      "          [-9.2705e-08, -3.8740e-07,  1.4857e-06],\n",
      "          [-2.0278e-06, -1.1407e-06, -7.0333e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4685e-06,  2.3718e-06,  9.8557e-07],\n",
      "          [ 3.1001e-06, -5.2100e-07, -5.1064e-06],\n",
      "          [ 2.9235e-06,  3.6320e-06,  4.1799e-06]],\n",
      "\n",
      "         [[ 2.3286e-07,  9.6692e-08,  4.0303e-07],\n",
      "          [-1.2352e-06, -3.3330e-07, -1.3457e-06],\n",
      "          [-8.5520e-07, -3.0947e-07, -1.1681e-06]],\n",
      "\n",
      "         [[ 2.8977e-06, -5.7429e-07,  2.0483e-06],\n",
      "          [ 1.7701e-06, -5.3224e-07,  4.1534e-07],\n",
      "          [-1.0267e-06, -2.0413e-06, -3.4041e-07]]],\n",
      "\n",
      "\n",
      "        [[[-9.2219e-07, -1.7292e-06, -1.0759e-06],\n",
      "          [-5.0364e-07, -4.7415e-08,  1.1795e-06],\n",
      "          [ 1.3617e-07, -1.4499e-06, -1.6204e-07]],\n",
      "\n",
      "         [[-1.8732e-06,  2.7336e-06,  6.7421e-07],\n",
      "          [-6.1404e-07,  3.8690e-06, -1.6907e-06],\n",
      "          [-4.4294e-07,  1.7321e-06, -7.6233e-07]],\n",
      "\n",
      "         [[ 6.4851e-07, -1.3242e-07,  1.4793e-07],\n",
      "          [-1.7230e-06,  7.4692e-07, -2.0637e-07],\n",
      "          [ 2.9177e-06,  2.0292e-06, -1.2291e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4495e-07, -6.1107e-07, -2.6289e-06],\n",
      "          [ 1.6399e-06,  1.3207e-06,  1.5299e-06],\n",
      "          [ 3.7471e-08, -9.1062e-07, -5.8045e-07]],\n",
      "\n",
      "         [[-1.2982e-06, -2.5688e-07, -1.1149e-06],\n",
      "          [ 1.1005e-06,  1.7228e-07,  1.4459e-06],\n",
      "          [ 4.7070e-08,  1.5441e-08, -3.8951e-07]],\n",
      "\n",
      "         [[-8.7403e-07, -6.7103e-07, -1.4182e-06],\n",
      "          [ 9.8502e-07,  2.4052e-06,  2.0470e-06],\n",
      "          [-1.8756e-07,  1.0291e-06, -9.4125e-07]]],\n",
      "\n",
      "\n",
      "        [[[-1.0434e-07,  1.3176e-06,  8.7505e-07],\n",
      "          [-1.8463e-06,  7.4239e-07,  1.4652e-06],\n",
      "          [-1.1921e-08,  3.2030e-07,  7.2646e-08]],\n",
      "\n",
      "         [[-7.1187e-07,  1.3983e-06,  1.1444e-06],\n",
      "          [-1.3910e-06,  2.5236e-06,  2.0610e-06],\n",
      "          [ 1.0802e-06,  2.4840e-08, -2.5719e-06]],\n",
      "\n",
      "         [[ 2.7010e-06, -9.5333e-07, -1.8647e-06],\n",
      "          [ 3.0779e-07,  2.0095e-06, -1.9498e-07],\n",
      "          [ 2.9219e-06,  2.3133e-06, -1.0722e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.9715e-08,  4.7367e-07, -1.6359e-07],\n",
      "          [-5.9422e-07,  1.0019e-07, -1.0587e-06],\n",
      "          [-5.5763e-07, -5.2779e-07,  3.0167e-07]],\n",
      "\n",
      "         [[-2.6150e-07, -7.6760e-07, -6.6305e-08],\n",
      "          [-1.2971e-06,  2.7536e-07, -2.7176e-07],\n",
      "          [ 1.8906e-07,  1.5493e-06, -2.2669e-07]],\n",
      "\n",
      "         [[-1.7351e-07,  1.6812e-06,  1.7453e-06],\n",
      "          [-1.9483e-06,  1.8807e-06,  1.8919e-07],\n",
      "          [-3.1096e-07, -1.0652e-06, -1.4941e-07]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.3618e-07, -1.6069e-06,  1.3192e-06],\n",
      "          [-7.0714e-07, -9.1496e-07,  1.6125e-06],\n",
      "          [ 2.0939e-06, -6.3802e-07, -1.5654e-06]],\n",
      "\n",
      "         [[-1.0019e-06,  6.3956e-06,  2.7550e-06],\n",
      "          [ 1.8548e-07,  3.0627e-07,  8.4557e-07],\n",
      "          [ 5.0482e-07, -2.1937e-06,  2.4779e-06]],\n",
      "\n",
      "         [[ 4.1622e-07,  2.7619e-07,  1.3395e-06],\n",
      "          [ 1.5131e-07, -1.9821e-07,  9.4936e-07],\n",
      "          [ 1.3581e-06, -7.4061e-07,  2.0546e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.7140e-07,  3.3017e-07, -1.1368e-07],\n",
      "          [-1.3132e-06, -4.6736e-07,  4.1369e-06],\n",
      "          [-1.1930e-06, -2.4633e-06, -3.3933e-07]],\n",
      "\n",
      "         [[-1.5305e-07, -6.0288e-07,  4.7406e-07],\n",
      "          [ 6.1446e-07, -3.2479e-07,  4.1471e-07],\n",
      "          [ 5.9003e-07, -7.5464e-07,  2.2265e-07]],\n",
      "\n",
      "         [[ 2.2375e-07,  6.3466e-06,  1.2255e-06],\n",
      "          [ 1.0782e-06,  7.6133e-07,  2.2125e-07],\n",
      "          [ 1.9334e-06,  1.8838e-08,  1.8970e-06]]],\n",
      "\n",
      "\n",
      "        [[[-2.1776e-06, -3.5291e-07,  1.0960e-08],\n",
      "          [-1.5441e-06, -1.3101e-06, -1.5316e-06],\n",
      "          [ 9.4067e-08, -1.1073e-06, -2.8518e-07]],\n",
      "\n",
      "         [[ 2.5790e-06,  2.5193e-06, -1.6680e-06],\n",
      "          [ 4.7305e-06,  3.5985e-06, -3.8212e-06],\n",
      "          [ 2.6327e-06, -2.1313e-06,  1.5786e-06]],\n",
      "\n",
      "         [[-1.3322e-06, -2.0385e-06,  3.5179e-06],\n",
      "          [-1.1522e-06, -9.1401e-07,  1.5264e-06],\n",
      "          [-8.8921e-07, -4.7010e-07, -1.8288e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1337e-06, -2.6288e-06, -3.1506e-06],\n",
      "          [ 3.3911e-07, -2.1225e-06,  1.9700e-06],\n",
      "          [-1.0832e-07, -1.8497e-07, -1.7214e-07]],\n",
      "\n",
      "         [[-2.5995e-07, -2.3911e-07,  6.2981e-07],\n",
      "          [ 4.1052e-07, -2.0850e-06, -1.7653e-07],\n",
      "          [-5.8451e-07, -3.9093e-07,  2.6396e-07]],\n",
      "\n",
      "         [[ 2.1170e-06, -1.0642e-07,  1.7342e-06],\n",
      "          [ 9.8637e-07,  8.0576e-07, -5.1987e-07],\n",
      "          [ 1.2843e-06,  1.2884e-06, -7.8116e-07]]],\n",
      "\n",
      "\n",
      "        [[[-4.0064e-07,  2.8023e-06,  1.2142e-06],\n",
      "          [-1.6197e-06,  1.2588e-06,  2.8356e-06],\n",
      "          [-1.4933e-07,  2.5761e-06,  2.5652e-07]],\n",
      "\n",
      "         [[ 2.4722e-06,  1.3305e-06, -2.0166e-06],\n",
      "          [-6.7166e-06, -1.4760e-06,  4.1285e-08],\n",
      "          [-3.7652e-06,  8.2769e-07, -3.5689e-06]],\n",
      "\n",
      "         [[-5.3200e-06, -7.1221e-07, -7.3406e-07],\n",
      "          [-3.1436e-06, -1.6156e-06, -7.4759e-07],\n",
      "          [ 4.8877e-07, -1.2227e-06, -1.3408e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4151e-06,  3.4721e-06, -5.1320e-07],\n",
      "          [-4.0518e-06, -1.7473e-06, -3.0812e-06],\n",
      "          [ 1.2202e-06,  2.9110e-06,  3.7807e-06]],\n",
      "\n",
      "         [[-1.6475e-06,  3.1825e-07, -7.5309e-08],\n",
      "          [-2.7475e-06,  3.6416e-07,  4.2480e-07],\n",
      "          [-8.3532e-07, -8.5860e-08,  9.0259e-08]],\n",
      "\n",
      "         [[-7.2867e-06, -2.7449e-07, -3.0878e-07],\n",
      "          [-5.1303e-06, -7.2984e-07, -1.4143e-06],\n",
      "          [-2.0437e-06,  1.4288e-06, -1.4654e-06]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[-1.4483e-05, -4.4145e-05, -6.7116e-05,  ..., -9.1007e-05,\n",
      "         -4.9900e-05, -7.5689e-05],\n",
      "        [ 4.4741e-06,  2.0597e-05,  1.0651e-05,  ..., -5.2841e-05,\n",
      "         -2.7888e-05, -3.6252e-06],\n",
      "        [ 1.5405e-05,  3.8502e-05,  1.5123e-05,  ...,  1.3664e-04,\n",
      "          6.2224e-05,  5.3719e-05],\n",
      "        [-1.1538e-05, -2.4633e-05,  1.2432e-05,  ..., -7.4811e-06,\n",
      "         -2.0493e-05, -2.2066e-05],\n",
      "        [ 6.1525e-06,  9.6967e-06,  2.8917e-05,  ...,  1.4701e-05,\n",
      "          3.6068e-05,  4.7666e-05]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([-7.6080e-05, -1.0555e-05,  9.3458e-05, -3.4960e-05,  2.8145e-05],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls.show_2djoin(x_support_set_task, y_support_set_task, title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
