{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16249129",
   "metadata": {},
   "source": [
    "## [참고]\n",
    "### https://cocoa-t.tistory.com/entry/PyHessian-Loss-Landscape-%EC%8B%9C%EA%B0%81%ED%99%94-PyHessian-Neural-Networks-Through-the-Lens-of-the-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af476",
   "metadata": {},
   "source": [
    "# 0. Dataset 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7235fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset=\"mini_imagenet_full_size\"\n",
    "# dataset=\"tiered_imagenet\"\n",
    "dataset=\"CIFAR_FS\"\n",
    "# dataset=\"CUB\"\n",
    "\n",
    "# title = 'miniImageNet'\n",
    "# title = 'tieredImageNet'\n",
    "title = 'CIFAR-FS'\n",
    "# title = 'CUB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005193c",
   "metadata": {},
   "source": [
    "# 1. MAML 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_maml = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_JM\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_maml.im_shape = (2, 3, args_maml.image_height, args_maml.image_width)\n",
    "\n",
    "args_maml.use_cuda = torch.cuda.is_available()\n",
    "args_maml.seed = 104\n",
    "args_maml.reverse_channels=False\n",
    "args_maml.labels_as_int=False\n",
    "args_maml.reset_stored_filepaths=False\n",
    "args_maml.num_of_gpus=1\n",
    "\n",
    "args_maml.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9052a",
   "metadata": {},
   "source": [
    "## 2. Arbiter 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_arbiter = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_arbiter.im_shape = (2, 3, args_arbiter.image_height, args_arbiter.image_width)\n",
    "\n",
    "args_arbiter.use_cuda = torch.cuda.is_available()\n",
    "args_arbiter.seed = 104\n",
    "args_arbiter.reverse_channels=False\n",
    "args_arbiter.labels_as_int=False\n",
    "args_arbiter.reset_stored_filepaths=False\n",
    "args_arbiter.num_of_gpus=1\n",
    "\n",
    "args_arbiter.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f7d8",
   "metadata": {},
   "source": [
    "## 3. Model 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803156ee",
   "metadata": {},
   "source": [
    "### 3.1. MAML Model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85286c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\anaconda3\\envs\\metal\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_JM\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_maml = MAMLFewShotClassifier(args=args_maml, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_maml.image_height, args_maml.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model_maml, data=data, args=args_maml, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3acf",
   "metadata": {},
   "source": [
    "### 3.2.  Arbiter 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25651dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n",
      "Arbiter\n",
      "linear1.weight torch.Size([20, 20])\n",
      "linear1.bias torch.Size([20])\n",
      "linear2.weight torch.Size([10, 20])\n",
      "linear2.bias torch.Size([10])\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "arbiter.linear1.weight torch.Size([20, 20]) cuda:0 True\n",
      "arbiter.linear1.bias torch.Size([20]) cuda:0 True\n",
      "arbiter.linear2.weight torch.Size([10, 20]) cuda:0 True\n",
      "arbiter.linear2.bias torch.Size([10]) cuda:0 True\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_arbiter = MAMLFewShotClassifier(args=args_arbiter, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_arbiter.image_height, args_arbiter.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "arbiter_system = ExperimentBuilder(model=model_arbiter, data=data, args=args_arbiter, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2ff6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6029333329200744,\n",
       " 'best_val_iter': 33000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 66,\n",
       " 'train_loss_mean': 0.6170567286610603,\n",
       " 'train_loss_std': 0.13234031447214586,\n",
       " 'train_accuracy_mean': 0.7684133331775665,\n",
       " 'train_accuracy_std': 0.059865535968887475,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 1.021280496120453,\n",
       " 'val_loss_std': 0.151031871638687,\n",
       " 'val_accuracy_mean': 0.6020222207903863,\n",
       " 'val_accuracy_std': 0.06521008437328708,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 2.5037e-02, -4.4997e-01,  1.7932e-01],\n",
       "                         [-2.0632e-01, -8.2393e-02,  1.0448e-01],\n",
       "                         [-8.5987e-02,  4.5530e-01, -2.0294e-02]],\n",
       "               \n",
       "                        [[ 2.0816e-01, -3.3887e-01,  2.5749e-01],\n",
       "                         [-1.0006e-01,  5.3326e-02,  1.8420e-01],\n",
       "                         [-1.7416e-01,  2.6081e-01, -1.5041e-01]],\n",
       "               \n",
       "                        [[ 3.4229e-01, -2.0577e-01, -1.6573e-02],\n",
       "                         [ 1.0237e-01,  1.2955e-01, -1.9383e-01],\n",
       "                         [-1.7447e-01,  1.7498e-01, -3.1870e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.4395e-01,  5.2128e-01,  2.6883e-01],\n",
       "                         [ 1.8075e-01, -1.4931e-01,  2.0894e-02],\n",
       "                         [-3.1864e-01, -3.6641e-01, -2.7889e-01]],\n",
       "               \n",
       "                        [[-1.9905e-01, -3.8144e-02, -1.6151e-01],\n",
       "                         [ 1.1556e-01, -1.9046e-01, -1.1170e-01],\n",
       "                         [ 1.8464e-01, -1.2443e-02,  1.8620e-01]],\n",
       "               \n",
       "                        [[-2.2214e-01,  2.1734e-02, -2.4665e-01],\n",
       "                         [ 1.4903e-02,  1.2988e-01,  2.0735e-01],\n",
       "                         [ 4.7411e-02,  1.2747e-01,  6.4476e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.2861e-01, -3.2424e-01,  3.7238e-01],\n",
       "                         [ 1.6947e-01, -2.8258e-01, -2.1689e-01],\n",
       "                         [ 2.2934e-01,  3.4886e-01, -1.3557e-01]],\n",
       "               \n",
       "                        [[-2.6556e-01, -2.8397e-01,  4.4669e-01],\n",
       "                         [ 2.9654e-01, -2.5358e-01, -1.0882e-01],\n",
       "                         [-1.4640e-01,  4.6053e-02,  8.3873e-03]],\n",
       "               \n",
       "                        [[ 1.7290e-02, -3.1139e-01,  2.2739e-01],\n",
       "                         [ 3.2377e-01, -1.4450e-01, -2.1537e-01],\n",
       "                         [-1.3449e-01,  1.3412e-01, -2.9386e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.9325e-01, -2.1398e-01, -3.0456e-01],\n",
       "                         [ 4.1278e-01, -3.9784e-01, -5.7591e-01],\n",
       "                         [ 4.6934e-01, -3.3650e-02, -4.2129e-01]],\n",
       "               \n",
       "                        [[ 9.6772e-02, -1.8275e-02,  3.0180e-02],\n",
       "                         [ 5.4257e-02, -5.9888e-02, -4.9876e-02],\n",
       "                         [ 1.2510e-01,  1.3736e-02, -1.0880e-01]],\n",
       "               \n",
       "                        [[ 1.0151e-01, -1.7353e-02,  1.0037e-01],\n",
       "                         [ 4.7628e-02,  3.8733e-02,  1.5967e-01],\n",
       "                         [-4.2467e-02, -5.2167e-02,  1.3743e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.8640e-01, -1.7409e-01,  2.0937e-01],\n",
       "                         [-1.7410e-01,  2.2463e-01,  2.2544e-01],\n",
       "                         [-1.4909e-01,  3.5473e-04,  2.1723e-01]],\n",
       "               \n",
       "                        [[ 3.2799e-01,  2.4585e-01,  5.4571e-02],\n",
       "                         [-2.4341e-02, -2.2892e-01, -3.5030e-02],\n",
       "                         [-4.1045e-03, -2.0666e-01, -3.0218e-01]],\n",
       "               \n",
       "                        [[ 1.9905e-01, -1.1782e-01, -3.8158e-01],\n",
       "                         [ 2.2489e-01, -1.4274e-01, -2.4078e-01],\n",
       "                         [ 1.4335e-01,  2.5220e-01,  4.5388e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1991e-02,  8.7496e-02,  1.2082e-01],\n",
       "                         [ 7.1742e-02,  2.5593e-01,  1.7615e-01],\n",
       "                         [ 6.6174e-02,  1.3048e-01,  1.6797e-01]],\n",
       "               \n",
       "                        [[-5.1880e-01, -4.4543e-01, -1.6444e-01],\n",
       "                         [-4.7992e-01, -4.3828e-01, -2.3753e-01],\n",
       "                         [-2.1924e-01, -7.1212e-02,  1.7914e-02]],\n",
       "               \n",
       "                        [[-9.6762e-02,  1.3703e-01,  3.1958e-01],\n",
       "                         [-6.6336e-02,  5.8481e-02,  3.0807e-01],\n",
       "                         [ 9.8960e-02,  2.5364e-01,  1.7082e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-0.0568,  0.2195, -0.1661, -0.0412, -0.0560,  0.0091, -0.0181,  0.1939,\n",
       "                       -0.0443, -0.1362,  0.0576, -0.1648,  0.0532, -0.1614,  0.0253, -0.0556,\n",
       "                       -0.1005, -0.1600, -0.2095, -0.0349, -0.0650,  0.0165, -0.0696, -0.0848,\n",
       "                        0.0406,  0.0287, -0.0043,  0.0845, -0.0733, -0.0238, -0.1847,  0.0197,\n",
       "                       -0.0142,  0.0345, -0.0010,  0.0155,  0.0182,  0.1267, -0.0299, -0.2206,\n",
       "                       -0.0345, -0.1168,  0.0246,  0.0189, -0.0208, -0.0123, -0.1200,  0.0599],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1178,  0.1109, -0.7902, -0.0772, -0.8277, -0.7771,  0.1709,  2.9743,\n",
       "                       -0.7568, -0.7098, -0.7086,  0.0255,  0.3214, -0.3074, -0.8943, -1.0388,\n",
       "                       -0.6050, -0.0139,  0.7373, -0.5944, -0.5067, -0.6364, -0.2760, -0.1611,\n",
       "                       -0.0798, -0.4247, -0.3395,  0.5578,  0.0140, -0.2211, -0.1097, -0.5261,\n",
       "                        0.0403, -0.4826, -0.0179, -0.4850,  0.5551,  0.2775, -0.6246, -0.0137,\n",
       "                       -0.1414,  0.0579, -0.5953, -0.1344,  0.0267,  0.0265, -0.4142, -0.1630],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.8807, 1.0831, 0.9570, 1.6960, 0.7824, 0.9485, 1.1983, 0.6926, 0.6111,\n",
       "                       0.9716, 0.8494, 1.3870, 0.8375, 0.7375, 1.2100, 0.7998, 0.5389, 0.9345,\n",
       "                       0.9370, 0.7303, 0.8977, 0.6418, 0.8176, 0.7296, 0.4506, 0.8586, 0.7235,\n",
       "                       0.7300, 1.1509, 0.7444, 0.9437, 0.5987, 1.3781, 0.6057, 0.6205, 0.6865,\n",
       "                       0.7709, 0.7630, 0.7231, 1.0815, 0.8458, 1.2721, 0.9503, 1.6399, 0.9478,\n",
       "                       0.7760, 0.8290, 1.0448], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 1.1723e-01, -2.2486e-01,  4.8115e-02],\n",
       "                         [ 1.3085e-01,  6.6258e-02,  2.8897e-02],\n",
       "                         [-2.9060e-02, -7.4014e-02, -1.5619e-01]],\n",
       "               \n",
       "                        [[ 5.9316e-02,  7.1605e-02,  6.9364e-02],\n",
       "                         [ 3.8951e-03,  6.2399e-02,  6.4783e-02],\n",
       "                         [ 1.7448e-01, -2.6770e-01,  3.4395e-02]],\n",
       "               \n",
       "                        [[-4.2440e-01, -3.7015e-01, -2.3283e-01],\n",
       "                         [-3.1611e-01, -3.3282e-01,  1.0062e-02],\n",
       "                         [-7.0869e-02,  6.4721e-01,  5.3067e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.6915e-01,  3.3147e-02, -1.8707e-01],\n",
       "                         [ 7.1431e-02,  1.7973e-01, -1.1686e-01],\n",
       "                         [ 1.2885e-02,  1.6110e-01, -1.5294e-01]],\n",
       "               \n",
       "                        [[-5.3997e-02,  4.5447e-02,  1.6804e-02],\n",
       "                         [-8.9433e-02, -5.2284e-02, -7.5497e-02],\n",
       "                         [-1.7821e-01, -1.1751e-02,  1.1624e-01]],\n",
       "               \n",
       "                        [[ 2.2381e-01, -3.6369e-01, -2.8259e-01],\n",
       "                         [ 3.6872e-01, -2.5336e-01,  2.6568e-02],\n",
       "                         [ 1.0620e-01, -3.3906e-01,  4.2335e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.8204e-02, -1.2502e-01,  3.9517e-01],\n",
       "                         [ 9.9335e-02,  5.6419e-01,  5.4226e-02],\n",
       "                         [ 1.3095e-01,  1.7714e-01, -1.2594e-01]],\n",
       "               \n",
       "                        [[ 4.3101e-02,  2.7285e-03,  2.5926e-01],\n",
       "                         [-2.0115e-01,  1.2104e-01,  1.7631e-01],\n",
       "                         [ 2.2271e-01,  1.5099e-01, -9.2619e-02]],\n",
       "               \n",
       "                        [[ 1.3939e-02, -4.3660e-01, -3.3420e-01],\n",
       "                         [ 5.3788e-01, -1.5989e-01, -5.9395e-01],\n",
       "                         [-1.4097e-01,  3.5848e-01,  1.2849e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.3552e-01, -9.3785e-02,  3.0068e-01],\n",
       "                         [ 7.5629e-02,  8.2245e-02,  9.3090e-02],\n",
       "                         [ 2.2206e-01,  3.0816e-01,  8.6614e-02]],\n",
       "               \n",
       "                        [[-2.1481e-01, -3.8560e-01, -3.2627e-01],\n",
       "                         [-7.6834e-03,  1.7724e-01, -3.5507e-02],\n",
       "                         [ 2.2014e-01,  1.7437e-02,  5.6633e-01]],\n",
       "               \n",
       "                        [[-2.6161e-01, -1.6674e-01, -1.4716e-02],\n",
       "                         [-2.1234e-01, -3.1047e-01, -5.1113e-01],\n",
       "                         [ 2.2891e-01, -1.5843e-01, -3.5421e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 9.5501e-03,  7.8331e-02, -7.4933e-02],\n",
       "                         [ 8.0321e-03, -7.0850e-02,  1.3408e-01],\n",
       "                         [ 2.6340e-01,  3.5210e-01, -1.2834e-01]],\n",
       "               \n",
       "                        [[ 1.4506e-01, -6.2499e-02, -1.2241e-01],\n",
       "                         [-1.4040e-01,  4.5845e-01,  2.9100e-02],\n",
       "                         [ 1.8178e-01, -3.7339e-01,  3.4046e-01]],\n",
       "               \n",
       "                        [[-1.1853e-02, -8.7244e-02, -4.3762e-02],\n",
       "                         [-1.9227e-01,  9.7253e-02,  6.1234e-01],\n",
       "                         [ 7.8001e-02,  2.5905e-01,  6.9914e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.1611e-02, -3.7756e-02, -1.2249e-01],\n",
       "                         [-5.5894e-02, -1.8564e-01,  1.1535e-01],\n",
       "                         [ 1.4011e-01,  4.6145e-02, -2.5308e-01]],\n",
       "               \n",
       "                        [[ 9.6727e-02,  4.6357e-02, -2.1699e-01],\n",
       "                         [-8.5529e-02, -2.5254e-01,  5.9169e-02],\n",
       "                         [-4.4068e-03, -3.1867e-01, -4.8196e-02]],\n",
       "               \n",
       "                        [[ 1.2215e-01, -3.4195e-01, -3.4973e-01],\n",
       "                         [-6.3272e-02,  2.5267e-02, -1.1772e-01],\n",
       "                         [-2.4811e-01,  2.6574e-01,  1.0269e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.0986e-01, -4.6881e-01, -2.1790e-01],\n",
       "                         [-4.7496e-02,  4.3363e-02, -4.8696e-02],\n",
       "                         [ 3.7602e-01,  2.8405e-01,  1.6903e-02]],\n",
       "               \n",
       "                        [[ 1.0033e-01,  1.8641e-01,  2.4904e-02],\n",
       "                         [ 1.6156e-01,  5.3520e-01,  3.9007e-02],\n",
       "                         [-5.7115e-02, -8.8734e-02, -3.1473e-01]],\n",
       "               \n",
       "                        [[ 2.6404e-01,  3.7046e-01, -7.6691e-02],\n",
       "                         [-8.0378e-02,  2.5661e-01, -1.9596e-01],\n",
       "                         [-1.4172e-01, -1.2200e-01, -2.0363e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.6754e-01,  7.3480e-02,  1.1622e-01],\n",
       "                         [ 7.0173e-02,  1.2383e-01, -1.7169e-01],\n",
       "                         [ 2.4898e-01, -9.2098e-02, -4.6312e-01]],\n",
       "               \n",
       "                        [[ 6.3293e-01,  2.5839e-01, -6.8058e-02],\n",
       "                         [-4.4857e-01, -2.2408e-01,  1.1252e-01],\n",
       "                         [-1.5572e-01,  1.1284e-01, -1.7335e-01]],\n",
       "               \n",
       "                        [[ 2.6170e-01,  4.4473e-01,  8.2485e-02],\n",
       "                         [ 4.4856e-01,  2.6471e-01,  3.9288e-01],\n",
       "                         [ 4.7014e-02,  3.4229e-02,  3.9225e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.7307e-02,  4.0675e-02, -1.5118e-01],\n",
       "                         [-1.5274e-02, -4.3358e-02, -4.2711e-02],\n",
       "                         [-1.9259e-01, -2.0651e-01,  9.8924e-02]],\n",
       "               \n",
       "                        [[-1.9144e-01, -5.3280e-02,  1.4052e-01],\n",
       "                         [-3.1058e-01, -1.7295e-01,  1.4374e-01],\n",
       "                         [-7.9022e-04, -5.3641e-02,  1.6725e-01]],\n",
       "               \n",
       "                        [[-5.2518e-01,  2.6836e-03,  7.5389e-02],\n",
       "                         [-8.3898e-01, -5.1489e-01,  5.1944e-01],\n",
       "                         [-3.2122e-01, -7.9167e-01,  3.6454e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.0374e-01,  2.2827e-01, -9.4024e-02],\n",
       "                         [-4.1874e-02,  2.5801e-02,  2.1840e-01],\n",
       "                         [-3.4656e-02,  7.3601e-02,  1.2317e-01]],\n",
       "               \n",
       "                        [[-3.5726e-01,  3.9791e-02, -1.6023e-01],\n",
       "                         [-3.6451e-01,  2.4205e-02,  2.9742e-01],\n",
       "                         [-2.0247e-01, -6.3653e-02,  1.0011e-01]],\n",
       "               \n",
       "                        [[ 2.0065e-01, -2.5123e-01, -3.2276e-01],\n",
       "                         [ 1.2319e-02, -5.7657e-01, -3.6513e-01],\n",
       "                         [-2.5569e-01, -4.9363e-01, -4.3979e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.5333e-01, -1.2538e-01,  5.4321e-01],\n",
       "                         [-5.4745e-01, -3.4796e-01,  2.7170e-01],\n",
       "                         [ 1.3192e-01,  5.4271e-02,  3.1668e-01]],\n",
       "               \n",
       "                        [[-2.3032e-01, -1.4286e-01,  4.9233e-01],\n",
       "                         [-7.3377e-01, -3.8872e-01,  2.4394e-01],\n",
       "                         [ 8.4159e-02,  1.6312e-01,  1.0642e-01]],\n",
       "               \n",
       "                        [[-1.4595e-02,  2.7501e-01,  2.4184e-01],\n",
       "                         [-2.0186e-01, -8.3606e-02, -2.0816e-01],\n",
       "                         [ 8.1960e-03, -3.1166e-02, -2.6199e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.8838e-01, -5.6943e-03, -7.6997e-02],\n",
       "                         [-2.3682e-02,  6.5188e-02, -2.3139e-01],\n",
       "                         [-6.1312e-02,  1.1627e-01, -1.8899e-01]],\n",
       "               \n",
       "                        [[-3.2381e-01, -1.3341e-01,  5.2094e-02],\n",
       "                         [-4.4858e-01, -1.9210e-01,  2.0638e-01],\n",
       "                         [ 1.3600e-01, -1.5267e-01, -1.3990e-02]],\n",
       "               \n",
       "                        [[-1.4009e-01,  1.5119e-01, -9.7849e-02],\n",
       "                         [-7.5604e-02,  2.4380e-01,  1.0850e-01],\n",
       "                         [ 4.0984e-04,  2.5637e-01,  4.5012e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 0.0059,  0.0179,  0.0464, -0.0243, -0.0036, -0.0288, -0.0034,  0.0121,\n",
       "                        0.0116,  0.0073,  0.0188,  0.0118, -0.0117,  0.0087, -0.0220,  0.0077,\n",
       "                        0.0228,  0.0384,  0.0025,  0.0091,  0.0202,  0.0579,  0.0164, -0.0135,\n",
       "                        0.0266,  0.0075,  0.0365, -0.0596,  0.0227,  0.0234, -0.0156, -0.0046,\n",
       "                        0.0052,  0.0002,  0.0042,  0.0016, -0.0119, -0.0054, -0.0308, -0.0152,\n",
       "                        0.0185,  0.0005,  0.0245, -0.0316,  0.0270, -0.0193, -0.0262,  0.0128],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.3482, -0.4582, -0.3668, -0.5350,  0.1087, -0.6147, -0.3951, -0.4760,\n",
       "                       -0.5476, -0.3157, -0.4880, -0.4163, -0.4559, -0.2741, -0.3840, -0.4875,\n",
       "                       -0.5368, -0.4536, -0.3777, -0.6277, -0.1977, -0.3475, -0.5605, -0.6755,\n",
       "                       -0.3217, -0.5682, -0.3373, -0.7194, -0.3823, -0.4244, -0.5390, -0.6882,\n",
       "                       -0.3076, -0.5692, -0.6515, -0.5146, -0.6122, -0.4158, -0.6060, -0.5107,\n",
       "                       -0.3592, -0.4583, -0.5558, -0.4330, -0.4815, -0.6314, -0.2706, -0.9414],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.1071, 0.9161, 0.8322, 0.8865, 1.0059, 1.0026, 1.0423, 0.8880, 0.9502,\n",
       "                       0.9909, 0.8596, 0.9968, 0.8817, 0.9192, 0.9583, 0.7205, 1.0283, 0.9072,\n",
       "                       0.8178, 0.8236, 0.9170, 0.8469, 1.1259, 1.0443, 1.0330, 0.9423, 1.1965,\n",
       "                       1.4433, 0.8633, 0.9365, 0.9605, 0.8016, 0.8934, 0.9058, 0.9686, 0.7292,\n",
       "                       1.1284, 0.9806, 1.0979, 1.2054, 1.0810, 1.0336, 1.1102, 0.6741, 0.9195,\n",
       "                       0.7565, 0.8966, 1.4231], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-4.8275e-02,  1.0676e-01,  2.5025e-01],\n",
       "                         [-4.0337e-01, -2.3295e-01, -1.1092e-01],\n",
       "                         [-3.0320e-01,  6.7789e-02,  6.8503e-02]],\n",
       "               \n",
       "                        [[-1.2478e-01,  2.3419e-01,  2.1155e-01],\n",
       "                         [-8.8580e-02,  1.8990e-01, -2.0579e-04],\n",
       "                         [ 3.3726e-02, -3.5016e-02, -1.5610e-01]],\n",
       "               \n",
       "                        [[-1.6755e-01, -2.9691e-01, -3.0679e-01],\n",
       "                         [-1.2511e-01,  3.7282e-01, -6.6927e-02],\n",
       "                         [ 1.5504e-01,  8.4938e-01,  3.2541e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.6847e-01, -3.7877e-01, -2.7585e-01],\n",
       "                         [-4.1303e-01, -4.1132e-01, -4.7550e-01],\n",
       "                         [-9.9584e-02, -3.6191e-01, -2.4970e-01]],\n",
       "               \n",
       "                        [[ 4.2101e-01,  1.0947e-01,  2.0296e-01],\n",
       "                         [ 1.9783e-01,  4.3689e-01, -9.6289e-02],\n",
       "                         [-3.3644e-01, -2.5695e-02, -1.8325e-01]],\n",
       "               \n",
       "                        [[ 6.3821e-02,  4.1665e-02,  2.5820e-01],\n",
       "                         [ 3.9545e-02, -7.5021e-02, -1.6536e-01],\n",
       "                         [-2.7939e-01, -4.7172e-01, -2.9780e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.7262e-01, -1.9487e-01, -1.8059e-01],\n",
       "                         [-2.3575e-02,  1.9537e-01,  2.2485e-01],\n",
       "                         [-8.1234e-02, -8.7750e-02, -8.2514e-02]],\n",
       "               \n",
       "                        [[-2.9202e-01, -2.6992e-01,  4.9123e-02],\n",
       "                         [-1.2796e-01, -1.4548e-01, -7.4813e-02],\n",
       "                         [ 1.7581e-01,  4.9964e-01,  1.5534e-01]],\n",
       "               \n",
       "                        [[-1.5003e-01,  6.1185e-02, -1.3899e-01],\n",
       "                         [ 9.4895e-02,  2.5260e-01, -1.1106e-01],\n",
       "                         [-1.3077e-01,  4.1205e-01, -2.0080e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2003e-01, -2.5372e-01, -2.3488e-02],\n",
       "                         [-1.7999e-01, -3.6867e-01,  8.1485e-02],\n",
       "                         [-3.5793e-03, -1.2956e-01, -1.4693e-01]],\n",
       "               \n",
       "                        [[-2.4457e-02, -8.8486e-02, -2.5571e-01],\n",
       "                         [-2.1084e-01,  1.5326e-01,  9.3773e-02],\n",
       "                         [-2.2072e-01, -5.3838e-02,  8.7295e-02]],\n",
       "               \n",
       "                        [[-3.3586e-01, -4.4808e-01, -6.9804e-01],\n",
       "                         [ 2.7132e-01,  5.2178e-01,  3.6476e-01],\n",
       "                         [-2.1507e-01,  1.7051e-01,  1.4623e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.0626e-01, -4.7490e-03,  1.1064e-01],\n",
       "                         [ 8.0119e-03,  1.6276e-01, -5.7380e-02],\n",
       "                         [-4.3877e-02,  7.0060e-02,  1.9635e-01]],\n",
       "               \n",
       "                        [[-5.2146e-02, -3.7111e-01, -2.4656e-02],\n",
       "                         [-3.7414e-02, -4.1102e-01, -2.0627e-02],\n",
       "                         [ 8.8074e-02, -1.6319e-01,  2.8225e-02]],\n",
       "               \n",
       "                        [[ 1.6722e-01,  1.6966e-01, -9.7760e-03],\n",
       "                         [-9.7418e-02, -7.9746e-02, -1.1699e-01],\n",
       "                         [-2.5112e-01, -2.0086e-01, -4.9813e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 8.6635e-02, -2.6379e-01, -1.7073e-01],\n",
       "                         [-2.1371e-03, -1.1621e-01,  6.6615e-02],\n",
       "                         [ 1.1864e-01, -1.2420e-01, -2.1610e-02]],\n",
       "               \n",
       "                        [[-1.7136e-01, -1.0243e-01, -1.5949e-01],\n",
       "                         [ 6.8331e-03,  1.8804e-03,  5.7051e-04],\n",
       "                         [ 5.2456e-02,  1.3981e-01, -3.7211e-01]],\n",
       "               \n",
       "                        [[ 7.3805e-02, -3.2453e-01, -5.9279e-01],\n",
       "                         [ 1.8092e-01, -1.2191e-01, -5.5461e-01],\n",
       "                         [ 3.0013e-01, -3.9664e-01, -5.7219e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.2521e-01,  3.1455e-01,  6.1523e-02],\n",
       "                         [ 1.3361e-01, -1.2973e-01, -2.6836e-01],\n",
       "                         [ 2.1610e-01,  4.1889e-02, -4.2994e-01]],\n",
       "               \n",
       "                        [[-3.5435e-02, -1.1472e-01, -1.0583e-01],\n",
       "                         [ 2.2659e-02,  8.0952e-02, -1.3790e-01],\n",
       "                         [ 2.2752e-01, -1.5975e-01, -1.8167e-01]],\n",
       "               \n",
       "                        [[-2.9078e-01, -1.2871e-01, -1.8751e-01],\n",
       "                         [-1.8504e-01, -1.1498e-01, -3.2690e-02],\n",
       "                         [-2.8099e-01,  1.2091e-02, -7.8271e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.1615e-02,  1.2573e-01, -6.3643e-02],\n",
       "                         [-3.2885e-02, -1.0712e-01, -1.1536e-01],\n",
       "                         [ 2.0438e-01,  1.8530e-01,  1.5420e-01]],\n",
       "               \n",
       "                        [[ 7.5532e-02,  5.1726e-01,  1.4386e-01],\n",
       "                         [ 3.7653e-01,  2.1200e-01, -4.4838e-01],\n",
       "                         [ 2.5890e-01, -1.5498e-02, -4.9306e-01]],\n",
       "               \n",
       "                        [[ 8.0420e-03,  2.4430e-01,  2.3225e-01],\n",
       "                         [ 1.1631e-01,  3.0547e-01, -6.4478e-02],\n",
       "                         [ 2.2236e-01,  1.4792e-01, -1.4772e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-9.6203e-02,  1.0692e-01,  6.5262e-02],\n",
       "                         [ 1.0129e-01,  6.9964e-02,  1.0961e-01],\n",
       "                         [ 1.8868e-01, -3.4987e-01, -1.0299e-01]],\n",
       "               \n",
       "                        [[ 2.6404e-01,  4.3812e-02, -1.3430e-01],\n",
       "                         [-2.3716e-02,  6.7540e-02, -2.3641e-02],\n",
       "                         [-3.4915e-01, -9.7474e-02,  2.2185e-01]],\n",
       "               \n",
       "                        [[ 2.9706e-01, -5.9129e-02,  5.3920e-02],\n",
       "                         [-9.4320e-02, -2.1426e-02, -1.4473e-01],\n",
       "                         [-1.0030e-01, -2.3161e-01,  7.0026e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.4568e-02, -4.8759e-02,  4.1992e-02],\n",
       "                         [ 1.6358e-01, -9.5074e-02, -8.2767e-02],\n",
       "                         [ 5.8289e-02,  1.7744e-01,  2.1886e-02]],\n",
       "               \n",
       "                        [[ 3.9879e-01, -6.3629e-02,  2.2657e-01],\n",
       "                         [ 2.6939e-01,  2.3070e-01,  2.7081e-01],\n",
       "                         [ 2.9925e-01, -1.8115e-01, -6.7829e-02]],\n",
       "               \n",
       "                        [[-1.5049e-02, -9.4559e-02, -1.1158e-01],\n",
       "                         [ 2.4958e-02,  9.7797e-02, -1.1996e-02],\n",
       "                         [ 1.8326e-01,  2.0815e-01,  2.1825e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.1435e-01, -1.8627e-01, -7.7109e-02],\n",
       "                         [-2.0777e-01, -1.3712e-01,  2.5167e-03],\n",
       "                         [ 1.1879e-01,  1.6598e-01,  8.8118e-02]],\n",
       "               \n",
       "                        [[-2.4300e-01,  9.7609e-02, -1.3486e-01],\n",
       "                         [ 1.3260e-01,  2.3450e-01, -9.0084e-02],\n",
       "                         [ 2.0798e-02,  1.4736e-01, -1.7467e-01]],\n",
       "               \n",
       "                        [[ 1.9154e-01, -1.3226e-01,  5.6997e-03],\n",
       "                         [ 1.2738e-01,  3.6395e-02, -1.5417e-01],\n",
       "                         [-3.1352e-02, -4.9823e-01, -1.5983e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.4481e-01, -2.0148e-01, -3.2177e-01],\n",
       "                         [-1.8596e-01, -1.7522e-01, -3.4640e-02],\n",
       "                         [-2.4183e-02, -9.5458e-02, -4.5932e-02]],\n",
       "               \n",
       "                        [[ 2.1486e-01,  1.6697e-01, -3.5126e-01],\n",
       "                         [ 5.3297e-02,  6.1913e-02, -5.2980e-01],\n",
       "                         [ 1.4346e-01,  3.3854e-01, -1.4936e-01]],\n",
       "               \n",
       "                        [[ 2.7606e-01,  3.3845e-01,  2.4213e-01],\n",
       "                         [-5.8999e-01, -7.7531e-01, -5.7579e-01],\n",
       "                         [-2.1931e-01, -3.9817e-02,  1.6721e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 0.0545, -0.0294,  0.0040,  0.0934, -0.0308, -0.0279,  0.0257,  0.0406,\n",
       "                        0.0282,  0.0748,  0.0329, -0.0121, -0.0047,  0.0388, -0.0134,  0.0451,\n",
       "                       -0.0337, -0.0033,  0.0041,  0.0111, -0.0448,  0.0065, -0.0476,  0.0298,\n",
       "                       -0.1085,  0.0270, -0.1591, -0.0626, -0.0343, -0.0530, -0.0014, -0.0241,\n",
       "                        0.0116, -0.0825,  0.0901, -0.0215, -0.0165, -0.0648, -0.0165, -0.0017,\n",
       "                        0.0365, -0.0115,  0.0093, -0.0380, -0.0086, -0.0005,  0.0377, -0.0782],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.4761, -0.4186, -0.5149, -0.7613, -0.5021, -0.5152, -0.5934, -0.7443,\n",
       "                       -0.3596, -0.7735, -0.5760, -0.9348, -0.5609, -0.8213, -0.5069, -0.4557,\n",
       "                       -0.5619, -0.5460, -0.9057, -0.6198, -0.3344, -0.6367, -0.4838, -1.3632,\n",
       "                       -0.7278, -0.6610, -0.7669, -0.7729, -1.6288, -0.9827, -0.2708, -0.6797,\n",
       "                       -0.9141, -0.4484, -0.6455, -0.6919, -0.7613, -0.4344, -0.8088, -0.6701,\n",
       "                       -0.6658, -0.7135, -0.5309, -1.2416, -0.6467, -0.4226, -0.5946, -0.6136],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.8222, 0.8021, 0.8066, 0.8849, 0.7799, 0.8619, 0.9059, 0.9851, 0.8140,\n",
       "                       0.8900, 0.9193, 0.9796, 0.8199, 0.8531, 0.7360, 0.8790, 0.8088, 0.7291,\n",
       "                       0.8571, 0.9202, 0.7229, 0.8293, 0.6758, 0.8409, 0.9964, 0.8795, 0.9078,\n",
       "                       0.9226, 0.9075, 1.0000, 0.6192, 0.8530, 0.9010, 0.6595, 0.8962, 0.9692,\n",
       "                       0.8418, 0.7921, 0.8220, 0.8976, 0.7863, 0.8013, 0.8664, 1.1205, 0.9238,\n",
       "                       0.6728, 0.9710, 0.8050], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-2.4393e-01, -1.6256e-01, -7.6074e-02],\n",
       "                         [-1.0031e-01, -8.2417e-02, -1.7764e-01],\n",
       "                         [ 1.0110e-01,  1.5303e-01, -3.4870e-02]],\n",
       "               \n",
       "                        [[ 1.5108e-01, -1.7032e-01,  9.6311e-04],\n",
       "                         [ 1.8988e-01, -1.3140e-01, -1.9165e-01],\n",
       "                         [ 2.0496e-01, -7.0907e-02, -1.5981e-01]],\n",
       "               \n",
       "                        [[-2.0313e-01, -1.0257e-01,  1.3925e-01],\n",
       "                         [-1.2162e-01, -2.2351e-01,  1.0560e-01],\n",
       "                         [-1.2636e-01, -1.0668e-01,  1.5469e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.4153e-01, -2.1150e-01, -9.4324e-02],\n",
       "                         [-1.8183e-01, -1.9546e-01, -2.3810e-01],\n",
       "                         [ 4.6776e-04, -1.1580e-01,  1.9740e-02]],\n",
       "               \n",
       "                        [[-3.7316e-02,  7.7690e-02, -2.2152e-03],\n",
       "                         [-2.8499e-01,  9.4306e-02, -1.6167e-01],\n",
       "                         [-2.2099e-01, -7.0278e-02, -4.1620e-02]],\n",
       "               \n",
       "                        [[-8.7149e-02,  3.8947e-02, -3.2221e-01],\n",
       "                         [ 1.7469e-01,  1.5686e-01, -1.9848e-02],\n",
       "                         [-5.8697e-03,  2.4584e-02, -4.6971e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.6776e-02,  7.9011e-02, -3.9424e-02],\n",
       "                         [ 1.0277e-01,  1.2664e-01, -3.5528e-02],\n",
       "                         [-1.4127e-01, -2.7255e-01, -3.8540e-01]],\n",
       "               \n",
       "                        [[-1.8514e-01, -9.9454e-02, -1.3245e-01],\n",
       "                         [-4.8259e-02, -3.8198e-02,  9.8478e-03],\n",
       "                         [-2.2798e-01, -3.0192e-01, -1.7376e-01]],\n",
       "               \n",
       "                        [[ 7.6053e-02, -1.3935e-01, -1.6534e-01],\n",
       "                         [-7.2108e-02, -3.2039e-02,  1.1386e-01],\n",
       "                         [-8.6764e-02,  4.8105e-02, -1.2165e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.0144e-01,  6.9039e-02, -2.1694e-03],\n",
       "                         [ 2.8395e-01,  2.6856e-01,  1.2569e-01],\n",
       "                         [ 3.4111e-01,  1.4565e-01,  4.9898e-02]],\n",
       "               \n",
       "                        [[-3.8837e-02,  3.7011e-02, -4.6223e-02],\n",
       "                         [-1.2672e-02,  1.2301e-01, -5.7843e-02],\n",
       "                         [-2.7001e-01, -4.3256e-01, -3.0397e-01]],\n",
       "               \n",
       "                        [[ 2.3397e-01,  2.1629e-01,  1.0481e-01],\n",
       "                         [ 8.2658e-02,  3.0328e-02,  1.4612e-01],\n",
       "                         [-1.2753e-01, -6.3040e-02,  8.8474e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.0231e-03,  8.5759e-02, -5.4231e-02],\n",
       "                         [ 6.5083e-02, -4.8214e-02,  8.0839e-02],\n",
       "                         [ 2.0578e-01,  3.4303e-01, -7.3844e-02]],\n",
       "               \n",
       "                        [[-3.9731e-02,  4.6501e-02, -1.5407e-01],\n",
       "                         [ 1.8223e-01,  9.8463e-02,  1.8482e-01],\n",
       "                         [ 1.7269e-02,  2.2765e-02, -1.7241e-01]],\n",
       "               \n",
       "                        [[-1.5460e-01, -4.7904e-01, -2.7330e-01],\n",
       "                         [-1.2498e-02, -4.1316e-01, -2.2065e-01],\n",
       "                         [-2.4386e-01, -4.8370e-01, -3.9690e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.7916e-02,  3.1693e-02, -1.0263e-01],\n",
       "                         [ 3.4124e-01,  6.8841e-02, -3.2765e-03],\n",
       "                         [ 2.9454e-01,  9.0875e-02,  1.1286e-01]],\n",
       "               \n",
       "                        [[-7.8779e-02, -8.8295e-02, -3.8616e-01],\n",
       "                         [ 1.1367e-01,  7.5823e-02, -8.3223e-02],\n",
       "                         [-8.9660e-02,  2.6585e-02, -7.7953e-02]],\n",
       "               \n",
       "                        [[ 1.3686e-01,  3.7933e-01,  1.6034e-01],\n",
       "                         [ 6.1773e-02,  1.3655e-01, -1.6103e-02],\n",
       "                         [-2.3204e-01, -9.1595e-02, -8.4355e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.2835e-01,  1.5567e-01, -1.7454e-01],\n",
       "                         [ 9.2437e-02,  3.2571e-02,  2.7283e-02],\n",
       "                         [ 1.9368e-01,  3.6604e-01, -2.8392e-02]],\n",
       "               \n",
       "                        [[ 2.1545e-01,  1.4240e-01,  1.9670e-01],\n",
       "                         [-1.0197e-01, -3.6138e-01, -1.6210e-01],\n",
       "                         [ 5.8989e-02,  1.2509e-01,  1.8738e-01]],\n",
       "               \n",
       "                        [[ 1.4206e-02,  4.4768e-03, -7.0198e-02],\n",
       "                         [ 1.2549e-01, -4.1027e-02,  1.1328e-01],\n",
       "                         [-1.5778e-01, -1.9091e-01, -1.5419e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 8.7017e-02,  2.3463e-01,  9.1578e-02],\n",
       "                         [-2.7767e-02,  1.3496e-01,  1.5680e-01],\n",
       "                         [ 3.3266e-01,  2.4962e-01,  3.2114e-01]],\n",
       "               \n",
       "                        [[ 1.0655e-01,  1.0961e-01, -4.1866e-02],\n",
       "                         [ 6.6390e-02, -6.8127e-02, -7.2113e-02],\n",
       "                         [ 4.7949e-02, -1.4790e-01, -2.6580e-01]],\n",
       "               \n",
       "                        [[ 2.1657e-01, -6.9499e-02, -2.6617e-02],\n",
       "                         [ 1.3639e-01,  3.5626e-02, -1.9102e-01],\n",
       "                         [-1.2926e-02, -1.0226e-01, -2.2401e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.8684e-01, -1.0744e-01, -2.6231e-01],\n",
       "                         [-1.5032e-01, -3.3186e-01, -2.9336e-01],\n",
       "                         [-1.3466e-01, -1.5470e-01, -2.5788e-01]],\n",
       "               \n",
       "                        [[ 6.0035e-02, -8.3980e-02,  7.6845e-03],\n",
       "                         [ 2.5939e-01,  5.2874e-02,  1.8119e-02],\n",
       "                         [ 3.2006e-01,  4.0496e-02,  1.6263e-01]],\n",
       "               \n",
       "                        [[-1.5812e-01, -1.2299e-01, -1.5293e-01],\n",
       "                         [ 3.1314e-02, -3.6119e-02, -1.3861e-02],\n",
       "                         [-2.1349e-01,  3.4100e-02, -1.4652e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.8519e-01, -1.8036e-01, -6.2217e-02],\n",
       "                         [-3.2027e-01, -1.6399e-01, -1.3872e-01],\n",
       "                         [-2.0412e-01, -9.5625e-02, -5.7085e-03]],\n",
       "               \n",
       "                        [[-1.3966e-01, -5.0533e-02, -3.3339e-01],\n",
       "                         [-1.9492e-01, -1.3573e-01, -2.8660e-01],\n",
       "                         [-2.5774e-01, -2.0109e-01, -2.2481e-01]],\n",
       "               \n",
       "                        [[ 2.7967e-01,  3.0126e-03,  1.6508e-01],\n",
       "                         [ 3.4856e-02, -1.2025e-01,  6.8871e-02],\n",
       "                         [ 2.2067e-01, -1.7907e-01,  1.8686e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.6410e-01, -1.7526e-01, -4.0414e-01],\n",
       "                         [-9.9362e-02, -6.0485e-02, -1.2055e-01],\n",
       "                         [-9.3918e-02, -7.2719e-02, -1.1958e-01]],\n",
       "               \n",
       "                        [[-1.1558e-01, -1.6246e-01, -9.2651e-02],\n",
       "                         [ 6.7016e-02,  1.1733e-01,  1.4765e-01],\n",
       "                         [-1.1664e-02,  2.0142e-02,  1.5004e-01]],\n",
       "               \n",
       "                        [[-3.2239e-01, -1.1068e-01, -4.8240e-01],\n",
       "                         [-2.6360e-01, -2.2647e-01, -3.8438e-01],\n",
       "                         [-2.0448e-01, -2.9460e-01, -4.4413e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3546e-01,  4.4783e-02,  1.0188e-01],\n",
       "                         [ 1.2089e-01, -8.6764e-02,  7.3263e-02],\n",
       "                         [ 1.0097e-01, -6.6628e-02,  1.3123e-01]],\n",
       "               \n",
       "                        [[ 3.6972e-01,  1.5312e-01,  6.4615e-02],\n",
       "                         [ 3.7663e-01, -5.5978e-02,  1.2358e-01],\n",
       "                         [ 3.3274e-01,  1.1516e-01,  1.9350e-01]],\n",
       "               \n",
       "                        [[ 1.0976e-02, -2.3031e-01, -2.0472e-01],\n",
       "                         [ 8.1310e-02, -1.4376e-01, -6.7653e-02],\n",
       "                         [-1.4942e-01, -2.2949e-02,  9.2257e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 0.0108,  0.0256,  0.0202,  0.0497,  0.0181, -0.4689, -0.0741,  0.0619,\n",
       "                        0.0068,  0.0890,  0.0078, -0.2167,  0.0310,  0.0358, -0.0756,  0.0721,\n",
       "                        0.0068, -0.2842,  0.0762, -0.0301,  0.0842,  0.0210,  0.0154, -0.0671,\n",
       "                       -0.0336,  0.0298,  0.0124, -0.1381, -0.2219, -0.3091, -0.0129, -0.0379,\n",
       "                        0.0304,  0.0123,  0.0332,  0.0161,  0.0134, -0.0111, -0.0087,  0.0760,\n",
       "                       -0.0316,  0.0550,  0.0248,  0.0265, -0.1987,  0.0095, -0.0261, -0.0412],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-1.3414, -1.1540, -1.0041, -1.3270, -1.6966, -2.2509, -1.8962, -1.2363,\n",
       "                       -0.9025, -1.4977, -2.1877, -1.3714, -1.1134, -1.1430, -1.0365, -1.5136,\n",
       "                       -1.0322, -1.4076, -1.1124, -0.9531, -0.7861, -0.9835, -1.2479, -1.2860,\n",
       "                       -1.1164, -1.1088, -1.4227, -0.9873, -2.2645, -1.3483, -1.2443, -0.7594,\n",
       "                       -1.7934, -0.8495, -0.7915, -0.8083, -1.5598, -0.4954, -1.4225, -1.9293,\n",
       "                       -1.0768, -1.2061, -1.0484, -1.1252, -1.4628, -0.6932, -1.7596, -1.4421],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.5065, 0.4254, 0.4639, 0.5266, 0.7932, 4.9134, 2.8481, 0.4630, 0.3816,\n",
       "                       0.8415, 0.9761, 3.7499, 0.3793, 3.3965, 3.0657, 2.3447, 0.2717, 3.7082,\n",
       "                       0.3005, 3.2432, 4.0394, 0.3361, 0.4039, 3.2574, 2.8367, 3.5554, 0.4783,\n",
       "                       3.7579, 4.9038, 3.9028, 0.3612, 3.3756, 3.0828, 0.1967, 3.2877, 0.2172,\n",
       "                       0.5964, 0.1610, 0.5416, 3.1711, 3.4815, 3.3578, 0.4246, 0.3929, 3.6624,\n",
       "                       0.1209, 4.1968, 0.7447], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[ 0.6506,  0.1314,  0.1255,  ...,  0.0357, -0.1291, -0.0257],\n",
       "                       [-0.0309,  0.0834, -0.0036,  ..., -0.0729,  0.0072, -0.0665],\n",
       "                       [ 0.6787,  0.2806,  0.2760,  ..., -0.0711, -0.0949,  0.0057],\n",
       "                       [-1.6509, -0.6054, -0.7238,  ...,  0.0343,  0.2407,  0.0924],\n",
       "                       [ 0.6508,  0.2425,  0.2889,  ..., -0.0706, -0.0871, -0.0742]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.2078,  0.3197, -0.2188,  0.4502, -0.2260], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3827998510599135,\n",
       "   1.290763833642006,\n",
       "   1.2295622889995574,\n",
       "   1.1832738929986955,\n",
       "   1.1405038301944732,\n",
       "   1.1100208464860917,\n",
       "   1.0845901787281036,\n",
       "   1.072832510948181,\n",
       "   1.0489890743494035,\n",
       "   1.0405344399213792,\n",
       "   1.0097493835687636,\n",
       "   0.9993626879453659,\n",
       "   0.9816959538459777,\n",
       "   0.9572139068841934,\n",
       "   0.946814087510109,\n",
       "   0.9354512060880661,\n",
       "   0.9160394587516785,\n",
       "   0.920455734193325,\n",
       "   0.8975413458347321,\n",
       "   0.8843331079483032,\n",
       "   0.8766135520935059,\n",
       "   0.8555644645690917,\n",
       "   0.8550927764177323,\n",
       "   0.8558069803714752,\n",
       "   0.84610875415802,\n",
       "   0.8373528547286987,\n",
       "   0.8168139300942421,\n",
       "   0.8273018574118615,\n",
       "   0.813343044936657,\n",
       "   0.823822079539299,\n",
       "   0.8021464664340019,\n",
       "   0.8010273220539093,\n",
       "   0.7896402115821839,\n",
       "   0.7763238592147828,\n",
       "   0.7958545460104942,\n",
       "   0.7649655148983002,\n",
       "   0.7708348118662834,\n",
       "   0.768103753566742,\n",
       "   0.7674686343669891,\n",
       "   0.7497745404243469,\n",
       "   0.7698955820202827,\n",
       "   0.7505355799794197,\n",
       "   0.7492374839186668,\n",
       "   0.7538739818930625,\n",
       "   0.7421146684885025,\n",
       "   0.7441011714339256,\n",
       "   0.7431449490189552,\n",
       "   0.7296682821512223,\n",
       "   0.7268862066268921,\n",
       "   0.7314285841584206,\n",
       "   0.732060464322567,\n",
       "   0.7066616643071174,\n",
       "   0.7224422740936279,\n",
       "   0.706265928864479,\n",
       "   0.7222972026467324,\n",
       "   0.7175356521606445,\n",
       "   0.7078733791708947,\n",
       "   0.7080762842297554,\n",
       "   0.7019470695853234,\n",
       "   0.7057755035758019,\n",
       "   0.704182672560215,\n",
       "   0.7000789467096329,\n",
       "   0.6848764058947563,\n",
       "   0.6840437971353531,\n",
       "   0.6918817499279976,\n",
       "   0.6970621973276139,\n",
       "   0.6958986286520958,\n",
       "   0.6719806236028671,\n",
       "   0.6770147794485092,\n",
       "   0.687423622071743,\n",
       "   0.6736363031864167,\n",
       "   0.6762020394802094,\n",
       "   0.6805517143011093,\n",
       "   0.6775967606902122,\n",
       "   0.6678170262575149,\n",
       "   0.666104705631733,\n",
       "   0.6740251885652542,\n",
       "   0.6696518015265465,\n",
       "   0.660695000231266,\n",
       "   0.6704835996031762,\n",
       "   0.6603739549517631,\n",
       "   0.6633225050568581,\n",
       "   0.6620231432914734,\n",
       "   0.6586690958142281,\n",
       "   0.6425662245750428,\n",
       "   0.653431859254837,\n",
       "   0.654295447409153,\n",
       "   0.6535185945034027,\n",
       "   0.6466650068163872,\n",
       "   0.6372940349578857,\n",
       "   0.6441785926818848,\n",
       "   0.6478465265035629,\n",
       "   0.6229611974954605,\n",
       "   0.6439402343034745,\n",
       "   0.6428235161900521,\n",
       "   0.6493279329538345,\n",
       "   0.6340072020292282,\n",
       "   0.637120962202549,\n",
       "   0.6289799667596817],\n",
       "  'train_loss_std': [0.1305706246995479,\n",
       "   0.11878783500645171,\n",
       "   0.1332054541725746,\n",
       "   0.12518576328597736,\n",
       "   0.12428474258450888,\n",
       "   0.12830727812683979,\n",
       "   0.13491803300699456,\n",
       "   0.13189237258438896,\n",
       "   0.1373431725947088,\n",
       "   0.13404885394862476,\n",
       "   0.1405684427126473,\n",
       "   0.14289470490579614,\n",
       "   0.13195363599860316,\n",
       "   0.1412893984490438,\n",
       "   0.1383983653674498,\n",
       "   0.14989149437977528,\n",
       "   0.13078368614294356,\n",
       "   0.1479257254628079,\n",
       "   0.1471659303504334,\n",
       "   0.15079773188637227,\n",
       "   0.14586820876930628,\n",
       "   0.13278491497333356,\n",
       "   0.14341991228691917,\n",
       "   0.14827646113681325,\n",
       "   0.1409467349036606,\n",
       "   0.15492529532250085,\n",
       "   0.13677878279686914,\n",
       "   0.14258920205456263,\n",
       "   0.14522700012542178,\n",
       "   0.153383690296289,\n",
       "   0.14219383475980643,\n",
       "   0.14491988283916002,\n",
       "   0.143210691593768,\n",
       "   0.14216651025261842,\n",
       "   0.14959752945015622,\n",
       "   0.15229156824672804,\n",
       "   0.13964660114664834,\n",
       "   0.14930365685721908,\n",
       "   0.15263911326638066,\n",
       "   0.1530143594552231,\n",
       "   0.14874151944453826,\n",
       "   0.14180048285753183,\n",
       "   0.14779895998680248,\n",
       "   0.15242998902219743,\n",
       "   0.14930965530422374,\n",
       "   0.14991409954112114,\n",
       "   0.14913027407708748,\n",
       "   0.1467996745595898,\n",
       "   0.14343049168060076,\n",
       "   0.14604270171988304,\n",
       "   0.14941043769206283,\n",
       "   0.14064496593211187,\n",
       "   0.14731358915701875,\n",
       "   0.1435090523591409,\n",
       "   0.15171172365279392,\n",
       "   0.14679690245404764,\n",
       "   0.13453236413812247,\n",
       "   0.14760946459625798,\n",
       "   0.14368885614459903,\n",
       "   0.1463565242966809,\n",
       "   0.15000879356885427,\n",
       "   0.1483652573431013,\n",
       "   0.14749344771150202,\n",
       "   0.1443195380111526,\n",
       "   0.14277543615826363,\n",
       "   0.14507284550631558,\n",
       "   0.1390602575428178,\n",
       "   0.14625049572832316,\n",
       "   0.13854913521443363,\n",
       "   0.15236510452499985,\n",
       "   0.13260687193782847,\n",
       "   0.1423530674153454,\n",
       "   0.1408487453377252,\n",
       "   0.14255310600730792,\n",
       "   0.1411607785137941,\n",
       "   0.1424262181220576,\n",
       "   0.1441181466132492,\n",
       "   0.14956802987333556,\n",
       "   0.1415681411790055,\n",
       "   0.14184196663375048,\n",
       "   0.14295098764168412,\n",
       "   0.1461195277339851,\n",
       "   0.14734280360022062,\n",
       "   0.14083061797063556,\n",
       "   0.1462260001370381,\n",
       "   0.13714735143130602,\n",
       "   0.13609864990929535,\n",
       "   0.14822727774207503,\n",
       "   0.1400652015957041,\n",
       "   0.1436515933254084,\n",
       "   0.1431444017597319,\n",
       "   0.14708112441453325,\n",
       "   0.13760680305480924,\n",
       "   0.14621627935158016,\n",
       "   0.14955942707022427,\n",
       "   0.15391061944175513,\n",
       "   0.13204427591709697,\n",
       "   0.13894568180707398,\n",
       "   0.13854031192156432],\n",
       "  'train_accuracy_mean': [0.42784000104665754,\n",
       "   0.47413333374261857,\n",
       "   0.5049866660833359,\n",
       "   0.5281466671228409,\n",
       "   0.550413332760334,\n",
       "   0.5643866654634476,\n",
       "   0.5781066660881042,\n",
       "   0.5816266661286355,\n",
       "   0.5908933328986168,\n",
       "   0.5956266656517982,\n",
       "   0.6068933330178261,\n",
       "   0.6117999984025955,\n",
       "   0.6206266663074493,\n",
       "   0.6334133314490318,\n",
       "   0.6358799999356269,\n",
       "   0.6426133332848549,\n",
       "   0.6476266659498214,\n",
       "   0.6477066665291786,\n",
       "   0.6549066657423973,\n",
       "   0.6631066667437553,\n",
       "   0.6663200005292892,\n",
       "   0.6731066661477089,\n",
       "   0.6722933317422867,\n",
       "   0.671426666021347,\n",
       "   0.6758266674876213,\n",
       "   0.680093332707882,\n",
       "   0.6886133333444595,\n",
       "   0.6832000004053116,\n",
       "   0.6898666675686836,\n",
       "   0.6863600006103515,\n",
       "   0.6957066675424576,\n",
       "   0.6950933338999749,\n",
       "   0.70148000061512,\n",
       "   0.705986665725708,\n",
       "   0.6965333344936371,\n",
       "   0.7112799990773201,\n",
       "   0.7078799993991852,\n",
       "   0.7090266678333282,\n",
       "   0.7083333342075347,\n",
       "   0.7173866661190986,\n",
       "   0.7061066662073135,\n",
       "   0.7144800001382827,\n",
       "   0.7163599992990494,\n",
       "   0.7149066669940949,\n",
       "   0.7174133339524269,\n",
       "   0.7163333343267441,\n",
       "   0.7194533326625824,\n",
       "   0.7250133342146874,\n",
       "   0.7236133330464363,\n",
       "   0.7222399985790253,\n",
       "   0.7218133333325386,\n",
       "   0.7351333332061768,\n",
       "   0.7262933336496353,\n",
       "   0.734719999551773,\n",
       "   0.7283066675066948,\n",
       "   0.7306666662096977,\n",
       "   0.7341066675782204,\n",
       "   0.7331333334445953,\n",
       "   0.7366933333873749,\n",
       "   0.7317466671466828,\n",
       "   0.7332400015592575,\n",
       "   0.7347066652774811,\n",
       "   0.741866668343544,\n",
       "   0.7445466666221618,\n",
       "   0.7397466675043106,\n",
       "   0.7361066671609878,\n",
       "   0.7385466667413711,\n",
       "   0.746599998831749,\n",
       "   0.744799999833107,\n",
       "   0.740933333992958,\n",
       "   0.74657333278656,\n",
       "   0.7442399996519089,\n",
       "   0.7428666661977767,\n",
       "   0.7456800013780593,\n",
       "   0.748533331990242,\n",
       "   0.748986667752266,\n",
       "   0.7449466660022735,\n",
       "   0.7472933324575424,\n",
       "   0.7504800000190734,\n",
       "   0.7459333329200745,\n",
       "   0.7504666653871537,\n",
       "   0.7512266669273376,\n",
       "   0.7504933315515518,\n",
       "   0.7519066668748856,\n",
       "   0.7573066675662994,\n",
       "   0.7531733324527741,\n",
       "   0.7560800004005432,\n",
       "   0.7553200006484986,\n",
       "   0.7543199988603592,\n",
       "   0.75989333319664,\n",
       "   0.7577333337068558,\n",
       "   0.7551733336448669,\n",
       "   0.7648933336734772,\n",
       "   0.7577466659545898,\n",
       "   0.7565199999809266,\n",
       "   0.7552933329343796,\n",
       "   0.7615333340167999,\n",
       "   0.7592266662120819,\n",
       "   0.7632266668081283],\n",
       "  'train_accuracy_std': [0.07027597133970723,\n",
       "   0.06699306715113512,\n",
       "   0.07299817091827956,\n",
       "   0.06852889476959285,\n",
       "   0.06935965988351207,\n",
       "   0.06849526674365759,\n",
       "   0.07278197127044209,\n",
       "   0.06891426741167019,\n",
       "   0.07167458647889681,\n",
       "   0.0698027908198388,\n",
       "   0.07318434770077763,\n",
       "   0.07227881230955416,\n",
       "   0.06806570745751633,\n",
       "   0.06967730564652475,\n",
       "   0.06792154869405652,\n",
       "   0.07246710787805823,\n",
       "   0.06802459261880019,\n",
       "   0.07213434004282897,\n",
       "   0.07251063024635539,\n",
       "   0.0744284425452634,\n",
       "   0.06995913065888165,\n",
       "   0.06583509567745995,\n",
       "   0.06816309471553934,\n",
       "   0.07238299419816464,\n",
       "   0.06651921283771381,\n",
       "   0.07427450792118746,\n",
       "   0.06677682088092234,\n",
       "   0.06832263352148908,\n",
       "   0.06770182975667541,\n",
       "   0.07076090754842883,\n",
       "   0.06709769629954718,\n",
       "   0.06840997470099995,\n",
       "   0.06537029918612047,\n",
       "   0.06860502024042957,\n",
       "   0.07034063084679038,\n",
       "   0.06994764271544286,\n",
       "   0.06384316843540855,\n",
       "   0.06949873888732358,\n",
       "   0.07044351608511057,\n",
       "   0.06874569414785338,\n",
       "   0.06838613967892554,\n",
       "   0.06657808044216965,\n",
       "   0.06599928912486297,\n",
       "   0.07016434855382511,\n",
       "   0.06599308278202683,\n",
       "   0.06983393107422665,\n",
       "   0.06683288484049701,\n",
       "   0.06751543034583558,\n",
       "   0.06636070893724892,\n",
       "   0.0680585050049262,\n",
       "   0.06878566290737902,\n",
       "   0.0631080560388075,\n",
       "   0.06584860299565028,\n",
       "   0.0649323533926706,\n",
       "   0.06955652643257812,\n",
       "   0.06731550903789077,\n",
       "   0.06369041252662928,\n",
       "   0.06903432235842986,\n",
       "   0.0671178023098404,\n",
       "   0.06945929340674652,\n",
       "   0.06857981710013779,\n",
       "   0.06925622807298809,\n",
       "   0.06840064954759334,\n",
       "   0.06584995612389645,\n",
       "   0.06626430308429367,\n",
       "   0.06528533713038523,\n",
       "   0.06537225815443064,\n",
       "   0.06633447406903957,\n",
       "   0.06481206968205126,\n",
       "   0.06795567183557427,\n",
       "   0.05924630406176243,\n",
       "   0.0648140966119069,\n",
       "   0.06390169444086433,\n",
       "   0.06195700944580612,\n",
       "   0.06303494656080713,\n",
       "   0.06451146127422207,\n",
       "   0.06548127260080477,\n",
       "   0.06679892438439054,\n",
       "   0.06384732385511871,\n",
       "   0.06481903040005897,\n",
       "   0.0655062676960236,\n",
       "   0.0646000157386631,\n",
       "   0.06560234552181247,\n",
       "   0.062412321919564695,\n",
       "   0.06712534970080083,\n",
       "   0.06280956239040869,\n",
       "   0.0614369607625322,\n",
       "   0.06411142963737637,\n",
       "   0.06429726020965947,\n",
       "   0.06457923493894371,\n",
       "   0.06568355111555127,\n",
       "   0.0672084072066381,\n",
       "   0.06307358869930565,\n",
       "   0.06558311348191206,\n",
       "   0.06775823541243778,\n",
       "   0.06969124358267996,\n",
       "   0.06106357309060565,\n",
       "   0.0626051270000115,\n",
       "   0.062203160119611994],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.427120145559311,\n",
       "   1.3635271112124125,\n",
       "   1.2850881866614023,\n",
       "   1.2672780952850977,\n",
       "   1.2510536547501883,\n",
       "   1.2472314321994782,\n",
       "   1.2036826147635777,\n",
       "   1.2041782907644907,\n",
       "   1.2012638648351033,\n",
       "   1.1969836219151815,\n",
       "   1.184735784928004,\n",
       "   1.1813520489136378,\n",
       "   1.1481729288895925,\n",
       "   1.1419991652170818,\n",
       "   1.1384888478120168,\n",
       "   1.1208961121241252,\n",
       "   1.180149313410123,\n",
       "   1.1017948756615321,\n",
       "   1.1227205791076025,\n",
       "   1.106395907998085,\n",
       "   1.0997973678509394,\n",
       "   1.0837579905986785,\n",
       "   1.0760856280724207,\n",
       "   1.0792343751589457,\n",
       "   1.084807769060135,\n",
       "   1.0796692498524985,\n",
       "   1.0808505276838938,\n",
       "   1.0799509833256404,\n",
       "   1.0672111493349075,\n",
       "   1.0602396881580354,\n",
       "   1.0566426052649815,\n",
       "   1.0843791792790094,\n",
       "   1.0587897767623267,\n",
       "   1.061067397793134,\n",
       "   1.0608269051710764,\n",
       "   1.0594582811991373,\n",
       "   1.0619730327526729,\n",
       "   1.0556769903500876,\n",
       "   1.0491408308347066,\n",
       "   1.078071386019389,\n",
       "   1.047118684053421,\n",
       "   1.0564910870790483,\n",
       "   1.052267267505328,\n",
       "   1.0412260021766027,\n",
       "   1.0533104548851648,\n",
       "   1.030008419553439,\n",
       "   1.044946368734042,\n",
       "   1.0470409395297369,\n",
       "   1.0537566830714544,\n",
       "   1.0499561776717503,\n",
       "   1.047319988210996,\n",
       "   1.0269423456986746,\n",
       "   1.045507535537084,\n",
       "   1.0532420603434245,\n",
       "   1.0476899427175521,\n",
       "   1.0506882854302724,\n",
       "   1.024925939043363,\n",
       "   1.0286306949456532,\n",
       "   1.044008606473605,\n",
       "   1.0489837779601414,\n",
       "   1.046958018342654,\n",
       "   1.043016421397527,\n",
       "   1.0413110162814458,\n",
       "   1.0471129965782167,\n",
       "   1.0366451116402944,\n",
       "   1.0105235530932744,\n",
       "   1.0463576068480809,\n",
       "   1.0546520747741064,\n",
       "   1.0525312064091366,\n",
       "   1.0371409773826599,\n",
       "   1.029666802684466,\n",
       "   1.0276809805631637,\n",
       "   1.0693193024396896,\n",
       "   1.0287656678756079,\n",
       "   1.030766369700432,\n",
       "   1.0357609482606251,\n",
       "   1.0325623613595962,\n",
       "   1.042489298582077,\n",
       "   1.046970168352127,\n",
       "   1.058817773660024,\n",
       "   1.0353227587540945,\n",
       "   1.0240529880921045,\n",
       "   1.040330029129982,\n",
       "   1.0229158480962117,\n",
       "   1.051594411333402,\n",
       "   1.033131770292918,\n",
       "   1.029660669763883,\n",
       "   1.0500559987624487,\n",
       "   1.0390480180581412,\n",
       "   1.0339660108089448,\n",
       "   1.0291594765583674,\n",
       "   1.0397388170162836,\n",
       "   1.032144349416097,\n",
       "   1.0185593674580256,\n",
       "   1.0363900125026704,\n",
       "   1.02152443865935,\n",
       "   1.0233245323101678,\n",
       "   1.0385038781166076,\n",
       "   1.0249879093964895],\n",
       "  'val_loss_std': [0.09708733059966176,\n",
       "   0.1066284357764446,\n",
       "   0.1116230535455675,\n",
       "   0.11663567423323025,\n",
       "   0.12459084386835202,\n",
       "   0.12025391836368005,\n",
       "   0.12563620542382686,\n",
       "   0.12440320847652488,\n",
       "   0.13556751081861665,\n",
       "   0.12842333454396557,\n",
       "   0.13463306975051562,\n",
       "   0.13989152686149403,\n",
       "   0.13177164263698618,\n",
       "   0.1372604540402876,\n",
       "   0.13715515208709173,\n",
       "   0.14138161867373292,\n",
       "   0.15182702610687812,\n",
       "   0.13420398479865947,\n",
       "   0.14196401716501908,\n",
       "   0.14124464864894312,\n",
       "   0.14408938620932676,\n",
       "   0.15058056303432468,\n",
       "   0.14339648924801732,\n",
       "   0.13953630267668812,\n",
       "   0.1453057734787438,\n",
       "   0.1496927656326886,\n",
       "   0.14889813892489093,\n",
       "   0.1472005354234091,\n",
       "   0.14384213110217164,\n",
       "   0.1484361899329593,\n",
       "   0.1459925739367154,\n",
       "   0.14639220525207056,\n",
       "   0.14261803744521348,\n",
       "   0.15886012467046254,\n",
       "   0.14908512703553103,\n",
       "   0.14925676661814558,\n",
       "   0.14413248307997165,\n",
       "   0.15734364734201517,\n",
       "   0.1500621858801472,\n",
       "   0.14655682631908032,\n",
       "   0.14631547104979822,\n",
       "   0.1527224938667584,\n",
       "   0.14996944274434168,\n",
       "   0.14321718868183358,\n",
       "   0.14546954412552426,\n",
       "   0.1412954236595394,\n",
       "   0.14489076251971242,\n",
       "   0.1409699290366351,\n",
       "   0.15169246131671746,\n",
       "   0.14772785498023547,\n",
       "   0.15008414842968887,\n",
       "   0.1500212561693215,\n",
       "   0.15636922159065192,\n",
       "   0.1565944998542774,\n",
       "   0.14530977088535557,\n",
       "   0.1557130264014539,\n",
       "   0.1523597969015609,\n",
       "   0.1502655370776341,\n",
       "   0.1529512032105393,\n",
       "   0.1441824176052424,\n",
       "   0.14904211467841466,\n",
       "   0.14363859959820086,\n",
       "   0.14757986754268188,\n",
       "   0.14130961144336976,\n",
       "   0.1539527777329747,\n",
       "   0.14317736541232154,\n",
       "   0.14867304312713508,\n",
       "   0.14931756024130305,\n",
       "   0.15123813330648975,\n",
       "   0.14639557263005398,\n",
       "   0.14939403345865754,\n",
       "   0.1503390495952206,\n",
       "   0.15284461413405753,\n",
       "   0.14675256641758033,\n",
       "   0.15095559006707507,\n",
       "   0.1431507730138981,\n",
       "   0.1471243161760993,\n",
       "   0.14209013478698113,\n",
       "   0.14307148645079973,\n",
       "   0.14282394094720882,\n",
       "   0.1527637128977847,\n",
       "   0.14084977454401656,\n",
       "   0.14587887846092887,\n",
       "   0.14981662548009175,\n",
       "   0.13979693896077497,\n",
       "   0.14612003146748476,\n",
       "   0.14749153803182166,\n",
       "   0.1531360712094259,\n",
       "   0.15030248465439353,\n",
       "   0.151442801183937,\n",
       "   0.15105044545148544,\n",
       "   0.14299421943489624,\n",
       "   0.14616744018236605,\n",
       "   0.14982387533719868,\n",
       "   0.1455549074216591,\n",
       "   0.14826558761129938,\n",
       "   0.14714854156470586,\n",
       "   0.14414438545934394,\n",
       "   0.14567361145333196],\n",
       "  'val_accuracy_mean': [0.3998666676382224,\n",
       "   0.43864444543917974,\n",
       "   0.4738000013430913,\n",
       "   0.4829777772227923,\n",
       "   0.4914222219586372,\n",
       "   0.4902888885140419,\n",
       "   0.5108666669329007,\n",
       "   0.5142222224672636,\n",
       "   0.515977776646614,\n",
       "   0.5171333341797193,\n",
       "   0.5267555552721024,\n",
       "   0.5230444447199504,\n",
       "   0.5381555557250977,\n",
       "   0.5445333334803582,\n",
       "   0.5439111132423083,\n",
       "   0.5515333334604899,\n",
       "   0.530999998152256,\n",
       "   0.5582666664322218,\n",
       "   0.5527777771155039,\n",
       "   0.5581999991337458,\n",
       "   0.5628444451093674,\n",
       "   0.5694666665792465,\n",
       "   0.5735777776439984,\n",
       "   0.5701555550098419,\n",
       "   0.5699333337942759,\n",
       "   0.5732222218314806,\n",
       "   0.5720222216844558,\n",
       "   0.5695111104846,\n",
       "   0.5774888876080513,\n",
       "   0.5816888877749443,\n",
       "   0.5831777761379878,\n",
       "   0.5676666649182638,\n",
       "   0.5818888874848683,\n",
       "   0.5833555546402931,\n",
       "   0.5809111114343007,\n",
       "   0.5795111114780108,\n",
       "   0.5813333328564961,\n",
       "   0.5857111103336017,\n",
       "   0.5861111111442248,\n",
       "   0.5747555537025134,\n",
       "   0.5865999998648962,\n",
       "   0.5845777753988902,\n",
       "   0.5839999998609225,\n",
       "   0.590133333603541,\n",
       "   0.5842444422841072,\n",
       "   0.5939555541674296,\n",
       "   0.5887333323558172,\n",
       "   0.5876666642228763,\n",
       "   0.5850222234924635,\n",
       "   0.5863777767618498,\n",
       "   0.5872888876994451,\n",
       "   0.5979999984304111,\n",
       "   0.5888222209612528,\n",
       "   0.5875777777036031,\n",
       "   0.5926888869206111,\n",
       "   0.5881555539369583,\n",
       "   0.5997555549939474,\n",
       "   0.5944444434841474,\n",
       "   0.5965555543700855,\n",
       "   0.585199998319149,\n",
       "   0.5885555542508761,\n",
       "   0.5885999988516172,\n",
       "   0.5891111103693644,\n",
       "   0.5863777764638265,\n",
       "   0.5981111109256745,\n",
       "   0.6029333329200744,\n",
       "   0.5924222213029862,\n",
       "   0.58417777856191,\n",
       "   0.5863999988635381,\n",
       "   0.5945999989906947,\n",
       "   0.5933777765432994,\n",
       "   0.5970444439848264,\n",
       "   0.5843999993801117,\n",
       "   0.5970888874928156,\n",
       "   0.5975999998052915,\n",
       "   0.5928888863325119,\n",
       "   0.595844444334507,\n",
       "   0.5892444415887197,\n",
       "   0.5881333333253861,\n",
       "   0.5832444428404172,\n",
       "   0.5925777774055799,\n",
       "   0.5972666657964388,\n",
       "   0.5964222218592962,\n",
       "   0.6012222216526667,\n",
       "   0.5864666650692621,\n",
       "   0.5982666662335396,\n",
       "   0.5978888884186745,\n",
       "   0.5921777777870496,\n",
       "   0.5956666652361552,\n",
       "   0.5941111100713412,\n",
       "   0.5990666649738947,\n",
       "   0.5897555537025134,\n",
       "   0.5947555542985599,\n",
       "   0.5992666668693225,\n",
       "   0.5932444435358047,\n",
       "   0.5998888885974885,\n",
       "   0.6021333325902621,\n",
       "   0.5965333324670792,\n",
       "   0.5995777770876884],\n",
       "  'val_accuracy_std': [0.0574003222459537,\n",
       "   0.059385241680208,\n",
       "   0.06533506613675863,\n",
       "   0.06527905486163046,\n",
       "   0.06522592784962727,\n",
       "   0.06439281667226919,\n",
       "   0.06711618445344289,\n",
       "   0.06379906691258946,\n",
       "   0.06589530996062433,\n",
       "   0.06308786233694207,\n",
       "   0.06534007396877782,\n",
       "   0.06968334259578629,\n",
       "   0.06672223884866417,\n",
       "   0.06587674119350109,\n",
       "   0.06641591389773902,\n",
       "   0.06740824197090448,\n",
       "   0.06784759443779399,\n",
       "   0.06403438759809593,\n",
       "   0.0667409763132502,\n",
       "   0.06449476568829812,\n",
       "   0.07016529295847433,\n",
       "   0.06709316815422722,\n",
       "   0.06486977995968382,\n",
       "   0.06561711125537496,\n",
       "   0.06469869762910836,\n",
       "   0.06596398104649964,\n",
       "   0.06734917007855527,\n",
       "   0.0648089907906009,\n",
       "   0.06553533005890684,\n",
       "   0.06776729900462067,\n",
       "   0.0682878841783533,\n",
       "   0.06529278769251454,\n",
       "   0.0650844647891755,\n",
       "   0.068513310008534,\n",
       "   0.06435710109454904,\n",
       "   0.066722627495487,\n",
       "   0.06652874687310366,\n",
       "   0.06640709004768114,\n",
       "   0.0692102728772847,\n",
       "   0.06700176521844853,\n",
       "   0.06642285203254676,\n",
       "   0.06874595921861328,\n",
       "   0.06663554794478728,\n",
       "   0.06583130090502629,\n",
       "   0.06364647413564804,\n",
       "   0.06847047050788711,\n",
       "   0.06335808936114472,\n",
       "   0.06254154281902803,\n",
       "   0.06590199972392292,\n",
       "   0.06282301680441381,\n",
       "   0.06462127380345636,\n",
       "   0.06734873355371275,\n",
       "   0.06616660984894213,\n",
       "   0.06651916706948338,\n",
       "   0.06464054129796863,\n",
       "   0.06551226758118493,\n",
       "   0.06642020779690555,\n",
       "   0.06750546504689629,\n",
       "   0.061705293700438535,\n",
       "   0.06360931614245677,\n",
       "   0.06503892612832829,\n",
       "   0.06515141268124894,\n",
       "   0.06566093305971842,\n",
       "   0.062228296366343236,\n",
       "   0.0652322549267483,\n",
       "   0.06187432796880483,\n",
       "   0.06734476903411296,\n",
       "   0.06373113328652068,\n",
       "   0.06554559628239306,\n",
       "   0.06467258832416549,\n",
       "   0.06634811496843225,\n",
       "   0.06357968305815773,\n",
       "   0.06371385256901302,\n",
       "   0.06297300620640746,\n",
       "   0.06334049072214831,\n",
       "   0.06429407744073531,\n",
       "   0.06329757747080868,\n",
       "   0.06257339014186303,\n",
       "   0.06152323297959609,\n",
       "   0.06313412533062983,\n",
       "   0.06794131232731454,\n",
       "   0.06151732091198448,\n",
       "   0.06464099949362462,\n",
       "   0.0635130992919081,\n",
       "   0.06154399698484454,\n",
       "   0.0652717625244106,\n",
       "   0.06413517810819676,\n",
       "   0.0635411648767836,\n",
       "   0.06354496932975014,\n",
       "   0.06424826424214762,\n",
       "   0.0643556313819259,\n",
       "   0.061488446904360006,\n",
       "   0.06318033772981323,\n",
       "   0.06623568490206108,\n",
       "   0.06511749889509846,\n",
       "   0.06266302296849352,\n",
       "   0.06542542350507045,\n",
       "   0.06214544548325076,\n",
       "   0.06271132730416154],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6556666655341784,\n",
       " 'best_val_iter': 27000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 54,\n",
       " 'train_loss_mean': 0.477478191524744,\n",
       " 'train_loss_std': 0.1250028197980929,\n",
       " 'train_accuracy_mean': 0.8291599999666214,\n",
       " 'train_accuracy_std': 0.05516928511889338,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.937552748521169,\n",
       " 'val_loss_std': 0.14005408862324878,\n",
       " 'val_accuracy_mean': 0.6363555536667506,\n",
       " 'val_accuracy_std': 0.06325064268432667,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[-2.2942e-02, -5.1323e-01,  3.4699e-01],\n",
       "                         [-2.3975e-01,  2.0178e-02,  1.1792e-01],\n",
       "                         [-1.1916e-01,  3.9857e-01, -2.9855e-02]],\n",
       "               \n",
       "                        [[ 2.8082e-01, -3.7710e-01,  3.2376e-01],\n",
       "                         [-1.6387e-01,  1.1056e-01,  5.5978e-02],\n",
       "                         [-2.3164e-01,  1.6199e-01, -1.6589e-01]],\n",
       "               \n",
       "                        [[ 2.7834e-01, -2.0440e-01, -1.3289e-02],\n",
       "                         [ 6.4747e-02,  2.0243e-01, -2.8534e-01],\n",
       "                         [-5.0292e-02,  2.1952e-01, -2.2636e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.9931e-01,  6.3607e-01,  1.4439e-01],\n",
       "                         [ 6.3523e-02, -4.3499e-02,  4.3638e-02],\n",
       "                         [-3.9675e-01, -6.1118e-01, -1.7299e-01]],\n",
       "               \n",
       "                        [[-1.7901e-03,  2.3580e-01, -3.8468e-02],\n",
       "                         [ 2.3421e-02, -9.0706e-02, -7.9763e-02],\n",
       "                         [ 4.2246e-02, -2.5070e-01,  2.0299e-01]],\n",
       "               \n",
       "                        [[-2.9690e-02,  1.1731e-01, -1.7040e-01],\n",
       "                         [-8.4267e-02,  9.9170e-03,  9.1295e-02],\n",
       "                         [ 4.7462e-02, -7.1012e-02,  8.3238e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-9.0468e-02, -1.7381e-01,  2.1703e-01],\n",
       "                         [ 2.4936e-01, -2.0838e-01, -2.9733e-01],\n",
       "                         [ 1.0182e-01,  3.3432e-01, -1.6809e-01]],\n",
       "               \n",
       "                        [[-2.3581e-01, -1.2636e-01,  3.7825e-01],\n",
       "                         [ 2.8334e-01, -9.6847e-02, -6.1032e-02],\n",
       "                         [-3.9515e-01,  1.2086e-01,  1.1350e-01]],\n",
       "               \n",
       "                        [[ 7.0083e-02, -1.9091e-01,  1.4680e-01],\n",
       "                         [ 3.6481e-01, -7.0714e-02, -2.8135e-01],\n",
       "                         [-2.7920e-01,  2.0222e-01, -5.7191e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.6981e-01, -9.5101e-02, -2.9419e-01],\n",
       "                         [ 4.9809e-01, -1.7688e-01, -4.6034e-01],\n",
       "                         [ 4.3917e-01,  8.0561e-02, -2.8719e-01]],\n",
       "               \n",
       "                        [[-5.4237e-02,  7.9365e-02,  7.5868e-02],\n",
       "                         [-1.2598e-01,  1.8704e-02,  6.1193e-02],\n",
       "                         [-7.4538e-02,  1.7593e-02, -3.3036e-02]],\n",
       "               \n",
       "                        [[-1.4256e-01, -1.2577e-02,  1.5855e-01],\n",
       "                         [-2.3650e-01,  7.4495e-02,  2.7634e-01],\n",
       "                         [-3.0469e-01, -4.5964e-02,  2.4306e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.0674e-02,  4.3863e-02, -7.2779e-02],\n",
       "                         [ 8.5915e-02,  2.2172e-01,  8.7347e-02],\n",
       "                         [ 9.2837e-02,  6.9198e-02,  2.8271e-01]],\n",
       "               \n",
       "                        [[-6.3758e-02, -2.4566e-02, -3.3046e-01],\n",
       "                         [-3.4164e-01, -3.6346e-01, -2.0423e-02],\n",
       "                         [-3.1155e-01, -4.6835e-01, -2.2344e-01]],\n",
       "               \n",
       "                        [[ 2.0556e-01,  1.8200e-01, -1.9931e-01],\n",
       "                         [ 3.1299e-01,  1.7831e-01,  1.9151e-01],\n",
       "                         [ 7.7306e-02,  2.3913e-01,  3.3365e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.7938e-01, -2.0858e-01,  5.6528e-04],\n",
       "                         [-1.7425e-01, -9.1155e-02,  8.3869e-03],\n",
       "                         [ 2.1811e-02, -3.6614e-02,  2.4613e-01]],\n",
       "               \n",
       "                        [[-1.7372e-01, -2.8422e-01, -3.7533e-01],\n",
       "                         [-3.2153e-01, -3.8954e-01, -5.2928e-01],\n",
       "                         [-2.9942e-01, -2.4679e-01, -1.7140e-01]],\n",
       "               \n",
       "                        [[ 1.7762e-01,  3.3638e-01,  1.6964e-01],\n",
       "                         [ 1.2448e-01,  1.0720e-01,  2.2634e-02],\n",
       "                         [ 1.5306e-01,  2.0030e-01, -1.7971e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([ 0.0084,  0.0191,  0.0169,  0.0031,  0.0102, -0.0188, -0.0061,  0.0012,\n",
       "                        0.0112, -0.0085, -0.0049, -0.0025,  0.0113, -0.0069, -0.0160, -0.0060,\n",
       "                        0.0024,  0.0035, -0.0163, -0.0024,  0.0341,  0.0004, -0.0042, -0.0033,\n",
       "                       -0.0081, -0.0031,  0.0031,  0.0007,  0.0013,  0.0019, -0.0090,  0.0095,\n",
       "                       -0.0239,  0.0054, -0.0010, -0.0058, -0.0077,  0.0029,  0.0043, -0.0227,\n",
       "                       -0.0091,  0.0292,  0.0093,  0.0036,  0.0070,  0.0014,  0.0055, -0.0008],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.3786, -0.0895,  0.0372, -0.0771, -0.3881, -0.1374, -0.6407,  0.6620,\n",
       "                        0.2342, -0.5148,  2.8337, -0.7010, -0.4836, -1.0246, -0.1415,  0.4447,\n",
       "                       -0.5160, -0.3523,  0.2883, -0.5961,  0.7240, -0.5124, -0.0328,  0.4538,\n",
       "                        0.8699, -0.4778,  0.0873,  1.4304, -0.8277, -0.0576, -0.1220, -0.4430,\n",
       "                        0.2128, -0.2660, -0.2965, -0.4926, -0.4818,  0.2293, -0.6394, -0.1121,\n",
       "                       -0.1154, -0.0542, -0.2063, -0.0986, -0.0299, -0.1661,  0.4110, -0.6318],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.0555, 0.9757, 0.9629, 0.5619, 0.5726, 1.0142, 0.8088, 1.3242, 0.6852,\n",
       "                       1.1200, 0.6148, 1.0065, 0.6991, 0.8769, 1.1189, 0.6516, 0.7901, 0.6830,\n",
       "                       1.0597, 0.7662, 0.8345, 0.6992, 0.8191, 0.7875, 0.6861, 0.8327, 0.7085,\n",
       "                       0.6681, 0.7774, 0.8099, 1.1934, 0.7879, 1.2120, 1.1068, 0.5395, 0.7664,\n",
       "                       0.7149, 0.6890, 0.6327, 1.2182, 1.0558, 1.3346, 0.8121, 1.1467, 1.2484,\n",
       "                       1.3710, 0.8633, 0.6327], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-3.6473e-01, -4.0112e-02, -5.8233e-03],\n",
       "                         [-7.7518e-02,  1.3329e-01,  1.7318e-01],\n",
       "                         [ 1.1477e-01,  2.6262e-01, -8.0032e-02]],\n",
       "               \n",
       "                        [[ 3.1300e-01,  1.5301e-01, -1.1053e-01],\n",
       "                         [-1.9624e-01, -2.1137e-01,  4.2572e-02],\n",
       "                         [-1.7574e-01, -4.6474e-02, -4.8419e-03]],\n",
       "               \n",
       "                        [[ 2.7277e-01, -3.4236e-02, -8.1735e-02],\n",
       "                         [-9.4206e-02, -1.1476e-01, -3.1154e-01],\n",
       "                         [-2.7214e-01, -2.6645e-02,  1.9106e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.2289e-01, -2.2096e-01, -2.1876e-01],\n",
       "                         [-2.3146e-01,  1.4277e-01,  6.1704e-02],\n",
       "                         [-4.4952e-02,  2.6654e-01,  2.7717e-02]],\n",
       "               \n",
       "                        [[-2.5934e-01, -6.5039e-02, -3.5492e-02],\n",
       "                         [-2.8395e-01,  2.9760e-01,  9.0480e-02],\n",
       "                         [-2.0866e-01,  4.7836e-01,  1.5359e-01]],\n",
       "               \n",
       "                        [[-2.7426e-02,  1.0626e-01,  6.5720e-02],\n",
       "                         [-6.8558e-02,  1.0777e-01,  2.1116e-01],\n",
       "                         [ 6.1869e-02,  2.7812e-01,  2.9874e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.8433e-01,  1.3821e-01,  1.3654e-01],\n",
       "                         [ 2.3332e-01, -7.7663e-02,  8.9797e-02],\n",
       "                         [ 1.1008e-01, -8.5004e-02,  2.1039e-01]],\n",
       "               \n",
       "                        [[-2.3657e-01,  8.8427e-03,  1.0016e-01],\n",
       "                         [ 1.6524e-01, -2.2699e-01,  4.2571e-02],\n",
       "                         [ 1.8443e-01,  2.6340e-01, -5.7862e-02]],\n",
       "               \n",
       "                        [[ 1.2734e-01,  1.6781e-01,  3.1310e-01],\n",
       "                         [ 2.0597e-01, -3.9859e-02, -6.6469e-02],\n",
       "                         [-1.6053e-01, -9.8036e-02,  2.5372e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.7863e-01, -3.8849e-01, -3.0060e-01],\n",
       "                         [-7.1138e-01, -6.3812e-01, -9.9385e-02],\n",
       "                         [-5.3415e-02, -5.0265e-01,  1.7714e-02]],\n",
       "               \n",
       "                        [[ 2.1620e-01,  1.9614e-03, -3.6420e-02],\n",
       "                         [ 3.6514e-01, -9.2283e-02, -3.0138e-01],\n",
       "                         [ 2.4033e-01,  1.0728e-01, -1.7168e-01]],\n",
       "               \n",
       "                        [[-1.8181e-01, -1.5388e-01, -6.6486e-02],\n",
       "                         [-2.7995e-01, -1.5348e-01, -7.2717e-02],\n",
       "                         [-1.9017e-01, -1.4705e-01, -4.7858e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.3427e-02,  4.0369e-01, -1.1405e-01],\n",
       "                         [ 3.8847e-01,  4.1945e-01, -2.1807e-01],\n",
       "                         [ 2.5052e-01, -6.7016e-02, -1.8256e-01]],\n",
       "               \n",
       "                        [[-2.1034e-02, -7.6247e-02, -4.9441e-02],\n",
       "                         [ 3.0397e-01, -1.1004e-01, -2.6864e-01],\n",
       "                         [ 5.9315e-02, -2.7443e-01, -4.2410e-01]],\n",
       "               \n",
       "                        [[-3.7662e-01, -2.3602e-01,  2.9388e-01],\n",
       "                         [ 1.9726e-01, -1.9012e-01, -1.8295e-02],\n",
       "                         [ 1.4332e-01,  3.6027e-02,  6.6164e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.9985e-01, -1.1246e-01,  2.2630e-01],\n",
       "                         [-1.4074e-01, -5.4375e-02,  2.7199e-01],\n",
       "                         [ 1.1500e-01,  5.3119e-01, -1.4237e-01]],\n",
       "               \n",
       "                        [[ 2.0488e-01, -4.1750e-02, -1.6768e-01],\n",
       "                         [ 1.4990e-01, -2.0305e-01, -2.5946e-01],\n",
       "                         [ 2.0883e-01, -3.9907e-02, -1.3626e-01]],\n",
       "               \n",
       "                        [[-6.4501e-02,  1.2402e-01,  1.8912e-02],\n",
       "                         [-3.2261e-01, -1.5048e-01,  1.9180e-02],\n",
       "                         [-4.6793e-01,  1.3293e-02,  2.5547e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.6525e-01, -1.6403e-01, -6.8230e-02],\n",
       "                         [-2.8274e-01,  1.2114e-01,  1.1006e-01],\n",
       "                         [-3.3076e-02,  5.4885e-02,  4.0228e-01]],\n",
       "               \n",
       "                        [[ 1.0693e-01,  6.2132e-02, -6.2233e-03],\n",
       "                         [-1.1168e-01, -4.2224e-02,  6.0106e-02],\n",
       "                         [-6.5048e-02, -3.4805e-01, -3.4049e-01]],\n",
       "               \n",
       "                        [[-1.8662e-02, -1.4935e-01, -7.2401e-02],\n",
       "                         [-9.1347e-02,  3.2243e-01,  8.0350e-02],\n",
       "                         [-1.2787e-01,  2.0998e-01,  3.6199e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.1653e-01,  9.5059e-02,  5.9458e-02],\n",
       "                         [ 2.1819e-01, -3.5292e-01, -2.8408e-02],\n",
       "                         [-5.0232e-02, -2.6090e-01,  1.8615e-01]],\n",
       "               \n",
       "                        [[-1.3598e-01, -9.6671e-02,  3.3114e-01],\n",
       "                         [-4.7833e-02, -1.4225e-01,  2.3673e-01],\n",
       "                         [-7.9791e-02,  7.6075e-02,  2.2338e-02]],\n",
       "               \n",
       "                        [[-1.5512e-01, -1.8882e-01, -1.1988e-01],\n",
       "                         [-8.5661e-03, -5.2205e-02, -1.7985e-01],\n",
       "                         [-1.4299e-01, -1.9223e-01, -3.7548e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.6310e-02, -2.3718e-01, -1.8772e-01],\n",
       "                         [-4.3946e-01,  1.1655e-01, -1.8847e-01],\n",
       "                         [-2.0963e-01,  1.0038e-01,  1.9381e-01]],\n",
       "               \n",
       "                        [[ 3.5058e-01,  3.5961e-01,  2.0445e-01],\n",
       "                         [ 1.1239e-01,  7.1185e-02,  1.5605e-01],\n",
       "                         [ 5.0435e-02,  1.0832e-02,  9.4583e-02]],\n",
       "               \n",
       "                        [[-4.2203e-01, -1.4508e-01,  7.8811e-02],\n",
       "                         [-3.6888e-01,  4.5399e-02,  1.8544e-01],\n",
       "                         [-1.7978e-01, -2.4665e-01,  9.0967e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.1428e-02, -2.6137e-02, -7.9288e-02],\n",
       "                         [ 2.0496e-01, -1.0465e-01,  9.7384e-02],\n",
       "                         [ 4.0218e-01,  3.3593e-02,  3.0642e-01]],\n",
       "               \n",
       "                        [[-1.1199e-01,  8.1653e-02, -1.1104e-01],\n",
       "                         [-2.2317e-02,  2.2897e-01,  3.5053e-02],\n",
       "                         [ 3.3011e-01,  1.4389e-01,  9.3321e-02]],\n",
       "               \n",
       "                        [[-2.1959e-01,  2.9482e-01,  5.8973e-02],\n",
       "                         [-7.9925e-03,  1.4982e-01, -4.1320e-02],\n",
       "                         [ 4.9807e-02,  8.7692e-02, -1.4952e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.8591e-01, -1.2289e-01, -1.8411e-01],\n",
       "                         [ 2.2286e-01,  9.6124e-02, -2.8321e-02],\n",
       "                         [ 9.2329e-02,  2.5024e-01,  4.2374e-01]],\n",
       "               \n",
       "                        [[-4.1171e-02,  5.6783e-02,  1.5039e-01],\n",
       "                         [ 4.1655e-02,  2.0029e-01,  7.7433e-02],\n",
       "                         [-3.1412e-01,  2.7337e-02,  1.4667e-01]],\n",
       "               \n",
       "                        [[-2.0568e-01, -4.9265e-02,  2.8599e-01],\n",
       "                         [-3.1279e-01, -2.3635e-02,  1.8680e-01],\n",
       "                         [-8.2745e-02,  6.6839e-04,  1.3832e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.5223e-01, -5.0377e-01, -7.8561e-02],\n",
       "                         [-7.1766e-02, -8.1196e-02, -1.1797e-02],\n",
       "                         [ 1.8971e-01,  3.0405e-01,  8.3600e-02]],\n",
       "               \n",
       "                        [[-2.3506e-01, -1.8624e-01,  2.4475e-01],\n",
       "                         [-3.6893e-02, -1.4811e-01, -3.4322e-03],\n",
       "                         [ 2.4156e-01,  2.0924e-01, -1.1545e-01]],\n",
       "               \n",
       "                        [[-4.1549e-01, -4.3482e-01, -7.2097e-01],\n",
       "                         [-7.4617e-02, -6.7676e-02, -4.2421e-01],\n",
       "                         [ 2.5417e-01,  2.5544e-01,  1.4118e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-6.6048e-07,  9.1414e-06,  8.0851e-02,  2.5518e-06,  6.7065e-03,\n",
       "                        1.8735e-06, -1.5808e-06,  2.2132e-07, -5.3613e-04,  4.4162e-02,\n",
       "                        4.1695e-05,  1.3912e-06, -8.9676e-09,  2.2552e-06, -1.1169e-02,\n",
       "                       -1.0088e-07, -4.3977e-04,  7.5452e-06,  1.8670e-04,  1.4459e-06,\n",
       "                       -5.4347e-07, -1.4865e-06, -2.5093e-03,  7.5560e-07, -5.8523e-06,\n",
       "                       -3.9851e-05,  1.9142e-06,  2.3663e-06,  7.1043e-08,  2.3373e-06,\n",
       "                       -1.3157e-06,  2.9076e-04,  2.3799e-06, -5.1017e-07, -2.7853e-05,\n",
       "                       -6.3859e-07,  1.0699e-06,  1.1410e-04, -2.4062e-07, -7.6894e-07,\n",
       "                       -2.5853e-07, -8.6395e-07, -3.8953e-07, -9.1784e-07, -7.4163e-07,\n",
       "                       -9.8259e-05, -1.0236e-06, -7.7779e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.1452, -0.4804, -0.3735, -0.4230, -0.5313, -0.2646, -0.2209, -0.4143,\n",
       "                       -0.5259, -0.6089, -0.3575, -0.4518, -0.6807, -0.7685, -0.5851, -0.5710,\n",
       "                       -0.5275, -0.5675, -0.4184, -0.3656, -0.6497, -0.5162, -0.4020, -0.5044,\n",
       "                       -0.6724, -0.4604, -0.4812, -0.2243, -0.2080, -0.3384, -0.9345, -0.3100,\n",
       "                       -0.5405, -0.6498, -0.5532, -0.5971, -0.6090, -0.6206, -0.5784, -0.3248,\n",
       "                       -0.6202, -0.3400, -0.6785, -0.2055, -0.7218, -0.6030, -0.4025, -0.5858],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.4835, 0.8937, 0.7147, 0.8378, 0.9032, 0.8546, 0.7746, 0.6067, 0.8204,\n",
       "                       1.2157, 1.0183, 0.8821, 0.7089, 1.1356, 0.7542, 1.1541, 0.9071, 0.9221,\n",
       "                       0.8868, 0.6614, 0.9463, 0.9052, 0.7585, 0.9314, 0.8874, 1.2189, 0.9940,\n",
       "                       1.0114, 0.8121, 1.1409, 0.9996, 0.9480, 0.8821, 0.8679, 0.7892, 1.1620,\n",
       "                       0.7198, 1.0427, 0.9902, 0.8483, 1.1296, 0.9385, 1.0388, 0.7602, 0.9314,\n",
       "                       0.7646, 0.9847, 0.9696], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[ 9.8932e-02, -1.1531e-01, -2.1590e-01],\n",
       "                         [-1.9630e-01, -6.0750e-02, -1.0571e-01],\n",
       "                         [-1.9773e-02,  2.1758e-01, -3.3825e-02]],\n",
       "               \n",
       "                        [[ 4.8028e-02,  3.9696e-01,  5.4782e-01],\n",
       "                         [ 5.7055e-02,  2.5099e-01,  1.1618e-01],\n",
       "                         [ 9.2139e-02, -7.2377e-01, -1.2643e-01]],\n",
       "               \n",
       "                        [[-1.9213e-01, -5.0397e-02,  1.8088e-01],\n",
       "                         [-1.9224e-01, -1.6904e-01,  7.9672e-02],\n",
       "                         [-3.6693e-01, -2.2086e-01, -6.4367e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.8758e-01, -1.5978e-02,  1.6333e-01],\n",
       "                         [-8.9208e-02, -1.6072e-01,  1.8233e-01],\n",
       "                         [-1.0851e-01, -2.4792e-01,  1.2858e-01]],\n",
       "               \n",
       "                        [[ 6.4490e-01,  1.2786e-01, -3.0422e-02],\n",
       "                         [ 1.6224e-01, -2.4275e-02, -2.8951e-01],\n",
       "                         [ 4.0218e-02,  6.1067e-02, -4.6693e-02]],\n",
       "               \n",
       "                        [[ 7.9684e-02,  2.9326e-01,  8.6007e-02],\n",
       "                         [-2.3378e-01,  1.4808e-01,  1.1029e-01],\n",
       "                         [-5.0543e-01, -1.8171e-01, -4.8108e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.6674e-02,  7.0490e-04, -1.9575e-01],\n",
       "                         [-2.1091e-01, -8.3008e-02, -2.3395e-02],\n",
       "                         [-3.6885e-01,  1.5612e-02, -2.5314e-01]],\n",
       "               \n",
       "                        [[ 2.7819e-01,  1.1892e-01, -2.1323e-01],\n",
       "                         [ 5.3108e-02, -1.4488e-01,  3.5889e-01],\n",
       "                         [ 3.7265e-01, -2.1272e-01,  4.8622e-02]],\n",
       "               \n",
       "                        [[ 3.2488e-01,  2.2019e-01,  1.8886e-01],\n",
       "                         [ 1.2734e-01,  2.7634e-01,  2.1359e-01],\n",
       "                         [-2.0275e-01, -2.0766e-01, -4.1076e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.9522e-02,  2.7099e-01, -1.7575e-01],\n",
       "                         [-1.9239e-01,  2.9771e-01,  1.6933e-01],\n",
       "                         [ 2.4655e-01, -1.8770e-01, -2.2555e-01]],\n",
       "               \n",
       "                        [[ 4.6190e-02, -9.4371e-02, -3.2618e-01],\n",
       "                         [ 2.7703e-01, -8.7499e-02, -1.7652e-02],\n",
       "                         [ 3.4910e-02, -2.7530e-01, -4.1140e-01]],\n",
       "               \n",
       "                        [[ 1.4110e-01, -1.2463e-01,  4.6773e-02],\n",
       "                         [-2.9340e-01, -2.3219e-01, -1.4366e-01],\n",
       "                         [-4.5429e-02,  3.4306e-02, -4.0656e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.3417e-01, -1.8833e-02, -1.6221e-01],\n",
       "                         [ 4.0839e-02,  1.1122e-01,  2.1260e-02],\n",
       "                         [-9.5172e-02, -7.1145e-02, -1.2137e-01]],\n",
       "               \n",
       "                        [[ 4.6191e-02, -1.0267e-01,  1.0612e-01],\n",
       "                         [ 1.6416e-01,  2.2984e-01, -1.4395e-01],\n",
       "                         [-8.3228e-02, -3.5093e-01,  2.2076e-01]],\n",
       "               \n",
       "                        [[-3.1405e-01, -3.0261e-02,  1.8441e-03],\n",
       "                         [-4.6899e-01, -2.2167e-01, -1.0270e-01],\n",
       "                         [ 1.0856e-02, -7.9634e-02,  4.0050e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2368e-01, -2.0134e-01, -4.3982e-01],\n",
       "                         [-2.5098e-01, -3.8694e-01,  1.2861e-03],\n",
       "                         [-1.4772e-01, -1.8198e-01, -4.4373e-01]],\n",
       "               \n",
       "                        [[-3.7802e-02,  2.6889e-01, -9.5306e-02],\n",
       "                         [-1.6416e-01,  9.5318e-02, -1.4292e-01],\n",
       "                         [-1.6053e-01, -1.2155e-01, -5.6502e-02]],\n",
       "               \n",
       "                        [[ 6.0616e-01,  4.2022e-01, -4.6314e-01],\n",
       "                         [ 3.4366e-01, -1.2713e-01, -3.3460e-01],\n",
       "                         [-1.7840e-01, -1.6332e-01, -6.6107e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.7849e-01,  1.5502e-01, -1.0594e-01],\n",
       "                         [ 4.5747e-02,  2.5630e-01, -2.6004e-02],\n",
       "                         [ 2.2119e-01,  1.5996e-01, -1.4387e-01]],\n",
       "               \n",
       "                        [[-9.8062e-02, -1.7522e-01,  1.5493e-03],\n",
       "                         [-3.1516e-01, -3.7706e-01, -2.1686e-01],\n",
       "                         [ 4.9494e-02,  1.0257e-01,  1.0344e-01]],\n",
       "               \n",
       "                        [[ 1.7393e-01, -2.7720e-01,  1.3590e-01],\n",
       "                         [-1.2460e-01, -1.9478e-01,  7.3238e-02],\n",
       "                         [-4.8464e-03,  1.2942e-02, -3.5248e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1912e-01, -3.9872e-01, -4.1281e-01],\n",
       "                         [-3.3781e-01,  3.3951e-01, -1.9151e-01],\n",
       "                         [ 1.3106e-02,  4.1145e-01, -1.2855e-01]],\n",
       "               \n",
       "                        [[ 1.2681e-01,  3.8849e-02, -2.6013e-01],\n",
       "                         [ 1.0442e-01, -1.4226e-01, -5.0100e-01],\n",
       "                         [-4.1003e-02,  2.5558e-01, -4.5810e-01]],\n",
       "               \n",
       "                        [[ 3.6544e-01,  1.5849e-01,  1.9659e-01],\n",
       "                         [ 1.5300e-01,  2.1413e-01, -3.0124e-01],\n",
       "                         [ 4.6766e-01,  3.4526e-01, -1.0218e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.2898e-02, -5.8362e-02, -1.2010e-01],\n",
       "                         [-2.2170e-01, -3.3008e-01, -4.1006e-01],\n",
       "                         [-2.2024e-01,  3.5493e-02,  7.0047e-02]],\n",
       "               \n",
       "                        [[-2.5349e-01,  6.9771e-02,  5.4015e-02],\n",
       "                         [-3.4874e-01,  2.0941e-02,  4.2399e-01],\n",
       "                         [ 6.3069e-02,  2.4317e-01,  2.9644e-01]],\n",
       "               \n",
       "                        [[-8.2439e-02,  4.9355e-01,  1.0440e-01],\n",
       "                         [-1.6102e-01, -1.2412e-02, -1.1202e-01],\n",
       "                         [ 1.5589e-02, -2.4527e-01,  2.5217e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.7245e-02, -2.0611e-03,  5.4128e-02],\n",
       "                         [ 9.8916e-02, -1.7254e-01,  1.8345e-01],\n",
       "                         [-5.0226e-01, -4.4856e-01, -5.6195e-01]],\n",
       "               \n",
       "                        [[ 2.4963e-01,  4.6462e-02, -4.0470e-01],\n",
       "                         [-2.5633e-01, -1.2446e-01,  8.6971e-02],\n",
       "                         [-2.8040e-01, -6.4567e-02,  1.8045e-01]],\n",
       "               \n",
       "                        [[-3.3467e-01, -2.4820e-01,  1.0833e-01],\n",
       "                         [ 8.5592e-02, -6.6720e-02,  2.1079e-01],\n",
       "                         [-1.1589e-01,  1.9220e-01, -4.1095e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-9.1594e-02,  6.7274e-02, -9.4313e-02],\n",
       "                         [ 3.8809e-02, -5.3072e-03, -2.0057e-02],\n",
       "                         [ 1.5991e-01, -4.2559e-01, -1.9277e-01]],\n",
       "               \n",
       "                        [[ 1.0643e-01,  4.8187e-01,  2.8334e-01],\n",
       "                         [ 1.8022e-01,  3.0712e-01,  2.0872e-01],\n",
       "                         [-1.8646e-01, -3.1433e-01, -2.6716e-01]],\n",
       "               \n",
       "                        [[ 3.5844e-01,  9.6758e-02, -2.2941e-02],\n",
       "                         [ 4.2312e-01, -1.1746e-01,  8.0113e-02],\n",
       "                         [ 2.8050e-01, -1.6815e-01,  7.9450e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.3443e-01, -4.5141e-01, -5.0010e-01],\n",
       "                         [-1.8321e-01, -1.7587e-01, -2.9376e-01],\n",
       "                         [-2.8259e-01,  1.1199e-02,  1.6369e-01]],\n",
       "               \n",
       "                        [[ 3.5061e-01,  1.1439e-01,  3.6183e-01],\n",
       "                         [-4.2381e-01,  8.2156e-02, -2.4876e-02],\n",
       "                         [-5.9844e-01, -6.4083e-02,  1.2755e-01]],\n",
       "               \n",
       "                        [[-3.5920e-01,  9.8480e-02,  4.8244e-02],\n",
       "                         [-1.3543e-01, -9.5744e-02, -2.0936e-01],\n",
       "                         [ 2.7985e-01,  2.7615e-02,  4.0154e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 3.1673e-07, -1.2554e-06, -1.8217e-05,  2.5735e-05, -4.7073e-05,\n",
       "                        5.9343e-07, -7.4059e-06,  3.4797e-07,  9.9643e-07,  6.0451e-05,\n",
       "                       -9.3746e-07, -6.7640e-07, -2.0093e-06,  7.3831e-06,  2.1272e-06,\n",
       "                        1.1097e-07, -1.1204e-06, -4.1625e-03,  1.3486e-05,  7.9453e-08,\n",
       "                       -6.2104e-07, -6.0103e-06,  4.9982e-05, -1.8502e-05,  5.3227e-05,\n",
       "                        1.6698e-06,  1.5726e-05,  1.1196e-04,  1.8844e-06,  1.1062e-06,\n",
       "                       -7.8516e-07,  2.9177e-07, -9.9098e-02, -5.8207e-06,  8.0439e-05,\n",
       "                       -2.1584e-05, -1.5802e-06, -3.0011e-05,  7.5731e-07,  2.2659e-06,\n",
       "                        1.8130e-06, -1.8327e-04, -1.8993e-04, -4.2161e-06, -6.1411e-07,\n",
       "                       -7.3136e-05,  7.1835e-05, -1.3822e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.5950, -0.6266, -0.7896, -1.0163, -0.4129, -0.4480, -1.3226, -0.5879,\n",
       "                       -0.4249, -0.4517, -0.6808, -0.6662, -0.5773, -0.5842, -1.1731, -0.7673,\n",
       "                       -0.2605, -0.7563, -0.4284, -0.7638, -1.4478, -0.6612, -0.6664, -0.7714,\n",
       "                       -1.2354, -0.5339, -0.8580, -0.7975, -0.7200, -0.6948, -1.1063, -0.6119,\n",
       "                       -0.2095, -0.4087, -0.7075, -0.4443, -0.6252, -0.8572, -1.0818, -0.7114,\n",
       "                       -0.8089, -1.1827, -0.9073, -0.6656, -0.7301, -0.7758, -0.6947, -0.8192],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.6968, 0.6877, 0.9484, 1.0820, 0.7283, 0.6955, 1.1923, 0.6299, 0.5874,\n",
       "                       0.8089, 0.7546, 0.6882, 0.7331, 0.6640, 1.1180, 0.7925, 0.4426, 0.9093,\n",
       "                       0.6518, 0.8528, 1.3338, 0.8168, 0.8067, 0.8724, 1.0732, 0.6722, 0.8721,\n",
       "                       1.0137, 0.6581, 0.5783, 1.1740, 0.5937, 0.4392, 0.5244, 0.9953, 0.7080,\n",
       "                       0.7667, 1.0709, 0.9736, 0.7531, 0.9271, 1.3014, 0.9339, 0.7462, 0.8941,\n",
       "                       0.8167, 0.6823, 0.8329], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-7.2285e-02,  1.2952e-01,  1.3632e-01],\n",
       "                         [ 3.6227e-02,  1.2858e-01, -1.2034e-02],\n",
       "                         [ 2.0528e-02,  3.9409e-02,  4.0717e-02]],\n",
       "               \n",
       "                        [[-1.8158e-02,  5.6468e-02,  3.2166e-02],\n",
       "                         [ 2.1484e-02,  4.3788e-02, -4.4769e-02],\n",
       "                         [ 4.8750e-02, -1.5219e-02, -1.9417e-02]],\n",
       "               \n",
       "                        [[-1.7505e-04, -2.0318e-02,  1.2336e-02],\n",
       "                         [ 4.1532e-02, -4.2223e-02,  4.3268e-02],\n",
       "                         [-6.5873e-02, -3.9985e-02, -1.7348e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.4256e-02,  9.9961e-02,  1.6907e-01],\n",
       "                         [-2.3336e-02,  8.1654e-02,  4.4574e-02],\n",
       "                         [-7.6280e-02,  5.3562e-02,  1.7158e-05]],\n",
       "               \n",
       "                        [[-8.6221e-03,  2.1814e-02, -3.2022e-02],\n",
       "                         [ 4.9186e-02, -1.5249e-03,  2.0264e-02],\n",
       "                         [ 3.1644e-02,  3.9076e-02,  4.2679e-02]],\n",
       "               \n",
       "                        [[ 8.8628e-02,  5.9010e-03, -9.5360e-03],\n",
       "                         [ 4.0630e-02, -1.3643e-03, -4.8556e-02],\n",
       "                         [-4.5290e-02,  6.6330e-03, -7.6175e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.9443e-03,  2.0421e-01,  3.2105e-02],\n",
       "                         [ 1.8555e-01,  8.0912e-02,  2.3156e-01],\n",
       "                         [ 1.5879e-01,  2.4014e-01,  4.7352e-02]],\n",
       "               \n",
       "                        [[ 7.9789e-02, -2.6131e-03, -1.0554e-01],\n",
       "                         [ 1.5561e-01, -4.5533e-02,  6.0557e-02],\n",
       "                         [ 1.4930e-02,  4.9609e-02, -1.5226e-01]],\n",
       "               \n",
       "                        [[ 1.8360e-02,  2.1838e-02, -8.3459e-02],\n",
       "                         [-5.7114e-02, -2.3444e-02, -2.6835e-01],\n",
       "                         [ 1.0096e-01, -1.9964e-01, -2.1654e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.9130e-02, -1.7729e-01, -2.1150e-01],\n",
       "                         [-1.1243e-03, -3.5394e-01, -2.6151e-01],\n",
       "                         [-2.1897e-02, -8.7807e-02, -3.0872e-01]],\n",
       "               \n",
       "                        [[-6.7911e-02,  4.1333e-01,  1.5892e-01],\n",
       "                         [ 4.8056e-02, -1.5900e-01, -1.7240e-01],\n",
       "                         [-1.2741e-01, -1.0246e-01, -4.3390e-02]],\n",
       "               \n",
       "                        [[ 2.8456e-01,  6.3790e-02, -2.6119e-02],\n",
       "                         [-3.9936e-02, -1.0685e-01, -7.9973e-03],\n",
       "                         [-2.5163e-01, -1.2238e-01,  1.4365e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.7789e-01, -1.9903e-02,  2.4218e-02],\n",
       "                         [-4.6813e-02, -9.2524e-02, -6.0376e-01],\n",
       "                         [-1.9057e-01, -2.9343e-01, -2.3600e-01]],\n",
       "               \n",
       "                        [[ 2.6817e-02,  4.8376e-02, -1.9665e-01],\n",
       "                         [-1.9563e-01, -3.0930e-01, -1.4733e-01],\n",
       "                         [ 1.6156e-01, -2.1107e-01, -2.4042e-01]],\n",
       "               \n",
       "                        [[ 4.3817e-02,  5.7814e-02,  1.0771e-01],\n",
       "                         [ 2.0361e-01,  5.6917e-02,  1.8237e-01],\n",
       "                         [ 4.1821e-02,  2.9812e-01,  1.4479e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.4471e-01, -2.7030e-01,  1.4983e-02],\n",
       "                         [-3.8585e-01,  2.0177e-01, -1.4218e-01],\n",
       "                         [-6.5440e-01, -3.7878e-01,  8.5304e-02]],\n",
       "               \n",
       "                        [[ 9.0403e-02,  3.1498e-01,  1.2600e-01],\n",
       "                         [ 2.6901e-01,  1.0399e-01,  2.0562e-01],\n",
       "                         [ 2.3581e-01,  3.5275e-01,  2.7580e-01]],\n",
       "               \n",
       "                        [[-7.4436e-02,  1.7195e-01,  3.5744e-03],\n",
       "                         [-7.2357e-02, -2.0117e-01,  1.5608e-01],\n",
       "                         [-5.1731e-01, -3.4342e-01, -1.7674e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 7.7162e-02,  1.6626e-01, -6.6978e-02],\n",
       "                         [ 3.2814e-01,  1.8964e-01, -5.1258e-02],\n",
       "                         [ 1.8511e-01,  2.8761e-01,  2.9791e-02]],\n",
       "               \n",
       "                        [[-9.2085e-02, -6.1773e-02,  4.0806e-02],\n",
       "                         [ 1.1633e-02,  1.9499e-02,  3.7593e-02],\n",
       "                         [-2.6724e-01, -1.7222e-01,  6.7344e-03]],\n",
       "               \n",
       "                        [[-1.0094e-01, -8.9701e-02, -2.0049e-01],\n",
       "                         [-2.5369e-01,  9.4932e-03,  1.5203e-01],\n",
       "                         [-2.5336e-01,  2.0305e-02, -9.5816e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.9007e-01,  1.1876e-01,  1.1593e-01],\n",
       "                         [-2.1117e-01,  2.5078e-01,  7.9219e-03],\n",
       "                         [-1.2710e-01, -4.3465e-02,  2.5163e-01]],\n",
       "               \n",
       "                        [[ 1.6206e-01, -6.7327e-02, -3.6065e-02],\n",
       "                         [ 9.6293e-02, -7.2192e-02, -1.6491e-01],\n",
       "                         [ 1.6045e-01,  1.4076e-01,  2.2104e-01]],\n",
       "               \n",
       "                        [[-2.2884e-02, -1.1490e-01, -9.9213e-02],\n",
       "                         [-2.1032e-01, -1.4729e-01, -2.9446e-01],\n",
       "                         [ 1.9447e-01,  4.5458e-02, -4.2567e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.9006e-01,  2.3254e-01,  2.2924e-01],\n",
       "                         [ 2.9871e-01,  7.7600e-02, -7.4937e-02],\n",
       "                         [ 1.8113e-01, -8.1734e-02, -1.9249e-01]],\n",
       "               \n",
       "                        [[ 9.5582e-02, -7.0885e-02, -3.0405e-01],\n",
       "                         [ 2.3104e-03, -2.1753e-01,  7.3426e-02],\n",
       "                         [ 1.9939e-02,  2.5372e-01,  1.8808e-01]],\n",
       "               \n",
       "                        [[-6.2163e-02,  2.9615e-01,  1.1705e-01],\n",
       "                         [ 3.3130e-02, -3.1189e-02, -1.2653e-01],\n",
       "                         [-3.4063e-02, -1.0585e-01, -2.0637e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.7107e-01,  1.7341e-01,  1.0122e-01],\n",
       "                         [-4.1102e-01,  4.4307e-02,  1.5873e-01],\n",
       "                         [-1.8565e-01,  7.5498e-02,  1.2226e-01]],\n",
       "               \n",
       "                        [[ 1.4071e-02,  6.0967e-02, -3.0689e-01],\n",
       "                         [-6.4715e-02, -3.9008e-01, -1.6507e-01],\n",
       "                         [-8.5952e-02, -2.9506e-03, -3.8241e-02]],\n",
       "               \n",
       "                        [[ 4.1687e-02, -3.9496e-02,  3.6047e-01],\n",
       "                         [-7.2736e-03, -9.7493e-02,  1.1206e-01],\n",
       "                         [-1.5341e-02, -4.3962e-02,  1.6203e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.7172e-02,  9.4900e-02,  1.0673e-01],\n",
       "                         [ 1.5826e-02,  1.4813e-01,  4.0323e-02],\n",
       "                         [ 1.8971e-02, -2.3944e-03, -3.8908e-02]],\n",
       "               \n",
       "                        [[ 1.8795e-03,  5.8293e-02,  2.0998e-02],\n",
       "                         [-3.7437e-02,  6.2186e-02, -3.7032e-02],\n",
       "                         [ 7.9786e-02,  8.2380e-03, -6.4565e-02]],\n",
       "               \n",
       "                        [[ 4.0801e-02,  2.2584e-02, -1.3347e-03],\n",
       "                         [ 3.3124e-02,  4.2179e-02, -2.0561e-02],\n",
       "                         [-6.0494e-02, -4.0668e-02, -4.5550e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.6802e-02,  1.6957e-01,  1.3878e-01],\n",
       "                         [ 3.6247e-02,  8.5175e-03,  2.9759e-02],\n",
       "                         [-5.3314e-03,  2.1580e-02,  3.0749e-02]],\n",
       "               \n",
       "                        [[-2.5731e-02,  2.2599e-02, -5.8043e-02],\n",
       "                         [ 9.9285e-02, -3.1215e-02,  5.0025e-02],\n",
       "                         [-1.0269e-02,  7.7690e-02, -1.7112e-02]],\n",
       "               \n",
       "                        [[ 4.8452e-02, -4.8495e-02, -1.1780e-01],\n",
       "                         [ 6.3499e-02,  3.2541e-02, -3.1135e-02],\n",
       "                         [-1.0394e-02,  6.6096e-03, -5.8675e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 8.7020e-06, -3.8275e-07, -1.3180e-06, -2.9053e-08,  1.0229e-06,\n",
       "                       -3.4953e-05, -3.4615e-06,  1.0914e-07,  4.5877e-07, -4.3072e-07,\n",
       "                       -9.9929e-07, -9.1894e-06,  8.9930e-02,  3.1831e-06, -1.1492e-06,\n",
       "                        1.4394e-07,  9.5139e-08, -3.2362e-07, -8.3064e-07,  1.1420e-06,\n",
       "                       -7.7577e-07,  9.7474e-06,  1.0281e-06, -6.2394e-07,  1.8810e-07,\n",
       "                       -1.9100e-05, -7.5671e-09,  2.8859e-06,  2.4547e-06,  6.0552e-07,\n",
       "                        4.0530e-06,  4.8651e-06,  2.7076e-07, -7.6341e-07,  8.3423e-09,\n",
       "                        1.0287e-07,  3.4171e-05, -3.2073e-08,  6.0822e-07, -6.0733e-06,\n",
       "                       -2.1206e-06, -3.4283e-05, -6.8856e-05, -3.4757e-06, -1.8367e-06,\n",
       "                       -2.9971e-08,  3.4818e-08,  2.8426e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.5155, -0.4130, -0.6322, -0.3571, -0.5585, -0.6454, -0.5449, -0.5896,\n",
       "                       -0.8937, -0.6988, -0.1839, -0.8066, -0.3286, -0.6241, -0.6140, -0.6388,\n",
       "                       -0.7701, -0.6477, -0.8078, -0.6693, -0.4274, -0.4952, -0.7127, -0.8579,\n",
       "                       -0.2085, -0.3866, -0.2037, -0.5033, -0.6021, -0.7046, -0.7324, -0.4417,\n",
       "                       -0.1724, -0.5739, -0.3354, -0.6650, -0.2828, -0.1740, -0.7040, -0.4580,\n",
       "                       -0.4270, -0.4962, -0.2742, -0.4916, -0.5018, -0.3147, -0.3335, -0.5186],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.4616, 0.5463, 0.5748, 0.1332, 0.5035, 0.5486, 0.5458, 0.2333, 0.4373,\n",
       "                       0.6406, 0.2840, 0.2811, 0.3924, 0.2508, 0.5767, 0.2125, 0.2554, 0.6407,\n",
       "                       0.2883, 0.3532, 0.4040, 0.4795, 0.3513, 0.4177, 0.2701, 0.4858, 0.0749,\n",
       "                       0.5000, 0.6172, 0.2417, 0.6116, 0.5677, 0.0564, 0.2390, 0.1002, 0.1266,\n",
       "                       0.1073, 0.0728, 0.3779, 0.3493, 0.5073, 0.4559, 0.3896, 0.2716, 0.4922,\n",
       "                       0.1162, 0.1662, 0.4770], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[ 0.0202, -0.1535, -0.2425,  ..., -0.1886, -0.1033, -0.1707],\n",
       "                       [-0.0925, -0.2597, -0.3819,  ..., -0.2293, -0.1366, -0.1484],\n",
       "                       [ 0.0249, -0.3184, -0.2924,  ..., -0.2028, -0.1277, -0.1888],\n",
       "                       [ 0.0694,  0.7111,  0.7824,  ..., -0.2517, -0.1356, -0.1499],\n",
       "                       [-0.1050, -0.2535, -0.3359,  ...,  0.5270,  0.3306,  0.4626]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0324, -0.0125, -0.0333, -0.0278, -0.0081], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[-2.3126e-01, -1.5249e-01,  8.8088e-02,  1.2593e-01, -7.8502e-02,\n",
       "                        -1.1202e-01, -8.0736e-02,  2.5986e-01, -2.2481e-01, -4.0364e-02,\n",
       "                         1.5610e-01,  9.5269e-02,  3.8862e-02,  1.6429e-02,  5.4435e-02,\n",
       "                         4.8785e-02, -1.7130e-01,  1.5534e-01,  1.2717e-01, -1.0206e-01],\n",
       "                       [ 1.0613e+00, -1.2954e+00,  6.0683e-01, -6.7394e-01,  9.3297e-01,\n",
       "                        -6.7736e-01,  1.0187e+00, -5.5808e-01,  5.5797e-01, -6.9031e-01,\n",
       "                        -1.0050e+00, -7.1359e-01, -5.3640e-01, -6.0335e-01, -7.0932e-01,\n",
       "                        -8.5977e-01, -3.7685e-01, -5.2492e-01,  4.5410e-01, -8.8876e-01],\n",
       "                       [-1.7650e-01, -7.4263e-02, -1.9603e-01,  1.7395e-01, -1.2017e-01,\n",
       "                         1.5592e-01, -2.8765e-03,  7.1014e-02,  7.4614e-02,  1.7035e-01,\n",
       "                        -1.8075e-01, -3.1389e-03, -6.3424e-02,  4.2344e-02, -8.9193e-02,\n",
       "                         1.8377e-01, -5.3476e-02,  1.4562e-01, -1.4323e-01, -1.9829e-01],\n",
       "                       [ 6.5726e-01, -7.0299e-01,  6.7191e-01, -7.5874e-01,  8.3528e-01,\n",
       "                        -7.3585e-01,  9.1340e-01, -4.8081e-01,  4.7174e-01, -6.3051e-01,\n",
       "                        -7.6995e-01, -7.1814e-01, -4.7943e-01, -8.8832e-01, -6.9014e-01,\n",
       "                        -5.6771e-01, -5.7030e-01, -8.5391e-01, -1.2410e-01, -5.8533e-01],\n",
       "                       [ 5.6687e-01, -1.5991e+00,  9.9012e-01, -7.0767e-01,  6.3072e-01,\n",
       "                        -7.4149e-01,  9.6615e-01, -8.3849e-01,  3.5042e-01, -8.9985e-01,\n",
       "                         1.2931e-01, -8.7115e-01, -7.4728e-01, -8.3028e-01, -7.5495e-01,\n",
       "                        -6.0420e-01, -3.5904e-01, -8.4235e-01,  3.9576e-01, -6.8380e-01],\n",
       "                       [-1.4191e-01,  9.4669e-02, -1.5038e-01, -1.4658e-02,  1.4405e-03,\n",
       "                         1.4693e-01,  4.8979e-03, -6.1230e-02, -1.7574e-01,  1.5559e-01,\n",
       "                         1.7111e-01, -1.6245e-01, -6.6190e-02, -9.3800e-02, -1.1772e-01,\n",
       "                         1.9119e-01,  5.5412e-02, -1.3313e-01, -8.8193e-02, -7.0477e-02],\n",
       "                       [-3.1918e-03,  1.6702e-01, -2.5259e-02, -5.8045e-02, -1.9568e-01,\n",
       "                        -1.8738e-01, -1.0919e-01, -2.0999e-01, -1.4216e-01,  3.0850e-02,\n",
       "                        -1.5226e-01,  1.8288e-01,  1.7433e-01, -8.3347e-02, -3.3173e-03,\n",
       "                         2.0585e-01,  3.2247e-02, -1.9702e-02, -1.5549e-01, -3.7919e-02],\n",
       "                       [-5.2119e-02,  5.8366e-02, -9.5376e-02,  4.1151e-02, -1.4242e-02,\n",
       "                        -1.3989e-01, -2.9618e-02, -4.6825e-02,  1.5251e-02, -5.1655e-03,\n",
       "                        -1.6485e-01,  1.5387e-01,  1.4536e-01,  3.0741e-02, -1.5371e-01,\n",
       "                         2.2002e-02, -5.1757e-02, -5.1059e-02, -1.4909e-02,  2.1917e-01],\n",
       "                       [ 8.1635e-01, -5.1851e-01,  8.1091e-01, -5.1868e-01,  8.8691e-01,\n",
       "                        -7.2625e-01,  8.1301e-01, -7.5557e-01,  7.7062e-01, -5.7799e-01,\n",
       "                        -8.1891e-01, -7.8807e-01, -4.7914e-01, -4.7465e-01, -8.6371e-01,\n",
       "                        -7.5349e-01, -6.0449e-01, -4.9772e-01, -9.8340e-02, -5.6282e-01],\n",
       "                       [ 8.8590e-01, -8.9171e-01,  6.8789e-01, -4.9882e-01,  8.5729e-01,\n",
       "                        -5.9152e-01,  7.0951e-01, -6.7555e-01,  6.7904e-01, -6.8043e-01,\n",
       "                        -8.0204e-01, -7.1268e-01, -1.0067e+00, -8.2626e-01, -8.7671e-01,\n",
       "                        -8.5335e-01, -6.5277e-01, -6.9748e-01,  1.7417e-01, -5.8834e-01],\n",
       "                       [ 8.1869e-01, -6.7591e-01,  5.7524e-01, -7.0863e-01,  6.5656e-01,\n",
       "                        -5.4038e-01,  8.2694e-01, -5.2491e-01,  1.2526e+00, -4.3053e-01,\n",
       "                        -1.5109e+00, -3.8262e-01, -9.8378e-01, -3.1709e-01, -9.9231e-01,\n",
       "                        -4.3986e-01, -9.0114e-01, -3.6594e-01,  9.4537e-02, -7.2334e-01],\n",
       "                       [-1.4173e-01,  1.8151e-01, -1.4494e-01,  2.1049e-01,  5.0123e-02,\n",
       "                         8.6908e-03,  1.1468e-01, -1.8511e-01,  2.0745e-01, -1.6201e-01,\n",
       "                        -1.1351e-01, -8.9195e-02,  2.3488e-02,  1.7444e-01, -1.1875e-01,\n",
       "                         8.5277e-02,  2.2247e-02, -3.2567e-02, -6.4565e-02,  2.3121e-01],\n",
       "                       [ 8.5641e-04,  7.8033e-02, -1.6028e-01,  3.1002e-03, -5.5865e-02,\n",
       "                         1.3764e-01, -1.2977e-01,  3.5730e-02,  1.8650e-01, -1.7713e-01,\n",
       "                        -1.1385e-01, -1.3867e-01,  1.0251e-01,  1.4555e-01, -1.7363e-01,\n",
       "                        -1.6093e-01,  1.0745e-01,  6.0484e-03,  3.7424e-02, -1.8967e-01],\n",
       "                       [ 5.1581e-02, -5.6762e-03, -1.3594e-01, -5.4797e-02,  8.6435e-02,\n",
       "                         1.2539e-02, -1.9003e-01,  2.1271e-01, -2.1802e-01, -2.8491e-02,\n",
       "                        -1.4078e-01,  6.5167e-02,  1.2081e-01, -1.8004e-01, -2.2841e-01,\n",
       "                        -2.9989e-02,  5.4311e-02, -2.5880e-02,  1.3265e-01, -1.9531e-01],\n",
       "                       [ 2.1353e-01,  2.0006e-01, -1.8942e-01,  7.9496e-02,  4.1478e-02,\n",
       "                        -1.1179e-01,  5.3119e-02,  1.8549e-01, -3.3323e-03, -1.2598e-01,\n",
       "                         2.5711e-02, -2.9604e-03, -2.0496e-02,  1.7962e-01, -1.9416e-02,\n",
       "                        -4.8471e-02, -7.4401e-02,  2.2016e-01, -8.8840e-03, -1.8109e-01],\n",
       "                       [ 6.2563e-01, -1.8436e+00,  8.8351e-01, -8.7205e-01,  8.7130e-01,\n",
       "                        -5.8424e-01,  6.8131e-01, -6.4999e-01,  7.4391e-02, -9.8012e-01,\n",
       "                        -1.8438e-01, -7.5027e-01, -7.0945e-01, -9.6888e-01, -7.8072e-01,\n",
       "                        -6.1998e-01, -2.9741e-01, -9.7928e-01,  1.1652e-01, -7.3769e-01],\n",
       "                       [ 8.5691e-01, -7.6635e-01,  6.7101e-01, -7.0076e-01,  7.5873e-01,\n",
       "                        -8.0651e-01,  5.7658e-01, -8.5658e-01,  6.6949e-01, -5.4281e-01,\n",
       "                        -1.6841e-01, -7.4174e-01, -6.6619e-01, -8.6188e-01, -5.6662e-01,\n",
       "                        -7.2463e-01, -5.1038e-01, -6.8463e-01,  3.8644e-02, -4.4553e-01],\n",
       "                       [ 1.5881e-01,  1.8694e-01, -9.5665e-02, -1.6845e-01,  7.3781e-02,\n",
       "                         1.4416e-01, -1.7335e-01, -1.3803e-01,  1.9204e-01, -1.5731e-01,\n",
       "                        -4.9296e-02,  2.4749e-02,  1.6346e-02,  1.5047e-01,  8.7614e-02,\n",
       "                        -4.4198e-02,  3.5807e-02, -1.8876e-01,  6.7670e-02,  1.8616e-01],\n",
       "                       [ 7.9702e-01, -1.1319e+00,  6.1157e-01, -6.7663e-01,  8.8199e-01,\n",
       "                        -7.6968e-01,  7.4581e-01, -7.3196e-01,  3.4411e-01, -7.6845e-01,\n",
       "                         2.1576e-02, -4.7091e-01, -5.5741e-01, -5.5828e-01, -6.4738e-01,\n",
       "                        -5.2360e-01, -4.2661e-01, -7.8090e-01,  2.3422e-01, -6.7514e-01],\n",
       "                       [ 8.9742e-02, -1.4763e-01,  1.2815e-01,  1.8031e-01, -2.2619e-01,\n",
       "                        -1.6096e-01,  4.1973e-02, -1.9481e-01, -1.8318e-01,  1.2415e-01,\n",
       "                        -1.2490e-01, -1.0639e-01, -5.7765e-02,  8.1214e-02,  1.6965e-01,\n",
       "                         2.4375e-02,  1.3092e-01,  1.7509e-01,  7.3107e-02,  2.2213e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([ 1.2503e-02,  8.9297e-01,  6.4409e-02,  7.0168e-01,  8.1756e-01,\n",
       "                       -6.4857e-02,  7.1124e-02, -2.0821e-01,  8.1017e-01,  6.9722e-01,\n",
       "                        8.0959e-01, -1.4202e-01,  5.5165e-02,  4.8305e-04, -9.7103e-03,\n",
       "                        6.0587e-01,  8.6854e-01, -1.4429e-01,  7.3198e-01,  2.2558e-03],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[-1.7376e-01, -1.2397e-01, -1.9416e-01,  7.6560e-02,  1.1020e-01,\n",
       "                         1.4656e-01,  3.6239e-02,  2.3965e-02,  2.0286e-02, -6.7065e-03,\n",
       "                        -3.9112e-01, -1.1422e-02, -5.0856e-02, -1.2952e-01, -1.1449e-01,\n",
       "                         2.3503e-02,  1.3481e-01, -1.8167e-02,  1.6775e-01, -1.9716e-01],\n",
       "                       [ 9.9007e-02,  1.9322e+00,  2.0148e-02,  1.6135e+00,  1.9169e+00,\n",
       "                         1.4606e-01,  1.4924e-01,  1.6985e-02,  1.7863e+00,  1.6088e+00,\n",
       "                         2.0909e+00, -1.0159e-01,  1.9558e-01, -1.6544e-01,  1.4378e-01,\n",
       "                         1.9422e+00,  2.0431e+00,  1.1773e-01,  1.8328e+00,  1.6398e-01],\n",
       "                       [ 9.2001e-03, -2.6113e-01, -2.0589e-01, -1.1619e-01, -3.5279e-01,\n",
       "                         2.1428e-01,  1.5543e-01, -1.1048e-01,  2.0785e-02, -6.1802e-03,\n",
       "                        -4.2830e-01,  3.0204e-02, -7.0261e-02, -1.9066e-01,  3.9366e-03,\n",
       "                        -2.7438e-01, -1.6800e-01, -2.9052e-02, -1.1490e-01, -1.1373e-01],\n",
       "                       [ 6.7105e-02, -2.4418e-01,  1.6478e-01, -6.4341e-02, -2.1997e-01,\n",
       "                        -2.4123e-01,  6.4815e-02, -1.5311e-01,  1.4679e-02, -3.3848e-02,\n",
       "                        -1.4718e-01, -1.6415e-02,  1.8645e-01, -6.1107e-02, -1.8708e-01,\n",
       "                        -7.6395e-02, -2.9540e-01,  6.4915e-02, -1.1139e-01,  9.4135e-02],\n",
       "                       [ 1.0028e-01,  1.8488e-01,  2.1410e-01,  2.4227e-01,  9.6949e-03,\n",
       "                         1.2092e-01,  6.1226e-02, -5.4656e-02,  2.3875e-01,  1.8320e-01,\n",
       "                         3.4363e-01,  1.5878e-01, -4.3637e-02,  4.0902e-02,  9.8967e-02,\n",
       "                         2.6611e-02,  1.9396e-01,  8.1670e-02,  5.8250e-02,  2.3577e-02],\n",
       "                       [ 9.6611e-03, -1.4670e-01,  9.4219e-02, -2.3468e-03, -1.6353e-01,\n",
       "                         2.0540e-01,  1.8008e-01, -1.9471e-01, -1.2091e-02, -2.6832e-01,\n",
       "                         1.3326e-01, -1.4088e-01,  1.7385e-01,  1.2608e-01,  2.2125e-01,\n",
       "                         2.2086e-02, -2.9399e-01, -4.3063e-02, -3.6112e-01, -1.7573e-01],\n",
       "                       [-3.4873e-02,  6.1492e-01, -2.1800e-01,  5.6572e-01,  4.6054e-01,\n",
       "                        -1.2199e-01,  7.4694e-02, -2.1838e-01,  6.6516e-01,  7.8094e-01,\n",
       "                         8.6649e-01, -1.8468e-01, -6.6531e-02, -2.1359e-02, -8.1823e-02,\n",
       "                         5.1784e-01,  6.2247e-01, -2.1515e-01,  6.9684e-01, -9.2496e-02],\n",
       "                       [-1.5284e-01, -1.8186e-01, -1.7027e-01, -2.5385e-02, -9.0100e-02,\n",
       "                        -2.1133e-01,  4.6392e-02, -7.1841e-02, -7.7207e-02,  2.6922e-02,\n",
       "                         2.0911e-02,  3.3497e-02,  3.3285e-02, -1.1838e-01, -7.6839e-02,\n",
       "                        -3.5342e-01, -1.8229e-03,  1.9570e-01, -3.5389e-01, -2.0229e-01],\n",
       "                       [-1.5637e-01,  1.1775e+00,  6.9845e-02,  1.1317e+00,  1.2808e+00,\n",
       "                        -7.8530e-02, -1.7155e-01, -9.1709e-02,  1.0257e+00,  1.1302e+00,\n",
       "                         8.1662e-01, -2.2718e-02,  2.8528e-02, -1.7518e-01,  2.1412e-01,\n",
       "                         1.3831e+00,  1.1863e+00,  1.6001e-01,  1.3364e+00, -1.4517e-01],\n",
       "                       [-7.8632e-02, -6.1202e-02,  9.2624e-02, -9.5798e-02, -7.0666e-02,\n",
       "                         2.1377e-01,  1.8529e-01,  1.1874e-01, -4.4167e-02, -2.0418e-01,\n",
       "                        -6.6320e-01, -5.5086e-02,  2.0842e-01, -1.1268e-01, -1.3026e-01,\n",
       "                        -3.7869e-01, -1.5474e-01,  2.0398e-01, -2.7425e-01,  2.8446e-02]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.0093,  1.9797, -0.0885, -0.1887,  0.1806, -0.2021,  0.0633,  0.0183,\n",
       "                        1.2007, -0.2726], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3831243230104446,\n",
       "   1.1470604515075684,\n",
       "   1.041866049170494,\n",
       "   0.9936886278390884,\n",
       "   0.9519963531494141,\n",
       "   0.919091225862503,\n",
       "   0.888294777572155,\n",
       "   0.8591080405712128,\n",
       "   0.8385973524451256,\n",
       "   0.8180697364211083,\n",
       "   0.7867452147006988,\n",
       "   0.7764555708765983,\n",
       "   0.761035330414772,\n",
       "   0.7391470678448677,\n",
       "   0.7290361112952233,\n",
       "   0.7263712412714958,\n",
       "   0.7037619164586068,\n",
       "   0.7000290148258209,\n",
       "   0.6954379646182061,\n",
       "   0.6824293164610863,\n",
       "   0.6777028104066849,\n",
       "   0.6556024571061134,\n",
       "   0.6540636216700078,\n",
       "   0.6612339159846305,\n",
       "   0.6539602423906327,\n",
       "   0.6400975578427315,\n",
       "   0.6195762794613838,\n",
       "   0.6337540436983109,\n",
       "   0.6816097396016121,\n",
       "   0.6451957045197487,\n",
       "   0.6148601350188255,\n",
       "   0.6106130024194717,\n",
       "   0.6115672317147255,\n",
       "   0.5956797205805778,\n",
       "   0.6074242236912251,\n",
       "   0.5866141303479672,\n",
       "   0.5919423958063126,\n",
       "   0.5863612085580826,\n",
       "   0.5896440166831016,\n",
       "   0.5706056029200554,\n",
       "   0.5818952191472053,\n",
       "   0.5731495276391506,\n",
       "   0.5716661645770073,\n",
       "   0.5811663694381713,\n",
       "   0.5679175012111664,\n",
       "   0.5667157185375691,\n",
       "   0.561230297088623,\n",
       "   0.5556488479971886,\n",
       "   0.5562693585157394,\n",
       "   0.552275339782238,\n",
       "   0.5593676807880401,\n",
       "   0.5426625583767891,\n",
       "   0.5496853390336036,\n",
       "   0.5387125526666641,\n",
       "   0.5429729154407978,\n",
       "   0.5467862339615822,\n",
       "   0.5409147626757622,\n",
       "   0.5375215468108654,\n",
       "   0.5355064628124238,\n",
       "   0.5374778520166874,\n",
       "   0.5346088425815105,\n",
       "   0.5435071705281734,\n",
       "   0.5262536410391331,\n",
       "   0.5224203266501427,\n",
       "   0.5331993159651757,\n",
       "   0.5345236032605171,\n",
       "   0.5352906672656537,\n",
       "   0.5119254511892796,\n",
       "   0.5187522521913052,\n",
       "   0.5303958544433117,\n",
       "   0.5177936653196812,\n",
       "   0.5350036357343196,\n",
       "   0.5312577618956565,\n",
       "   0.5220882703065872,\n",
       "   0.5182907997369767,\n",
       "   0.5149709592461587,\n",
       "   0.5249440445303917,\n",
       "   0.5298956387341023,\n",
       "   0.5111568813920021,\n",
       "   0.5279957401454449,\n",
       "   0.5099749073982239,\n",
       "   0.5162140392363072,\n",
       "   0.5245638361275196,\n",
       "   0.5157140246629714,\n",
       "   0.5119781843423843,\n",
       "   0.520851419389248,\n",
       "   0.5144646825790405,\n",
       "   0.5093003856539726,\n",
       "   0.5012278590798378,\n",
       "   0.5111925230324268,\n",
       "   0.500324750483036,\n",
       "   0.5054419267773628,\n",
       "   0.4920826796889305,\n",
       "   0.4960034486055374,\n",
       "   0.503710396707058,\n",
       "   0.509914936631918,\n",
       "   0.5008178343176841,\n",
       "   0.5004837462902069,\n",
       "   0.48181268176436426],\n",
       "  'train_loss_std': [0.17236133259741993,\n",
       "   0.13471731315858013,\n",
       "   0.153358637573573,\n",
       "   0.14052413581890327,\n",
       "   0.1409125156415443,\n",
       "   0.13752291108763398,\n",
       "   0.1446889769251445,\n",
       "   0.13423866852780503,\n",
       "   0.1453406847310494,\n",
       "   0.13766246226280637,\n",
       "   0.13891789705976645,\n",
       "   0.14576517689430124,\n",
       "   0.13180598661109155,\n",
       "   0.1450494633718983,\n",
       "   0.13515801384179585,\n",
       "   0.14157089585088872,\n",
       "   0.132250297193832,\n",
       "   0.1387619133101519,\n",
       "   0.1406679065412931,\n",
       "   0.14331299675682962,\n",
       "   0.13718091999867696,\n",
       "   0.12744475965990495,\n",
       "   0.13642415133209915,\n",
       "   0.13942032388846076,\n",
       "   0.13159551493898547,\n",
       "   0.1398595223224282,\n",
       "   0.13184076143518472,\n",
       "   0.12990900504297867,\n",
       "   0.15926910502982006,\n",
       "   0.14290812898944721,\n",
       "   0.12950956738930705,\n",
       "   0.1314282004360023,\n",
       "   0.1337174470880632,\n",
       "   0.13381787431492678,\n",
       "   0.131807851657008,\n",
       "   0.14008494692154025,\n",
       "   0.12949171638191215,\n",
       "   0.13417046384261525,\n",
       "   0.14042782779506807,\n",
       "   0.13437203841451834,\n",
       "   0.1337030391914784,\n",
       "   0.13540438124557042,\n",
       "   0.13320540974175835,\n",
       "   0.1376776093685393,\n",
       "   0.13401364855649744,\n",
       "   0.14024310278197943,\n",
       "   0.1384953170307961,\n",
       "   0.1353730118949873,\n",
       "   0.13168887495955017,\n",
       "   0.1330424628931702,\n",
       "   0.13401620681824047,\n",
       "   0.12717384347344549,\n",
       "   0.13250396910152248,\n",
       "   0.13646880196231398,\n",
       "   0.1409772343956475,\n",
       "   0.13626729631744666,\n",
       "   0.1315430161730352,\n",
       "   0.1380931452420121,\n",
       "   0.12522090431866958,\n",
       "   0.1311408898332432,\n",
       "   0.1351964577815077,\n",
       "   0.13402385885046358,\n",
       "   0.1332111632805472,\n",
       "   0.1345342146238443,\n",
       "   0.13228019091260007,\n",
       "   0.13153690833451004,\n",
       "   0.138809481291848,\n",
       "   0.1357745402999404,\n",
       "   0.12657430543955348,\n",
       "   0.13551578554282281,\n",
       "   0.1264147177237242,\n",
       "   0.13137936389400282,\n",
       "   0.12849619757843062,\n",
       "   0.13038436338785292,\n",
       "   0.13237862792585955,\n",
       "   0.1267025336373583,\n",
       "   0.13842452365820393,\n",
       "   0.1341965584290962,\n",
       "   0.1250180819269217,\n",
       "   0.13427161897345677,\n",
       "   0.13171954519801465,\n",
       "   0.14159818370109162,\n",
       "   0.14038944079053994,\n",
       "   0.12890076624189542,\n",
       "   0.1313236395380696,\n",
       "   0.12568755672344714,\n",
       "   0.12555043649788503,\n",
       "   0.13460138161304414,\n",
       "   0.12491721171436088,\n",
       "   0.12909181234338266,\n",
       "   0.13444819356251644,\n",
       "   0.1352670752361038,\n",
       "   0.12748700382134104,\n",
       "   0.12955865576006734,\n",
       "   0.1300575214865637,\n",
       "   0.14041276324787172,\n",
       "   0.12236200123620039,\n",
       "   0.12676108729785177,\n",
       "   0.1221616404123333],\n",
       "  'train_accuracy_mean': [0.42805333352088926,\n",
       "   0.5485599985122681,\n",
       "   0.6001066665053367,\n",
       "   0.6180799989104271,\n",
       "   0.639119998216629,\n",
       "   0.6520133327245712,\n",
       "   0.6655733327269554,\n",
       "   0.6780266664028167,\n",
       "   0.6856933342218399,\n",
       "   0.6938533325195313,\n",
       "   0.7087866668701172,\n",
       "   0.7121466673612594,\n",
       "   0.7180666676163674,\n",
       "   0.7271599997282028,\n",
       "   0.7302533336877823,\n",
       "   0.7327999987006187,\n",
       "   0.7401733322143554,\n",
       "   0.744013332247734,\n",
       "   0.7446000002622605,\n",
       "   0.7518133331537247,\n",
       "   0.7529733335971832,\n",
       "   0.7628799995183945,\n",
       "   0.7618266674280166,\n",
       "   0.7570933336019516,\n",
       "   0.7608399994373322,\n",
       "   0.766426666021347,\n",
       "   0.7749066655635833,\n",
       "   0.7693600002527237,\n",
       "   0.7510799994468689,\n",
       "   0.7667733331918717,\n",
       "   0.7761333332061767,\n",
       "   0.7785466666221619,\n",
       "   0.7791200004816056,\n",
       "   0.7853733323812485,\n",
       "   0.7785200001001358,\n",
       "   0.7892666671276093,\n",
       "   0.7872799990177155,\n",
       "   0.788799999833107,\n",
       "   0.7864666681289673,\n",
       "   0.7951866661310196,\n",
       "   0.7904000017642975,\n",
       "   0.7944933326244354,\n",
       "   0.7940800007581711,\n",
       "   0.7920666658878326,\n",
       "   0.7964266672134399,\n",
       "   0.793426666378975,\n",
       "   0.7975333330631256,\n",
       "   0.7993199993371963,\n",
       "   0.7989066677093506,\n",
       "   0.7998399987220765,\n",
       "   0.7987600010633469,\n",
       "   0.8059466670751572,\n",
       "   0.8025333334207535,\n",
       "   0.806693333029747,\n",
       "   0.8047333327531815,\n",
       "   0.803866664648056,\n",
       "   0.8056266660690308,\n",
       "   0.8071999998092652,\n",
       "   0.8088533331155777,\n",
       "   0.8079599995613098,\n",
       "   0.807039999127388,\n",
       "   0.803173332452774,\n",
       "   0.8125333342552185,\n",
       "   0.813519998550415,\n",
       "   0.8079333344697952,\n",
       "   0.8084133338928222,\n",
       "   0.8064933348894119,\n",
       "   0.8162266671657562,\n",
       "   0.812933333516121,\n",
       "   0.8099866684675217,\n",
       "   0.8133466665744782,\n",
       "   0.8065866659879685,\n",
       "   0.8088266664743423,\n",
       "   0.8113333328962326,\n",
       "   0.8139466669559479,\n",
       "   0.8145199999809265,\n",
       "   0.8110000003576279,\n",
       "   0.8083333333730698,\n",
       "   0.8177466673851013,\n",
       "   0.8109733326435089,\n",
       "   0.8166666672229766,\n",
       "   0.8163066667318344,\n",
       "   0.8115199995040894,\n",
       "   0.8149066656827927,\n",
       "   0.8153999986648559,\n",
       "   0.8133466663360596,\n",
       "   0.8152933348417282,\n",
       "   0.817359999537468,\n",
       "   0.8194266664981842,\n",
       "   0.8164533340930938,\n",
       "   0.8210666669607163,\n",
       "   0.8197066665887832,\n",
       "   0.8249333333969117,\n",
       "   0.821319999575615,\n",
       "   0.8181466673612594,\n",
       "   0.8161733322143555,\n",
       "   0.8196933327913284,\n",
       "   0.8185199990272521,\n",
       "   0.826280000925064],\n",
       "  'train_accuracy_std': [0.09557050648164318,\n",
       "   0.06802853626351944,\n",
       "   0.07590513028708973,\n",
       "   0.06747149435241877,\n",
       "   0.06806747579166232,\n",
       "   0.0685516174490967,\n",
       "   0.06920536492592574,\n",
       "   0.06651595924199213,\n",
       "   0.06894013037254527,\n",
       "   0.0663873854752469,\n",
       "   0.06539890326910933,\n",
       "   0.06775587414465918,\n",
       "   0.06275805405151853,\n",
       "   0.06613135811638794,\n",
       "   0.06219701894346246,\n",
       "   0.06424453191092643,\n",
       "   0.06151036938112407,\n",
       "   0.06257283482727315,\n",
       "   0.06359521362069466,\n",
       "   0.06584561645789483,\n",
       "   0.06044100206865123,\n",
       "   0.058142113926557,\n",
       "   0.06082211597805909,\n",
       "   0.06248551977527311,\n",
       "   0.05921941238868767,\n",
       "   0.06263641315847827,\n",
       "   0.058736437816568794,\n",
       "   0.05957358882208386,\n",
       "   0.06850653119911149,\n",
       "   0.06379959746873325,\n",
       "   0.057576075701086214,\n",
       "   0.05950106445917942,\n",
       "   0.058620654827854016,\n",
       "   0.060794869210770174,\n",
       "   0.05855926272012378,\n",
       "   0.0637069332737747,\n",
       "   0.05807410434562686,\n",
       "   0.05985374888841843,\n",
       "   0.06303899981293816,\n",
       "   0.059139275963521774,\n",
       "   0.05953240012587169,\n",
       "   0.05935794371942797,\n",
       "   0.057605346475909744,\n",
       "   0.059579974370628806,\n",
       "   0.059346328901516386,\n",
       "   0.06197125170551677,\n",
       "   0.061728113213868285,\n",
       "   0.059312955492322725,\n",
       "   0.05988028942493751,\n",
       "   0.05890535752670895,\n",
       "   0.05938327564373663,\n",
       "   0.05585072851394442,\n",
       "   0.05745398271770186,\n",
       "   0.05905627699559394,\n",
       "   0.06106532093127058,\n",
       "   0.0581368504430762,\n",
       "   0.058607420260121834,\n",
       "   0.06115248671679721,\n",
       "   0.05507990741534753,\n",
       "   0.05701008336963022,\n",
       "   0.05977173754352164,\n",
       "   0.05857015713765736,\n",
       "   0.05800750997085558,\n",
       "   0.058310745109523617,\n",
       "   0.05747962024633982,\n",
       "   0.05741597105857096,\n",
       "   0.06038297315852427,\n",
       "   0.05912760125619544,\n",
       "   0.054160420391515175,\n",
       "   0.05835751879472331,\n",
       "   0.054810379347360744,\n",
       "   0.05755455519929137,\n",
       "   0.0560218315153551,\n",
       "   0.058824789044847996,\n",
       "   0.05916738425804537,\n",
       "   0.05619739694042022,\n",
       "   0.05875731632854294,\n",
       "   0.058731593740228946,\n",
       "   0.05173769805910055,\n",
       "   0.05824419490125138,\n",
       "   0.058015323743044356,\n",
       "   0.06007664834458585,\n",
       "   0.06090357306601149,\n",
       "   0.05698354800106724,\n",
       "   0.05816982945352076,\n",
       "   0.054393627643769746,\n",
       "   0.05457209930239578,\n",
       "   0.05667654240602463,\n",
       "   0.05485763607423365,\n",
       "   0.05671330392420355,\n",
       "   0.058986402124637176,\n",
       "   0.05999631929156148,\n",
       "   0.05567541075484186,\n",
       "   0.05614536573638727,\n",
       "   0.05684001617301003,\n",
       "   0.061624138726741245,\n",
       "   0.05418318182758634,\n",
       "   0.05523795758321489,\n",
       "   0.05369487334927156],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.344914110104243,\n",
       "   1.2029183451334635,\n",
       "   1.1524400542179742,\n",
       "   1.1205498671531677,\n",
       "   1.0881222585837047,\n",
       "   1.0672575829426447,\n",
       "   1.0486708426475524,\n",
       "   1.0294380019108453,\n",
       "   0.9934572752316793,\n",
       "   1.0049500902493795,\n",
       "   0.9880114465951919,\n",
       "   0.9805617499351501,\n",
       "   0.9934762020905813,\n",
       "   1.0028977757692337,\n",
       "   0.9564629516998927,\n",
       "   0.9627957437435786,\n",
       "   0.9776364014546076,\n",
       "   0.9455163655678431,\n",
       "   0.9512509767214458,\n",
       "   0.9429633859793345,\n",
       "   0.9537615871429443,\n",
       "   0.9421991487344106,\n",
       "   0.9427571632464726,\n",
       "   0.9618750880161921,\n",
       "   0.9256183715661367,\n",
       "   0.9332147518793742,\n",
       "   0.940244032740593,\n",
       "   0.9297376275062561,\n",
       "   0.9426372104883194,\n",
       "   0.9334066985050837,\n",
       "   0.8999025634924571,\n",
       "   0.9178580957651138,\n",
       "   0.9159815307458242,\n",
       "   0.9298174502452214,\n",
       "   0.919751008550326,\n",
       "   0.9259122765064239,\n",
       "   0.91890789270401,\n",
       "   0.913161249756813,\n",
       "   0.9121177754799525,\n",
       "   0.9065400461355845,\n",
       "   0.9335646351178487,\n",
       "   0.9379937448104223,\n",
       "   0.9082650993267695,\n",
       "   0.9093311256170273,\n",
       "   0.9314141569534937,\n",
       "   0.9165400326251983,\n",
       "   0.9288300553957621,\n",
       "   0.9159002604087194,\n",
       "   0.9108860794703165,\n",
       "   0.9162280472119649,\n",
       "   0.9187991180022558,\n",
       "   0.939987697203954,\n",
       "   0.9143218302726746,\n",
       "   0.9019512422879536,\n",
       "   0.9372996509075164,\n",
       "   0.9575381636619568,\n",
       "   0.9235058055321376,\n",
       "   0.9259264719486237,\n",
       "   0.9379360884428024,\n",
       "   0.9348233850797018,\n",
       "   0.9401046643654506,\n",
       "   0.9212624605496724,\n",
       "   0.9437316109736761,\n",
       "   0.934078643321991,\n",
       "   0.9239691092570623,\n",
       "   0.9206885931889216,\n",
       "   0.9062416297197342,\n",
       "   0.9257521784305572,\n",
       "   0.9105148661136627,\n",
       "   0.9330389732122422,\n",
       "   0.9378560856978099,\n",
       "   0.9300699126720429,\n",
       "   0.9429550304015477,\n",
       "   0.9252929608027141,\n",
       "   0.9405155316988627,\n",
       "   0.9577122165759404,\n",
       "   0.9275214791297912,\n",
       "   0.9424492835998535,\n",
       "   0.9556366789340973,\n",
       "   0.9431165293852488,\n",
       "   0.9481919987996419,\n",
       "   0.9434578039248784,\n",
       "   0.9310424894094467,\n",
       "   0.931773685614268,\n",
       "   0.9627634431918463,\n",
       "   0.9340996847550074,\n",
       "   0.9424677377939225,\n",
       "   0.9467354973157247,\n",
       "   0.950321779847145,\n",
       "   0.9477016820510229,\n",
       "   0.9405332112312317,\n",
       "   0.943105363647143,\n",
       "   0.9465104564030965,\n",
       "   0.9405541521310806,\n",
       "   0.9480538713932037,\n",
       "   0.9475417178869248,\n",
       "   0.9262124609947204,\n",
       "   0.9553034381071727,\n",
       "   0.9330294992526372],\n",
       "  'val_loss_std': [0.11325503200293692,\n",
       "   0.11824855058408427,\n",
       "   0.12712258535870558,\n",
       "   0.1321257389228303,\n",
       "   0.12953729281182633,\n",
       "   0.13702270928652574,\n",
       "   0.13437890400820535,\n",
       "   0.12969246610417426,\n",
       "   0.13260102838493074,\n",
       "   0.13292365211750967,\n",
       "   0.13053341287533565,\n",
       "   0.1316764373269921,\n",
       "   0.1390326425166993,\n",
       "   0.1328659045442342,\n",
       "   0.12776224865990307,\n",
       "   0.13249881806796687,\n",
       "   0.13713048158766922,\n",
       "   0.13034118663926014,\n",
       "   0.12935101340640895,\n",
       "   0.13095251115236467,\n",
       "   0.1273956632461203,\n",
       "   0.12841673179242688,\n",
       "   0.12495240701406772,\n",
       "   0.12781978546838021,\n",
       "   0.1264345566526659,\n",
       "   0.13021262766834713,\n",
       "   0.12918078012356968,\n",
       "   0.13060945506257796,\n",
       "   0.1319075142512808,\n",
       "   0.12499913451674037,\n",
       "   0.13013422664526833,\n",
       "   0.1307987158455077,\n",
       "   0.12898614147305226,\n",
       "   0.12641975218810797,\n",
       "   0.1277606554911566,\n",
       "   0.1316533179535921,\n",
       "   0.12915684064811883,\n",
       "   0.13323924406642282,\n",
       "   0.13034358046290295,\n",
       "   0.1326226459425335,\n",
       "   0.12570449624982274,\n",
       "   0.12390746894669818,\n",
       "   0.12938816661007826,\n",
       "   0.13155942743751137,\n",
       "   0.12367306367655824,\n",
       "   0.1363657768115907,\n",
       "   0.12136054916036854,\n",
       "   0.12812265154421762,\n",
       "   0.13165069440471092,\n",
       "   0.12817717815311624,\n",
       "   0.12893172703889033,\n",
       "   0.12554957847262507,\n",
       "   0.12713588473801796,\n",
       "   0.12836038168965883,\n",
       "   0.13291537302780806,\n",
       "   0.13353459781624857,\n",
       "   0.13554701605911712,\n",
       "   0.12716101510046302,\n",
       "   0.13614976754792923,\n",
       "   0.1288344065078874,\n",
       "   0.1330197055095379,\n",
       "   0.13472396169037706,\n",
       "   0.13236778466708338,\n",
       "   0.13182598840500645,\n",
       "   0.13045425644259226,\n",
       "   0.13894795348427294,\n",
       "   0.1370968040748917,\n",
       "   0.13279788067442347,\n",
       "   0.13017579555159361,\n",
       "   0.12946907848596137,\n",
       "   0.1307960532349062,\n",
       "   0.12198706514718316,\n",
       "   0.13734805080677184,\n",
       "   0.13080186096978685,\n",
       "   0.13391438454405968,\n",
       "   0.12935575328884588,\n",
       "   0.13556141593098248,\n",
       "   0.13360976011581577,\n",
       "   0.13638173463284395,\n",
       "   0.1331586254391046,\n",
       "   0.13850525401991456,\n",
       "   0.13217434552012333,\n",
       "   0.13073030493735924,\n",
       "   0.13834232552919318,\n",
       "   0.12884699186228238,\n",
       "   0.13788206409923107,\n",
       "   0.1367232943651513,\n",
       "   0.14060090414906512,\n",
       "   0.13045831538501265,\n",
       "   0.13259259826018852,\n",
       "   0.13133511972305786,\n",
       "   0.136008688216888,\n",
       "   0.14265311001151743,\n",
       "   0.1303339032120267,\n",
       "   0.143537204734925,\n",
       "   0.14414601510042893,\n",
       "   0.13861199040876446,\n",
       "   0.13420232081885858,\n",
       "   0.14304259729121135],\n",
       "  'val_accuracy_mean': [0.45095555583635966,\n",
       "   0.5263333317637443,\n",
       "   0.5467999985814095,\n",
       "   0.5603777782122294,\n",
       "   0.5721777779857318,\n",
       "   0.5817555555701256,\n",
       "   0.5907333326339722,\n",
       "   0.5988222231467565,\n",
       "   0.6127333322167396,\n",
       "   0.6088222221533457,\n",
       "   0.6179555542270343,\n",
       "   0.619888888100783,\n",
       "   0.6162222203612328,\n",
       "   0.6125555564959844,\n",
       "   0.6330666667222977,\n",
       "   0.6284888882438342,\n",
       "   0.6224444439013799,\n",
       "   0.6370666667819023,\n",
       "   0.6332444433371226,\n",
       "   0.6372444440921148,\n",
       "   0.6323333332935969,\n",
       "   0.6403333334128062,\n",
       "   0.6377555561065674,\n",
       "   0.6285555530587832,\n",
       "   0.6432666659355164,\n",
       "   0.6421555555860201,\n",
       "   0.6392222229639689,\n",
       "   0.6423333332935969,\n",
       "   0.6362666648626327,\n",
       "   0.6409777742624283,\n",
       "   0.6552888882160187,\n",
       "   0.6492222206791242,\n",
       "   0.6470666653911272,\n",
       "   0.6449111113945644,\n",
       "   0.6471999988953272,\n",
       "   0.6443111100792884,\n",
       "   0.6470666657884916,\n",
       "   0.6489999985694885,\n",
       "   0.6516444428761801,\n",
       "   0.653866666952769,\n",
       "   0.6426444433132807,\n",
       "   0.641311111052831,\n",
       "   0.6536222209533056,\n",
       "   0.6529333333174387,\n",
       "   0.6434888882438342,\n",
       "   0.6511999996503194,\n",
       "   0.6422444455822309,\n",
       "   0.6514888863762219,\n",
       "   0.6495333318909009,\n",
       "   0.6501333338022232,\n",
       "   0.6462000000476837,\n",
       "   0.640866663257281,\n",
       "   0.6487999991575877,\n",
       "   0.6556666655341784,\n",
       "   0.6429555544257164,\n",
       "   0.629622222383817,\n",
       "   0.6475777793924014,\n",
       "   0.6463999980688095,\n",
       "   0.6407777779301007,\n",
       "   0.638244442542394,\n",
       "   0.6392222218712171,\n",
       "   0.6449999976158142,\n",
       "   0.6383777781327565,\n",
       "   0.6365777764717738,\n",
       "   0.64659999837478,\n",
       "   0.6469999995827674,\n",
       "   0.6518888884782791,\n",
       "   0.6424000001947086,\n",
       "   0.648688888947169,\n",
       "   0.6419999983906746,\n",
       "   0.6376444425185521,\n",
       "   0.6406888885299364,\n",
       "   0.6386666672428449,\n",
       "   0.6476444431145986,\n",
       "   0.6382000003258387,\n",
       "   0.6264222218592962,\n",
       "   0.6472222203016281,\n",
       "   0.6361111125349999,\n",
       "   0.6326444426178932,\n",
       "   0.6394666649897893,\n",
       "   0.6316444436709087,\n",
       "   0.6346444431940714,\n",
       "   0.6417999998728434,\n",
       "   0.6425555536150932,\n",
       "   0.6270888895789782,\n",
       "   0.6405777765313784,\n",
       "   0.6369111107786497,\n",
       "   0.6380000001192093,\n",
       "   0.6284444435437521,\n",
       "   0.6327333333094914,\n",
       "   0.6369111101826032,\n",
       "   0.634600000778834,\n",
       "   0.6356666675209999,\n",
       "   0.6350444460908572,\n",
       "   0.6309333332379659,\n",
       "   0.635711110830307,\n",
       "   0.6434888887405396,\n",
       "   0.6285555555423101,\n",
       "   0.6420222221811612],\n",
       "  'val_accuracy_std': [0.06204041212674087,\n",
       "   0.0618076454109917,\n",
       "   0.06419401043970376,\n",
       "   0.06382387431234049,\n",
       "   0.06071791105357385,\n",
       "   0.06532902079396445,\n",
       "   0.06351129372384531,\n",
       "   0.06073334005499528,\n",
       "   0.0633912845348019,\n",
       "   0.06459175368171827,\n",
       "   0.06127749398725615,\n",
       "   0.06594825875793137,\n",
       "   0.06558191511154632,\n",
       "   0.06497283375026061,\n",
       "   0.06235088407410042,\n",
       "   0.06411053910811393,\n",
       "   0.06512172975716116,\n",
       "   0.062180049508099314,\n",
       "   0.06307661330812454,\n",
       "   0.062466285723404374,\n",
       "   0.0626007331220435,\n",
       "   0.06175249409640443,\n",
       "   0.06355982643503139,\n",
       "   0.0630401611898353,\n",
       "   0.05986772025250435,\n",
       "   0.06151138051150965,\n",
       "   0.05979336859062807,\n",
       "   0.061740495237480576,\n",
       "   0.0625624680448143,\n",
       "   0.060857511963318484,\n",
       "   0.06015558573838546,\n",
       "   0.06051742059594294,\n",
       "   0.0606103791876578,\n",
       "   0.06198948099687664,\n",
       "   0.05724033937925872,\n",
       "   0.06116710144130725,\n",
       "   0.06204528975614883,\n",
       "   0.060295876684149485,\n",
       "   0.059050195314743634,\n",
       "   0.06261946323800509,\n",
       "   0.06056254513847421,\n",
       "   0.05930885638484532,\n",
       "   0.0576611259335358,\n",
       "   0.059723960521346944,\n",
       "   0.05923316930809062,\n",
       "   0.061445888782757606,\n",
       "   0.06054199265289114,\n",
       "   0.05878528180745336,\n",
       "   0.06008577791631483,\n",
       "   0.05583690761793687,\n",
       "   0.061119835443444,\n",
       "   0.061792878886523225,\n",
       "   0.06081822403500432,\n",
       "   0.059227435390390354,\n",
       "   0.05706920891940726,\n",
       "   0.06238950205961736,\n",
       "   0.059238871775380966,\n",
       "   0.060299891441688784,\n",
       "   0.06223124996690612,\n",
       "   0.061991871974181476,\n",
       "   0.06250916111377891,\n",
       "   0.06133122086544163,\n",
       "   0.06254507963408815,\n",
       "   0.06178930486926352,\n",
       "   0.05920331539478771,\n",
       "   0.05881578455267484,\n",
       "   0.059577421912686855,\n",
       "   0.060917820755320186,\n",
       "   0.06246468415027,\n",
       "   0.06288025131729193,\n",
       "   0.061932517353674006,\n",
       "   0.05970767943947022,\n",
       "   0.06158883940381479,\n",
       "   0.05937145675075626,\n",
       "   0.0593260065607153,\n",
       "   0.06229749044240314,\n",
       "   0.06036299644130012,\n",
       "   0.057713105842589874,\n",
       "   0.058537480781371554,\n",
       "   0.056954928187961475,\n",
       "   0.06204747069277441,\n",
       "   0.06064761292164395,\n",
       "   0.06139022768606815,\n",
       "   0.05962713561478027,\n",
       "   0.059079101420565834,\n",
       "   0.06113948468370679,\n",
       "   0.0635057252184847,\n",
       "   0.05900031383988539,\n",
       "   0.06313614753024782,\n",
       "   0.06325558939469794,\n",
       "   0.06245187540000335,\n",
       "   0.06214936653398954,\n",
       "   0.06328799629915456,\n",
       "   0.06248109402165396,\n",
       "   0.06303569952540304,\n",
       "   0.06083716887735603,\n",
       "   0.06214777690566628,\n",
       "   0.06220982083643293,\n",
       "   0.06193831816177473],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb0c68",
   "metadata": {},
   "source": [
    "### 1.1 MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     #print(key)\n",
    "#     if value.requires_grad:\n",
    "#         print(key)\n",
    "#         print(value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a599c8",
   "metadata": {},
   "source": [
    "### 1.2 Arbiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ebc67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = arbiter_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = arbiter_system.state['best_epoch']\n",
    "\n",
    "state = arbiter_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "arbiter_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0531d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a86b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "\n",
    "x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "\n",
    "\n",
    "x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task = next(zip(x_support_set,y_support_set,x_target_set, y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdeb442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "647183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbiter_x_support_set, arbiter_x_target_set, arbiter_y_support_set, arbiter_y_target_set, seed = train_sample\n",
    "\n",
    "arbiter_x_support_set = torch.Tensor(arbiter_x_support_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_x_target_set = torch.Tensor(arbiter_x_target_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_y_support_set = torch.Tensor(arbiter_y_support_set).long().to(device=arbiter_system.model.device)\n",
    "arbiter_y_target_set = torch.Tensor(arbiter_y_target_set).long().to(device=arbiter_system.model.device)\n",
    "\n",
    "\n",
    "arbiter_x_support_set_task, arbiter_y_support_set_task, arbiter_x_target_set_task, arbiter_y_target_set_task = next(zip(arbiter_x_support_set,arbiter_y_support_set,arbiter_x_target_set, arbiter_y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce1c0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd4d6e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\MAML\\meta_neural_network_architectures.py:999: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  if param.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "names_weights_copy = arbiter_system.model.get_inner_loop_parameter_dict(arbiter_system.model.classifier.named_parameters())\n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = arbiter_x_target_set_task.shape\n",
    "\n",
    "arbiter_x_support_set_task = arbiter_x_support_set_task.view(-1, c, h, w)\n",
    "arbiter_y_support_set_task = arbiter_y_support_set_task.view(-1)\n",
    "arbiter_x_target_set_task = arbiter_x_target_set_task.view(-1, c, h, w)\n",
    "arbiter_y_target_set_task = arbiter_y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds = arbiter_system.model.net_forward(\n",
    "            x=arbiter_x_support_set_task,\n",
    "            y=arbiter_y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "            soft_target=None\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "    \n",
    "    if arbiter_system.model.args.arbiter:\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                retain_graph=True)\n",
    "\n",
    "        names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "\n",
    "        for key, weight in names_weights_copy.items():\n",
    "            weight_norm = torch.norm(weight, p=2)\n",
    "            per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "        for key, grad in names_grads_copy.items():\n",
    "            gradient_l2norm = torch.norm(grad, p=2)\n",
    "            per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "        per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                    per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        generated_gradient_rate = arbiter_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "            g += 1\n",
    "\n",
    "    names_weights_copy = arbiter_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_arbiter.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=arbiter_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in arbiter_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d16650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "y_support_set_task = y_support_set_task.view(-1)\n",
    "x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "y_target_set_task = y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds = maml_system.model.net_forward(\n",
    "            x=x_support_set_task,\n",
    "            y=y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "            soft_target=None\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "\n",
    "\n",
    "    names_weights_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_maml.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=maml_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575454f0",
   "metadata": {},
   "source": [
    "## landscape 함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aec9618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = loss_landscape_join.landscape(maml_system.model.classifier, arbiter_system.model.classifier, args_arbiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c578d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "tensor([ 1.7462e-09,  5.3551e-09, -9.3132e-10, -8.1491e-09,  6.9849e-10,\n",
      "         4.1910e-09,  8.3819e-09, -1.1642e-09,  3.1287e-10, -2.6776e-09,\n",
      "        -4.6566e-10, -8.3819e-09,  3.2596e-09, -1.5134e-09,  3.3760e-09,\n",
      "         4.6566e-10, -1.8626e-09, -9.5461e-09,  1.8626e-09, -4.6566e-10,\n",
      "        -1.9791e-09, -4.3656e-11,  1.8626e-09,  3.9581e-09, -5.8208e-10,\n",
      "         5.8208e-10,  9.3132e-10,  1.9092e-08,  0.0000e+00,  2.0955e-09,\n",
      "        -1.8626e-09, -2.3283e-10,  8.3819e-09, -2.9104e-11,  8.1491e-10,\n",
      "         1.0477e-09, -6.9849e-09, -7.1013e-09,  0.0000e+00, -1.8626e-09,\n",
      "        -2.5611e-09,  9.3132e-09, -8.6147e-09,  6.0536e-09,  1.3271e-08,\n",
      "        -5.8208e-10, -3.2596e-09, -6.2864e-09], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.1642e-10,  1.3388e-09, -3.4925e-10, -1.4552e-10, -3.2014e-10,\n",
      "        -4.6566e-10, -1.1642e-10, -7.2760e-11,  0.0000e+00, -2.9104e-11,\n",
      "        -8.7311e-11, -5.8208e-11, -2.6193e-10,  8.4401e-10,  1.2224e-09,\n",
      "         1.7462e-10, -2.3283e-10, -3.2014e-10,  1.6007e-10, -2.9104e-10,\n",
      "        -3.4925e-10,  5.8208e-11,  7.5670e-10,  9.6043e-10,  1.0186e-10,\n",
      "         3.4925e-10,  0.0000e+00, -1.1642e-09,  2.3283e-10,  2.9104e-10,\n",
      "         8.7311e-10,  0.0000e+00, -2.3283e-10,  3.4925e-10,  4.3656e-10,\n",
      "         4.7294e-11,  2.7649e-10,  5.2387e-10,  2.3283e-10,  6.4028e-10,\n",
      "        -2.6193e-10, -2.0373e-10,  3.4925e-10,  3.3469e-10, -2.9104e-10,\n",
      "        -7.2760e-11,  8.7311e-11,  0.0000e+00], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 7.0331e-03,  5.7055e-03,  3.2257e-03, -7.1942e-03, -8.4112e-03,\n",
      "         1.1243e-02, -1.0698e-02,  1.0922e-02, -6.7714e-03, -1.0837e-02,\n",
      "        -1.0646e-02,  6.9316e-03,  1.0273e-02,  6.2557e-03, -6.6994e-03,\n",
      "        -2.2782e-03, -1.1729e-02,  5.9218e-04, -6.6102e-03,  7.8959e-03,\n",
      "        -1.3617e-02,  2.7649e-02,  8.2350e-03,  1.0414e-02,  3.1442e-03,\n",
      "        -7.7614e-03, -1.1235e-02, -3.0476e-03,  2.1270e-03, -1.4861e-02,\n",
      "         2.5016e-03, -6.6961e-03,  1.4413e-02,  6.2598e-03,  1.6644e-02,\n",
      "        -2.5785e-03, -3.1719e-02,  1.5978e-02, -1.7284e-05,  2.1126e-02,\n",
      "         1.6371e-02,  6.4176e-03, -3.5523e-04, -7.3459e-03,  1.0090e-03,\n",
      "        -2.2650e-03,  1.1534e-02, -4.0571e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 0.0055,  0.0050,  0.0056, -0.0107, -0.0161,  0.0173, -0.0076,  0.0074,\n",
      "        -0.0141,  0.0005, -0.0087,  0.0069,  0.0135,  0.0034, -0.0024, -0.0005,\n",
      "        -0.0092, -0.0029, -0.0108,  0.0097, -0.0122,  0.0169,  0.0101,  0.0189,\n",
      "        -0.0078, -0.0161, -0.0071, -0.0076, -0.0012, -0.0142,  0.0045, -0.0099,\n",
      "         0.0116,  0.0063,  0.0191, -0.0037, -0.0398,  0.0105, -0.0005,  0.0172,\n",
      "         0.0164,  0.0062,  0.0008, -0.0083,  0.0035, -0.0041,  0.0107,  0.0060],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([[[[ 2.0695e-02,  1.4205e-02,  1.2521e-02],\n",
      "          [ 1.5480e-02,  1.0005e-02,  9.3098e-03],\n",
      "          [ 1.3772e-02,  9.4843e-03,  9.6003e-03]],\n",
      "\n",
      "         [[ 1.8979e-02,  1.2854e-02,  1.1552e-02],\n",
      "          [ 1.2295e-02,  7.2611e-03,  6.8182e-03],\n",
      "          [ 9.1388e-03,  5.1753e-03,  5.4924e-03]],\n",
      "\n",
      "         [[ 1.4412e-02,  9.3424e-03,  8.0478e-03],\n",
      "          [ 6.3848e-03,  1.8940e-03,  9.3349e-04],\n",
      "          [ 1.1835e-03, -2.5001e-03, -2.6764e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.3757e-03,  9.1844e-03,  8.7417e-03],\n",
      "          [ 9.2162e-03,  8.9815e-03,  8.3187e-03],\n",
      "          [ 9.8103e-03,  9.4638e-03,  8.5422e-03]],\n",
      "\n",
      "         [[ 9.0651e-03,  8.4606e-03,  7.9911e-03],\n",
      "          [ 9.0841e-03,  8.4429e-03,  7.7118e-03],\n",
      "          [ 9.9224e-03,  9.1801e-03,  8.1848e-03]],\n",
      "\n",
      "         [[ 1.1624e-02,  1.1147e-02,  1.0902e-02],\n",
      "          [ 1.1475e-02,  1.1000e-02,  1.0608e-02],\n",
      "          [ 1.2268e-02,  1.1752e-02,  1.1115e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.2783e-02,  4.9298e-02,  5.2614e-02],\n",
      "          [ 4.2196e-02,  4.7986e-02,  5.0878e-02],\n",
      "          [ 4.6980e-02,  5.1839e-02,  5.3509e-02]],\n",
      "\n",
      "         [[ 5.0595e-02,  5.6491e-02,  5.8883e-02],\n",
      "          [ 4.9978e-02,  5.4821e-02,  5.6493e-02],\n",
      "          [ 5.5411e-02,  5.9030e-02,  5.9573e-02]],\n",
      "\n",
      "         [[ 5.6954e-02,  6.5464e-02,  6.9675e-02],\n",
      "          [ 5.5322e-02,  6.2202e-02,  6.5100e-02],\n",
      "          [ 5.9138e-02,  6.4384e-02,  6.5645e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.3575e-03, -8.0695e-03, -9.9296e-03],\n",
      "          [-7.7739e-03, -1.0881e-02, -1.2227e-02],\n",
      "          [-8.4186e-03, -1.1521e-02, -1.2458e-02]],\n",
      "\n",
      "         [[-1.4537e-03, -5.2572e-03, -8.0598e-03],\n",
      "          [-6.2079e-03, -9.4990e-03, -1.1492e-02],\n",
      "          [-8.3192e-03, -1.1422e-02, -1.3054e-02]],\n",
      "\n",
      "         [[ 1.6204e-03, -2.5058e-03, -6.6682e-03],\n",
      "          [-3.6531e-04, -4.1062e-03, -7.9518e-03],\n",
      "          [-1.6931e-04, -3.7187e-03, -7.4000e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7558e-04, -2.7630e-05, -2.9594e-04],\n",
      "          [ 3.4046e-04, -5.3841e-05, -2.6196e-04],\n",
      "          [ 5.8961e-04,  3.4806e-04,  2.2459e-04]],\n",
      "\n",
      "         [[ 3.6550e-04, -1.6889e-05, -8.7307e-05],\n",
      "          [ 3.1089e-04,  2.8848e-05, -9.3172e-06],\n",
      "          [ 5.1902e-04,  3.6672e-04,  4.2407e-04]],\n",
      "\n",
      "         [[ 2.0662e-04, -3.0232e-05,  5.4887e-05],\n",
      "          [ 2.1133e-04,  2.6050e-05,  1.2082e-04],\n",
      "          [ 4.2537e-04,  3.2404e-04,  4.5744e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.6956e-04, -3.5364e-04, -3.3481e-04],\n",
      "          [-4.0374e-04, -3.8458e-04, -3.5763e-04],\n",
      "          [-4.3262e-04, -4.0793e-04, -3.8493e-04]],\n",
      "\n",
      "         [[-9.8130e-05, -9.8492e-05, -7.7077e-05],\n",
      "          [-9.9053e-05, -9.1489e-05, -5.9794e-05],\n",
      "          [-9.0039e-05, -7.0208e-05, -3.5672e-05]],\n",
      "\n",
      "         [[-2.2767e-04, -2.3030e-04, -2.0765e-04],\n",
      "          [-2.1100e-04, -2.0198e-04, -1.6751e-04],\n",
      "          [-1.9694e-04, -1.7780e-04, -1.3453e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 8.8476e-09,  1.1642e-09, -1.3039e-08,  1.3970e-09,  2.3283e-10,\n",
      "        -2.7940e-09, -5.8208e-11,  3.7835e-09, -6.9849e-10,  1.1642e-09,\n",
      "         2.6193e-09,  1.9645e-10,  8.3674e-11,  1.4552e-10, -2.6776e-09,\n",
      "         2.3283e-10, -2.8740e-10, -6.9849e-10, -1.3970e-08, -1.0186e-10,\n",
      "         5.5879e-09,  1.4552e-11, -4.6566e-10, -4.6566e-10, -3.2596e-09,\n",
      "        -4.1473e-10, -5.8208e-10,  8.9640e-09,  2.3283e-10, -7.5670e-10,\n",
      "         1.9791e-09, -5.8208e-10, -7.6834e-09,  1.8626e-09, -1.6735e-10,\n",
      "        -7.2760e-12,  0.0000e+00, -1.9209e-09,  1.0914e-10,  2.0955e-08,\n",
      "         2.3283e-09, -6.9849e-09,  3.9290e-10,  3.0705e-09, -2.7940e-09,\n",
      "         3.0268e-09, -5.5297e-10,  4.3656e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-1.2193e-03, -2.5999e-04,  9.4466e-04, -5.3164e-04,  5.7509e-04,\n",
      "         1.3024e-03, -2.3127e-04, -6.5285e-04,  1.5193e-03, -3.4137e-05,\n",
      "         1.9536e-03, -3.5340e-05,  8.6472e-04, -3.6335e-04, -3.8831e-04,\n",
      "         1.2627e-04, -1.4912e-03,  9.6224e-04,  2.5518e-03,  4.1826e-04,\n",
      "         4.6853e-04, -3.6822e-04,  2.1353e-04,  4.3875e-03,  1.1838e-03,\n",
      "        -2.8993e-04,  5.7095e-04,  1.8130e-03,  6.0246e-04, -1.1508e-03,\n",
      "        -4.0094e-03, -1.4473e-04, -1.1340e-03, -5.6004e-04,  6.8986e-04,\n",
      "        -9.0688e-04,  7.4733e-04, -3.4049e-05,  1.2040e-04,  5.6342e-03,\n",
      "        -6.1344e-04,  3.4095e-03,  3.1081e-03, -7.0910e-04, -2.6604e-03,\n",
      "        -2.9488e-04,  1.2416e-03,  2.1604e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-2.9104e-11, -4.3656e-11, -1.0186e-10, -2.9104e-11,  9.2768e-11,\n",
      "         9.4587e-11, -5.0932e-11,  9.4587e-11, -7.2760e-11,  4.7294e-11,\n",
      "         9.8225e-11, -1.1642e-10, -8.7311e-11, -2.9104e-11, -5.0932e-11,\n",
      "        -6.7303e-11,  2.9104e-11,  1.8190e-11, -7.2760e-11,  4.3656e-11,\n",
      "        -1.6007e-10,  0.0000e+00,  5.0932e-11,  1.4552e-11, -5.8208e-11,\n",
      "         2.4738e-10, -3.6380e-12, -3.9654e-10,  1.4552e-11, -2.0373e-10,\n",
      "         8.0036e-11,  1.6735e-10, -1.8554e-10, -1.0914e-11,  1.1642e-10,\n",
      "        -2.3283e-10, -4.3656e-11,  5.8208e-11,  2.5102e-10,  1.9645e-10,\n",
      "         1.6735e-10,  3.5652e-10, -1.1642e-10, -1.0550e-10,  2.9104e-11,\n",
      "         6.1846e-11,  1.1642e-10, -3.2742e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-2.1119e-04,  4.3422e-04,  5.0703e-04,  1.8986e-04,  2.6194e-03,\n",
      "         1.9197e-03,  2.1199e-03, -2.5485e-04, -2.0455e-03, -1.2197e-03,\n",
      "        -2.7032e-04,  1.2786e-03, -4.3103e-03,  6.8045e-04,  5.2585e-04,\n",
      "        -1.5768e-03, -1.5880e-04,  1.7711e-03,  1.7198e-03,  9.0422e-04,\n",
      "        -1.8876e-03, -1.4824e-03, -1.2337e-03,  3.0418e-04,  2.6371e-03,\n",
      "        -8.3801e-04,  1.6545e-03,  2.2767e-03, -3.8674e-03, -1.5019e-04,\n",
      "        -1.3311e-03,  4.8790e-04, -5.1165e-04,  2.3086e-03, -2.2782e-03,\n",
      "        -4.5469e-03,  1.0514e-05,  1.0677e-03,  1.7459e-03,  7.0513e-04,\n",
      "         1.8190e-03,  2.1357e-03,  2.2261e-03,  1.7340e-04,  9.6524e-04,\n",
      "        -8.1997e-04, -2.5153e-03, -3.4758e-03], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-4.0693e-05, -3.9401e-04,  7.4304e-05],\n",
      "          [ 5.1063e-05, -3.6145e-04, -1.3948e-04],\n",
      "          [ 1.6691e-04, -9.0746e-05,  6.1514e-04]],\n",
      "\n",
      "         [[ 5.6561e-05, -9.7183e-05, -1.7446e-04],\n",
      "          [ 5.1490e-05,  5.1179e-07, -2.0877e-04],\n",
      "          [ 8.0563e-05, -4.5985e-06, -1.8805e-04]],\n",
      "\n",
      "         [[-1.5796e-04, -1.9039e-04, -1.3623e-04],\n",
      "          [-6.1157e-05, -1.4550e-04,  2.4438e-05],\n",
      "          [-4.5185e-05, -2.5943e-04, -1.5701e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0370e-04,  3.5392e-04,  3.3122e-04],\n",
      "          [ 2.9157e-04,  1.2793e-04,  1.2676e-04],\n",
      "          [ 2.0046e-04,  1.1463e-05, -9.1575e-05]],\n",
      "\n",
      "         [[ 3.1374e-04,  1.1361e-05,  1.2588e-05],\n",
      "          [ 1.0987e-04, -5.2182e-04, -1.3867e-04],\n",
      "          [-1.1916e-05, -3.9107e-04, -6.1441e-04]],\n",
      "\n",
      "         [[-1.0419e-04, -2.2123e-04, -1.2095e-05],\n",
      "          [-3.0023e-04, -5.1780e-04, -7.0075e-04],\n",
      "          [-3.7636e-04, -5.8701e-04, -8.3957e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.0266e-04, -2.9655e-04, -1.3775e-04],\n",
      "          [-2.8365e-04,  1.0138e-04,  7.4284e-05],\n",
      "          [-2.7519e-04,  1.8151e-05,  6.6278e-05]],\n",
      "\n",
      "         [[ 3.5918e-05,  4.5341e-05,  8.3041e-05],\n",
      "          [ 3.4467e-05,  1.0285e-04,  2.4456e-05],\n",
      "          [ 1.8417e-04,  2.5712e-04,  9.9988e-05]],\n",
      "\n",
      "         [[-1.9956e-05,  6.0254e-05,  3.2878e-05],\n",
      "          [ 6.8984e-06,  6.3397e-05,  7.9583e-05],\n",
      "          [-3.5813e-05,  1.1064e-04,  7.0669e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3823e-04,  1.5114e-04,  1.6820e-04],\n",
      "          [ 8.0420e-05,  8.0105e-05,  7.1336e-05],\n",
      "          [ 2.0132e-04,  1.6216e-04,  1.2568e-04]],\n",
      "\n",
      "         [[-3.4641e-04, -2.2412e-05,  5.0244e-05],\n",
      "          [-4.4148e-04, -1.4326e-04, -2.0250e-04],\n",
      "          [-2.0090e-04, -2.5151e-04, -1.9831e-04]],\n",
      "\n",
      "         [[ 1.7865e-04,  2.0564e-04,  1.1101e-04],\n",
      "          [ 1.7769e-04,  1.4580e-04,  1.1243e-04],\n",
      "          [ 2.2530e-04,  1.3849e-04,  1.1710e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 4.3815e-04,  1.8996e-03, -8.1613e-05],\n",
      "          [ 5.0532e-04,  2.2054e-03,  3.1460e-04],\n",
      "          [ 1.7995e-04,  1.5797e-03, -4.1587e-04]],\n",
      "\n",
      "         [[ 4.8699e-04,  6.5768e-04, -3.3348e-05],\n",
      "          [ 4.5526e-04,  6.1425e-04, -3.1494e-04],\n",
      "          [ 2.8198e-04,  4.3735e-04, -3.8220e-04]],\n",
      "\n",
      "         [[-6.4139e-06,  7.1800e-04,  1.3343e-04],\n",
      "          [-6.1435e-05,  7.8384e-04,  1.8804e-04],\n",
      "          [ 3.5302e-05,  5.1892e-04, -1.3189e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1776e-04, -4.8504e-06,  9.4215e-05],\n",
      "          [ 2.5532e-04,  5.4933e-05, -4.6364e-05],\n",
      "          [ 1.5385e-04,  1.6382e-04, -8.5855e-05]],\n",
      "\n",
      "         [[-2.8487e-04, -3.6471e-04,  1.5355e-04],\n",
      "          [-3.9641e-04,  3.7071e-04, -8.2811e-05],\n",
      "          [-2.6140e-05,  3.7057e-04,  6.2971e-05]],\n",
      "\n",
      "         [[-4.9806e-04, -5.3121e-04, -2.3900e-04],\n",
      "          [-4.6661e-05, -5.2697e-05, -1.4322e-04],\n",
      "          [ 3.9442e-06,  2.2717e-04,  1.1643e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 9.2519e-06,  7.7279e-04,  2.8543e-05],\n",
      "          [ 1.9306e-04,  6.0469e-04,  2.0494e-04],\n",
      "          [-3.0531e-04, -5.0034e-05, -2.3920e-04]],\n",
      "\n",
      "         [[-1.3790e-04,  1.5936e-04,  1.4199e-04],\n",
      "          [-1.4828e-04,  8.1009e-05,  1.0143e-04],\n",
      "          [-1.9079e-05, -8.2870e-05, -6.1956e-05]],\n",
      "\n",
      "         [[-1.0318e-04,  2.3239e-06, -2.7868e-05],\n",
      "          [ 6.1654e-05, -1.1078e-04, -4.2213e-04],\n",
      "          [-5.4654e-05, -7.3363e-05, -1.9372e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3450e-04,  6.5937e-05,  1.4910e-04],\n",
      "          [ 8.8270e-05,  2.5689e-04,  2.0545e-04],\n",
      "          [-5.4029e-05,  8.1004e-05,  9.4566e-05]],\n",
      "\n",
      "         [[ 2.2591e-04,  2.3276e-04,  1.7605e-04],\n",
      "          [ 3.1799e-04,  4.6235e-04,  2.8428e-04],\n",
      "          [ 1.3939e-04,  1.7696e-04,  1.9175e-04]],\n",
      "\n",
      "         [[ 1.8618e-04,  1.2732e-04,  1.4411e-04],\n",
      "          [ 8.2244e-05,  1.9878e-04,  2.7709e-04],\n",
      "          [-8.5463e-05, -2.1672e-05,  1.0511e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.4326e-04,  4.8976e-06,  1.0911e-04],\n",
      "          [-2.3437e-04, -1.2133e-04, -1.7257e-04],\n",
      "          [-1.6133e-04, -5.4845e-05, -4.8548e-05]],\n",
      "\n",
      "         [[ 1.2364e-04,  2.3240e-04,  3.6685e-04],\n",
      "          [ 5.5623e-05,  5.2656e-05,  2.0872e-04],\n",
      "          [ 9.6270e-05,  2.4269e-05,  1.3856e-04]],\n",
      "\n",
      "         [[ 1.5337e-04,  1.3800e-04,  1.7860e-04],\n",
      "          [ 1.8768e-04,  1.9493e-04,  1.4063e-04],\n",
      "          [ 1.8596e-04,  1.7517e-04,  4.7940e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.7119e-04,  6.1541e-04,  4.7748e-04],\n",
      "          [ 3.0521e-04,  3.0003e-04,  3.4335e-04],\n",
      "          [ 9.8434e-05,  1.7336e-04,  2.9682e-04]],\n",
      "\n",
      "         [[ 9.5123e-07,  1.0258e-04, -6.0416e-05],\n",
      "          [-1.7704e-04, -6.3274e-05, -1.2216e-05],\n",
      "          [-9.4921e-05,  8.9237e-05,  4.5027e-05]],\n",
      "\n",
      "         [[ 2.6019e-04,  2.7222e-04,  8.4155e-05],\n",
      "          [-2.1090e-05,  4.0119e-05,  1.9819e-04],\n",
      "          [ 7.9446e-05,  6.0569e-05,  1.5807e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1639e-04,  5.0179e-04,  2.2715e-04],\n",
      "          [ 5.7189e-04,  5.7182e-04,  3.5231e-04],\n",
      "          [ 3.9918e-04,  5.3063e-04,  3.1578e-04]],\n",
      "\n",
      "         [[ 5.7825e-05, -5.0074e-05,  1.6550e-04],\n",
      "          [ 9.2265e-05, -4.6386e-06,  1.3173e-04],\n",
      "          [ 8.9140e-05,  3.0231e-05,  8.4523e-05]],\n",
      "\n",
      "         [[ 1.8004e-04,  6.0738e-05,  1.1440e-04],\n",
      "          [ 1.0417e-04, -2.7729e-05,  1.1276e-04],\n",
      "          [ 1.6039e-04,  1.0014e-05,  1.5884e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.5954e-05, -6.6235e-05,  2.3974e-05],\n",
      "          [ 7.4556e-06, -2.4862e-06, -7.6250e-06],\n",
      "          [-1.9087e-05,  1.6102e-05,  2.2387e-05]],\n",
      "\n",
      "         [[-4.1142e-04, -3.2717e-04, -3.7173e-04],\n",
      "          [-3.1744e-04, -4.7894e-05, -2.5092e-04],\n",
      "          [-1.4730e-04,  1.2164e-05, -1.3280e-04]],\n",
      "\n",
      "         [[-1.4546e-04, -2.1118e-04, -1.2122e-04],\n",
      "          [-2.3707e-06, -4.1170e-06,  5.4060e-05],\n",
      "          [ 5.7439e-05,  9.2318e-05,  9.9465e-05]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-2.9104e-11,  1.4552e-10, -1.8190e-11,  1.0186e-10, -2.9104e-11,\n",
      "         2.0373e-10,  1.1642e-10,  3.6380e-11, -1.4552e-10,  1.6007e-10,\n",
      "        -1.4552e-10, -3.1287e-10, -5.8208e-11, -5.8208e-11,  4.9477e-10,\n",
      "         2.1828e-10,  1.4552e-10, -8.7311e-11, -9.4587e-11,  1.1642e-10,\n",
      "         1.1642e-10, -1.4552e-10, -2.9104e-11,  7.2760e-11, -9.4587e-11,\n",
      "        -3.6380e-11,  5.8208e-11, -1.9281e-10, -5.0932e-11,  2.1828e-11,\n",
      "         1.4552e-11, -7.2760e-11,  2.1828e-11, -5.8208e-11,  8.7311e-11,\n",
      "         5.8208e-11, -1.4552e-10, -1.3097e-10, -1.0186e-10, -1.1642e-10,\n",
      "        -4.8749e-10,  6.9849e-10, -8.7311e-11,  1.4552e-11, -1.1642e-10,\n",
      "        -1.4552e-11, -3.6380e-11,  1.0914e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAG2CAYAAABRfK0WAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZ1UlEQVR4nO3dd1RU19oG8GeGoZehN6UoiNhALNhiSTSJLdFoNDHGnpiiad4Uvfcmpmt6j0mMURMTjSlqbmzRqElU7GJDUVCkSBGRDgPMnO8PPpAzQxfYU57fWmfBvMwZXkd0Hvbss7dCkiQJREREREZIKboBIiIiorowqBAREZHRYlAhIiIio8WgQkREREaLQYWIiIiMFoMKERERGS0GFSIiIjJaDCpERERktBhUiIiIyGgxqBAREZHREhpUtFotXnzxRXTo0AH29vYICQnBa6+9Bq7qT0RERACgEvnN33rrLSxbtgyrV69Gt27dcOTIEcyaNQtqtRpPPvmkyNaIiIjICChEbko4duxY+Pj4YMWKFdW1iRMnwt7eHmvWrBHVFhERERkJoSMqAwcOxFdffYXz588jLCwMJ06cwN69e/H+++/Xen+NRgONRlN9W6fTIScnBx4eHlAoFG3VNhEREd0ESZJQUFAAf39/KJUNzEKRBNJqtdILL7wgKRQKSaVSSQqFQnrzzTfrvP/ixYslADx48ODBgwcPMzhSUlIazApC3/pZt24dnnvuObzzzjvo1q0bYmNj8fTTT+P999/HjBkzDO6vP6KSl5eHwMBApKSkwMXFpS1bJyIiombKz89HQEAAcnNzoVar672v0KASEBCAhQsXYt68edW1119/HWvWrMG5c+caPD8/Px9qtRp5eXkMKkRERCaiKa/fQi9PLi4uNnhvysrKCjqdTlBHREREZEyETqa966678MYbbyAwMBDdunXD8ePH8f7772P27Nki2yIiIiIjIfStn4KCArz44ovYsGEDsrKy4O/vjylTpuCll16CjY1Ng+fzrR8iIiLT05TXb6FB5WYxqBAREZkek5mjYuwSsgpFt0BERGTRGFT0lFXosCk2DeM+24cR7/+FxKsMK0RERKIInUxrbCRJwl2f7EV8ZkF17dv9SXhlXHeBXREREVkujqjUoFAoMCzcS1b7+Wgq8kvLBXVERERk2RhU9EwfEAwr5Y19g4rKtFh/OEVgR0RERJaLQUVPO1d73NnNR1ZbHZMErc5kL44iIiIyWQwqtZg5sIPsdkpOCf48mymoGyIiIsvFoFKLvsFu6OYvv6575b4kMc0QERFZMAaVWigUCswaJB9Vibl4DWfT8wV1REREZJkYVOpwV6QfPJ3ky/iv3p8kphkiIiILxaBSB1uVFR7oFySrbTiehpyiMkEdERERWR4GlXo82D8Q1lY3LlXWVOiw9lCywI6IiIgsC4NKPbyd7TA2wl9W+y7mMsq1OkEdERERWRYGlQbMHBgsu52RX4ptpzPENENERGRhGFQaEBngil6BrrLayn2XxDRDRERkYRhUGkH/UuVjybk4kZIrphkiIiILwqDSCCO7+8LXxU5WW8VLlYmIiFodg0ojWFspMW2A/FLl309eQVZ+qaCOiIiILAODSiM9EB0IW9WNp6tcK2HNQV6qTERE1JoYVBrJzdEG90S1k9V+OHgZmgqtoI6IiIjMH4NKE8wcFCy7nV1Yhv+dSBfTDBERkQVgUGmCcF8XDOjoIaut3HcJkiQJ6oiIiMi8Mag00Sy9UZUzV/Jx5PJ1Mc0QERGZOQaVJhrexQcB7vayGheAIyIiah0MKk1kpVRgxoBgWW37mUyk5ZaIaYiIiMiMMag0w+S+AXC0saq+rdVJ+DYmSVxDREREZopBpRlc7Kxxb+/2stq6QykoLqsQ1BEREZF5YlBpphl6uyrnlZRjw/E0Mc0QERGZKQaVZuro5YRhnb1ktVX7knipMhERUQtiULkJ+rsqX8gqxL6Ea4K6ISIiMj8MKjdhSCdPhHg5ymq8VJmIiKjlMKjcBIVCgZl6oyq74rOQlF0kqCMiIiLzwqBykyb2agcXO1X1bUkCVu1PEtcQERGRGWFQuUkONircHx0oq/18NBUFpeWCOiIiIjIfDCotYPqAICgVN24Xairw05FUcQ0RERGZCaFBJTg4GAqFwuCYN2+eyLaarL2bA+7o6iurrY5JglbHS5WJiIhuhtCgcvjwYaSnp1cfO3bsAABMmjRJZFvNMlNvV+XL14qxJz5LTDNERERmQmhQ8fLygq+vb/Xx+++/IyQkBEOHDhXZVrP06+COLn4ustrKfUlimiEiIjITRjNHpaysDGvWrMHs2bOhUChqvY9Go0F+fr7sMBYKhQKz9EZV9iZk43xmgZiGiIiIzIDRBJWNGzciNzcXM2fOrPM+S5YsgVqtrj4CAgLarsFGuDvSHx6ONrIaR1WIiIiaz2iCyooVKzBq1Cj4+/vXeZ9FixYhLy+v+khJSWnDDhtmZ22FB/rJL1XecDwVucVlgjoiIiIybUYRVC5fvoydO3fioYceqvd+tra2cHFxkR3G5sH+QVDVuFa5tFyHtYeMK1ARERGZCqMIKitXroS3tzfGjBkjupWb5uNihzERfrLadzFJqNDqBHVERERkuoQHFZ1Oh5UrV2LGjBlQqVQNn2ACZg4Mlt2+kleKP+IyxTRDRERkwoQHlZ07dyI5ORmzZ88W3UqLiQp0Q88AV1mNuyoTERE1nfCgcscdd0CSJISFhYlupUXpX6p8OOk6TqfliWmGiIjIRAkPKuZqdA8/+LjYymrfcFSFiIioSRhUWom1lRLT+gfJar+fSMfVAo2gjoiIiEwPg0ormhIdCBvVjae4TKvD9wcvC+yIiIjItDCotCIPJ1uM7ylfwG7NgWRoKrSCOiIiIjItDCqtbObADrLb2YUabDmVLqgbIiIi08Kg0sq6+rugXwd3WW3lviRIkiSoIyIiItPBoNIGZg2Sj6qcTM3DseTrgrohIiIyHQwqbeD2rj5o72Yvq33DXZWJiIgaxKDSBqyUCswYECyrbTudgSu5JWIaIiIiMhEMKm1kct8AONhYVd/W6iR8G8NLlYmIiOrDoNJG1PbWmNirvay29lAyissqBHVERERk/BhU2pD+/j95JeX45WiqmGaIiIhMAINKG+ro5YTh4d6y2jf7kqDT8VJlIiKi2jCotLE5t8gvVb6UXYTd8VmCuiEiIjJuDCptbECIB8J9nWW1FXu5qzIREVFtGFTamEKhMBhV2Z94DXFX8gV1REREZLwYVAS4u6c/PJ1sZbVv9nFUhYiISB+DigC2KitM6x8kq/0WewVZBaWCOiIiIjJODCqCTO0fCBvVjae/TKvDmgPJAjsiIiIyPgwqgng62WJ8T39Z7fsDl1FarhXUERERkfFhUBFott6k2mtFZdgUmyaoGyIiIuPDoCJQuK8Lbgn1lNVW7L0ESeICcERERACDinD6lyqfzyzE3oRsQd0QEREZFwYVwYaGeaGjl6OsxgXgiIiIKjGoCKZUKjB7kHxUZU/8VSRkFQjqiIiIyHgwqBiBib3aw9XBWlZbuS9JTDNERERGhEHFCNjbWOGB6EBZ7ZdjqbheVCaoIyIiIuPAoGIkpg8IhkqpqL5dWq7DD4e4ABwREVk2BhUj4au2w9gIP1nt25gklFXoBHVEREQkHoOKEZlzS0fZ7cx8DbacShfUDRERkXgMKkakR3s1ooPdZTUuAEdERJaMQcXI6C+rfyotD4eTrgvqhoiISCwGFSNze1cfBLjby2or9l4U1A0REZFYDCpGxkqpwMyB8lGVP+IykXytWFBHRERE4jCoGKHJfdrDyVZVfVuSgJX7uaw+ERFZHuFBJS0tDQ8++CA8PDxgb2+PHj164MiRI6LbEsrZzhr39Q2Q1dYfTkF+abmgjoiIiMQQGlSuX7+OQYMGwdraGlu3bkVcXBzee+89uLm5iWzLKMwcGIwa67+hqEyL9YdTxDVEREQkgKrhu7Set956CwEBAVi5cmV1rUOHDvWcYTkC3B1wZzdfbD2dUV1buS8JMwcGQ2UlfCCMiIioTQh9xfvtt9/Qp08fTJo0Cd7e3oiKisLy5ctFtmRU5uhdqpyWW4I/4jIFdUNERNT2hAaVixcvYtmyZejUqRO2b9+Oxx57DE8++SRWr15d6/01Gg3y8/NlhznrHeSGyPZqWW3FXk6qJSIiyyE0qOh0OvTq1QtvvvkmoqKiMHfuXDz88MP44osvar3/kiVLoFarq4+AgIBa72cuFAqFwQJwRy9fR2xKrpiGiIiI2pjQoOLn54euXbvKal26dEFycu27Bi9atAh5eXnVR0qK+U8uHd3DD35qO1ntG46qEBGRhRAaVAYNGoT4+HhZ7fz58wgKCqr1/ra2tnBxcZEd5s7aSonpA4JltS2n0pGeVyKmISIiojYkNKg888wzOHDgAN58800kJCTghx9+wFdffYV58+aJbMvoPBAdCHtrq+rbFToJq/dfFtgRERGZu29jkvDCzyeRXagR2ofQoNK3b19s2LABa9euRffu3fHaa6/hww8/xNSpU0W2ZXTUDta4t3d7WW3toWQUl1UI6oiIiMzZtUIN3t0ejx+PpODWd/bg638uolyrE9KL8AU5xo4di1OnTqG0tBRnz57Fww8/LLolozRrULDsdl5JOX45miqmGSIiMmvv7TiP/NLKX4YLNBV4ffNZnEzNE9KL8KBCjdPRywnDw71ltW/2JUGnkwR1RERE5uh0Wh7WHpJf1HJ3pD96B4lZNZ5BxYToLwB3KbsIu+OzBHVDRETmRpIkvPzbGUg1fge2t7bCotHhwnpiUDEhA0I8EO7rLKtxATgiImopv524giOXr8tq824NgZ/aXlBHDCompbYF4PYnXkPcFfNeoZeIiFpfkaYCS7ack9UC3O3x0OCOgjqqxKBiYu6O9Ienk42s9s0+jqoQEdHN+XxPAjLyS2W1F8d0hV2N5TFEYFAxMXbWVniwv3xBvN9iryCroLSOM4iIiOp3+VoRlv8t/6V3cCdP3N7VR1BHNzComKAH+wfBRnXjr65Mq8OaA7VvO0BERNSQ1zefRVmNdVJUSgUW39UVCoVCYFeVGFRMkKeTLcb39JfVvj9wGaXlWkEdERGRqfr7/FXsiMuU1WYMDEaot3MdZ7QtBhUTpT+p9lpRGTbFpgnqhoiITFG5VodX/ndGVvNwtMGTwzsJ6sgQg4qJCvd1wS2hnrLair2XIElcAI6IiBpn9f4kJF4tktWeH9kZantrQR0ZYlAxYfoLwJ3PLMS+hGuCuiEiIlNytUCDj3ZekNUi2qsxqXeAoI5qx6BiwoaGeaGjl6OstmLvRUHdEBGRKXl3ezwKNPLNbRff1Q1KpfgJtDUxqJgwpVKB2YPkoyq7468iIatQUEdERGQKTqTkYv3RFFltQlQ7Yfv51IdBxcRN7NUerg7y9xK5ABwREdVFp5Pw8v/k+/k42ljhhVHi9vOpD4OKibO3scID0YGy2i9HU5FdqBHUERERGbONsWk4npwrqz0xvBN8XOzENNQABhUzMH1AMKytbrynqKnQ4dv9SeIaIiIio1SoqcCSrfL9fII9HDBrULCYhhqBQcUM+KrtMK5nO1ltdcxlFOlNkiIiIsv2ya4LuFogH3F/6a6usFWJ3c+nPgwqZuKRIfLdLfNKyvHj4ZQ67k1ERJbmUnYRvtkrn8N4a2cv3BYufj+f+jComIlOPs4Y0cVbVlux9xLKa+zdQEREluu13+NQrr0xg9baSoEXx3YV2FHjMKiYkUeGhshup+WWYPPJdEHdEBGRsdh9Lgu7zmXJarMHdUBHLydBHTUeg4oZ6RPkhl6BrrLaF38lcll9IiILVlahw6u/x8lqXs62mH9bqKCOmoZBxYwoFAo8qjeqci6jAH+dvyqoIyIiEm3lvku4lC3fz+eFkeFwtjOe/Xzqw6BiZkZ08TFYVv/Lv7isPhGRJcrKL8XHf8r38+kZ4IoJUe3qOMP4MKiYGaVSYXAFUMzFaziRkiumISIiEuatbfEoKtPKaq/cbXz7+dSHQcUMjY9qB29nW1ntq785qkJEZEmOJV/HL8dSZbVJvdsjMsBVTEPNxKBihmxVVph9i3yzwq2n05Gk9x4lERGZJ51Owiu/nZHVnG1VeH6kce7nUx8GFTP1QL9AONmqqm/rJGD5PxxVISKyBD8fS8WJ1DxZ7akRneClN9puChhUzJSLnTWm9pNvVvjT0VSDpZOJiMi85JeW4+1t8v18QrwcMX1AsJiGbhKDihmbNaiDbLPCsgodvo1JEtcQERG1uo93XkB2YZms9tJd3WCjMs2XfNPsmhrFV22He/QuQfuWmxUSEZmthKxCrNqfJKuN6OKDoWFeYhpqAQwqZm5uLZsVruNmhUREZkeSJLzyvzOo0N1YjdzGSokXx3YR2NXNY1Axc6HezhjRRb4z5op/LnKzQiIiM7PzbBb+uZAtqz00uAOCPBzrOMM0MKhYgEeHykdVruSV4veTVwR1Q0RELa20XIvX9Pbz8XGxxbxbTWM/n/owqFiAPsHu6BPkJqt9+ddFblZIRGQmVuy9hOScYlnt36O7wLHGMhWmikHFQjxSy2aFe7hZIRGRycvIK8VnuxNktT5Bbrg70l9QRy2LQcVCDA/3RojBZoWJgrohIqKWsnTrWRTX2M9HoQBevrsbFArT2c+nPkKDyssvvwyFQiE7wsNNb3lfU1C5WaF8VOXAxRzEcrNCIiKTdSQpBxtj5XMO7+8biO7t1II6annCR1S6deuG9PT06mPv3r2iWzJb46L84eOiv1khR1WIiExRhVaHxXr7+bjYqfDsHWGCOmodwoOKSqWCr69v9eHp6Sm6JbNlq7LC7EH6mxVm4BI3KyQiMjnfHbiMM1fyZbVnbg+Dh5Pp7edTH+FB5cKFC/D390fHjh0xdepUJCcn13lfjUaD/Px82UFNM6VfIJxrzAKXuFkhEZHJycwvxXt/nJfVwnyc8GD/IEEdtR6hQaVfv35YtWoVtm3bhmXLluHSpUsYPHgwCgoKar3/kiVLoFarq4+AgIA27tj0udhZ44H+8s0Kf+ZmhUREJuX1zWdRqLcdyhv39IC1lfDxhxankIxoMY3c3FwEBQXh/fffx5w5cwy+rtFooNHceEHNz89HQEAA8vLy4OLi0patmrTM/FIMfms3ymqsTjv/1lA8e2dngV0REVFj/HPhKqatOCSr3du7Pd6dFCmoo6bLz8+HWq1u1Ou3UUUvV1dXhIWFISEhodav29rawsXFRXZQ0/m41LZZYRI3KyQiMnKl5Vq8tEk+gVZtb41Fo8z3ilmjCiqFhYVITEyEn5+f6FbM3sN6mxXml1Zg7aG65wcREZF4X/510eACiIWjws1uAm1NQoPKs88+i7/++gtJSUnYv38/7rnnHlhZWWHKlCki27IIod5OuL2r3maFey9xs0IiIiOVlF2Ez/bI33HoFeiK+/qY93xNoUElNTUVU6ZMQefOnTF58mR4eHjgwIED8PLyEtmWxdDfrDA9rxT/O8HNComIjI0kSXjptzMoq7jxy6RSAbw+vgeUSvNYgbYuQncrWrdunchvb/F6B7mjb7AbDiddr659+ddF3BPVzmyWXiYiMgdbT2fgb7392WYO7ICu/uY/V9Oo5qhQ29NfVj8+swB74rlZIRGRsSjUVODV/8XJaj4utlhgZivQ1oVBxcLdFu6NUG8nWe0LblZIRGQ0PthxHhn5pbLaS2O7wclW6JsibYZBxcIplQrM1bsC6OClHBxPvl7HGURE1FbiruRj1f4kWW1ImBdG9/AV05AADCqE8T3b1bJZIZfVJyISSaeT8N+Np6DV3ViX1UalxKt3d7OoeYQMKgQblRJzbpFvVrjtTAYuXi0U1BEREf14JAXHknNltceHhSDY01FMQ4IwqBAAYEp0bZsVXhLYERGR5bpWqMHSredktWAPBzw6NKSOM8wXgwoBAJztrDFVb9fNX46lIqugtI4ziIiotSzdeg55JeWy2mvju8PO2kpQR+IwqFC12YOCYVNj582yCh1W603iIiKi1nU4KQc/HU2V1cZG+GFwJ8tcDJVBhap5u9hhQi/5ZoXfxVw22EqciIhaR7lWh/9uOC2rOdmq8OLYroI6Eo9BhWQeHtIRNSeT55dWYB03KyQiahPf7L2E+MwCWW3B7WHwcbET1JF4DCokE+LlhNu7GG5WWHN/CSIianlpuSX4cOcFWa2rnwumDwiq4wzLwKBCBh7Rm1XOzQqJiFrfq/87g5JybfVthQJ4457uUFlZ9ku1Zf/pqVa9g9wQHewuq335dyIkSarjDCIiuhm7zmVi+5lMWW1KdCCiAt0EdWQ8GFSoVo8MlS+rfz6zELvjswR1Q0RkvkrKtHhp0xlZzcPRBi/cGS6oI+PCoEK1urWzNzoZbFbIZfWJiFrap7svIPV6iay2aHQXqB2sBXVkXBhUqFa1bVZ46FIOjl7mZoVERC0lIavAYG+16A7umKi3VIQlY1ChOo3r2Q6+epfEfbLrQh33JiKippAkCS9uPINy7Y35fyqlAq+P725Rmw42hEGF6mSjUuKhwfLNCvfEX0VsSq6YhoiIzMim2CuIuXhNVntocEeE+TgL6sg4MahQvab2C4Knk42s9vGfHFUhIroZeSXleH1znKzWztUeTw4PFdSR8WJQoXrZ21gZzFXZdS4LJ1NzxTRERGQG3t0ej+zCMlnt5bu7wcFGVccZlotBhRr0YP8geDhyVIWIqCWcSMnFmoOXZbURXbxxe1efOs6wbAwq1CAHGxUe1htV2Xk2C6fT8gR1RERkmrQ6Cf/deBo118+0s1Zi8V3dxDVl5BhUqFGm9Q+Cu96oykccVSEiapLvD17GKb1f8p4c3gkB7g6COjJ+DCrUKI62KoMrgHbEZeLMFY6qEBE1RlZBKd7ZFi+rdfJ2wkO3dKzjDAIYVKgJpg8IhqveSomcq0JE1DhvbD6LAk2FrPba+O6wUfGluD58dqjRnGxVeHiwPPlvP5OJs+n5gjoiIjIN+xKysSlWvgv9hKh26N/RQ1BHpoNBhZpk+oAgqO05qkJE1FiaCi1e3HhaVnOxU+HfY7oI6si0NCuopKSkIDU1tfr2oUOH8PTTT+Orr75qscbIODnbWWPOLfK5KltPZ+BcBkdViIhq8/nuRFzMLpLVnh8ZDk8nW0EdmZZmBZUHHngAu3fvBgBkZGTg9ttvx6FDh/Cf//wHr776aos2SMZn5qBguNjJFyX6ZFeCoG6IiIzX2fR8fLZb/v9jZIArHogOFNSR6WlWUDl9+jSio6MBAOvXr0f37t2xf/9+fP/991i1alVL9kdGyMXOGrP1RlW2nErHhcwCQR0RERmfCq0Oz/98EhW6G4umWCkVeGN8dyiV3HSwsZoVVMrLy2FrWzlktXPnTtx9990AgPDwcKSnp7dcd2S0Zg3qAOcaoyqSBHzMURUiomrL/7lksGbKY0ND0L2dWlBHpqlZQaVbt2744osv8M8//2DHjh0YOXIkAODKlSvw8OAMZkugtrfGrEHyUZXfT15BQhZHVYiIErIK8cHO87JaqLcTnuCmg03WrKDy1ltv4csvv8SwYcMwZcoUREZGAgB+++236reEyPzNGdQBzrbyURXOVSEiS6fVSXj+5xMoq9BV1xQK4O17I2CrshLYmWlq1jaNw4YNQ3Z2NvLz8+Hm5lZdnzt3LhwcuAywpVA7WGPmoGBZOPnfiSt4cngnhHg5CeyMiEic1fuTcCw5V1abM6gDegW61X4C1atZIyolJSXQaDTVIeXy5cv48MMPER8fD29v7xZtkIzbnFs6wKnGqIpOAj7lqAoRWajL14rw9vZzslqQhwP+dUdnQR2ZvmYFlXHjxuHbb78FAOTm5qJfv3547733MH78eCxbtqxZjSxduhQKhQJPP/10s84nMVwdbDBjYJCstik2DRevFgrqiIhIDEmSsPCXUygt18nqb02MgL0N3/JprmYFlWPHjmHw4MEAgJ9//hk+Pj64fPkyvv32W3z88cdNfrzDhw/jyy+/RERERHPaIcHm3NIRDjX+Eeok4NPdHFUhIsuy9lAKYi5ek9Wm9Q/iMvk3qVlBpbi4GM7OzgCAP/74AxMmTIBSqUT//v1x+fLlJj1WYWEhpk6diuXLl8vmu5DpcHe0wfQBwbLaptgrSNJbiZGIyFxdyS3Bm1vOymrtXO3xwqhwQR2Zj2YFldDQUGzcuBEpKSnYvn077rjjDgBAVlYWXFxcmvRY8+bNw5gxYzBixIgG76vRaJCfny87yDg8PLgD7K1vjKpodRJHVYjIIkiShH9vOIVCvZ2Rl0zoIZvDR83TrKDy0ksv4dlnn0VwcDCio6MxYMAAAJWjK1FRUY1+nHXr1uHYsWNYsmRJo+6/ZMkSqNXq6iMgIKA57VMr8HCyxfQB8rkqG46n4fI1jqoQkXn79Vga9sRfldUm9W6PIWFegjoyL80KKvfeey+Sk5Nx5MgRbN++vbo+fPhwfPDBB416jJSUFDz11FP4/vvvYWdn16hzFi1ahLy8vOojJSWlOe1TK3l4SEeDUZXPdycK7IiIqHVl5Zfilf+dkdW8nW3x3zFdBXVkfhSSJEkN361uVbsot2/fvknnbdy4Effccw+srGq8sGm1UCgUUCqV0Gg0sq/VJj8/H2q1Gnl5eU1+y4laxxub47D8n0vVt1VKBXY/OwwB7lxfh4jMiyRJeOS7o/gjLlNWXz69D27v6iOoK9PQlNfvZo2o6HQ6vPrqq1Cr1QgKCkJQUBBcXV3x2muvQafTNfwAqBx9OXXqFGJjY6uPPn36YOrUqYiNjW0wpJBxmjskBHbWN36sKnQSPt/DuSpEZH42n0o3CCl3R/ozpLSwZs3y+c9//oMVK1Zg6dKlGDRoEABg7969ePnll1FaWoo33nijwcdwdnZG9+7dZTVHR0d4eHgY1Ml0eDnbYmq/IKzYe2NU5acjqZh3ayjau3FUhYjMQ05RGRZvkr/l4+Fog5fv7iaoI/PVrBGV1atX4+uvv8Zjjz2GiIgIRERE4PHHH8fy5cuxatWqFm6RTM0jQzvCVqU/qsK5KkRkPl753xlcKyqT18Z1g7ujjaCOzFezgkpOTg7Cww2vDQ8PD0dOTk6zm9mzZw8+/PDDZp9PxsHb2Q4P9AuU1X46koK03BJBHRERtZwdcZnYFHtFVruzmw/G9PAT1JF5a1ZQiYyMxKeffmpQ//TTT7m6LAEAHh0aApsaoyrlWgnLOFeFiExcXkk5/rPhlKzmYqfCa+O6Q6FQCOrKvDVrjsrbb7+NMWPGYOfOndVrqMTExCAlJQVbtmxp0QbJNPm42GFK3wCsjrmxUvH6w5VzVfzU9gI7IyJqvjc2xyGrQCOrvXRXN3i7NG6ZDWq6Zo2oDB06FOfPn8c999yD3Nxc5ObmYsKECThz5gy+++67lu6RTNSjw0JgY3XjR6xMq8MyzlUhIhP19/mrWH8kVVYbGuaFib3aCerIMtz0Oio1nThxAr169YJWq22ph6wX11Exfi9uPI3vDtwYVbGxUuLv52+Fr5q/fRCR6SjUVODOD/6WzbVzslVh+zND0M6Vo8RN1errqBA11mPDQmBtdeN92zKtDl/8xVEVIjItb209Z3BBwKLR4QwpbYBBhVqVv6s9JveR78n0w6FkZOaXCuqIiKhpDly8JhsZBoABHT0wpW9gHWdQS2JQoVb3+K2h8lGVCo6qEJFpKCnTYuEvJ2U1e2srLJ3YA0olr/JpC0266mfChAn1fj03N/dmeiEz1c7VHvf2DsDaQ8nVtR8OJuOxYSHwduZcFSIyXu/viEfStWJZ7bk7OyPIw1FQR5anSSMqarW63iMoKAjTp09vrV7JhD0+LASqGr99aCp0+OqviwI7IiKq37Hk67LtQACgV6ArZgwMFtOQhWrSiMrKlStbqw8ycwHuDri3d3usO5xSXVtz8DIeGRoCL2dbgZ0RERnSVGjx/M8noatxXayNSom3742EFd/yaVOco0JtZt6tobJ/4KXlOiz/h6MqRGR8PvkzAQlZhbLa0yM6IdTbSVBHlotBhdpMgLsDJkTJF0b6LuYysgs1dZxBRNT2TqflYZnehP8e7dSYO7ijoI4sG4MKtan5t8lHVUrKtRxVISKjUa7V4bmfT0Jb4z0flVKBt++NgMqKL5ki8FmnNhXk4YjxPQ1HVXL0tksnIhLhiz2JOJueL6vNuzUUXfy4+rkoDCrU5ubfFoqac9GKyziqQkTinc8swCe75Lu8d/ZxxrxbQwV1RACDCgnQwdNwVGXVviSuVktEwpRrdXjupxMo0+qqa0oF8M6kCNio+FIpEp99EkJ/VKWkXIv3/zgvriEismgf7DiPE6l5strDQzoior2rmIaoGoMKCdHRywn39ZXvAbT+aIrBe8NERK1tf0K2wVU+HT0d8cyIMEEdUU0MKiTMMyPC4GBjVX1bkoAlW88J7IiILM31ojI8sz4WUo2F3aytFPjo/ijYWVvVfSK1GQYVEsbbxQ6PDAmR1f4+fxV/nb8qqCMisiSSJOH5X04iM1++ltNzd3ZGj/ZqQV2RPgYVEurhIR3grbeE/pItZ2VrGBARtYY1B5OxIy5TVhvcyRMP3cKF3YwJgwoJ5WCjwrN3dJbVzmUU4JejqYI6IiJLEJ9RgNd/j5PV3B1t8N6kSCi5l49RYVAh4Sb2bo9wX2dZ7d0/4lFcViGoIyIyZ6XlWjy59jg0FTpZ/d1JEfB2sRPUFdWFQYWEs1Iq8O/RXWS1rAINlv99qY4ziIia780tZxGfWSCrzRwYjNvCfQR1RPVhUCGjMCTMC0PCvGS1L/9ORBYXgSOiFrQjLhPfxlyW1br4uWDhqHBBHVFDGFTIaPx7dLjB0vof7OQicETUMjLzS/H8zydkNTtrJT6Z0pOXIhsxBhUyGuG+LpjUW74I3I+HUxCfUVDHGUREjaPTSViwPhbXi8tl9ZfGdkOot3MdZ5ExYFAho7LgjjDY1/jNRicBS7aeFdgREZmDL/++iH0J12S1kd18MSU6oI4zyFgwqJBR8XGxw9wh8jUM9sRfxT8XuAgcETVPbEou3vsjXlbzU9th6cQeUCh4KbKxY1AhozN3SEd46S0C98ZmLgJHRE1XqKnAU+uOo6LG/x8KBfDBfT3h6mAjsDNqLAYVMjqOtir863b5ZmDnMgrw6zEuAkdETfPSxtO4fK1YVpt/ayj6d/QQ1BE1FYMKGaVJfQLQ2cdwEbiSMq2gjojI1Gw8noZfj6fJar0CXfHU8E6COqLmYFAho2SlVGDRaPm6Bpn5Gnz9z0VBHRGRKUm+Voz/bjwtqznbqvDR/VFQWfGlz5Twb4uM1tAwLwzu5CmrLfsrEVkFXASOiOpWrtXhyXXHUaiRb8PxxoQeCHB3ENQVNReDChkthUKBRaO6QKG3CNyHOy+Ia4qIjN6HO88jNiVXVru3d3vcHekvpiG6KUKDyrJlyxAREQEXFxe4uLhgwIAB2Lp1q8iWyMh09XfBvb3ay2rrDiXjQiYXgSMiQ/sTs/H5nkRZrYOnI165u5ugjuhmCQ0q7du3x9KlS3H06FEcOXIEt912G8aNG4czZ86IbIuMzL/u6Aw76xs/qpWLwJ0T2BERGaPrRWVY8OMJSDVWMrC2UuDj+6PgaKsS1xjdFKFB5a677sLo0aPRqVMnhIWF4Y033oCTkxMOHDggsi0yMr5qO8wdLF8Ebte5LOxLyBbUEREZG0mS8PwvJ5Ght5Hps3d0Ro/2akFdUUswmjkqWq0W69atQ1FREQYMGFDrfTQaDfLz82UHWYa5Q0Pg6WS4CJyOi8AREYDvDyZjR1ymrDa4kyce1vslh0yP8KBy6tQpODk5wdbWFo8++ig2bNiArl271nrfJUuWQK1WVx8BAdyjwVI42aqwQG8RuLj0fGzQWyOBiCzP+cwCvPZ7nKzm7miD9yZFQqnkEvmmTnhQ6dy5M2JjY3Hw4EE89thjmDFjBuLi4mq976JFi5CXl1d9pKSktHG3JNLkPu3RydtJVuMicESWrbRciyfXHoemQiervzspAt4udoK6opYkPKjY2NggNDQUvXv3xpIlSxAZGYmPPvqo1vva2tpWXyFUdZDlUFkp8e/RXWS19LxSfLPvkqCOiEi0JVvO4lyG/CrAmQODcVu4j6COqKUJDyr6dDodNBqN6DbISA3r7IVBofI9Oj7fnYCrBfyZIbI0O+MysTrmsqzWxc8FC0eF13EGmSKhQWXRokX4+++/kZSUhFOnTmHRokXYs2cPpk6dKrItMmIKhQL/Hi1fBK6oTIuP/jwvrikianOZ+aV47ucTspqdtRKfTOkJO2srQV1RaxAaVLKysjB9+nR07twZw4cPx+HDh7F9+3bcfvvtItsiI9fNX40JUfJF4NYeSkFCFheBI7IEOp2EBetjcb24XFZ/aWw3hHo713EWmSqhK+CsWLFC5LcnE/bsnWH4/eSV6gl0Wp2EpVvP4esZfQV3RkSt7at/LmJfwjVZbWQ3X0yJ5pWg5sjo5qgQNYaf2t5gfYSdZ7OwP5GLwBGZs9iUXLy7PV5W81PbYenEHlAoeCmyOWJQIZP16LAQeDrZyGpvbuEicETmKrtQg8fWHEVFjX/jCgXwwX094epgU8+ZZMoYVMhkOdmq8PQI+SJwp9PysekEF4EjMjflWh3mfX8M6XnyJfLn3xqK/h096jiLzAGDCpm0+/sGIMTLUVZ7Z1s8Ssu5CByROXlzy1kcvJQjq/Xv6I6nhncS1BG1FQYVMmm1LQJ3hYvAEZmVDcdTsXJfkqzmr7bDZw/0gsqKL2Pmjn/DZPJuC/fGgI76i8Al4lohF4EjMnWn0/Kw6NdTspqNSokvpvWGh95GpWSeGFTI5CkUCvxnjHxUpVBTgY/+vCCoIyJqCdeLyvDomqMoLZfv4/P6+O6IaO8qpilqcwwqZBa6t1NjQlQ7We37g8lIvFooqCMiuhlanYQn1x1H6vUSWX1a/yBM7sP1UiwJgwqZjX/d2Rm2qhs/0lWLwBGR6Xlnezz+uSBfF6lPkBteHNtVUEckCoMKmY12rvaYc0sHWW1HXCb2JXAROCJTsvlkOr74K1FW83a2xedTe8FGxZctS8O/cTIrjw0LgYejfOGnf284hZIyXq5MZAriMwoMNhu0tlJg2YO94O1iJ6grEolBhcyKs501nrldvgjc5WvF+GAnd1cmMnZ5JeV45LsjKNb7xWLxXd3QO8hdUFckGoMKmZ0HogPRJ8hNVvv6n4s4kZIrpiEiapBOJ+GZH2ORdK1YVp/cpz2m9gsU1BUZAwYVMjtKpQJLJ0bApsZCUDoJeOGXkyir0NVzJhGJ8uGfF7DrXJasFtlejVfHdedmgxaOQYXMUqi3E54aIV9a+1xGgcEEPSISb0dcJj7WW/fIw9EGyx7sDTtrK0FdkbFgUCGzNXdIR3Txc5HVPtl1ARcyCwR1RET6Eq8WYsGPsbKalVKBTx/oBX9XezFNkVFhUCGzZW2lxDv3RsBKeWPYuFwr4YVfTkJbY5t4IhKjUFOBR747igJNhaz+79FdMCCEOyJTJQYVMmvd26nx0GD52irHknPxbUySmIaICAAgSRKeXX8CCVny1aPH9fTH7EHBYpoio8SgQmbvmRFhCPZwkNXe3haPlJziOs4gotb2+Z5EbDuTIat18XPB0gkRnDxLMgwqZPbsrK2wdGKErFZSrsW/N5yCJPEtIKK2tic+C+/+ES+ruTpY46tpvWFvw8mzJMegQhahf0cPg7UY/rmQjZ+PpgrqiMgyJV8rxlPrYlHzdwSlAvj4/igEuDvUfSJZLAYVshgLR4XDTy1fgvu13+OQVVAqqCMiy1JcVoG53x1BXkm5rP7snZ0xJMxLUFdk7BhUyGI421njjXu6y2r5pRVYvOmMoI6ILIckSVj4yymcy5AvDzC6hy8eGxoiqCsyBQwqZFFuC/fBuJ7+strW0xnYdjpdUEdElmHF3kv47cQVWa2TtxPevjeSk2epXgwqZHFeGtsV7no7LL+46QzyisvrOIOIbsb+xGws2XpOVnO2VeHLab3hZKsS1BWZCgYVsjgeTrZYfFdXWe1qgQZvbIkT1BGR+UrLLcH8H44bLLL44f090dHLSVBXZEoYVMgi3R3pj9vCvWW19UdSsfdCtqCOiMxPabkWj353FDlFZbL60yM6YXgXH0FdkalhUCGLpFAo8Pr47gbDzgt/PYnisoo6ziKixpIkCf/deBqn0vJk9RFdvPHkbZ3qOIvIEIMKWSx/V3ssHBUuq6VeL8G7288L6ojIfKw5cNlgnaIOno54/76eUCo5eZYaj0GFLNoD0YGI7uAuq63cfwnHkq8L6ojI9P1z4Spe/p98zpeDjRW+nNYbLnbWgroiU8WgQhZNqVRg6YQesFXd+KcgScALP5+EpkIrsDMi03Q+swCPrzlmMHn2vUmRCPNxFtQVmTIGFbJ4Hb2c8MztYbLahaxCfLY7UVBHRKYpq6AUs1YeRoFGPs/r8WEhGNXDT1BXZOoYVIgAPHRLB3Rv5yKrLduTgHMZ+YI6IjItJWVaPLz6CNJyS2T1MT388OwdnQV1ReaAQYUIgMpKibcmRsCqxiS/cq2EF34+aTCETURyOp2Ep388jhOp8it8ogJd8d7kSE6epZvCoEL0/7r5q/Ho0I6y2onUPKzcd0lQR0SmYem2c9h+JlNWC3C3x/LpfWBnbSWoKzIXQoPKkiVL0LdvXzg7O8Pb2xvjx49HfHy8yJbIwj1xWyd09HKU1d79Ix6XrxUJ6ojIuK05cBlf/X1RVnOxU2HlzL7wdLIV1BWZE6FB5a+//sK8efNw4MAB7NixA+Xl5bjjjjtQVMQXBRLDztoKb0+MQM090krLdVj4yylIEt8CIqppd3wWXtp0WlZTKRX4YlpvhHrzCh9qGQrJiP73vXr1Kry9vfHXX39hyJAhDd4/Pz8farUaeXl5cHFxafD+RI21eNNprI65LKstndAD90cHCuqIyLjEXcnHpC/2o6hMfhn/O/dGYFKfAEFdkaloyuu3Uc1RycurnIjl7u5e69c1Gg3y8/NlB1FreG5kONq52stqb2w+i4y8UkEdERmPzPxSzFl92CCkPHFbKEMKtTijCSo6nQ5PP/00Bg0ahO7du9d6nyVLlkCtVlcfAQH8B0Gtw8lWhTfukf8cFmgq8OKm03wLiCxakaYCs1cdRrpeaL870h8L9NYjImoJRhNU5s2bh9OnT2PdunV13mfRokXIy8urPlJSUtqwQ7I0wzp7Y0KvdrLajrhMbDmVIagjIrG0OglPrTuOM1fko9l9gtzw9r0RUCh4GTK1PKMIKvPnz8fvv/+O3bt3o3379nXez9bWFi4uLrKDqDW9OKYrPBxtZLXFv53Gdb1t64ksweub47DzbJasFuThgK94GTK1IqFBRZIkzJ8/Hxs2bMCuXbvQoUMHke0QGXBztMEr47rJatmFZXhtc1wdZxCZp1X7LmHlviRZTW1vjZUz+8JdL8wTtSShQWXevHlYs2YNfvjhBzg7OyMjIwMZGRkoKSlp+GSiNjKmhx9u7+ojq/16LA174rPqOIPIvPx5NhOv/i4P59ZWCnw1rTc6ejkJ6ooshdCgsmzZMuTl5WHYsGHw8/OrPn788UeRbRHJKBQKvD6+O5ztVLL6C7+cRHahRlBXRG3jdFoenlh7HPo7Sbx9bwT6dfQQ0xRZFOFv/dR2zJw5U2RbRAZ8XOzwn9FdZLXMfA2e+OE4KrQ6QV0Rta4ruSWYveowivUuQ35mRBjuiap7PiFRSzKKybREpuC+vgEY3MlTVou5eA3v7zgvqCOi1lP4/5chZxXIRw0nRLXDk8NDBXVFlohBhaiRFAoFPrivJ3xd7GT1z/ckYmdcZh1nEZmeCq0O8384hnMZBbJ6vw7uWDKxBy9DpjbFoELUBJ5Otvhsai+o9Latf2Z9LJKvFQvqiqjlSJKEl/93Bnvir8rqHT0d8eW03rBV8TJkalsMKkRN1DvIDf8dI5+vUlBagUfXHEVpubaOs4hMw4q9l7DmQLKs5u5og5Wz+sLVgZchU9tjUCFqhhkDg3FXpL+sFpeeb7CTLJEp2X4mA29sOSur2aiU+GpabwR5OArqiiwdgwpRMygUCiyd0AOh3vI1JNYfScWPh5PrOIvIeJ1MzcVT645DfyurdydFok9w7RvFErUFBhWiZnK0VeGLB3vD0Ub+nv2Lm87gdFqeoK6Imi71ejHmrD6C0nL5pfbP3dkZd+uNHBK1NQYVopsQ6u2Et+6NkNXKKnR4dM1R5BZzPyAyfvml5Ziz6giu6l2GPKl3ezw+LERQV0Q3MKgQ3aSxEf6YPUi+T1Xq9RIsWH8COv3lPImMSLlWh3nfH0N8pvwy5IEhHnjjHl6GTMaBQYWoBSwaHY4+QW6y2q5zWfh8T4Kgjojqp9NJWPjLKfxzIVtWD/V2wrIHe8NGxZcHMg78SSRqAdZWSnz6QC94Oskv33xvx3n8c+FqHWcRiSFJEl7ffBa/HEuV1T2dbLByZl+o7a0FdUZkiEGFqIX4qu3w8ZQo1FwLTpKAp9bF4koudwQn4/HprgR8s++SrGarUmL59D4IcHcQ1BVR7RhUiFrQwBBPPHtnZ1ktp6gMj39/DGUV3LyQxPvuwGW8p7c/lUqpwLIHeyEq0K2Os4jEYVAhamGPDgnBiC4+slpsSi7e2BwnqCOiSpti02pdlPDdSZG4LdynljOIxGNQIWphSqUC702ORKDeEPrqmMvYFJsmqCuydLvjs/Cv9ScMFnR75e5uGB/VTkxTRI3AoELUCtT21lj2YC/Y6l05sfCXUzivdykoUWs7kpSDx9YcRYXe5fJPj+iEGQODxTRF1EgMKkStpJu/Gq+P7y6rlZRr8eh3R1FQWi6oK7I0cVfyMWvVYYNVZ2cODMZTwzsJ6oqo8RhUiFrRpD4BmBIdIKtdzC7CC7+chKQ/Bk/UwpKyizD9m0MoKK2Q1e+JaoeXxnblgm5kEhhUiFrZ4ru6oXs7F1lty6kMrNh7qY4ziG5eZn4pHlxxENmF8qXxh4d74+17I6BUMqSQaWBQIWpldtZWWDa1t8EiWku2nsOhSzmCuiJzlltchmkrDiL1unz9nuhgd3w2tResrfhfP5kO/rQStYEAdwd8eF9PWU2rkzD/h2PIKigV0xSZpeKyCsxadRjnMwtl9a5+Lvh6Zh/YWVvVcSaRcWJQIWojt4Z748nbQmW1rAINnvjhOCq0XAyObp6mQotHvjuK48m5snoHT0esnh0NFzsujU+mh0GFqA09NSIMgzt5ymoHL+Xg3T/O13EGUeNodRIW/HjCYJNBXxc7fDcnGl7OtoI6I7o5DCpEbchKqcBH90fBX20nq3/xVyK2n8kQ1BWZOkmS8N+Np7H5VLqs7upgje/mRKO9G/fvIdPFoELUxtwdbf5/QqP8qotn159AUnaRoK7IlL29PR5rDyXLag42Vlg1KxqdfJwFdUXUMhhUiASICnTDi2O7ymoFmgo8uuYoSsq0groiU/TV34lYtidRVrOxqtwJuWeAq5imiFoQgwqRINP6B2FcT39Z7VxGAf6z8RQXg6NGWX84BW9uOSerKRXAx1N6YlCoZx1nEZkWBhUiQRQKBZZM6IEwHydZ/ddjafj6Hy4GR/XbdjodC389aVBfMqEHRnb3E9ARUetgUCESyMFGhWUP9oajjXxtize2nMXG49xpmWq3LyEbT66Nhd4eg1g0Khz39Q0U0xRRK2FQIRIsxMsJ70yKNKg/+9MJ/H3+qoCOyJidSMnF3G+PoExv7Z1Hh4bgkaEhgroiaj0MKkRGYHQPPywcFS6rVegkPLbmKE6l5gnqiozNhcwCzFx5CEV6E66nRAfghZGdBXVF1LoYVIiMxCNDOmLWoGBZrahMi5krD/GyZULq9WJMW3EI14vLZfUxPfzw+vge3AmZzBaDCpGRUCgUeHFMV4yNkE+EvFZUhunfHMLVAk0dZ5K5u1qgwbQVh5CRL98XanAnT7x/XySsuBMymTEGFSIjolQq8N7kSAwM8ZDVk3OKMWvVIRRqKgR1RqLkFJVhxjeHcElvVC0q0BVfPNgbtipuMkjmjUGFyMjYqqzw5bTe6OrnIqufTsvHo98dRVkFNzC0FFcLNJjy1QHEpefL6mE+Tlg5sy8cbVWCOiNqO0KDyt9//4277roL/v7+UCgU2Lhxo8h2iIyGs501Vs3uiwB3e1l9b0I2nvv5BHT616WS2cnML8X9X8UgPrNAVm/vZo/v5vSDq4ONoM6I2pbQoFJUVITIyEh89tlnItsgMkreznZYPSsa7o7yF6RNsVewZOtZQV1RW7iSW4L7voxB4lX52z3+ajt8/1A/+LjY1XEmkfkROm44atQojBo1SmQLREato1flEP/9Xx1ASfmNS1KX/3MJ3s52eHhIR4HdUWtIySnGlOUHkHq9RFYPcLfHDw/1R4A7d0Imy8I5KkRGLjLAFcse7AWV3pUdXL3W/FzKLsLkL2MMQkoHT0esf2QAQwpZJJMKKhqNBvn5+bKDyBIM6+yNt++NMKhz9VrzkZBVgMlfxiA9T34JcidvJ/w4tz/81PZ1nElk3kwqqCxZsgRqtbr6CAgIEN0SUZuZ0Kt9ravXPrrmKE6m5oppilrE2fR83PflAYO1csJ9nbF2bn94c04KWTCTCiqLFi1CXl5e9ZGSkiK6JaI29ciQjpg9qIOsVlymxayVh7l6rYk6nZaHKcsP4FpRmazeo50aax/uD08nW0GdERkHkwoqtra2cHFxkR1ElkShUOC/Y7rgrkh/WZ2r15qm48nXMWX5AeTqLYsfFeiKNQ/1g5sjL0EmEhpUCgsLERsbi9jYWADApUuXEBsbi+TkZJFtERk1pVKBdydFYFAoV681ZYeTcjBtxSEUlMr/vqKD3fHdnH5Q21sL6ozIuAgNKkeOHEFUVBSioqIAAAsWLEBUVBReeuklkW0RGT1blRW+eJCr15qq/YnZmL7CMFQODPHAqtl94cQVZ4mqKSRJMtklLvPz86FWq5GXl8e3gcgiZRWUYuKy/UjJkV/OOq6nPz6Y3BNKblZndP46fxVzvz0CjV6YHBrmhS+n9YadNffuIfPXlNdvk5qjQkRy3s52+HZ2v1pXr31zC1evNTZ/ns3Ew6sNQ8qILj74ajpDClFtGFSITFwHT0esnNkXDjbyF7mv917C8r8vCuqK9G07nY5HvjuKMq08pIzu4YvPp/biLshEdWBQITIDkQGu+Hxq7avXbjieKqgrqvLbiSuY98NxVOhtJjmupz8+vj8KNir+V0xUF/7rIDITda1e+9xPJ7l6rUA/H03F0+uOQ6sXUib1bo/3J/eEyor/DRPVh/9CiMzIhF7tsYir1xqNtYeS8dzPJ6CXUTC1XyDemhgBK052JmoQgwqRmZnL1WuNwur9SVj06ynoX1c5a1AwXh/fnVdkETUSgwqRmalv9dr7vzqAuCvczLO1Lf/7Ihb/dsag/sjQjnhpbFcoFAwpRI3FoEJkhupavTYjvxSTv4zhnJVW9OmuC3ijlkvDnxzeCQtHhjOkEDURF3zTd+gQMHky4Otbefj51f65ry9gw304yLgVlJbj/q8O4IzeKIpKqcCbE3pgch/uQN5SJEnCBzvO4+NdCQZfe/aOMMy/rZOAroiMU1Nev7lOs77UVODy5cqjIe7ujQs07u4Af4siAZztrLFmTj/M/e4IDiddr65X6CQ8//NJpOYU45nbw/hb/k0qKdPipU2n8dNRw0vB/zO6Cx4e0lFAV0TmgUFFX0ZG4++bk1N5xMXVfz9r67pDjP5hb39z/RPpcXO0wXdz+uHZn07g95Ppsq99vCsBqddLsHRiBNfyaKaLVwvx+PfHcC6jwOBrr9zdDTMGBrd9U0RmhEFFX1OCSmOVlwMpKZVHQ1xcGg4zvr6AlxdgxZUsqXHsrK3w8f1RaOdmjy//kq9W++vxNGTkl2LZg725Y28TbT6Zjhd+OWmwuaBCAbwxvgce6BcoqDMi88E5KvoSE4FTpyoDS9WRni6/XVbWMt/rZiiVlWGlZnjx86s8/P2Bdu0qDz+/yhEdov/33YHLWLzptMHaHmE+Tlg5KxrtXDmq15CyCh3e3HIWq/YnGXzN0cYKb98biTERfm3fGJGJaMrrN4NKU0kScP16/UGm6va1a23TU30UCsDbWx5eqo6aNTc3zqOxIH+ezcT8H46jpFwrq3s72+KbmX3RvZ1aUGfGL/V6Meb9cBwnUnINvtbZxxmfP9gLIV5Obd8YkQlhUDEWZWVAVlbdQabm7dJSsb3a2RmGF/3bfn6V9yOzcDI1F7NXHUF2oUZWd7CxwmdTe+HWzt6COjNeu85l4pkfTyCvpNzgaxN7tcfr47vD3oZvyRI1hEHF1EgSUFBQe4jRDzRZWTBY6rItubremENT9VaT/uHrC6jVHKExASk5xZi16jASsgpldSulAq+N6845Fv+vQqvD+zvO4/M9iQZfs1Up8eq4bpjcJ4BXTxE1EoOKOauoALKz6w4yV64AaWmVh8i5NPb2tYcZ/ZqXV+V8GxImr7gcc787goOXcgy+9viwEDx7R2eLXu49K78UT6w9XuvzE+zhgM+n9kZXfwv5/4eohTCoUOWoy7VrlYGlZnipOqpqVwWvUGplVTmHpuakYB8fw6ucfHw4StOKNBVaPP/zSWyKvWLwtbsj/fHOpAjYqizvLY39idl4cm2swdtjADCquy/eujcCLnacrE7UVAwq1HgaTeVITEOBpqREdKeArW3DYabqcwcH0d2aHEmS8N4f5/HpbsOVVft1cMdX0/pA7WAZL8o6nYTP9yTg/R3nDa6OUikV+PfoLpg1KJhv9ZD5KCsD8vJuHLm58s+7dAFGjmyxb8egQi1Lkip/WNPT5UfV2001j7w80d1WcnauDC5Vk4H9/eWfV00O5gJ7BtYeSsZ/N56GVu8VOsTLEatmRSPA3bxD4PWiMjyzPhZ74g1HG/3Vdvh0ai/0CnQT0BlRHbTaynmO+uGiKZ839MvozJnAypUt1jKX0KeWpVBUTqJ1da1M1fUpKak9wOjXWntScEFB5ZFgODog4+ZWe5Cp+bmPD6CynH8qU6ID4au2w/zvj6Go7Mbly4lXi3DP5/vwzcy+iGjvKq7BVnQs+Trmf38MV/IMr8Ib1tkLH0zuCTdH7vFFLai8/EZYyM+Xj2rUPOr7WmFhw9/nZuXmtv73qANHVEiMioobl25nZt6YEFzz86ojP7/hx2tNSqXh6Ez79kBgYOURFFRZN7OF9U6n5WH2qsPIKpDPz7C3tsInU6IwoquPoM5aniRJWLkvCUu2nkW5Vv5folIBLLg9DI8PC7XoScVUi6orNqtGJ6pGKKo+179dW8gwhrfVG+PWW4Fdu1rs4fjWD5mXkpKGw0xmptj1aJTKygBTFVxq+2iCP6NpuSWYtfIQzmfKf2NTKir3sZk2IFhMYy0ov7QcL/x8EltPG26f4elki4+n9MTAEE8BnVGrqwoa169XHrm5Nz42NnTodOL6by3W1pUXL6jVlSPpajXQqxfwzjst9i0YVMgy1VyPJjOzciJwzYnCNScLFxe3fX+urvUHGV9fo7xUO6+kHI+tOYr9iYYrLc8d0hELR4ab7EhD3JV8PP79USRdM/x5iO7gjk+nRMHbhYscGrWKihvhoSpw1Awd+p/rBxJzCxoKhWHIqO12fZ/b2bX6FZYMKkT1qQo0+gGmts8rKhp+vJZibQ0EBABhYUB4eOV8oKrDU+xv9GUVOiz89SR+PZZm8LUxPfzw3uRI2FmbzuXLkiRh/ZEUvLTpDDQVhi9Ujw0Lwb9uD4PKyviCo9nSaivDQ05O5dIKVUddt3NyKu9fYLhrtcmysroRGFxc5CGj5lHf15ycjPIXHn0MKkQtQaerXFxPP8CkpVXuhJ2cDFy+DBQVtX4vnp7y4FJ1BAS02doykiThg50X8PGfFwy+FtFejTm3dMDtXX3gYGPcE49LyrT478bT+OVYqsHX1PbWeH9yJIZ3MZ/5N21OkiondzYUNPRv5+aKXXW7JdQcnaj5sa7RC/3A4eBgMWtFMagQtZWqTSovX74RXGp+TE6ufCuqtTg6Go6+dOkChIS02uTe9UdS8O9fT6FCf4ERVO4TdGc3X4zr6Y9bQj2NZkSitFyLfy5kY+vpdOyIy0RBqeFIWWR7NT59oJfZX37dJCUlhqMYtY1s1Kzl5FReyWJqlMrKEOHmdiNc1AwZDX3u7GwSIxnGgkGFyJiUlgKpqfWHmZbe7sDaGggNlYeX7t2Bbt1a5FLrfy5cxWNrjqFQU/dbY55ONhgb4Y9xPf3RM8C1zRdHKynTYk98FraezsCfZzNll1rrmzkwGP8e3QU2KjN8oak5YbTq7ZKqj1WfVx36wUP0ZqlNZWNTGTRqHlXhQ/9z/dvOzhYzmmEMGFSITIlOV3mpdnIykJgInDsHnD1beZw/37IhxtERiI4GBg4EBgyoPNzdm/VQZ9Pz8dDqI0jLbfjyymAPB9zdsx3G9/RHRy+nZn2/xijUVGDXuSxsO52O3eeuoqS87nACAI42Vnjr3giMjfBvtZ5aTGlp3UGjvhBy/Xrl/A9T4uAAeHhU/mx6eNw4at52d688aoYNLuBoMhhUiMxFRQVw6dKN4FLzaKlJhOHhlYFl4MDKIzy80UPYJWVabDmVjo2xadiXkG2w3HxtIturMa5nO4yN9IO3881fUZNXUo4/z2Ziy6kM/H3hKspqmRxbm4EhHnhtfHeEtGJwMlBebhgi6vu85m1TG90AKkfvqsKFfuioL4jY8Uorc8egQmTuJKlyYm9tASYz8+Ye29UV6N//xqhLv36Vw+INyCooxe8n0rEpNg0nUhveSkGpAAaFemJ8z3a4s7svnGwb/5bU9aIy7IjLxJbT6diXkG2wSFttFAqgb7A7RnX3xcjuvvBTt8Bv35JUGSSSkiqPy5cr/17qCiBtMfG6NSgUlSMW+iMatX2s+TnfTqE6MKgQWbLr1w3DS1xc5QtpcyiVQI8e8lGXjh3rfQG6eLUQm2KvYFNsWq1rlOizs1ZiRBcfjO/ZDkPCvGqdK3K1QIM/4jKw9VQGYi5eM9iLqDZWSgX6d3THyO5+uLObT9NHcCSp8sqvmkGk6vOq222xfHlLUSpvBI6aH93c6n57xcOjctKolelcfk7Gj0GFiAxlZQEHDgD791cehw83/+0EL68bIy4DBwJRUZXrN+iRJAknUvOw8Xgafj95BdmFDc+3cXOwxugefhgf1Q4Bbg7YfiYDW06l43BSTqPeWlIpFRgU6onRPXxxe1dfuNe3N48kVT4v+uGj5uciFgdsiItL7YGjZvCo7Wsc4SAjwaBCRA0rKwNOnABiYm6El5SU5j9eUBDQtWvllUU1P/5/gKnQ6rAv8Ro2Hk/D9jMZKK7nKpymsrFSYkiYJ0Z198OILj5QO9S4NFurrQwc589XHhcuVB5VQUTU3A8Hh7rDhf7nNWuurha1SSaZJwYVImqe1NQbwSUmBjh27ObXxAgMNAgvxaFh2JFSjE2xV/D3+au1rsnSEDtrJYaFeWNUD1/c1tkLzjlXbwSRmh8TE1t/XQ+lsnLxveDgyg0rq942qSuAuLoCtrat2xOREWNQIaKWUVICHD0qH3XJymqZxw4IALp1Q0mnzjju7I/fyt3we7krCm3rWHBNktBOW4QJziUYYZWHroUZsL6YWBlGEhJad6KqSnUjiAQFVX6sOsx092yi1mRyQeWzzz7DO++8g4yMDERGRuKTTz5BdHR0g+cxqBC1MUmqvFy6asRl/37g5MkW3ditwNsP59zaI9alHQpsHdEpLx0RJVfhn5UC6/zcFvs+MtbWlSM/dQURf3++3ULUgkwqqPz444+YPn06vvjiC/Tr1w8ffvghfvrpJ8THx8Pb27vecxlUiIxAaSkQH195ZdGZMzc+JiQY1860rq6VGz6GhQGdOlVuM1AVRPz8eFULURsyqaDSr18/9O3bF59++ikAQKfTISAgAE888QQWLlxY77kMKkRGrLS08m2ZquBSFWISElpvpVQHh8oQUhVGan708OAVL0RGoimv30LHMsvKynD06FEsWrSouqZUKjFixAjExMQY3F+j0UCj0VTfzsurXFQqPz+/9Zsloqarevtk9OgbNY2mMqycPVs5EnP2bOW2AYmJjRuBUamADh0qR0RCQ298DA2tHBmpK4y01Eq+RHTTql63GzNWIjSoZGdnQ6vVwsdHvqW6j48Pzp07Z3D/JUuW4JVXXjGoBwQEtFqPRGRkKipuXGJMRCatoKAAarW63vuY1OywRYsWYcGCBdW3dTodcnJy4OHh0eY7sxqj/Px8BAQEICUlhW+FtSI+z22Dz3Pb4PPcdvhc3yBJEgoKCuDv3/CGoEKDiqenJ6ysrJCptzdJZmYmfH19De5va2sLW721B1xdXVuzRZPk4uJi8f8I2gKf57bB57lt8HluO3yuKzU0klKlcVukthIbGxv07t0bf/75Z3VNp9Phzz//xIABAwR2RkRERMZA+Fs/CxYswIwZM9CnTx9ER0fjww8/RFFREWbNmiW6NSIiIhJMeFC57777cPXqVbz00kvIyMhAz549sW3bNoMJttQwW1tbLF682ODtMWpZfJ7bBp/ntsHnue3wuW4e4euoEBEREdVF6BwVIiIiovowqBAREZHRYlAhIiIio8WgQkREREaLQcXE5eTkYOrUqXBxcYGrqyvmzJmDwsLCRp0rSRJGjRoFhUKBjRs3tm6jJq6pz3NOTg6eeOIJdO7cGfb29ggMDMSTTz5ZvT8VVfrss88QHBwMOzs79OvXD4cOHar3/j/99BPCw8NhZ2eHHj16YMuWLW3UqWlryvO8fPlyDB48GG5ubnBzc8OIESMa/HuhSk39ea6ybt06KBQKjB8/vnUbNFEMKiZu6tSpOHPmDHbs2IHff/8df//9N+bOnduocz/88ENuPdBITX2er1y5gitXruDdd9/F6dOnsWrVKmzbtg1z5sxpw66N248//ogFCxZg8eLFOHbsGCIjI3HnnXciKyur1vvv378fU6ZMwZw5c3D8+HGMHz8e48ePx+nTp9u4c9PS1Od5z549mDJlCnbv3o2YmBgEBATgjjvuQFpaWht3blqa+jxXSUpKwrPPPovBgwe3UacmSCKTFRcXJwGQDh8+XF3bunWrpFAopLS0tHrPPX78uNSuXTspPT1dAiBt2LChlbs1XTfzPNe0fv16ycbGRiovL2+NNk1OdHS0NG/evOrbWq1W8vf3l5YsWVLr/SdPniyNGTNGVuvXr5/0yCOPtGqfpq6pz7O+iooKydnZWVq9enVrtWgWmvM8V1RUSAMHDpS+/vpracaMGdK4cePaoFPTwxEVExYTEwNXV1f06dOnujZixAgolUocPHiwzvOKi4vxwAMP4LPPPqt1TyWSa+7zrC8vLw8uLi5QqYSvsyhcWVkZjh49ihEjRlTXlEolRowYgZiYmFrPiYmJkd0fAO68884670/Ne571FRcXo7y8HO7u7q3Vpslr7vP86quvwtvbmyOtDeD/mCYsIyMD3t7esppKpYK7uzsyMjLqPO+ZZ57BwIEDMW7cuNZu0Sw093muKTs7G6+99lqj35Yzd9nZ2dBqtQYrUPv4+ODcuXO1npORkVHr/Rv7d2CJmvM863vhhRfg7+9vEBLphuY8z3v37sWKFSsQGxvbBh2aNo6oGKGFCxdCoVDUezT2Pxl9v/32G3bt2oUPP/ywZZs2Qa35PNeUn5+PMWPGoGvXrnj55ZdvvnGiNrJ06VKsW7cOGzZsgJ2dneh2zEZBQQGmTZuG5cuXw9PTU3Q7Ro8jKkboX//6F2bOnFnvfTp27AhfX1+DiVoVFRXIycmp8y2dXbt2ITExEa6urrL6xIkTMXjwYOzZs+cmOjctrfk8VykoKMDIkSPh7OyMDRs2wNra+mbbNguenp6wsrJCZmamrJ6ZmVnnc+rr69uk+1Pznucq7777LpYuXYqdO3ciIiKiNds0eU19nhMTE5GUlIS77rqruqbT6QBUjtbGx8cjJCSkdZs2JaInyVDzVU3yPHLkSHVt+/bt9U7yTE9Pl06dOiU7AEgfffSRdPHixbZq3aQ053mWJEnKy8uT+vfvLw0dOlQqKipqi1ZNSnR0tDR//vzq21qtVmrXrl29k2nHjh0rqw0YMICTaRvQ1OdZkiTprbfeklxcXKSYmJi2aNEsNOV5LikpMfh/eNy4cdJtt90mnTp1StJoNG3ZutFjUDFxI0eOlKKioqSDBw9Ke/fulTp16iRNmTKl+uupqalS586dpYMHD9b5GOBVPw1q6vOcl5cn9evXT+rRo4eUkJAgpaenVx8VFRWi/hhGZd26dZKtra20atUqKS4uTpo7d67k6uoqZWRkSJIkSdOmTZMWLlxYff99+/ZJKpVKevfdd6WzZ89KixcvlqytraVTp06J+iOYhKY+z0uXLpVsbGykn3/+WfZzW1BQIOqPYBKa+jzr41U/dWNQMXHXrl2TpkyZIjk5OUkuLi7SrFmzZP+hXLp0SQIg7d69u87HYFBpWFOf5927d0sAaj0uXbok5g9hhD755BMpMDBQsrGxkaKjo6UDBw5Uf23o0KHSjBkzZPdfv369FBYWJtnY2EjdunWTNm/e3MYdm6amPM9BQUG1/twuXry47Rs3MU39ea6JQaVuCkmSpLZ+u4mIiIioMXjVDxERERktBhUiIiIyWgwqREREZLQYVIiIiMhoMagQERGR0WJQISIiIqPFoEJERERGi0GFiIzSzJkzMX78eJN7bCJqWQwqRBZo5syZ1TtE29jYIDQ0FK+++ioqKipu6jGN7cU/KSkJCoUCsbGxsvpHH32EVatWCemJiJqGuycTWaiRI0di5cqV0Gg02LJlC+bNmwdra2ssWrSoSY+j1WqhUCharK+WfrzaqNXqVn18Imo5HFEhslC2trbw9fVFUFAQHnvsMYwYMQK//fYbNBoNnn32WbRr1w6Ojo7o168f9uzZU33eqlWr4Orqit9++w1du3aFra0tZs+ejdWrV2PTpk3VIzV79uzBnj17oFAokJubW31+bGwsFAoFkpKS6ny85OTk6vu/8sor8PLygouLCx599FGUlZVVf23btm245ZZb4OrqCg8PD4wdOxaJiYnVX+/QoQMAICoqCgqFAsOGDQNgOPqj0Wjw5JNPwtvbG3Z2drjllltw+PDh6q9X/Tn+/PNP9OnTBw4ODhg4cCDi4+Nb4G+CiOrDoEJEAAB7e3uUlZVh/vz5iImJwbp163Dy5ElMmjQJI0eOxIULF6rvW1xcjLfeegtff/01zpw5g48//hiTJ0/GyJEjkZ6ejvT0dAwcOLDR31v/8by9vQEAf/75J86ePYs9e/Zg7dq1+PXXX/HKK69Un1dUVIQFCxbgyJEj+PPPP6FUKnHPPfdAp9MBAA4dOgQA2LlzJ9LT0/Hrr7/W+v2ff/55/PLLL1i9ejWOHTuG0NBQ3HnnncjJyZHd7z//+Q/ee+89HDlyBCqVCrNnz270n5GImkn0rohE1PZq7tSq0+mkHTt2SLa2ttLMmTMlKysrKS0tTXb/4cOHS4sWLZIkSZJWrlwpAZBiY2PrfMwqVbtIX79+vbp2/Phx2S7S9T2eu7u7VFRUVF1btmyZ5OTkJGm12lr/XFevXpUASKdOnZIk6cau1sePH6+z18LCQsna2lr6/vvvq79eVlYm+fv7S2+//bbsz7Fz587q+2zevFkCIJWUlNTaCxG1DI6oEFmo33//HU5OTrCzs8OoUaNw33334d5774VWq0VYWBicnJyqj7/++kv2loqNjQ0iIiJarJe6Hi8yMhIODg7VtwcMGIDCwkKkpKQAAC5cuIApU6agY8eOcHFxQXBwMADI3jpqSGJiIsrLyzFo0KDqmrW1NaKjo3H27FnZfWv26OfnBwDIyspq9PcioqbjZFoiC3Xrrbdi2bJlsLGxgb+/P1QqFX788UdYWVnh6NGjsLKykt3fycmp+nN7e/tGTXhVKit/F5IkqbpWXl5ucL/GPp6+u+66C0FBQVi+fDn8/f2h0+nQvXt32TyWlmRtbV39eVW/VW8zEVHrYFAhslCOjo4IDQ2V1aKioqDVapGVlYXBgwc36fFsbGyg1WplNS8vLwBAeno63NzcAMDgUuH6nDhxAiUlJbC3twcAHDhwAE5OTggICMC1a9cQHx+P5cuXV/e6d+9eg54AGPRVU0hICGxsbLBv3z4EBQUBqAxThw8fxtNPP93oXomodfCtHyKqFhYWhqlTp2L69On49ddfcenSJRw6dAhLlizB5s2b6z03ODgYJ0+eRHx8PLKzs1FeXo7Q0FAEBATg5ZdfxoULF7B582a89957je6nrKwMc+bMQVxcHLZs2YLFixdj/vz5UCqVcHNzg4eHB7766iskJCRg165dWLBggex8b29v2NvbY9u2bcjMzEReXp7B93B0dMRjjz2G5557Dtu2bUNcXBwefvhhFBcXY86cOY3ulYhaB4MKEcmsXLkS06dPx7/+9S907twZ48ePx+HDhxEYGFjveQ8//DA6d+6MPn36wMvLC/v27YO1tTXWrl2Lc+fOISIiAm+99RZef/31RvcyfPhwdOrUCUOGDMF9992Hu+++Gy+//DKAyreV1q1bh6NHj6J79+545pln8M4778jOV6lU+Pjjj/Hll1/C398f48aNq/X7LF26FBMnTsS0adPQq1cvJCQkYPv27dWjQEQkjkKq+eYxERERkRHhiAoREREZLQYVIiIiMloMKkRERGS0GFSIiIjIaDGoEBERkdFiUCEiIiKjxaBCRERERotBhYiIiIwWgwoREREZLQYVIiIiMloMKkRERGS0GFSIiIjIaP0fRS93CwKtH0IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls.show_2djoin(x_support_set_task, y_support_set_task, title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
