{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16249129",
   "metadata": {},
   "source": [
    "## [참고]\n",
    "### https://cocoa-t.tistory.com/entry/PyHessian-Loss-Landscape-%EC%8B%9C%EA%B0%81%ED%99%94-PyHessian-Neural-Networks-Through-the-Lens-of-the-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5f86c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyhessian\n",
    "#!pip install pytorchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36ee9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "253a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af476",
   "metadata": {},
   "source": [
    "# 0. Dataset 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7235fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"mini_imagenet_full_size\"\n",
    "# dataset=\"tiered_imagenet\"\n",
    "# dataset=\"CIFAR_FS\"\n",
    "# dataset=\"CUB\"\n",
    "\n",
    "title = 'miniImageNet'\n",
    "# title = 'tieredImageNet'\n",
    "# title = 'CIFAR-FS'\n",
    "# title = 'CUB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005193c",
   "metadata": {},
   "source": [
    "# 1. MAML 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f0d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_maml = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":4,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_5way_1shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 150,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 151,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_maml.im_shape = (2, 3, args_maml.image_height, args_maml.image_width)\n",
    "\n",
    "args_maml.use_cuda = torch.cuda.is_available()\n",
    "args_maml.seed = 104\n",
    "args_maml.reverse_channels=False\n",
    "args_maml.labels_as_int=False\n",
    "args_maml.reset_stored_filepaths=False\n",
    "args_maml.num_of_gpus=1\n",
    "\n",
    "args_maml.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9052a",
   "metadata": {},
   "source": [
    "## 2. Arbiter 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "199f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args_arbiter = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":4,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":dataset,\n",
    "  \"dataset_path\":dataset,\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_1shot\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 150,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.0001,\n",
    "  \"meta_learning_rate\":0.0001,   \"total_epochs_before_pause\": 151,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args_arbiter.im_shape = (2, 3, args_arbiter.image_height, args_arbiter.image_width)\n",
    "\n",
    "args_arbiter.use_cuda = torch.cuda.is_available()\n",
    "args_arbiter.seed = 104\n",
    "args_arbiter.reverse_channels=False\n",
    "args_arbiter.labels_as_int=False\n",
    "args_arbiter.reset_stored_filepaths=False\n",
    "args_arbiter.num_of_gpus=1\n",
    "\n",
    "args_arbiter.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f7d8",
   "metadata": {},
   "source": [
    "## 3. Model 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803156ee",
   "metadata": {},
   "source": [
    "### 3.1. MAML Model 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f85286c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML_5way_1shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "75000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_maml = MAMLFewShotClassifier(args=args_maml, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_maml.image_height, args_maml.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model_maml, data=data, args=args_maml, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3acf",
   "metadata": {},
   "source": [
    "### 3.2.  Arbiter 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25651dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_1shot\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "75000 75000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model_arbiter = MAMLFewShotClassifier(args=args_arbiter, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args_arbiter.image_height, args_arbiter.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "arbiter_system = ExperimentBuilder(model=model_arbiter, data=data, args=args_arbiter, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179503e",
   "metadata": {},
   "source": [
    "## 0. 모델 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a2ff6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.49008888800938927,\n",
       " 'best_val_iter': 68500,\n",
       " 'current_iter': 75000,\n",
       " 'best_epoch': 137,\n",
       " 'train_loss_mean': 0.8438593364357948,\n",
       " 'train_loss_std': 0.11332703653923111,\n",
       " 'train_accuracy_mean': 0.6819400010108948,\n",
       " 'train_accuracy_std': 0.06164948072695627,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 1.3849907732009887,\n",
       " 'val_loss_std': 0.14854044243951514,\n",
       " 'val_accuracy_mean': 0.4589111105600993,\n",
       " 'val_accuracy_std': 0.055844356440995245,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0388, -0.1226,  0.0672],\n",
       "                         [-0.0668, -0.0297,  0.0048],\n",
       "                         [-0.0444,  0.1443, -0.0402]],\n",
       "               \n",
       "                        [[ 0.0790, -0.1145,  0.0826],\n",
       "                         [-0.0515,  0.0199,  0.0601],\n",
       "                         [-0.0679,  0.0657, -0.0550]],\n",
       "               \n",
       "                        [[ 0.0792, -0.0546, -0.0110],\n",
       "                         [ 0.0145,  0.0658, -0.0572],\n",
       "                         [-0.0629,  0.0578, -0.0681]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0695,  0.1519,  0.0675],\n",
       "                         [ 0.0616, -0.0557,  0.0074],\n",
       "                         [-0.1177, -0.1125, -0.0683]],\n",
       "               \n",
       "                        [[-0.0285,  0.0311, -0.0136],\n",
       "                         [ 0.0618, -0.0675, -0.0628],\n",
       "                         [ 0.0627, -0.0536,  0.0801]],\n",
       "               \n",
       "                        [[-0.0859,  0.0428, -0.0952],\n",
       "                         [-0.0170,  0.0604,  0.0897],\n",
       "                         [-0.0174, -0.0045,  0.0095]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0074, -0.0447,  0.0686],\n",
       "                         [ 0.0047, -0.0623, -0.0349],\n",
       "                         [ 0.0861,  0.0671, -0.0762]],\n",
       "               \n",
       "                        [[-0.0781, -0.0504,  0.0796],\n",
       "                         [ 0.0894, -0.0266,  0.0469],\n",
       "                         [-0.0885, -0.0377,  0.0578]],\n",
       "               \n",
       "                        [[ 0.0500, -0.0848,  0.0283],\n",
       "                         [ 0.0955, -0.0064, -0.0676],\n",
       "                         [-0.0613,  0.0346,  0.0050]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0412, -0.1333, -0.0723],\n",
       "                         [-0.0203, -0.0985,  0.0455],\n",
       "                         [-0.0255,  0.0390, -0.0345]],\n",
       "               \n",
       "                        [[ 0.0752,  0.0152,  0.0967],\n",
       "                         [ 0.0600, -0.0004, -0.0152],\n",
       "                         [ 0.0339,  0.0685,  0.0068]],\n",
       "               \n",
       "                        [[ 0.0039, -0.1228, -0.0355],\n",
       "                         [-0.0488,  0.0070,  0.0401],\n",
       "                         [-0.0241,  0.0375,  0.0501]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0327, -0.0161, -0.0690],\n",
       "                         [ 0.0102,  0.0736, -0.0821],\n",
       "                         [ 0.0909,  0.1115, -0.0763]],\n",
       "               \n",
       "                        [[ 0.0605,  0.0365,  0.0729],\n",
       "                         [ 0.0151, -0.0486, -0.0308],\n",
       "                         [-0.0283, -0.0946,  0.0265]],\n",
       "               \n",
       "                        [[-0.0282, -0.0138, -0.0080],\n",
       "                         [-0.0206, -0.0263,  0.1180],\n",
       "                         [-0.0613, -0.0159,  0.0512]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0794,  0.0127, -0.0593],\n",
       "                         [-0.0580,  0.0247,  0.0537],\n",
       "                         [ 0.0869,  0.0799,  0.0971]],\n",
       "               \n",
       "                        [[-0.0376, -0.0697,  0.0207],\n",
       "                         [ 0.0339, -0.0992, -0.0607],\n",
       "                         [-0.0188, -0.0058, -0.0109]],\n",
       "               \n",
       "                        [[ 0.0655, -0.0074,  0.0564],\n",
       "                         [ 0.0124, -0.0184, -0.0089],\n",
       "                         [-0.0237, -0.0410, -0.0158]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([ 5.0699e-03, -3.7895e-03, -2.4842e-02,  1.0001e-02, -2.4530e-02,\n",
       "                        9.9211e-03, -1.3987e-03, -9.0022e-03,  2.5962e-02, -1.1086e-02,\n",
       "                        8.3875e-04,  1.0691e-02,  4.0063e-02, -8.2120e-03, -6.5286e-03,\n",
       "                       -2.0157e-03,  2.7542e-02,  7.5789e-03,  2.9582e-02, -3.9190e-03,\n",
       "                       -7.0375e-03, -2.0234e-02, -8.3720e-03,  4.1449e-03, -1.4004e-02,\n",
       "                        6.7702e-03, -1.3976e-02, -1.1722e-02, -1.8077e-02, -1.4847e-02,\n",
       "                        6.4757e-03, -3.8741e-04,  3.2551e-03, -1.5920e-02,  1.8428e-02,\n",
       "                        8.3694e-03, -4.4457e-03,  1.5647e-02,  7.5965e-04,  2.5716e-02,\n",
       "                        3.1303e-03,  2.7086e-02,  6.9993e-04, -8.0018e-03, -1.2226e-02,\n",
       "                       -2.7752e-03,  2.7872e-03,  8.5301e-03, -9.9896e-03, -1.4408e-02,\n",
       "                       -3.6759e-02, -2.7962e-02,  1.6713e-02, -2.2453e-02, -6.0961e-03,\n",
       "                        1.1836e-02,  1.3099e-03,  1.6424e-02,  9.3129e-03,  1.3876e-02,\n",
       "                        9.4670e-03, -4.2988e-03,  7.1954e-03,  6.2694e-03,  9.9207e-03,\n",
       "                        1.1828e-02,  2.4424e-03, -7.3596e-03,  5.9282e-03, -7.2310e-03,\n",
       "                       -2.7273e-02,  1.4343e-02, -6.0167e-03,  1.1105e-02, -3.7588e-02,\n",
       "                       -1.0728e-03, -1.5203e-02,  9.5777e-03,  1.3850e-03,  7.4337e-03,\n",
       "                        1.9314e-02,  2.7374e-02,  3.6764e-05,  3.5337e-03,  3.9874e-02,\n",
       "                        3.1960e-03, -1.6160e-02,  2.9474e-02,  4.0934e-03,  3.5039e-02,\n",
       "                       -1.1015e-02,  1.2691e-03, -8.5393e-03, -9.5396e-03, -6.1358e-03,\n",
       "                        1.6116e-02,  3.6739e-03,  1.7601e-03,  3.4439e-02,  1.0536e-02,\n",
       "                       -1.3805e-02,  3.3069e-02,  1.5626e-02, -1.0841e-02,  1.1547e-02,\n",
       "                       -1.0760e-02,  1.6005e-02,  4.5204e-03,  1.0110e-02, -7.0456e-03,\n",
       "                        1.3584e-02, -1.0632e-03, -5.6836e-03,  6.4489e-03, -1.2884e-02,\n",
       "                        1.3070e-02, -2.5156e-03, -2.3869e-02,  4.6812e-03, -9.0895e-03,\n",
       "                        1.2945e-02,  5.1526e-03,  1.9552e-02, -2.9238e-02,  8.7367e-04,\n",
       "                        2.7314e-02,  3.1107e-03, -2.6651e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1706,  0.1537,  0.1237, -0.1936,  0.0869,  0.0305, -0.1025, -0.2072,\n",
       "                       -0.1484, -0.1800,  0.0058, -0.1342,  0.2651, -0.1803,  0.3678, -0.0202,\n",
       "                        0.3623,  0.1935,  0.2023, -0.0524,  0.1013, -0.1188, -0.1849, -0.0104,\n",
       "                        0.0927, -0.1842, -0.1762,  0.3318,  0.4611,  0.1363,  0.1034, -0.0891,\n",
       "                       -0.0962,  0.0224, -0.0575, -0.1583, -0.0983,  0.0545, -0.1234, -0.1303,\n",
       "                       -0.2238,  0.4554, -0.2711,  0.0059,  0.4473, -0.1194, -0.0776, -0.2339,\n",
       "                       -0.0744,  0.0283,  0.5743, -0.0443,  0.4696,  0.1889,  0.2609, -0.1564,\n",
       "                       -0.1088,  0.1661, -0.0335,  0.3196, -0.1194,  0.2881,  0.0239, -0.3056,\n",
       "                        0.2868, -0.1554,  0.3954, -0.1878, -0.0779,  0.1142,  0.1482,  0.7383,\n",
       "                       -0.1063, -0.1196,  0.1052, -0.0828, -0.0953,  0.1972, -0.0217, -0.1279,\n",
       "                        0.4469, -0.1741, -0.2976, -0.2027,  0.3603,  0.3396,  0.2870,  0.1641,\n",
       "                       -0.1363,  0.0927, -0.0772, -0.1825,  0.1063,  0.0470,  0.2935,  0.1419,\n",
       "                        0.1411,  0.3352,  0.0061, -0.1501,  0.2036,  0.3038, -0.1676, -0.0034,\n",
       "                        0.1775, -0.1383,  0.1115, -0.1637, -0.0780,  0.3084,  0.2459,  0.5742,\n",
       "                       -0.0839,  0.1092, -0.1000, -0.0823, -0.1427,  0.5819, -0.1038,  0.4306,\n",
       "                       -0.1181, -0.1336, -0.1234,  0.0636,  0.4885, -0.0410, -0.0889, -0.2468],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([1.0043, 1.1351, 0.9557, 0.9500, 0.9716, 0.9488, 0.9057, 0.9320, 0.9131,\n",
       "                       0.9523, 1.0278, 0.9272, 1.0600, 0.8880, 1.0990, 1.1368, 0.9497, 1.0532,\n",
       "                       0.9969, 0.9422, 0.9377, 0.8426, 0.9269, 0.9876, 0.9521, 0.8497, 0.9490,\n",
       "                       0.9605, 1.0364, 0.9494, 1.0955, 0.9009, 1.1468, 1.0455, 0.9007, 0.9353,\n",
       "                       0.8985, 1.0305, 0.9409, 0.9669, 1.0130, 1.1194, 0.8048, 1.0329, 1.0516,\n",
       "                       1.1005, 0.8950, 0.8886, 0.9770, 1.0008, 1.1128, 0.9855, 1.1063, 0.9711,\n",
       "                       1.0364, 0.9039, 0.9968, 1.0684, 0.9761, 1.0721, 1.0219, 0.9753, 0.9570,\n",
       "                       0.8725, 1.0258, 0.8493, 1.0756, 0.8628, 1.1227, 0.9741, 0.9897, 1.1259,\n",
       "                       0.8912, 0.9277, 0.9920, 0.9113, 0.9405, 1.1263, 1.1209, 1.0014, 1.0391,\n",
       "                       0.9764, 0.8423, 0.8737, 1.1292, 1.1485, 1.0469, 1.0450, 0.9652, 0.9655,\n",
       "                       0.8671, 0.8901, 0.9306, 1.0020, 0.9986, 0.9711, 1.0318, 0.9917, 0.9067,\n",
       "                       0.9340, 0.9893, 1.0516, 0.9175, 1.0342, 1.0134, 0.9530, 1.0242, 0.9071,\n",
       "                       1.0866, 1.0276, 1.0294, 1.1761, 0.9953, 1.0597, 1.0453, 1.1675, 0.8911,\n",
       "                       1.0443, 0.9954, 0.9851, 0.8653, 0.9214, 1.0499, 1.0735, 1.0597, 0.9587,\n",
       "                       1.0118, 0.9001], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-0.0083,  0.0336, -0.0149],\n",
       "                         [ 0.0031,  0.0067, -0.0157],\n",
       "                         [ 0.0184, -0.0037, -0.0336]],\n",
       "               \n",
       "                        [[ 0.0213,  0.0103, -0.0364],\n",
       "                         [ 0.0212, -0.0323, -0.0288],\n",
       "                         [-0.0251,  0.0232, -0.0106]],\n",
       "               \n",
       "                        [[ 0.0422, -0.1033, -0.0395],\n",
       "                         [-0.0339, -0.0179,  0.0696],\n",
       "                         [ 0.0534, -0.0149, -0.0195]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0173, -0.0784, -0.0397],\n",
       "                         [-0.0184, -0.0656, -0.0059],\n",
       "                         [-0.0095, -0.0597, -0.0179]],\n",
       "               \n",
       "                        [[ 0.0578, -0.0123, -0.0443],\n",
       "                         [ 0.0362,  0.0101, -0.0111],\n",
       "                         [ 0.0550, -0.0576,  0.0049]],\n",
       "               \n",
       "                        [[-0.0050,  0.0057, -0.0190],\n",
       "                         [ 0.0262,  0.0215,  0.0057],\n",
       "                         [-0.0445, -0.0713,  0.0204]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0050, -0.0263,  0.0308],\n",
       "                         [ 0.0616,  0.0070, -0.0362],\n",
       "                         [ 0.0609,  0.0159, -0.0336]],\n",
       "               \n",
       "                        [[ 0.0609, -0.0013,  0.0066],\n",
       "                         [-0.0739,  0.0385,  0.0087],\n",
       "                         [ 0.0086, -0.0537,  0.0624]],\n",
       "               \n",
       "                        [[ 0.0060,  0.0646, -0.0039],\n",
       "                         [-0.0057, -0.0281,  0.0248],\n",
       "                         [-0.0741, -0.0105,  0.0045]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0223, -0.0047,  0.0028],\n",
       "                         [-0.0519, -0.0473, -0.0566],\n",
       "                         [-0.0233,  0.0026, -0.0765]],\n",
       "               \n",
       "                        [[-0.0511, -0.0520, -0.0079],\n",
       "                         [ 0.0532, -0.0410,  0.0037],\n",
       "                         [ 0.0567,  0.0019, -0.0049]],\n",
       "               \n",
       "                        [[-0.0308, -0.0295,  0.0301],\n",
       "                         [ 0.0285, -0.0362, -0.0040],\n",
       "                         [-0.0201, -0.0129,  0.0254]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0541, -0.0755, -0.0021],\n",
       "                         [-0.0264, -0.0296, -0.0391],\n",
       "                         [ 0.0057,  0.0003,  0.0100]],\n",
       "               \n",
       "                        [[ 0.0795, -0.0112, -0.0620],\n",
       "                         [ 0.0348, -0.0208, -0.0937],\n",
       "                         [ 0.0030,  0.0363, -0.0034]],\n",
       "               \n",
       "                        [[-0.0480,  0.0485, -0.0661],\n",
       "                         [ 0.0013, -0.0499,  0.0365],\n",
       "                         [ 0.0164,  0.0107,  0.1014]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0134,  0.0220,  0.0144],\n",
       "                         [-0.0297, -0.0341, -0.0051],\n",
       "                         [-0.0008, -0.0232, -0.0393]],\n",
       "               \n",
       "                        [[ 0.0700,  0.0354,  0.0340],\n",
       "                         [-0.0470,  0.0129, -0.0110],\n",
       "                         [-0.0172,  0.0094,  0.0031]],\n",
       "               \n",
       "                        [[-0.0437, -0.0462,  0.0112],\n",
       "                         [-0.0395,  0.0097,  0.0259],\n",
       "                         [ 0.0354,  0.0007, -0.0248]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0341, -0.0400, -0.0428],\n",
       "                         [ 0.0355,  0.0358, -0.0102],\n",
       "                         [ 0.0635, -0.0191, -0.0632]],\n",
       "               \n",
       "                        [[-0.0363,  0.0403,  0.0325],\n",
       "                         [-0.0490, -0.0091,  0.0171],\n",
       "                         [-0.0695, -0.0005, -0.0104]],\n",
       "               \n",
       "                        [[ 0.0111, -0.0224, -0.0097],\n",
       "                         [ 0.0871, -0.0277, -0.0133],\n",
       "                         [ 0.0316, -0.0193, -0.0547]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0684, -0.0388, -0.0185],\n",
       "                         [ 0.0185, -0.0058,  0.0457],\n",
       "                         [ 0.0131,  0.0548,  0.0141]],\n",
       "               \n",
       "                        [[-0.0639, -0.0228, -0.0483],\n",
       "                         [-0.0135, -0.0194, -0.0583],\n",
       "                         [-0.0554,  0.0835,  0.0408]],\n",
       "               \n",
       "                        [[ 0.0200, -0.0305, -0.0039],\n",
       "                         [-0.0247, -0.0225,  0.0120],\n",
       "                         [ 0.0035,  0.0101, -0.0421]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0411, -0.0351,  0.0511],\n",
       "                         [-0.0219, -0.0445,  0.0681],\n",
       "                         [-0.0317,  0.0595,  0.0345]],\n",
       "               \n",
       "                        [[-0.0051,  0.0285, -0.0839],\n",
       "                         [-0.0540,  0.0075,  0.0360],\n",
       "                         [-0.0705, -0.0466,  0.0620]],\n",
       "               \n",
       "                        [[-0.0245,  0.0649,  0.0466],\n",
       "                         [ 0.0240,  0.0055, -0.0224],\n",
       "                         [-0.0286, -0.0072,  0.0466]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0312, -0.0023,  0.0651],\n",
       "                         [-0.0392,  0.0016, -0.0397],\n",
       "                         [-0.0425, -0.0175, -0.0488]],\n",
       "               \n",
       "                        [[-0.0762,  0.0693,  0.0852],\n",
       "                         [-0.1050, -0.0200,  0.0020],\n",
       "                         [-0.0698, -0.0081, -0.0243]],\n",
       "               \n",
       "                        [[-0.0341, -0.0169,  0.0065],\n",
       "                         [-0.0115,  0.0351,  0.0015],\n",
       "                         [ 0.0282, -0.0096, -0.0189]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0005,  0.0227, -0.0124],\n",
       "                         [ 0.0898,  0.0721, -0.0316],\n",
       "                         [ 0.0181,  0.0581, -0.0553]],\n",
       "               \n",
       "                        [[-0.0205,  0.0258,  0.0817],\n",
       "                         [ 0.0581, -0.0481, -0.0672],\n",
       "                         [-0.0087, -0.0431, -0.0049]],\n",
       "               \n",
       "                        [[-0.0518, -0.0395, -0.0264],\n",
       "                         [-0.0049, -0.0013, -0.0524],\n",
       "                         [ 0.0176, -0.0370, -0.0225]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0269,  0.0569, -0.0158],\n",
       "                         [ 0.0338,  0.0698, -0.0617],\n",
       "                         [-0.0184, -0.0098, -0.0477]],\n",
       "               \n",
       "                        [[-0.0619, -0.0268, -0.0203],\n",
       "                         [ 0.0376,  0.0753, -0.0225],\n",
       "                         [ 0.0100,  0.0468, -0.0662]],\n",
       "               \n",
       "                        [[-0.0436,  0.0243, -0.0158],\n",
       "                         [-0.0367, -0.0045,  0.0328],\n",
       "                         [ 0.0139, -0.0291, -0.0114]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 3.0009e-03,  1.7546e-03, -8.1436e-05,  5.2545e-05, -1.3901e-03,\n",
       "                       -2.5206e-03, -2.4125e-03,  2.1798e-03, -9.4918e-04,  3.2247e-03,\n",
       "                        1.4933e-03, -1.4473e-03, -4.0977e-04, -4.2015e-03, -6.4778e-03,\n",
       "                       -7.5354e-03, -1.1371e-03, -2.2452e-03,  3.5273e-03, -2.8347e-03,\n",
       "                        4.2376e-03, -1.3102e-03, -6.6563e-03, -2.6935e-03,  6.2052e-04,\n",
       "                        2.1561e-04, -3.0060e-03,  1.6312e-03,  3.7914e-03, -2.6637e-03,\n",
       "                        5.4050e-03, -3.2663e-03, -2.0373e-03,  4.9387e-03, -4.5086e-03,\n",
       "                       -4.2828e-03, -2.1492e-03,  9.1008e-04,  8.6745e-05,  2.9715e-03,\n",
       "                       -5.8335e-04, -1.3102e-03,  1.1516e-03, -1.4056e-03,  3.2477e-03,\n",
       "                       -1.9262e-04,  3.8244e-04, -1.1048e-04, -3.8958e-03, -8.7672e-03,\n",
       "                       -9.0614e-04,  5.0618e-03, -1.6579e-03, -5.1019e-03, -1.8327e-03,\n",
       "                       -3.7269e-03,  2.8483e-03, -8.2917e-03,  1.3511e-03, -5.0935e-03,\n",
       "                        3.8802e-03, -2.0888e-03, -2.4761e-04,  3.0835e-03,  5.4658e-04,\n",
       "                        7.7302e-03, -3.1260e-03, -4.1355e-03, -6.3349e-03,  3.8736e-03,\n",
       "                        1.2082e-03,  2.4950e-03, -1.0857e-03,  4.7702e-03,  2.5783e-03,\n",
       "                        5.9160e-03, -1.9404e-03,  3.7192e-03,  3.7137e-03,  1.9318e-03,\n",
       "                       -6.7149e-03,  2.4957e-03, -9.0113e-04,  1.7454e-04, -3.9449e-03,\n",
       "                        2.2248e-03,  1.2506e-03, -1.9670e-03, -4.0734e-03,  1.8569e-03,\n",
       "                       -6.5452e-03,  3.1774e-03,  3.6876e-03,  3.3112e-03, -3.9644e-06,\n",
       "                        4.3991e-03, -4.6370e-04,  2.4771e-05,  1.2715e-03,  8.3163e-04,\n",
       "                       -2.1126e-03, -1.7806e-03, -3.1942e-03, -5.8469e-03,  4.4419e-03,\n",
       "                        5.5260e-03,  2.4395e-03,  6.2301e-04, -4.8840e-03,  7.5325e-04,\n",
       "                        9.4787e-04, -4.3185e-03,  5.5922e-03, -7.8584e-04,  2.4494e-03,\n",
       "                       -2.7484e-04, -8.2680e-03, -1.4630e-03, -2.2813e-03,  4.4508e-03,\n",
       "                       -2.9440e-03, -1.9070e-03, -1.5875e-03, -4.6258e-04, -1.9929e-03,\n",
       "                       -1.6960e-03,  2.6047e-03, -2.3221e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.1385, -0.1502, -0.1105, -0.2272, -0.2330, -0.1612, -0.1600, -0.2059,\n",
       "                       -0.1333,  0.0163, -0.2071, -0.3193, -0.1527, -0.1466, -0.2049, -0.2385,\n",
       "                       -0.2401, -0.0617, -0.1053, -0.2566, -0.2976, -0.1627, -0.1520, -0.0954,\n",
       "                       -0.1237, -0.2410, -0.1552, -0.1776, -0.1961, -0.0629, -0.0650, -0.1701,\n",
       "                       -0.1958, -0.1862, -0.2697, -0.1443,  0.0214, -0.1794, -0.2661, -0.1639,\n",
       "                       -0.1232, -0.1309, -0.1271, -0.1613, -0.1391, -0.1803, -0.1460, -0.2554,\n",
       "                       -0.0824, -0.1572, -0.1650, -0.1936, -0.2372, -0.1135, -0.0842, -0.2052,\n",
       "                       -0.2193, -0.1720, -0.1001, -0.1203, -0.1955, -0.1971, -0.1637, -0.1314,\n",
       "                       -0.2169, -0.0777, -0.1786, -0.1087, -0.1868, -0.2413, -0.1628, -0.2159,\n",
       "                       -0.2196, -0.1837, -0.2172, -0.1229, -0.1891, -0.1599, -0.1164, -0.1527,\n",
       "                       -0.2045, -0.1554, -0.0759, -0.1510, -0.1785, -0.1677, -0.1603, -0.1836,\n",
       "                       -0.1006, -0.1326, -0.1455, -0.3136, -0.0588, -0.1907, -0.2297, -0.0957,\n",
       "                       -0.1373, -0.2048, -0.2413, -0.2852, -0.1130, -0.2545, -0.2065, -0.1271,\n",
       "                       -0.2359, -0.2010, -0.2512, -0.2550, -0.1406, -0.2143, -0.2001, -0.1563,\n",
       "                       -0.1697, -0.1313, -0.1307, -0.1398, -0.1310, -0.1855, -0.1825, -0.1158,\n",
       "                       -0.1642, -0.1532, -0.0617, -0.0931,  0.0121, -0.2351, -0.1216, -0.2280],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([1.0434, 1.0028, 1.1275, 1.0462, 0.8685, 1.1062, 0.9390, 0.9694, 1.0417,\n",
       "                       0.9710, 1.0578, 1.0871, 1.0206, 1.0208, 0.8893, 1.1105, 0.8775, 0.8900,\n",
       "                       0.9617, 0.9318, 1.0308, 0.9969, 1.0113, 1.0233, 0.9526, 1.1132, 0.9528,\n",
       "                       0.9087, 1.0329, 1.0470, 0.9974, 0.9504, 1.1253, 1.0385, 0.9885, 0.9725,\n",
       "                       1.1254, 1.0467, 0.9271, 0.8884, 1.0197, 1.0994, 0.9395, 0.9289, 0.9134,\n",
       "                       1.0323, 0.9422, 1.0669, 1.1978, 1.0896, 1.0342, 0.8923, 1.0088, 0.9492,\n",
       "                       0.9445, 1.0864, 0.8655, 0.9959, 1.0204, 1.0256, 0.9186, 0.9763, 1.0073,\n",
       "                       1.0547, 0.9106, 0.9232, 0.9597, 1.0402, 0.9547, 1.1253, 0.9820, 0.9002,\n",
       "                       1.1833, 0.9775, 1.1021, 1.0066, 1.0916, 1.0394, 1.0579, 0.8589, 1.0626,\n",
       "                       0.9322, 0.9496, 1.0152, 0.8778, 0.8987, 1.0569, 0.9611, 1.1037, 1.1139,\n",
       "                       1.0805, 1.0612, 0.9830, 1.0640, 1.0511, 0.9097, 1.0566, 1.0089, 1.0081,\n",
       "                       1.0817, 0.9524, 1.0577, 0.9122, 1.0019, 1.0815, 0.9228, 1.0272, 0.8804,\n",
       "                       0.9944, 0.9681, 0.9809, 1.1596, 1.0139, 1.0300, 0.9322, 0.9584, 1.1828,\n",
       "                       1.0844, 0.9072, 1.0372, 0.9461, 0.9389, 1.0889, 0.9077, 1.1085, 1.0128,\n",
       "                       1.0835, 1.0950], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-0.0016,  0.1241, -0.0118],\n",
       "                         [-0.0188,  0.0058,  0.0018],\n",
       "                         [ 0.0549,  0.0020,  0.0427]],\n",
       "               \n",
       "                        [[ 0.0240,  0.0399,  0.0112],\n",
       "                         [-0.0361,  0.0487, -0.0480],\n",
       "                         [ 0.0291,  0.0037, -0.0651]],\n",
       "               \n",
       "                        [[-0.0436,  0.0269, -0.0030],\n",
       "                         [-0.0413,  0.0980, -0.0798],\n",
       "                         [-0.0490, -0.0555, -0.1237]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0846, -0.0060, -0.0311],\n",
       "                         [ 0.0361,  0.0205,  0.0517],\n",
       "                         [ 0.0171, -0.0073, -0.0256]],\n",
       "               \n",
       "                        [[-0.0577, -0.0696, -0.0191],\n",
       "                         [-0.0240, -0.0791, -0.0510],\n",
       "                         [ 0.0037, -0.0363, -0.0397]],\n",
       "               \n",
       "                        [[-0.0741, -0.0019,  0.0325],\n",
       "                         [-0.0389, -0.0438, -0.0335],\n",
       "                         [-0.0451, -0.0421,  0.0189]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0417,  0.0259,  0.0648],\n",
       "                         [ 0.0308,  0.0281, -0.0532],\n",
       "                         [ 0.0078,  0.0383, -0.0285]],\n",
       "               \n",
       "                        [[-0.0355,  0.0266,  0.0207],\n",
       "                         [-0.0080,  0.0619,  0.0007],\n",
       "                         [-0.0614, -0.0306,  0.0110]],\n",
       "               \n",
       "                        [[ 0.0235,  0.0547,  0.0309],\n",
       "                         [-0.0502,  0.0394,  0.0528],\n",
       "                         [-0.0141,  0.0372,  0.0184]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0116,  0.0436,  0.0064],\n",
       "                         [-0.0013,  0.0201,  0.0394],\n",
       "                         [-0.0379,  0.0152,  0.0049]],\n",
       "               \n",
       "                        [[-0.0299, -0.0577, -0.0365],\n",
       "                         [-0.0845, -0.0463, -0.0246],\n",
       "                         [-0.0401, -0.0266, -0.0056]],\n",
       "               \n",
       "                        [[ 0.0104, -0.0338, -0.0780],\n",
       "                         [-0.0293,  0.0349, -0.0031],\n",
       "                         [-0.0720,  0.0459, -0.0125]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0743,  0.0076,  0.0003],\n",
       "                         [-0.0146, -0.0081,  0.0596],\n",
       "                         [-0.0064, -0.0134,  0.0078]],\n",
       "               \n",
       "                        [[ 0.0582,  0.0427,  0.0198],\n",
       "                         [-0.0644,  0.0304, -0.0228],\n",
       "                         [-0.1049,  0.0130, -0.0452]],\n",
       "               \n",
       "                        [[-0.0089, -0.1058,  0.0306],\n",
       "                         [-0.0319, -0.0191, -0.0735],\n",
       "                         [-0.0330, -0.0256, -0.0467]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0443,  0.0300,  0.0237],\n",
       "                         [-0.0160, -0.0010,  0.0062],\n",
       "                         [-0.0206,  0.0159,  0.0851]],\n",
       "               \n",
       "                        [[-0.0459, -0.0653,  0.0088],\n",
       "                         [ 0.0286, -0.0197, -0.0441],\n",
       "                         [-0.0015, -0.0549,  0.0393]],\n",
       "               \n",
       "                        [[-0.0318, -0.0100, -0.0006],\n",
       "                         [ 0.0504,  0.1019,  0.0699],\n",
       "                         [-0.0051,  0.0482,  0.0449]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0194,  0.0365,  0.0254],\n",
       "                         [ 0.0705, -0.0068,  0.0381],\n",
       "                         [-0.0022,  0.0036, -0.0008]],\n",
       "               \n",
       "                        [[-0.0049, -0.0337, -0.0414],\n",
       "                         [ 0.0026,  0.0204, -0.0494],\n",
       "                         [ 0.0280, -0.0077,  0.0195]],\n",
       "               \n",
       "                        [[ 0.0089, -0.0255, -0.0509],\n",
       "                         [-0.0236, -0.0453,  0.0577],\n",
       "                         [-0.0023, -0.0057, -0.0341]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0371, -0.0330, -0.0307],\n",
       "                         [ 0.0056, -0.0201, -0.0622],\n",
       "                         [-0.0068, -0.0192,  0.0143]],\n",
       "               \n",
       "                        [[ 0.0932,  0.0383,  0.0666],\n",
       "                         [-0.0049,  0.0294, -0.0257],\n",
       "                         [-0.0701,  0.0572, -0.0092]],\n",
       "               \n",
       "                        [[ 0.0645,  0.0039, -0.0331],\n",
       "                         [-0.0151, -0.0150, -0.0412],\n",
       "                         [ 0.0060,  0.0750,  0.0414]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0115,  0.0073, -0.0799],\n",
       "                         [ 0.0197, -0.0433, -0.0310],\n",
       "                         [ 0.0444,  0.0587, -0.0344]],\n",
       "               \n",
       "                        [[ 0.0206, -0.0495,  0.0253],\n",
       "                         [ 0.0102,  0.0054,  0.0351],\n",
       "                         [ 0.0187,  0.0407,  0.0600]],\n",
       "               \n",
       "                        [[-0.0697,  0.0047, -0.0705],\n",
       "                         [-0.0345, -0.0143,  0.0162],\n",
       "                         [-0.0933, -0.1154, -0.0156]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0008, -0.0227,  0.0288],\n",
       "                         [-0.0722, -0.0116, -0.0872],\n",
       "                         [-0.0039, -0.0397, -0.0372]],\n",
       "               \n",
       "                        [[-0.0964, -0.0539, -0.0668],\n",
       "                         [-0.0402, -0.0535,  0.0296],\n",
       "                         [ 0.0223, -0.0694, -0.0807]],\n",
       "               \n",
       "                        [[ 0.0232,  0.0281, -0.0218],\n",
       "                         [ 0.0585,  0.0596, -0.0128],\n",
       "                         [ 0.0601,  0.0160,  0.0090]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0311, -0.0261, -0.0698],\n",
       "                         [-0.0204, -0.0276,  0.0095],\n",
       "                         [-0.0056,  0.0383, -0.0056]],\n",
       "               \n",
       "                        [[-0.0444,  0.0419,  0.0437],\n",
       "                         [-0.0790, -0.0190,  0.0273],\n",
       "                         [-0.0947, -0.0719,  0.0319]],\n",
       "               \n",
       "                        [[-0.0004,  0.0546, -0.0668],\n",
       "                         [ 0.0581, -0.0110, -0.0491],\n",
       "                         [ 0.0479, -0.0210,  0.0312]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0374,  0.0639,  0.0126],\n",
       "                         [ 0.0742,  0.0594, -0.0252],\n",
       "                         [-0.0001,  0.0454, -0.0048]],\n",
       "               \n",
       "                        [[ 0.0332,  0.0824, -0.0232],\n",
       "                         [-0.0304,  0.0602, -0.0413],\n",
       "                         [-0.0201,  0.0030, -0.0997]],\n",
       "               \n",
       "                        [[-0.0371, -0.0378, -0.0063],\n",
       "                         [ 0.0129,  0.0251,  0.0058],\n",
       "                         [ 0.0864,  0.0198, -0.0152]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-1.9435e-03,  2.0728e-03,  1.6199e-03,  5.8443e-03, -1.2079e-03,\n",
       "                        3.5288e-03, -5.2398e-03, -2.4989e-03,  4.9730e-03,  1.0132e-02,\n",
       "                        1.9595e-03,  4.5374e-04,  8.8144e-04,  2.7000e-03, -2.8738e-03,\n",
       "                       -1.6397e-03, -1.0622e-03,  2.2442e-04,  1.8661e-03, -1.4846e-03,\n",
       "                       -5.3747e-04,  3.0512e-04, -7.4335e-04, -1.2864e-03,  1.1444e-04,\n",
       "                       -9.7772e-04, -4.4175e-03,  7.0316e-03, -1.3164e-03, -3.2446e-03,\n",
       "                       -7.2042e-03,  3.4929e-03, -7.5637e-03, -5.9017e-03,  1.9502e-03,\n",
       "                       -9.1094e-04, -4.4233e-03,  3.9447e-03,  4.0641e-03, -2.5910e-03,\n",
       "                       -4.5941e-03,  7.6494e-03,  6.5628e-03, -2.7371e-03,  1.2620e-02,\n",
       "                       -5.4039e-04, -1.2650e-02,  1.5286e-03,  2.9403e-03,  5.8201e-03,\n",
       "                       -9.9829e-03,  1.9995e-04,  6.6934e-03,  3.3838e-03, -7.7022e-03,\n",
       "                        9.1024e-04, -1.2526e-03, -7.8928e-04,  1.0864e-03, -5.9145e-03,\n",
       "                       -3.2894e-03,  1.5673e-03, -1.1643e-02,  1.1680e-03,  2.8806e-03,\n",
       "                        5.4671e-03, -3.1151e-04,  7.8865e-05, -9.4047e-04, -1.2644e-03,\n",
       "                        9.0335e-03,  3.9895e-03, -2.7226e-05, -3.7444e-03, -4.0505e-04,\n",
       "                        4.8491e-03, -4.6742e-03,  1.1874e-02,  3.3946e-03, -8.4274e-04,\n",
       "                        4.6366e-03, -7.1008e-03, -1.8129e-03,  1.5956e-03, -2.3002e-03,\n",
       "                        4.9473e-04, -3.4776e-03,  5.6054e-04,  1.2556e-02,  3.3017e-03,\n",
       "                        4.6407e-03, -3.8395e-03,  3.5479e-03,  6.1973e-04,  2.6463e-03,\n",
       "                        5.3534e-06,  4.5437e-03, -2.1856e-03, -6.9410e-04, -1.0977e-02,\n",
       "                        7.5467e-04,  4.7011e-03,  5.8671e-04,  8.1188e-03,  3.2466e-03,\n",
       "                        7.5537e-04, -2.0689e-03, -2.4286e-03,  1.9940e-04,  3.4309e-03,\n",
       "                       -3.7515e-03,  1.2660e-03, -7.2227e-04,  2.0541e-03,  6.8350e-03,\n",
       "                        1.1298e-03, -1.6084e-03, -8.7877e-03, -5.0637e-04,  2.9644e-03,\n",
       "                       -1.9621e-03,  2.6360e-03, -1.7760e-04,  4.1208e-03,  7.1704e-04,\n",
       "                        2.1572e-04, -2.6393e-03,  6.3076e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.0757, -0.1057, -0.2190, -0.1013, -0.2026, -0.2144, -0.2381, -0.2397,\n",
       "                       -0.2109, -0.2432, -0.1085, -0.1588, -0.1863, -0.0361, -0.2291, -0.1605,\n",
       "                       -0.1146, -0.2207, -0.2625, -0.1767, -0.0314, -0.1262, -0.1824, -0.1758,\n",
       "                       -0.1553, -0.0999, -0.2446, -0.1329, -0.1769, -0.3328, -0.1912, -0.1571,\n",
       "                       -0.1791, -0.2403, -0.1591, -0.2403, -0.1676, -0.1384, -0.1358, -0.1394,\n",
       "                       -0.2147, -0.2362, -0.1608, -0.1859, -0.1847, -0.1895, -0.0314, -0.1943,\n",
       "                       -0.1505, -0.3289, -0.2361, -0.2156, -0.1299, -0.1773, -0.2099, -0.2011,\n",
       "                       -0.2530, -0.1426, -0.2083, -0.0901, -0.1957, -0.1484, -0.1339, -0.1054,\n",
       "                       -0.1747, -0.2282, -0.2149, -0.1166, -0.1553, -0.1085, -0.1803, -0.2849,\n",
       "                       -0.2088, -0.0514, -0.1070, -0.1048, -0.0407, -0.1084, -0.1568, -0.1900,\n",
       "                       -0.1123, -0.0492, -0.2228, -0.1008, -0.1477, -0.2357, -0.1429, -0.1356,\n",
       "                       -0.2173, -0.1622, -0.1610, -0.0824, -0.0341, -0.1980, -0.1786, -0.2130,\n",
       "                       -0.1699, -0.2444, -0.1419, -0.1965, -0.1692, -0.1060, -0.0947, -0.1178,\n",
       "                       -0.0747, -0.0787, -0.1537, -0.2165, -0.0384, -0.0998, -0.1370, -0.1275,\n",
       "                       -0.2643, -0.0984, -0.2353, -0.2885, -0.1483, -0.1036, -0.1749, -0.1332,\n",
       "                       -0.2548, -0.1328, -0.1463, -0.2111, -0.1691, -0.1951, -0.0352, -0.3199],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([1.0133, 0.9400, 0.9715, 0.9199, 0.9971, 0.9557, 1.0142, 1.0431, 0.9573,\n",
       "                       1.0472, 0.9203, 1.0477, 0.9543, 0.8819, 0.8805, 0.9219, 0.9784, 0.9710,\n",
       "                       0.9755, 0.9954, 0.9827, 1.0744, 1.0138, 0.8600, 1.0193, 0.9702, 0.9383,\n",
       "                       1.0068, 1.0183, 1.0428, 1.0475, 1.0237, 1.0391, 0.9870, 1.0287, 1.0109,\n",
       "                       1.0424, 1.0130, 0.9638, 0.8808, 0.9948, 1.0954, 0.9847, 0.9379, 1.0000,\n",
       "                       1.0076, 1.0427, 1.0088, 0.9580, 1.0190, 1.0976, 0.9603, 0.9876, 0.9975,\n",
       "                       1.0023, 0.9212, 0.9426, 1.0214, 0.9328, 0.9774, 1.0568, 0.9380, 1.0426,\n",
       "                       0.9950, 0.8580, 1.0078, 0.9192, 0.9853, 0.8844, 0.9647, 0.9922, 1.0806,\n",
       "                       0.9971, 0.9362, 0.8662, 0.9087, 0.9608, 1.0333, 0.9291, 0.9553, 0.9957,\n",
       "                       1.0473, 1.0113, 1.0000, 0.9939, 1.0897, 0.8791, 0.9185, 1.1027, 1.0132,\n",
       "                       0.9579, 0.9670, 0.9459, 1.0352, 0.8953, 1.0379, 0.9253, 0.9466, 0.9476,\n",
       "                       1.0929, 0.9853, 0.9026, 0.9466, 0.9662, 1.0090, 0.9340, 0.9510, 0.9871,\n",
       "                       0.9250, 0.9599, 0.9968, 1.0239, 1.0293, 0.9069, 0.9799, 0.8951, 1.0673,\n",
       "                       1.0189, 0.9634, 0.8915, 1.0564, 1.0084, 1.0184, 1.0046, 0.9893, 1.1311,\n",
       "                       0.9732, 1.0953], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-0.0274, -0.0205, -0.0205],\n",
       "                         [ 0.0063,  0.0112, -0.0039],\n",
       "                         [ 0.0326,  0.0058,  0.0436]],\n",
       "               \n",
       "                        [[-0.0501, -0.0424, -0.0324],\n",
       "                         [-0.0248, -0.0294, -0.0142],\n",
       "                         [-0.0405, -0.0306, -0.0251]],\n",
       "               \n",
       "                        [[ 0.0435, -0.0080,  0.0415],\n",
       "                         [ 0.0002,  0.0027,  0.0482],\n",
       "                         [ 0.0101,  0.0430,  0.0156]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0395, -0.0042, -0.0089],\n",
       "                         [ 0.0160, -0.0199, -0.0157],\n",
       "                         [-0.0312, -0.0185, -0.0059]],\n",
       "               \n",
       "                        [[-0.0564,  0.0056, -0.0253],\n",
       "                         [-0.0159,  0.0320,  0.0096],\n",
       "                         [-0.0040, -0.0366, -0.0343]],\n",
       "               \n",
       "                        [[ 0.0140, -0.0339, -0.0162],\n",
       "                         [ 0.0379, -0.0040, -0.0144],\n",
       "                         [ 0.0197, -0.0375, -0.0007]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0100, -0.0306, -0.0186],\n",
       "                         [-0.0055, -0.0135, -0.0047],\n",
       "                         [-0.0121, -0.0200, -0.0399]],\n",
       "               \n",
       "                        [[-0.0290, -0.0155, -0.0064],\n",
       "                         [-0.0493, -0.0235, -0.0392],\n",
       "                         [ 0.0098,  0.0201, -0.0005]],\n",
       "               \n",
       "                        [[ 0.0206,  0.0078,  0.0137],\n",
       "                         [ 0.0213, -0.0406,  0.0110],\n",
       "                         [-0.0242, -0.0116, -0.0023]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0192,  0.0040,  0.0298],\n",
       "                         [-0.0179,  0.0159,  0.0392],\n",
       "                         [ 0.0666,  0.0076, -0.0008]],\n",
       "               \n",
       "                        [[-0.0032,  0.0284,  0.0059],\n",
       "                         [-0.0002,  0.0263, -0.0164],\n",
       "                         [ 0.0483,  0.0050,  0.0134]],\n",
       "               \n",
       "                        [[ 0.0192, -0.0182,  0.0069],\n",
       "                         [-0.0175, -0.0392, -0.0369],\n",
       "                         [-0.0182, -0.0086, -0.0170]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0373,  0.0077,  0.0503],\n",
       "                         [ 0.0048,  0.0076,  0.0340],\n",
       "                         [ 0.0412,  0.0334,  0.0080]],\n",
       "               \n",
       "                        [[ 0.0601,  0.0437,  0.0512],\n",
       "                         [ 0.0425,  0.0399,  0.0191],\n",
       "                         [ 0.0471,  0.0223,  0.0383]],\n",
       "               \n",
       "                        [[ 0.1064,  0.0444,  0.0323],\n",
       "                         [-0.0052,  0.0125, -0.0043],\n",
       "                         [ 0.0575,  0.0214, -0.0043]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0779, -0.0130, -0.0736],\n",
       "                         [-0.0426, -0.0431, -0.0374],\n",
       "                         [-0.0752, -0.0288, -0.0397]],\n",
       "               \n",
       "                        [[ 0.0492,  0.0185,  0.0113],\n",
       "                         [ 0.0150, -0.0196, -0.0308],\n",
       "                         [-0.0028, -0.0184, -0.0134]],\n",
       "               \n",
       "                        [[ 0.0089,  0.0171,  0.0097],\n",
       "                         [-0.0158,  0.0042, -0.0399],\n",
       "                         [ 0.0152,  0.0209,  0.0002]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0505, -0.0295, -0.0184],\n",
       "                         [-0.0126, -0.0107, -0.0039],\n",
       "                         [-0.0478, -0.0414, -0.0441]],\n",
       "               \n",
       "                        [[-0.0574, -0.0021, -0.0378],\n",
       "                         [-0.0262, -0.0670, -0.0310],\n",
       "                         [-0.0747, -0.0660, -0.0185]],\n",
       "               \n",
       "                        [[-0.0051,  0.0490,  0.0153],\n",
       "                         [-0.0089,  0.0108, -0.0262],\n",
       "                         [-0.0403,  0.0195, -0.0125]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0053,  0.0211, -0.0199],\n",
       "                         [ 0.0388,  0.0482,  0.0459],\n",
       "                         [ 0.0162, -0.0024,  0.0094]],\n",
       "               \n",
       "                        [[-0.0132,  0.0380,  0.0149],\n",
       "                         [ 0.0099,  0.0425,  0.0271],\n",
       "                         [ 0.0157,  0.0037, -0.0010]],\n",
       "               \n",
       "                        [[-0.0651, -0.0075, -0.0588],\n",
       "                         [-0.0509, -0.0630, -0.0616],\n",
       "                         [-0.1223, -0.0908, -0.1112]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0267, -0.0016,  0.0189],\n",
       "                         [ 0.0017, -0.0244,  0.0189],\n",
       "                         [ 0.0162,  0.0090,  0.0151]],\n",
       "               \n",
       "                        [[ 0.0830,  0.0267,  0.0561],\n",
       "                         [ 0.0566,  0.0330,  0.0395],\n",
       "                         [ 0.0361,  0.0229,  0.0281]],\n",
       "               \n",
       "                        [[ 0.0579,  0.0153,  0.0343],\n",
       "                         [ 0.0177,  0.0191,  0.0068],\n",
       "                         [ 0.0270,  0.0280, -0.0017]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0967, -0.0701, -0.0690],\n",
       "                         [-0.0851, -0.0591, -0.0411],\n",
       "                         [-0.0918, -0.0271, -0.0582]],\n",
       "               \n",
       "                        [[-0.0183, -0.0470, -0.0275],\n",
       "                         [-0.0440, -0.0518, -0.0613],\n",
       "                         [-0.0235, -0.0700, -0.0094]],\n",
       "               \n",
       "                        [[ 0.0049,  0.0011,  0.0136],\n",
       "                         [-0.0049,  0.0027, -0.0193],\n",
       "                         [ 0.0061, -0.0081, -0.0110]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0452, -0.0595, -0.0560],\n",
       "                         [ 0.0110, -0.0054, -0.0396],\n",
       "                         [-0.0266, -0.0351, -0.0428]],\n",
       "               \n",
       "                        [[ 0.1062,  0.0769,  0.0966],\n",
       "                         [ 0.0820, -0.0037,  0.0404],\n",
       "                         [ 0.1102,  0.0656,  0.0819]],\n",
       "               \n",
       "                        [[-0.0232, -0.0071,  0.0080],\n",
       "                         [-0.0263,  0.0270, -0.0017],\n",
       "                         [ 0.0425,  0.0208,  0.0443]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0317, -0.0167, -0.0296],\n",
       "                         [-0.0092, -0.0299, -0.0233],\n",
       "                         [-0.0383,  0.0229, -0.0375]],\n",
       "               \n",
       "                        [[ 0.0240,  0.0542,  0.0766],\n",
       "                         [ 0.0176, -0.0143,  0.0107],\n",
       "                         [ 0.0039,  0.0426,  0.0327]],\n",
       "               \n",
       "                        [[-0.0090,  0.0049, -0.0199],\n",
       "                         [-0.0073, -0.0077, -0.0188],\n",
       "                         [ 0.0147, -0.0025, -0.0225]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 0.0141,  0.0351,  0.0626, -0.0182,  0.0631,  0.0430,  0.0335,  0.0177,\n",
       "                        0.0822, -0.0261,  0.0252,  0.0301,  0.0632,  0.0221,  0.0410,  0.0281,\n",
       "                        0.0088,  0.0209, -0.0283,  0.0340,  0.0400,  0.0491,  0.0708,  0.0316,\n",
       "                        0.0087,  0.0063,  0.0125,  0.0101,  0.0422, -0.0362,  0.0055,  0.0483,\n",
       "                        0.0335,  0.0257,  0.0299,  0.0273,  0.0157, -0.0001, -0.0158,  0.0143,\n",
       "                        0.0322,  0.0730,  0.0171,  0.0297,  0.0665, -0.0025,  0.0532, -0.0296,\n",
       "                        0.0462,  0.0165, -0.0072, -0.0345, -0.0039,  0.0770, -0.0318,  0.0504,\n",
       "                        0.0372,  0.0092,  0.0458,  0.0106,  0.0995,  0.0512, -0.0094,  0.0209,\n",
       "                       -0.0178,  0.0163,  0.0087, -0.0261,  0.0089,  0.0502,  0.0545,  0.0380,\n",
       "                        0.0345,  0.0575,  0.0413,  0.0009,  0.0060,  0.0053, -0.0140, -0.0341,\n",
       "                        0.0446, -0.0099,  0.0108,  0.0269,  0.0100, -0.0461,  0.0458,  0.0569,\n",
       "                       -0.0337, -0.0355,  0.0010, -0.0422,  0.0202, -0.0083,  0.0281,  0.0579,\n",
       "                        0.0173,  0.0189,  0.0140,  0.0336, -0.0784,  0.0063,  0.0238,  0.0106,\n",
       "                        0.0397,  0.0649,  0.0499,  0.0336, -0.0065,  0.0024,  0.0193,  0.0101,\n",
       "                        0.0150,  0.0046,  0.0137,  0.0361,  0.0136, -0.0121,  0.0246,  0.0097,\n",
       "                        0.0269, -0.0108,  0.0495,  0.0128,  0.0314,  0.0368,  0.0414,  0.0614],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.3152, -0.2995, -0.3308, -0.3886, -0.0856, -0.5815, -0.3950, -0.2181,\n",
       "                       -0.1120, -0.2190, -0.2786, -0.5619, -0.6169, -0.4979, -0.2985, -0.5421,\n",
       "                       -0.2314, -0.3636, -0.0055, -0.4421, -0.2575, -0.1395, -0.2091, -0.3434,\n",
       "                       -0.4172, -0.3988, -0.4398, -0.2220, -0.4758, -0.3489, -0.3736, -0.2428,\n",
       "                       -0.4958, -0.5034, -0.1616, -0.3574, -0.1536, -0.5120, -0.2405, -0.2860,\n",
       "                       -0.1508, -0.2458, -0.3309, -0.3076, -0.1776, -0.3390, -0.5320, -0.2152,\n",
       "                        0.0091, -0.0819, -0.1451, -0.0401, -0.4679, -0.0188, -0.3988, -0.5742,\n",
       "                       -0.2936, -0.3164, -0.4420, -0.2474,  0.0021, -0.4383, -0.3415, -0.2095,\n",
       "                       -0.3460, -0.4973, -0.4337, -0.0445, -0.2226, -0.3935, -0.5602, -0.3814,\n",
       "                       -0.1507, -0.1896, -0.5829, -0.3616, -0.3121, -0.3417, -0.2956, -0.3662,\n",
       "                       -0.5204, -0.1592, -0.2332, -0.3993, -0.3787, -0.3915, -0.5979, -0.1667,\n",
       "                       -0.4034, -0.1774, -0.0776, -0.0564, -0.1612, -0.3792, -0.5075, -0.5815,\n",
       "                       -0.5213, -0.3435, -0.2285, -0.4934, -0.3467, -0.1896, -0.3943, -0.2649,\n",
       "                       -0.3437, -0.6096, -0.4691, -0.2730, -0.2352, -0.5131, -0.0830, -0.3402,\n",
       "                       -0.4275, -0.3344, -0.5429, -0.6165, -0.4073, -0.0878, -0.4085, -0.3994,\n",
       "                       -0.5160, -0.1098, -0.2787, -0.3699, -0.3452, -0.1928, -0.1506, -0.2544],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([1.4160, 1.1340, 1.5827, 1.2931, 1.6960, 1.3326, 1.4613, 1.1536, 1.5475,\n",
       "                       1.2915, 1.0701, 1.1859, 1.3950, 1.4556, 1.4674, 1.2395, 1.7516, 1.3523,\n",
       "                       1.4108, 1.6153, 1.5576, 1.6465, 1.5639, 0.9898, 0.5103, 0.6026, 0.7077,\n",
       "                       1.4007, 1.2511, 0.5291, 0.9601, 1.6630, 1.4830, 1.2734, 1.3108, 1.3484,\n",
       "                       1.4657, 0.6795, 1.1391, 1.4949, 1.5578, 1.3688, 1.1871, 0.9223, 1.6031,\n",
       "                       1.2536, 1.4174, 1.4022, 1.6729, 1.4718, 1.2358, 1.4681, 0.6377, 1.7374,\n",
       "                       1.2578, 1.6576, 1.2801, 0.9369, 1.4838, 1.4362, 1.6398, 1.3455, 1.0968,\n",
       "                       1.3611, 1.4120, 0.6880, 0.5843, 1.5440, 1.4377, 1.3094, 1.7202, 0.9911,\n",
       "                       1.2383, 1.5200, 1.2725, 1.2166, 1.2204, 0.7681, 1.1892, 0.4905, 1.0678,\n",
       "                       1.5456, 1.0445, 1.1059, 0.5859, 0.5934, 1.4798, 1.2934, 1.2400, 1.3931,\n",
       "                       1.3801, 1.4581, 1.3112, 0.4807, 1.3517, 1.3861, 1.2934, 1.4056, 1.0638,\n",
       "                       1.3401, 0.5837, 1.5417, 0.6180, 1.7800, 1.3710, 1.5934, 1.2130, 1.9190,\n",
       "                       1.4762, 1.3281, 1.5336, 1.2815, 1.0562, 0.9741, 1.1945, 1.2391, 0.9156,\n",
       "                       1.5432, 0.9367, 0.7273, 0.7504, 1.3918, 1.3713, 0.9035, 1.5051, 0.9692,\n",
       "                       1.6098, 1.5835], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0094, -0.0036,  0.0082,  ..., -0.0044,  0.0091, -0.0104],\n",
       "                       [-0.0064, -0.0049,  0.0033,  ..., -0.0056,  0.0093, -0.0059],\n",
       "                       [-0.0011, -0.0070,  0.0141,  ..., -0.0043,  0.0100, -0.0054],\n",
       "                       [-0.0061, -0.0047,  0.0094,  ..., -0.0016,  0.0033, -0.0033],\n",
       "                       [-0.0012,  0.0001, -0.0011,  ..., -0.0061,  0.0024, -0.0133]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([ 0.0180, -0.0869,  0.0190,  0.0178, -0.0670], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.8318467121124267,\n",
       "   1.5983083968162537,\n",
       "   1.5430345573425293,\n",
       "   1.5083630273342132,\n",
       "   1.4910247814655304,\n",
       "   1.4608267521858216,\n",
       "   1.4340785319805145,\n",
       "   1.4145927634239197,\n",
       "   1.3996416285037994,\n",
       "   1.3747078623771667,\n",
       "   1.3607904794216157,\n",
       "   1.3409236319065094,\n",
       "   1.341164960861206,\n",
       "   1.3283679914474487,\n",
       "   1.3163312566280365,\n",
       "   1.3115412558317185,\n",
       "   1.2962443249225617,\n",
       "   1.2923316864967347,\n",
       "   1.2765611706972122,\n",
       "   1.2701425309181213,\n",
       "   1.262922222018242,\n",
       "   1.2610854940414429,\n",
       "   1.2470033650398253,\n",
       "   1.2460899232625962,\n",
       "   1.2271174730062484,\n",
       "   1.2249810993671417,\n",
       "   1.2250787972211838,\n",
       "   1.2112787744998932,\n",
       "   1.2127141485214232,\n",
       "   1.1938127961158753,\n",
       "   1.195285102725029,\n",
       "   1.1816516741514207,\n",
       "   1.1898823568820953,\n",
       "   1.1711176308393478,\n",
       "   1.1710917117595672,\n",
       "   1.166148725748062,\n",
       "   1.1586723541021346,\n",
       "   1.1477791343927384,\n",
       "   1.1442804923057557,\n",
       "   1.1410921235084535,\n",
       "   1.1340575044155121,\n",
       "   1.1319040760993957,\n",
       "   1.114160883665085,\n",
       "   1.120978896379471,\n",
       "   1.108312455534935,\n",
       "   1.1094423418045043,\n",
       "   1.0921668326854705,\n",
       "   1.1024008923768998,\n",
       "   1.0996985973119735,\n",
       "   1.0799782016277313,\n",
       "   1.0811318504810332,\n",
       "   1.0735118397474288,\n",
       "   1.070364094018936,\n",
       "   1.081956598162651,\n",
       "   1.0725402553081513,\n",
       "   1.079026174068451,\n",
       "   1.0602641402482986,\n",
       "   1.0693339935541153,\n",
       "   1.0588311381340028,\n",
       "   1.0614249033927918,\n",
       "   1.0621320962905885,\n",
       "   1.0392640874385835,\n",
       "   1.0419712617397308,\n",
       "   1.0351883593797684,\n",
       "   1.0324410486221314,\n",
       "   1.031361035823822,\n",
       "   1.0397289422750473,\n",
       "   1.0225966469049454,\n",
       "   1.0161720789670945,\n",
       "   1.0088632398843764,\n",
       "   1.009213131904602,\n",
       "   1.0156853744983674,\n",
       "   1.0081839593648911,\n",
       "   1.0092095991373062,\n",
       "   0.9974531508684158,\n",
       "   1.0004725872278213,\n",
       "   0.9896835037469864,\n",
       "   0.9879749362468719,\n",
       "   1.0061366568803787,\n",
       "   0.9853436852693558,\n",
       "   0.9807116059064865,\n",
       "   0.9778145459890366,\n",
       "   0.9866289068460464,\n",
       "   0.9681597218513489,\n",
       "   0.9662101818323136,\n",
       "   0.9701725223064422,\n",
       "   0.9585104268789292,\n",
       "   0.973819406747818,\n",
       "   0.9624878883361816,\n",
       "   0.9632994604110717,\n",
       "   0.9499297029972077,\n",
       "   0.9442514389753341,\n",
       "   0.9466090531349182,\n",
       "   0.9450236101150513,\n",
       "   0.9448442112207412,\n",
       "   0.936452612042427,\n",
       "   0.9437068248987198,\n",
       "   0.9459252759218216,\n",
       "   0.9317153687477112,\n",
       "   0.9291333380937576,\n",
       "   0.9329384281635285,\n",
       "   0.9519873490333557,\n",
       "   0.9493791848421097,\n",
       "   0.9366050963401794,\n",
       "   0.9341552958488465,\n",
       "   0.9252304375171662,\n",
       "   0.9288655401468277,\n",
       "   0.9225431883335113,\n",
       "   0.915434630393982,\n",
       "   0.9227971856594086,\n",
       "   0.9103666851520539,\n",
       "   0.922907907485962,\n",
       "   0.9189257556200028,\n",
       "   0.9198251608610153,\n",
       "   0.903761478304863,\n",
       "   0.9088799324035645,\n",
       "   0.9331154893636704,\n",
       "   0.9072175570726395,\n",
       "   0.9049256544113159,\n",
       "   0.9076299633979797,\n",
       "   0.8938433399200439,\n",
       "   0.9042160888910293,\n",
       "   0.8940214012861252,\n",
       "   0.9051282244920731,\n",
       "   0.8976612440347671,\n",
       "   0.8855648146867752,\n",
       "   0.8902094473838806,\n",
       "   0.88288112616539,\n",
       "   0.8787294689416886,\n",
       "   0.8814283108711243,\n",
       "   0.8702052093744278,\n",
       "   0.8785008816719055,\n",
       "   0.8865549991130829,\n",
       "   0.8771972962617874,\n",
       "   0.8673693441152572,\n",
       "   0.8508515096902848,\n",
       "   0.8694924570322037,\n",
       "   0.8584951921701431,\n",
       "   0.8819297049045562,\n",
       "   0.8648730691671371,\n",
       "   0.8560707029104233,\n",
       "   0.866009321808815,\n",
       "   0.8478461540937424,\n",
       "   0.8427363322973251,\n",
       "   0.862855496764183,\n",
       "   0.845456717967987,\n",
       "   0.8548695590496064,\n",
       "   0.8454525594711304,\n",
       "   0.8440196956396103],\n",
       "  'train_loss_std': [0.25430728678725373,\n",
       "   0.084993271046978,\n",
       "   0.08375380470876247,\n",
       "   0.08445574280740095,\n",
       "   0.07961509068337601,\n",
       "   0.08309896083206332,\n",
       "   0.08398734438530799,\n",
       "   0.08344440798520025,\n",
       "   0.0872600512560799,\n",
       "   0.08577531061463353,\n",
       "   0.08391023378803657,\n",
       "   0.08852135064015293,\n",
       "   0.0901474745468707,\n",
       "   0.08731090436641428,\n",
       "   0.08939504237431631,\n",
       "   0.09402506805328714,\n",
       "   0.09341650095197736,\n",
       "   0.09271051067735596,\n",
       "   0.10184151006759526,\n",
       "   0.09116618780811374,\n",
       "   0.10343405506012397,\n",
       "   0.09876189079379723,\n",
       "   0.09846813690981182,\n",
       "   0.10071172225177179,\n",
       "   0.09763278117036188,\n",
       "   0.10397094813279456,\n",
       "   0.10674817025316521,\n",
       "   0.10548467256780979,\n",
       "   0.10236928379453124,\n",
       "   0.10312824047612261,\n",
       "   0.10559665209918664,\n",
       "   0.10140992728486102,\n",
       "   0.09757671650164622,\n",
       "   0.10839923310194048,\n",
       "   0.1036139431375372,\n",
       "   0.10314840452871259,\n",
       "   0.11023762339049521,\n",
       "   0.1071390984188472,\n",
       "   0.10787154260084413,\n",
       "   0.11273799882365046,\n",
       "   0.10691848112779953,\n",
       "   0.10847768353167586,\n",
       "   0.10827981145662749,\n",
       "   0.11209516406070467,\n",
       "   0.11017308658961784,\n",
       "   0.10739596400805719,\n",
       "   0.1134873267336262,\n",
       "   0.11411276337639416,\n",
       "   0.11049318324223942,\n",
       "   0.1130082342184702,\n",
       "   0.11049433251073852,\n",
       "   0.10993174105164452,\n",
       "   0.1085623957137897,\n",
       "   0.10757337132704647,\n",
       "   0.11242312252478646,\n",
       "   0.11456290910979415,\n",
       "   0.10532850967367308,\n",
       "   0.11601125042059908,\n",
       "   0.11204571710392543,\n",
       "   0.11110046941311837,\n",
       "   0.1141594874779681,\n",
       "   0.11514013999811318,\n",
       "   0.10923839970206081,\n",
       "   0.11614529999547146,\n",
       "   0.10735897074172417,\n",
       "   0.11064994461725695,\n",
       "   0.11771046578355412,\n",
       "   0.1106178771448733,\n",
       "   0.120420679333906,\n",
       "   0.11826470988251533,\n",
       "   0.11003370541320137,\n",
       "   0.11160874070258278,\n",
       "   0.11239780002963612,\n",
       "   0.11362189605562487,\n",
       "   0.10915940017809887,\n",
       "   0.11718432605162146,\n",
       "   0.11427815689085755,\n",
       "   0.11693868992411961,\n",
       "   0.11152526016167968,\n",
       "   0.11570689791722714,\n",
       "   0.11496582669529307,\n",
       "   0.11248364177957869,\n",
       "   0.11421849161652878,\n",
       "   0.11006540729500916,\n",
       "   0.11511958783722853,\n",
       "   0.11803298524388896,\n",
       "   0.11305106814136968,\n",
       "   0.11674152655606353,\n",
       "   0.11456577634886456,\n",
       "   0.11741536442071861,\n",
       "   0.11522761842997042,\n",
       "   0.11579926206441513,\n",
       "   0.10738557029736848,\n",
       "   0.12142256764853167,\n",
       "   0.1158472193622679,\n",
       "   0.11369715959649508,\n",
       "   0.11564106801876753,\n",
       "   0.1138792467872833,\n",
       "   0.11287865879707784,\n",
       "   0.11900999388856959,\n",
       "   0.11926457672699836,\n",
       "   0.11842782961904354,\n",
       "   0.1173687503223554,\n",
       "   0.11559321513474287,\n",
       "   0.11635888367781452,\n",
       "   0.11299892947024147,\n",
       "   0.1184021590369636,\n",
       "   0.12061755624871169,\n",
       "   0.11275006546593663,\n",
       "   0.11656587535983456,\n",
       "   0.11300891116123327,\n",
       "   0.12195442181322838,\n",
       "   0.11231699005200764,\n",
       "   0.10946475145126998,\n",
       "   0.11792776223763554,\n",
       "   0.10752567906456433,\n",
       "   0.12207011691830477,\n",
       "   0.11906396331671412,\n",
       "   0.11652073469893186,\n",
       "   0.11515075241473433,\n",
       "   0.11428786451420896,\n",
       "   0.1149768832603518,\n",
       "   0.11450125174906578,\n",
       "   0.11364654611145841,\n",
       "   0.10741771758362788,\n",
       "   0.11954268232380394,\n",
       "   0.11464580350307306,\n",
       "   0.1160913384866323,\n",
       "   0.10970057821986173,\n",
       "   0.11568395625934072,\n",
       "   0.11305833230764384,\n",
       "   0.11344421055740107,\n",
       "   0.1152109067990924,\n",
       "   0.11072609150675691,\n",
       "   0.11233687850243745,\n",
       "   0.11069280188361806,\n",
       "   0.1142066152539962,\n",
       "   0.11563178020471025,\n",
       "   0.1336949129228227,\n",
       "   0.10658984041635278,\n",
       "   0.10895258203795978,\n",
       "   0.11557076426309834,\n",
       "   0.10845407784789174,\n",
       "   0.11372342134459273,\n",
       "   0.1120583720602935,\n",
       "   0.11012068624693415,\n",
       "   0.11975816739697703,\n",
       "   0.11760073067740445,\n",
       "   0.11205435614480991],\n",
       "  'train_accuracy_mean': [0.3070666653513908,\n",
       "   0.32491333308815956,\n",
       "   0.3404133334755898,\n",
       "   0.35419999992847445,\n",
       "   0.36241333320736885,\n",
       "   0.37706000128388406,\n",
       "   0.39460666716098786,\n",
       "   0.40517999935150145,\n",
       "   0.4125199996829033,\n",
       "   0.42663333386182783,\n",
       "   0.4365666669011116,\n",
       "   0.4480599992275238,\n",
       "   0.4460600001811981,\n",
       "   0.45459333300590515,\n",
       "   0.45809999930858614,\n",
       "   0.46153333312273026,\n",
       "   0.47069999933242795,\n",
       "   0.4733666652441025,\n",
       "   0.48013999944925306,\n",
       "   0.48213999962806703,\n",
       "   0.4891133328080177,\n",
       "   0.4882333324551582,\n",
       "   0.4962399986386299,\n",
       "   0.4961866652965546,\n",
       "   0.5066599997282029,\n",
       "   0.5055999993085861,\n",
       "   0.5076866655945778,\n",
       "   0.5130733324885368,\n",
       "   0.5116933330893516,\n",
       "   0.5244466660022735,\n",
       "   0.5215399985909462,\n",
       "   0.5292733322978019,\n",
       "   0.5236333327889442,\n",
       "   0.5325866639614105,\n",
       "   0.5340999999642372,\n",
       "   0.5358599984049797,\n",
       "   0.5382999997138977,\n",
       "   0.5445533316135407,\n",
       "   0.5464533330202103,\n",
       "   0.5470133320689201,\n",
       "   0.5501999985575676,\n",
       "   0.5513866642713546,\n",
       "   0.5604333326816558,\n",
       "   0.5568799996376037,\n",
       "   0.5632333328723907,\n",
       "   0.562566665828228,\n",
       "   0.5694599976539612,\n",
       "   0.5620066668987275,\n",
       "   0.5662399987578393,\n",
       "   0.5767266654968262,\n",
       "   0.5763199990391731,\n",
       "   0.5771933334469795,\n",
       "   0.5798399978876114,\n",
       "   0.5738133338093757,\n",
       "   0.5786066652536392,\n",
       "   0.5757866660952569,\n",
       "   0.5841866652965546,\n",
       "   0.5808066660165787,\n",
       "   0.5858933341503143,\n",
       "   0.5824399989843368,\n",
       "   0.5831333324313164,\n",
       "   0.5945599985718727,\n",
       "   0.5934066649079323,\n",
       "   0.5981199987530709,\n",
       "   0.5968466661572457,\n",
       "   0.5999866656661034,\n",
       "   0.5955533333420754,\n",
       "   0.6064933323860169,\n",
       "   0.6077199986577034,\n",
       "   0.6104466669559478,\n",
       "   0.6082599985003472,\n",
       "   0.6060333323478698,\n",
       "   0.6077999997735023,\n",
       "   0.6073199996352195,\n",
       "   0.6145733326673508,\n",
       "   0.612666666507721,\n",
       "   0.6176599999070167,\n",
       "   0.6191066663265228,\n",
       "   0.6071466664075852,\n",
       "   0.6201999992132187,\n",
       "   0.6217333322763443,\n",
       "   0.6233999998569488,\n",
       "   0.6190733332633972,\n",
       "   0.6281400000452996,\n",
       "   0.628993332862854,\n",
       "   0.6254933348298073,\n",
       "   0.6313000001311302,\n",
       "   0.6243066662549972,\n",
       "   0.6274533337950706,\n",
       "   0.6270666673183442,\n",
       "   0.6357799990177154,\n",
       "   0.6363933340907096,\n",
       "   0.6371066672205925,\n",
       "   0.6374600006341934,\n",
       "   0.6379533338546753,\n",
       "   0.6408466691374779,\n",
       "   0.6366533332467079,\n",
       "   0.6359399991035461,\n",
       "   0.6432266676425934,\n",
       "   0.6434133331179619,\n",
       "   0.640826667368412,\n",
       "   0.6314466668963432,\n",
       "   0.6346599998474121,\n",
       "   0.6415866671204566,\n",
       "   0.6411866652965545,\n",
       "   0.6447399994134903,\n",
       "   0.6450866677165031,\n",
       "   0.6501533325314521,\n",
       "   0.6504266664981843,\n",
       "   0.6503466671705246,\n",
       "   0.652080000936985,\n",
       "   0.6478799999952316,\n",
       "   0.6496200001835823,\n",
       "   0.648746666431427,\n",
       "   0.6575400012135506,\n",
       "   0.6543000007271766,\n",
       "   0.6474400005936622,\n",
       "   0.6575999990701675,\n",
       "   0.6556999994516373,\n",
       "   0.6537799995541572,\n",
       "   0.6604266669154167,\n",
       "   0.6550666675567627,\n",
       "   0.659653333902359,\n",
       "   0.656273333966732,\n",
       "   0.6584800008535385,\n",
       "   0.6640133343338966,\n",
       "   0.6633266672492028,\n",
       "   0.6669333330392837,\n",
       "   0.6670666687488556,\n",
       "   0.6650400002002717,\n",
       "   0.6697733343243599,\n",
       "   0.6664799994826317,\n",
       "   0.6634000017046928,\n",
       "   0.6663333334922791,\n",
       "   0.6735333343744278,\n",
       "   0.6788199992775917,\n",
       "   0.6706800013780594,\n",
       "   0.6749133334159851,\n",
       "   0.6672933346033096,\n",
       "   0.6740866659283637,\n",
       "   0.6769533339738846,\n",
       "   0.673753333568573,\n",
       "   0.681533333659172,\n",
       "   0.6822866665124894,\n",
       "   0.6716799997091293,\n",
       "   0.6817733328342438,\n",
       "   0.6789199997782707,\n",
       "   0.6835533339381218,\n",
       "   0.6806666684150696],\n",
       "  'train_accuracy_std': [0.03881145303878916,\n",
       "   0.043225677532622866,\n",
       "   0.04668530872403287,\n",
       "   0.04534367529007336,\n",
       "   0.04652165841966738,\n",
       "   0.04673044118484911,\n",
       "   0.049600635604112306,\n",
       "   0.04994075138300228,\n",
       "   0.051896099004607184,\n",
       "   0.051369456360191613,\n",
       "   0.04920626641195841,\n",
       "   0.0540007480305731,\n",
       "   0.05510161059318927,\n",
       "   0.05443804911540247,\n",
       "   0.052107271212261635,\n",
       "   0.05541283406294904,\n",
       "   0.05599463210762095,\n",
       "   0.05412309969735598,\n",
       "   0.05830115649097946,\n",
       "   0.05526198482547766,\n",
       "   0.058802799757961384,\n",
       "   0.0575454505264934,\n",
       "   0.056655254452774736,\n",
       "   0.05989687978149517,\n",
       "   0.05915873222679376,\n",
       "   0.05975185866862627,\n",
       "   0.06080207962852439,\n",
       "   0.061236147023480396,\n",
       "   0.059561913450971136,\n",
       "   0.05808063789599632,\n",
       "   0.05992889831987334,\n",
       "   0.05831432757212073,\n",
       "   0.05580102675584511,\n",
       "   0.060764556997394394,\n",
       "   0.05885170687391729,\n",
       "   0.05784187573449869,\n",
       "   0.06135560378809966,\n",
       "   0.06011027389671578,\n",
       "   0.0618823181739724,\n",
       "   0.0625972302015544,\n",
       "   0.06004668511853583,\n",
       "   0.06090164635867274,\n",
       "   0.06137689595108677,\n",
       "   0.062346691609472565,\n",
       "   0.0598539982755327,\n",
       "   0.06073083697001221,\n",
       "   0.06331681861248112,\n",
       "   0.06382228713768677,\n",
       "   0.06106495932574125,\n",
       "   0.061529548819257436,\n",
       "   0.0610090332973509,\n",
       "   0.05945540218301207,\n",
       "   0.06065290170825996,\n",
       "   0.05939989420619345,\n",
       "   0.06018188003845165,\n",
       "   0.062286284796995314,\n",
       "   0.05739439288238227,\n",
       "   0.06291258845049795,\n",
       "   0.06177685529642144,\n",
       "   0.06144629013049259,\n",
       "   0.0629666928007179,\n",
       "   0.061747928197320334,\n",
       "   0.06107422792061318,\n",
       "   0.06053886899916438,\n",
       "   0.05855131578662007,\n",
       "   0.058659468491559286,\n",
       "   0.0635514202524225,\n",
       "   0.05883586430581158,\n",
       "   0.06499574000001652,\n",
       "   0.06491259583366554,\n",
       "   0.060128521125485335,\n",
       "   0.060127818020251696,\n",
       "   0.061800432765437126,\n",
       "   0.062049227274557577,\n",
       "   0.0607103905406875,\n",
       "   0.06514428492805324,\n",
       "   0.06355690385677655,\n",
       "   0.06331966328430698,\n",
       "   0.060324610441334324,\n",
       "   0.06444846405916746,\n",
       "   0.06300173110601319,\n",
       "   0.06237374872469428,\n",
       "   0.06283070057221192,\n",
       "   0.059750837805992815,\n",
       "   0.062289895607438646,\n",
       "   0.06395867611959759,\n",
       "   0.06064046304402594,\n",
       "   0.06222064161629325,\n",
       "   0.06161676365314224,\n",
       "   0.06309336959180273,\n",
       "   0.061227013502062756,\n",
       "   0.06308929195050417,\n",
       "   0.05812272329046806,\n",
       "   0.06309968881153483,\n",
       "   0.06354954461722513,\n",
       "   0.058655822821673184,\n",
       "   0.0617709023553072,\n",
       "   0.05944918460194535,\n",
       "   0.06077526793359205,\n",
       "   0.06532580102124583,\n",
       "   0.06532674280718792,\n",
       "   0.061804499540661466,\n",
       "   0.05964744540763434,\n",
       "   0.06211937019257444,\n",
       "   0.061284516632318056,\n",
       "   0.06081227176444066,\n",
       "   0.063542228426611,\n",
       "   0.06566039611706187,\n",
       "   0.061802160591023884,\n",
       "   0.06250877179681201,\n",
       "   0.05918996046964667,\n",
       "   0.06311994837926987,\n",
       "   0.060514554375763696,\n",
       "   0.057968826879093674,\n",
       "   0.06392472730309523,\n",
       "   0.05746089541331939,\n",
       "   0.06347617136850742,\n",
       "   0.06372419816150579,\n",
       "   0.06393311571692112,\n",
       "   0.059302993345508495,\n",
       "   0.06287833199624934,\n",
       "   0.06195640287719849,\n",
       "   0.06152444834060283,\n",
       "   0.06012765034661519,\n",
       "   0.05967356321764717,\n",
       "   0.06455956675550724,\n",
       "   0.060901924210288894,\n",
       "   0.06096242878124117,\n",
       "   0.059775283012633795,\n",
       "   0.06126208451955903,\n",
       "   0.060010681822348554,\n",
       "   0.061355509051027296,\n",
       "   0.06046115441147021,\n",
       "   0.0607817586888545,\n",
       "   0.059245665188597973,\n",
       "   0.06100607420227179,\n",
       "   0.06060091561353365,\n",
       "   0.06264337619055697,\n",
       "   0.06611880364551519,\n",
       "   0.05871503005392358,\n",
       "   0.05793929961593449,\n",
       "   0.06175850123805309,\n",
       "   0.05811830713723732,\n",
       "   0.05835670111830683,\n",
       "   0.06057814015821149,\n",
       "   0.05758790149558905,\n",
       "   0.06258603039352348,\n",
       "   0.06151003938515468,\n",
       "   0.06067728923224827],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.688828178246816,\n",
       "   1.623844372431437,\n",
       "   1.5965785217285156,\n",
       "   1.5753108032544454,\n",
       "   1.5575758536656699,\n",
       "   1.541965634028117,\n",
       "   1.524037458896637,\n",
       "   1.5061699922879537,\n",
       "   1.4925887854894002,\n",
       "   1.4767595227559407,\n",
       "   1.4717185179392496,\n",
       "   1.4559179957707722,\n",
       "   1.455863382021586,\n",
       "   1.441533723672231,\n",
       "   1.44072346051534,\n",
       "   1.436180812517802,\n",
       "   1.4211229054133097,\n",
       "   1.413975820541382,\n",
       "   1.4234620022773743,\n",
       "   1.4142495727539062,\n",
       "   1.4038164321581523,\n",
       "   1.4050470026334128,\n",
       "   1.3819031246503195,\n",
       "   1.3892665108044941,\n",
       "   1.3945346665382385,\n",
       "   1.3839610060056051,\n",
       "   1.369941926797231,\n",
       "   1.3719466789563497,\n",
       "   1.3723469511667887,\n",
       "   1.3675373236338297,\n",
       "   1.3651344235738119,\n",
       "   1.3467641814549765,\n",
       "   1.3546836439768473,\n",
       "   1.3436996364593505,\n",
       "   1.3465730532010396,\n",
       "   1.3439765652020772,\n",
       "   1.3365069262186686,\n",
       "   1.3300005865097047,\n",
       "   1.3422380097707112,\n",
       "   1.342665978272756,\n",
       "   1.3205448492368062,\n",
       "   1.3214504432678222,\n",
       "   1.313732823530833,\n",
       "   1.3339821275075276,\n",
       "   1.3126584219932556,\n",
       "   1.300905512968699,\n",
       "   1.302627714474996,\n",
       "   1.3135455735524495,\n",
       "   1.3068626101811727,\n",
       "   1.299360686937968,\n",
       "   1.3080467224121093,\n",
       "   1.2994965100288391,\n",
       "   1.2893567514419555,\n",
       "   1.3074579493204752,\n",
       "   1.3085717225074769,\n",
       "   1.3004114270210265,\n",
       "   1.3403531130154926,\n",
       "   1.2932375224431356,\n",
       "   1.2854250391324362,\n",
       "   1.3247006376584372,\n",
       "   1.294384574095408,\n",
       "   1.2837775739034016,\n",
       "   1.3067864060401917,\n",
       "   1.283343288898468,\n",
       "   1.2825777006149293,\n",
       "   1.4350369739532471,\n",
       "   1.270222548643748,\n",
       "   1.2748551678657531,\n",
       "   1.2691165288289388,\n",
       "   1.2705410901705425,\n",
       "   1.289125723838806,\n",
       "   1.287690234184265,\n",
       "   1.2798044232527415,\n",
       "   1.2795835304260255,\n",
       "   1.2823264090220134,\n",
       "   1.2818079789479573,\n",
       "   1.2860322133700053,\n",
       "   1.2828986827532451,\n",
       "   1.293155874411265,\n",
       "   1.277217168013255,\n",
       "   1.260358635187149,\n",
       "   1.2532100518544516,\n",
       "   1.2928903142611186,\n",
       "   1.265773943265279,\n",
       "   1.2551272837320964,\n",
       "   1.2759689990679424,\n",
       "   1.266565617720286,\n",
       "   1.2681531484921773,\n",
       "   1.2611472781499227,\n",
       "   1.2746865860621135,\n",
       "   1.2775680947303771,\n",
       "   1.2610521999994915,\n",
       "   1.2725930261611937,\n",
       "   1.2817531180381776,\n",
       "   1.2553122409184774,\n",
       "   1.2714379465579986,\n",
       "   1.2638173302014668,\n",
       "   1.2527683234214784,\n",
       "   1.26398229042689,\n",
       "   1.2725456023216248,\n",
       "   1.27917645017306,\n",
       "   1.2734364827473958,\n",
       "   1.3033709541956584,\n",
       "   1.266082649230957,\n",
       "   1.2559835505485535,\n",
       "   1.2856933840115865,\n",
       "   1.2593405810991922,\n",
       "   1.276476579507192,\n",
       "   1.2909494805336,\n",
       "   1.2737098236878712,\n",
       "   1.2706426938374837,\n",
       "   1.285804069042206,\n",
       "   1.297734224796295,\n",
       "   1.2560364413261413,\n",
       "   1.2563812772432963,\n",
       "   1.2728838189442953,\n",
       "   1.2627532768249512,\n",
       "   1.251967339515686,\n",
       "   1.26441748380661,\n",
       "   1.2768745470046996,\n",
       "   1.2539046355088552,\n",
       "   1.262818994919459,\n",
       "   1.2937828866640726,\n",
       "   1.2663106671969095,\n",
       "   1.2703386028607686,\n",
       "   1.2599513204892476,\n",
       "   1.2662081336975097,\n",
       "   1.268808538913727,\n",
       "   1.265020968914032,\n",
       "   1.2727256902058919,\n",
       "   1.2588178054491679,\n",
       "   1.2949100565910339,\n",
       "   1.2595636336008709,\n",
       "   1.269987655878067,\n",
       "   1.2618183382352193,\n",
       "   1.2777381642659504,\n",
       "   1.2410528592268626,\n",
       "   1.2526150846481323,\n",
       "   1.2614172347386678,\n",
       "   1.2606123677889507,\n",
       "   1.3302349623044332,\n",
       "   1.2610533618927002,\n",
       "   1.2724095900853476,\n",
       "   1.295203946431478,\n",
       "   1.269315949678421,\n",
       "   1.285705449183782,\n",
       "   1.2942701466878255,\n",
       "   1.2740562089284262,\n",
       "   1.260830402771632],\n",
       "  'val_loss_std': [0.07424384458888526,\n",
       "   0.06835804224278538,\n",
       "   0.06775583207850289,\n",
       "   0.06988229205647949,\n",
       "   0.06883469642803168,\n",
       "   0.06884323230760288,\n",
       "   0.07646532571822683,\n",
       "   0.07201893021100818,\n",
       "   0.07355930165456803,\n",
       "   0.07730945043188633,\n",
       "   0.07960689758935956,\n",
       "   0.0748873807968394,\n",
       "   0.08344647759820652,\n",
       "   0.0791326591774454,\n",
       "   0.0770248443038329,\n",
       "   0.08312024752994046,\n",
       "   0.08793633171573423,\n",
       "   0.08355785634389426,\n",
       "   0.08584380576338262,\n",
       "   0.08874933407118347,\n",
       "   0.0867912245274332,\n",
       "   0.0856484534986393,\n",
       "   0.08604014244220341,\n",
       "   0.08755220428556897,\n",
       "   0.08726223264826387,\n",
       "   0.09379908591130985,\n",
       "   0.08658302691124428,\n",
       "   0.09252672381795346,\n",
       "   0.09092122057370558,\n",
       "   0.08910920338083715,\n",
       "   0.08821147140058842,\n",
       "   0.09456174611326179,\n",
       "   0.0947904245343887,\n",
       "   0.09764414783693583,\n",
       "   0.09853700215625066,\n",
       "   0.0984067092539961,\n",
       "   0.09734556404726374,\n",
       "   0.09412542015249414,\n",
       "   0.09660231061366793,\n",
       "   0.09793585286737526,\n",
       "   0.0968453641611865,\n",
       "   0.09258759367615085,\n",
       "   0.09766655128326138,\n",
       "   0.10074349424438912,\n",
       "   0.09449414354379464,\n",
       "   0.09890814335587461,\n",
       "   0.09588992102883982,\n",
       "   0.09682256752582855,\n",
       "   0.09152425628085702,\n",
       "   0.09648949911310087,\n",
       "   0.09890793396996055,\n",
       "   0.10276163275947102,\n",
       "   0.09771921551260326,\n",
       "   0.1010010673829055,\n",
       "   0.10314463524051826,\n",
       "   0.10330846696873748,\n",
       "   0.11503925848155017,\n",
       "   0.10324028112526527,\n",
       "   0.10094647438904208,\n",
       "   0.10721517610760523,\n",
       "   0.10158076977393354,\n",
       "   0.10268580043202295,\n",
       "   0.10094735774757373,\n",
       "   0.10522482607904061,\n",
       "   0.1068909372543734,\n",
       "   0.1367771583635889,\n",
       "   0.1056114192630532,\n",
       "   0.1028648963041631,\n",
       "   0.10363724951954996,\n",
       "   0.10515714417807173,\n",
       "   0.1058821556511469,\n",
       "   0.1075381186214853,\n",
       "   0.10883096728434347,\n",
       "   0.10669562013496711,\n",
       "   0.1067840605507656,\n",
       "   0.10554932208004675,\n",
       "   0.10715830873921281,\n",
       "   0.10470113252107174,\n",
       "   0.1137268676815279,\n",
       "   0.10979794010894879,\n",
       "   0.10780208025782295,\n",
       "   0.10714496496328998,\n",
       "   0.10740539170540288,\n",
       "   0.10737207297590956,\n",
       "   0.10905344921559317,\n",
       "   0.10453739967198356,\n",
       "   0.1144132008049904,\n",
       "   0.10567122276174977,\n",
       "   0.11044014192972311,\n",
       "   0.10778727588974203,\n",
       "   0.11238773811575131,\n",
       "   0.11046186014644652,\n",
       "   0.10429718787767474,\n",
       "   0.10409057230730064,\n",
       "   0.10759995444382954,\n",
       "   0.10968162076234318,\n",
       "   0.11497353608115908,\n",
       "   0.1080621025872931,\n",
       "   0.11009722778653161,\n",
       "   0.1145262847904145,\n",
       "   0.11523675897343381,\n",
       "   0.11126656495932441,\n",
       "   0.11696086326163421,\n",
       "   0.11232778227211111,\n",
       "   0.10823517748805127,\n",
       "   0.12268695696703803,\n",
       "   0.11195397494724832,\n",
       "   0.11711413444122866,\n",
       "   0.11226878100100181,\n",
       "   0.10589732640708054,\n",
       "   0.11115524692152537,\n",
       "   0.1144858761074933,\n",
       "   0.11640737190950957,\n",
       "   0.11152725829593514,\n",
       "   0.10784804647529506,\n",
       "   0.11698875209613407,\n",
       "   0.10726291934500513,\n",
       "   0.10917948049881804,\n",
       "   0.10908516898063995,\n",
       "   0.11878331226362107,\n",
       "   0.12256556433165881,\n",
       "   0.10844376043949557,\n",
       "   0.11235275698483589,\n",
       "   0.11002581939253013,\n",
       "   0.11519807721059193,\n",
       "   0.10713541998707471,\n",
       "   0.11264107334171472,\n",
       "   0.1205571995691026,\n",
       "   0.11694772906413799,\n",
       "   0.12035051385778799,\n",
       "   0.11473359227858965,\n",
       "   0.12203290816521493,\n",
       "   0.11341837333298849,\n",
       "   0.11662916933405039,\n",
       "   0.11529278259219988,\n",
       "   0.11567739669851126,\n",
       "   0.11215625811660289,\n",
       "   0.11300835894432094,\n",
       "   0.11080013480124004,\n",
       "   0.11698105347456035,\n",
       "   0.12202784070806288,\n",
       "   0.11337305292602878,\n",
       "   0.11471106880540588,\n",
       "   0.11773891462227606,\n",
       "   0.11780801969874802,\n",
       "   0.11265338746800853,\n",
       "   0.10834037996164524,\n",
       "   0.11863945861300237,\n",
       "   0.11751235263133836],\n",
       "  'val_accuracy_mean': [0.2943777783711751,\n",
       "   0.3050888878107071,\n",
       "   0.3104888884226481,\n",
       "   0.3191333331664403,\n",
       "   0.3270666661858559,\n",
       "   0.3331333331267039,\n",
       "   0.34515555560588834,\n",
       "   0.3521777777870496,\n",
       "   0.36255555589993793,\n",
       "   0.3690666667620341,\n",
       "   0.3737555545568466,\n",
       "   0.38177777886390685,\n",
       "   0.3859333328406016,\n",
       "   0.3924000000953674,\n",
       "   0.39148888885974886,\n",
       "   0.3935777777433395,\n",
       "   0.4026666671037674,\n",
       "   0.40506666640440625,\n",
       "   0.4015111110607783,\n",
       "   0.4055555556217829,\n",
       "   0.4092444443702698,\n",
       "   0.41257777671019236,\n",
       "   0.42102222363154096,\n",
       "   0.4167777794599533,\n",
       "   0.4163555556535721,\n",
       "   0.4195555557807287,\n",
       "   0.4281111115217209,\n",
       "   0.4295333337783813,\n",
       "   0.4250666664044062,\n",
       "   0.4297111101945241,\n",
       "   0.4292444453636805,\n",
       "   0.43806666711966197,\n",
       "   0.43637777626514435,\n",
       "   0.4423555548985799,\n",
       "   0.43842222313086193,\n",
       "   0.4436444459358851,\n",
       "   0.44473333179950714,\n",
       "   0.4501777780056,\n",
       "   0.44273333191871644,\n",
       "   0.4442222207784653,\n",
       "   0.45115555385748546,\n",
       "   0.4508666664361954,\n",
       "   0.4523999981085459,\n",
       "   0.4505777766307195,\n",
       "   0.45462222178777056,\n",
       "   0.46262222071488696,\n",
       "   0.4602222214142481,\n",
       "   0.45793333391348523,\n",
       "   0.458222220937411,\n",
       "   0.46035555481910706,\n",
       "   0.45655555645624796,\n",
       "   0.4641999993721644,\n",
       "   0.46577777604262033,\n",
       "   0.4583777771393458,\n",
       "   0.4652444436152776,\n",
       "   0.4632666669289271,\n",
       "   0.4531111115217209,\n",
       "   0.4688666671514511,\n",
       "   0.4731555543343226,\n",
       "   0.4548444430033366,\n",
       "   0.4646666661898295,\n",
       "   0.4696888877948125,\n",
       "   0.46215555528799696,\n",
       "   0.4722666658957799,\n",
       "   0.4709333318471909,\n",
       "   0.44797777811686196,\n",
       "   0.47908888836701713,\n",
       "   0.4747111095984777,\n",
       "   0.4785999979575475,\n",
       "   0.47895555317401883,\n",
       "   0.4660444442431132,\n",
       "   0.46904444237550097,\n",
       "   0.47217777689297996,\n",
       "   0.47437777916590373,\n",
       "   0.47137777666250863,\n",
       "   0.4729555545250575,\n",
       "   0.4700444455941518,\n",
       "   0.4711111106475194,\n",
       "   0.4686444427569707,\n",
       "   0.47315555612246196,\n",
       "   0.48171110928058625,\n",
       "   0.48646666467189786,\n",
       "   0.469511110385259,\n",
       "   0.48162222067515054,\n",
       "   0.4809777756532033,\n",
       "   0.47779999792575834,\n",
       "   0.481622219880422,\n",
       "   0.4801111098130544,\n",
       "   0.4823999996980031,\n",
       "   0.47442222078641255,\n",
       "   0.4747111095984777,\n",
       "   0.48233333210150403,\n",
       "   0.47239999810854594,\n",
       "   0.46839999794960024,\n",
       "   0.4830444419384003,\n",
       "   0.4767555546760559,\n",
       "   0.48148888647556304,\n",
       "   0.4844888867934545,\n",
       "   0.47782222112019856,\n",
       "   0.47586666643619535,\n",
       "   0.4769777768850327,\n",
       "   0.47631111204624177,\n",
       "   0.46546666622161864,\n",
       "   0.4774000000953674,\n",
       "   0.4825555553038915,\n",
       "   0.47553333222866057,\n",
       "   0.4846222201983134,\n",
       "   0.47146666526794434,\n",
       "   0.46779999872048694,\n",
       "   0.47682222167650856,\n",
       "   0.4767777740955353,\n",
       "   0.4735333329439163,\n",
       "   0.4717111090819041,\n",
       "   0.4827999997138977,\n",
       "   0.4809555552403132,\n",
       "   0.47915555357933043,\n",
       "   0.4817777758836746,\n",
       "   0.4847555551926295,\n",
       "   0.47799999992052716,\n",
       "   0.4781777765353521,\n",
       "   0.4875333338975906,\n",
       "   0.4806222212314606,\n",
       "   0.47060000121593476,\n",
       "   0.47968888700008394,\n",
       "   0.47575555404027303,\n",
       "   0.4808444450298945,\n",
       "   0.47575555443763734,\n",
       "   0.4785333309570948,\n",
       "   0.47868888775507606,\n",
       "   0.4752222232023875,\n",
       "   0.4813555536667506,\n",
       "   0.46991110980510714,\n",
       "   0.48086666703224185,\n",
       "   0.4767333328723907,\n",
       "   0.48051110963026683,\n",
       "   0.4749111107985179,\n",
       "   0.49008888800938927,\n",
       "   0.4848222227891286,\n",
       "   0.47968888739744825,\n",
       "   0.4795111097892125,\n",
       "   0.46857777615388235,\n",
       "   0.4826222213109334,\n",
       "   0.47964444319407146,\n",
       "   0.4709555538495382,\n",
       "   0.48028888742129006,\n",
       "   0.4692666663726171,\n",
       "   0.46446666578451795,\n",
       "   0.4762888874610265,\n",
       "   0.4810222202539444],\n",
       "  'val_accuracy_std': [0.03662044942380474,\n",
       "   0.03753062303365826,\n",
       "   0.0387783578872275,\n",
       "   0.04134127234327977,\n",
       "   0.041624280694906396,\n",
       "   0.043179608161087936,\n",
       "   0.04500466829333788,\n",
       "   0.04526463204790026,\n",
       "   0.044542807840021154,\n",
       "   0.044538987714660715,\n",
       "   0.04443938272616408,\n",
       "   0.04396407167398601,\n",
       "   0.047229730344615084,\n",
       "   0.0459067000641782,\n",
       "   0.04401100146549515,\n",
       "   0.048243359724595224,\n",
       "   0.05059790613207527,\n",
       "   0.0502743888420472,\n",
       "   0.047646207788852465,\n",
       "   0.047671199546685764,\n",
       "   0.04997057885799903,\n",
       "   0.04878546759211305,\n",
       "   0.050379224033833464,\n",
       "   0.051180748883997026,\n",
       "   0.048566635260020724,\n",
       "   0.04882116569431856,\n",
       "   0.0492994111483049,\n",
       "   0.053683614425301736,\n",
       "   0.05210632822959299,\n",
       "   0.05112502713683523,\n",
       "   0.049091218118569036,\n",
       "   0.05031349141949356,\n",
       "   0.052206053666382306,\n",
       "   0.0535627649942139,\n",
       "   0.05307394812115725,\n",
       "   0.05442541105618357,\n",
       "   0.049368260829320505,\n",
       "   0.054374266896754335,\n",
       "   0.05488448244303921,\n",
       "   0.05220992609680563,\n",
       "   0.05197718232063355,\n",
       "   0.05132673840645814,\n",
       "   0.04864401169860649,\n",
       "   0.050696987789176175,\n",
       "   0.05121311458581916,\n",
       "   0.05198943803669029,\n",
       "   0.053431390332449674,\n",
       "   0.05089665568467173,\n",
       "   0.05319658103704718,\n",
       "   0.0498874977778261,\n",
       "   0.055476946189036445,\n",
       "   0.053546005582511914,\n",
       "   0.05310599587581486,\n",
       "   0.052941249595287856,\n",
       "   0.05202185540571389,\n",
       "   0.05391827683363514,\n",
       "   0.05166511363574728,\n",
       "   0.05281390195860774,\n",
       "   0.052721189062418614,\n",
       "   0.050491783015175644,\n",
       "   0.054973393624006245,\n",
       "   0.053239285241964425,\n",
       "   0.052565495352368985,\n",
       "   0.05425497963418789,\n",
       "   0.05512694966332531,\n",
       "   0.050048711339780255,\n",
       "   0.05576258662545266,\n",
       "   0.05412220156358316,\n",
       "   0.05692394880205957,\n",
       "   0.052911015391420464,\n",
       "   0.052338476962064784,\n",
       "   0.057507919037539906,\n",
       "   0.05446709646686687,\n",
       "   0.05555795972765246,\n",
       "   0.0544483251035562,\n",
       "   0.05400210610732004,\n",
       "   0.05419851885794916,\n",
       "   0.055865799116951935,\n",
       "   0.05353447578664762,\n",
       "   0.052655113830905126,\n",
       "   0.056391141046338994,\n",
       "   0.054838791952950786,\n",
       "   0.05392503117233965,\n",
       "   0.05546535221733162,\n",
       "   0.055451739878299516,\n",
       "   0.05513865208011013,\n",
       "   0.05528342570484687,\n",
       "   0.05477822748463571,\n",
       "   0.054554246109942986,\n",
       "   0.05381930799203643,\n",
       "   0.05608483791655714,\n",
       "   0.057081813347668514,\n",
       "   0.053811906766269325,\n",
       "   0.054763760700137915,\n",
       "   0.05273263828284555,\n",
       "   0.056290181601876946,\n",
       "   0.058625662115433644,\n",
       "   0.05674763412030171,\n",
       "   0.0576104234521015,\n",
       "   0.055338259990948435,\n",
       "   0.05650348509487152,\n",
       "   0.054966854241456885,\n",
       "   0.056642190634848474,\n",
       "   0.05699105685532466,\n",
       "   0.05686816166287631,\n",
       "   0.05841464749174847,\n",
       "   0.057049407858131757,\n",
       "   0.05601253677134788,\n",
       "   0.05619124782439973,\n",
       "   0.05379430923670489,\n",
       "   0.05551831927905493,\n",
       "   0.05558607436207362,\n",
       "   0.054148950644848884,\n",
       "   0.055890211144694064,\n",
       "   0.054027889095750764,\n",
       "   0.05585853239193331,\n",
       "   0.05637791202757209,\n",
       "   0.05619456589480332,\n",
       "   0.053651825775295967,\n",
       "   0.05644083948775624,\n",
       "   0.059321384960100296,\n",
       "   0.05482144225179657,\n",
       "   0.05109768324814736,\n",
       "   0.05375437317688631,\n",
       "   0.0574725632958566,\n",
       "   0.0545543369533246,\n",
       "   0.05610426391528855,\n",
       "   0.05703991806235806,\n",
       "   0.05675273921730895,\n",
       "   0.05735195693282902,\n",
       "   0.05625483835529674,\n",
       "   0.05811858518645318,\n",
       "   0.055944986541568105,\n",
       "   0.057727931988238536,\n",
       "   0.055987744905889085,\n",
       "   0.054579464162540225,\n",
       "   0.05523142758412529,\n",
       "   0.05739213607084864,\n",
       "   0.05381634806430354,\n",
       "   0.059193826323653005,\n",
       "   0.05215126943543011,\n",
       "   0.0538840816477232,\n",
       "   0.05509472790509868,\n",
       "   0.055103047998954435,\n",
       "   0.05701713267033052,\n",
       "   0.055942919433219884,\n",
       "   0.05412992583266168,\n",
       "   0.05594380011133529,\n",
       "   0.05656061407487619],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fed56fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.5149999976158142,\n",
       " 'best_val_iter': 63500,\n",
       " 'current_iter': 75000,\n",
       " 'best_epoch': 127,\n",
       " 'train_loss_mean': 0.6405333166122437,\n",
       " 'train_loss_std': 0.12306425183415713,\n",
       " 'train_accuracy_mean': 0.7643866658210754,\n",
       " 'train_accuracy_std': 0.05608368177868576,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 1.256844596862793,\n",
       " 'val_loss_std': 0.13286672963334395,\n",
       " 'val_accuracy_mean': 0.5063555530707041,\n",
       " 'val_accuracy_std': 0.05552089533946327,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0105, -0.0684,  0.0652],\n",
       "                         [-0.0485, -0.0316,  0.0005],\n",
       "                         [-0.0371,  0.1022, -0.0269]],\n",
       "               \n",
       "                        [[ 0.0615, -0.0651,  0.0822],\n",
       "                         [-0.0323,  0.0213,  0.0604],\n",
       "                         [-0.0649,  0.0368, -0.0340]],\n",
       "               \n",
       "                        [[ 0.0525, -0.0161, -0.0195],\n",
       "                         [ 0.0270,  0.0670, -0.0560],\n",
       "                         [-0.0574,  0.0319, -0.0524]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0290,  0.0996,  0.0344],\n",
       "                         [ 0.0584, -0.0688,  0.0141],\n",
       "                         [-0.0823, -0.0518, -0.0422]],\n",
       "               \n",
       "                        [[-0.0396,  0.0182, -0.0197],\n",
       "                         [ 0.0557, -0.0827, -0.0516],\n",
       "                         [ 0.0689, -0.0345,  0.0862]],\n",
       "               \n",
       "                        [[-0.0699,  0.0628, -0.0927],\n",
       "                         [-0.0117,  0.0469,  0.0749],\n",
       "                         [-0.0083,  0.0004, -0.0143]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0094, -0.0168,  0.0586],\n",
       "                         [-0.0197, -0.0514, -0.0212],\n",
       "                         [ 0.0713,  0.0571, -0.0604]],\n",
       "               \n",
       "                        [[-0.0671, -0.0197,  0.0457],\n",
       "                         [ 0.0829, -0.0059,  0.0288],\n",
       "                         [-0.0680, -0.0260,  0.0492]],\n",
       "               \n",
       "                        [[ 0.0439, -0.0615,  0.0163],\n",
       "                         [ 0.0749,  0.0065, -0.0700],\n",
       "                         [-0.0655,  0.0384,  0.0070]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0367, -0.1029, -0.0440],\n",
       "                         [-0.0006, -0.0696,  0.0624],\n",
       "                         [-0.0119,  0.0783, -0.0406]],\n",
       "               \n",
       "                        [[ 0.0455, -0.0027,  0.0747],\n",
       "                         [ 0.0372, -0.0238, -0.0340],\n",
       "                         [ 0.0058,  0.0605, -0.0103]],\n",
       "               \n",
       "                        [[ 0.0072, -0.1026, -0.0376],\n",
       "                         [-0.0396,  0.0148,  0.0341],\n",
       "                         [-0.0292,  0.0395,  0.0385]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0375, -0.0132, -0.0385],\n",
       "                         [-0.0079,  0.0661, -0.0647],\n",
       "                         [ 0.0476,  0.0831, -0.0548]],\n",
       "               \n",
       "                        [[ 0.0471,  0.0235,  0.0720],\n",
       "                         [ 0.0186, -0.0331, -0.0299],\n",
       "                         [-0.0299, -0.0914,  0.0284]],\n",
       "               \n",
       "                        [[-0.0106, -0.0070, -0.0208],\n",
       "                         [ 0.0004, -0.0231,  0.0888],\n",
       "                         [-0.0505, -0.0241,  0.0176]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0619,  0.0165, -0.0487],\n",
       "                         [-0.0695,  0.0166,  0.0328],\n",
       "                         [ 0.0657,  0.0668,  0.0959]],\n",
       "               \n",
       "                        [[-0.0466, -0.0908,  0.0289],\n",
       "                         [ 0.0388, -0.0767, -0.0365],\n",
       "                         [-0.0149,  0.0150,  0.0258]],\n",
       "               \n",
       "                        [[ 0.0394, -0.0284,  0.0692],\n",
       "                         [ 0.0097,  0.0035,  0.0081],\n",
       "                         [-0.0350, -0.0359,  0.0080]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-3.6578e-04, -8.4689e-05, -7.0194e-04, -2.6497e-04, -5.4432e-05,\n",
       "                       -8.4954e-04,  2.9737e-05,  2.9809e-04,  9.9362e-04, -2.3285e-04,\n",
       "                        1.3230e-05,  3.0850e-04, -3.0058e-04,  1.5334e-04, -3.2856e-04,\n",
       "                       -7.9059e-05, -1.1051e-04,  3.4707e-04, -3.4265e-04,  2.2904e-05,\n",
       "                       -1.4584e-04, -8.8309e-06, -1.1731e-05,  1.9204e-04, -1.7849e-04,\n",
       "                       -2.6008e-04,  7.2150e-05,  5.1076e-05,  3.8917e-04, -1.6296e-04,\n",
       "                       -2.0220e-04,  1.4826e-04, -3.6382e-04, -2.9244e-03,  1.4504e-04,\n",
       "                       -1.3087e-04, -3.9849e-04,  5.4777e-05, -4.1107e-05,  1.4138e-04,\n",
       "                        1.7108e-04,  1.6799e-04,  6.4071e-04,  1.0781e-04, -1.0189e-04,\n",
       "                       -2.2725e-05, -1.8230e-04,  5.4204e-05,  3.1923e-04, -1.4061e-04,\n",
       "                        1.9522e-04, -1.3511e-04, -4.2536e-04, -8.1485e-05, -7.4279e-05,\n",
       "                        1.9474e-05, -1.3778e-04,  1.6728e-04,  2.8570e-04,  5.2962e-05,\n",
       "                       -1.7066e-04,  1.4593e-04, -3.4372e-04, -8.1523e-05,  1.0897e-04,\n",
       "                       -3.0040e-04, -4.0001e-05,  6.4929e-05, -2.8761e-05,  2.4954e-04,\n",
       "                        2.4948e-04, -5.1815e-04,  1.0172e-04,  3.9876e-05, -7.3716e-05,\n",
       "                        9.4737e-06,  7.3067e-05, -2.7148e-05,  1.5110e-04, -1.9265e-05,\n",
       "                        1.1353e-04, -9.2343e-05, -3.3340e-05, -2.0958e-04, -6.6891e-04,\n",
       "                        2.7591e-04, -5.9689e-05, -2.0456e-04,  3.9899e-04,  1.8189e-04,\n",
       "                        1.1379e-04,  3.1651e-04, -7.3980e-04, -4.8143e-04, -1.2706e-04,\n",
       "                       -1.0240e-04,  6.8675e-04,  2.0329e-04,  2.5903e-06, -5.5816e-05,\n",
       "                        2.1160e-04,  1.6916e-04, -7.9881e-04, -3.6286e-06,  5.1189e-05,\n",
       "                        4.9687e-04, -4.4200e-04, -1.2895e-04, -5.0412e-04,  1.9263e-04,\n",
       "                       -1.5057e-04,  2.3695e-04, -5.9561e-04, -6.8408e-04,  1.5110e-04,\n",
       "                       -1.2268e-05, -5.7303e-05,  2.8140e-04,  3.7393e-04,  3.3609e-04,\n",
       "                       -4.2717e-04,  9.9765e-05,  1.4345e-03,  1.9358e-04, -2.8868e-04,\n",
       "                       -1.7529e-04, -3.5312e-04,  2.4876e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1088,  0.1726,  0.1213, -0.1801, -0.0513, -0.0155, -0.3258, -0.1791,\n",
       "                        0.0659, -0.0907, -0.2274, -0.2324,  0.2101, -0.1395, -0.0203, -0.1313,\n",
       "                       -0.2876,  0.2169,  0.4909, -0.2187, -0.1248, -0.1550, -0.1685, -0.0572,\n",
       "                        0.0925, -0.3210, -0.0883,  0.0712,  0.1381,  0.2204, -0.1145, -0.0525,\n",
       "                       -0.0657, -0.2666, -0.0716, -0.1754, -0.0808, -0.0399, -0.2518,  0.0922,\n",
       "                       -0.3334, -0.1127, -0.1165, -0.1194,  0.2702, -0.1949, -0.1732, -0.2644,\n",
       "                       -0.1154, -0.2319,  0.2114, -0.3008,  0.4595, -0.1140, -0.2108, -0.1050,\n",
       "                       -0.0662,  0.1798, -0.0488,  0.2525, -0.0866,  0.6349,  0.0474, -0.1226,\n",
       "                       -0.0397, -0.1017, -0.1290, -0.1822, -0.0699,  0.0163,  0.0352, -0.0735,\n",
       "                       -0.2739, -0.1997,  0.1441, -0.1257,  0.0784, -0.0303, -0.0979, -0.1413,\n",
       "                        0.1731, -0.2784, -0.1830, -0.0339, -0.0259,  0.1383,  0.2102, -0.1015,\n",
       "                       -0.2120,  0.1918,  0.1087, -0.2293,  0.1626, -0.0870,  0.0534, -0.0646,\n",
       "                       -0.0178,  0.1453,  0.0835, -0.0525,  0.0321,  0.0783, -0.0293, -0.0942,\n",
       "                       -0.2755, -0.1818, -0.0998, -0.0012, -0.0765,  0.0085,  0.0706,  0.6802,\n",
       "                        0.0011, -0.1024, -0.2737, -0.0814, -0.1038,  0.1749, -0.0684,  0.1238,\n",
       "                       -0.0175, -0.1502, -0.1796, -0.0118,  0.4828, -0.0597, -0.1972, -0.2150],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.9741, 1.1432, 1.0315, 0.8291, 0.9516, 1.0112, 0.8298, 0.8597, 1.0649,\n",
       "                       0.9161, 0.9361, 0.8571, 1.0315, 0.9022, 1.0340, 0.9010, 0.9866, 1.0053,\n",
       "                       1.0406, 0.8547, 0.8442, 0.8437, 0.8324, 0.9658, 0.8737, 0.8111, 1.0590,\n",
       "                       0.9751, 0.9983, 0.9473, 1.0231, 0.8374, 1.1141, 1.2846, 0.8240, 0.9473,\n",
       "                       0.9849, 0.9929, 0.9014, 0.9766, 1.0003, 1.0957, 1.1280, 0.9877, 1.0874,\n",
       "                       0.8905, 0.8827, 0.8272, 1.0341, 0.8982, 0.9870, 0.8469, 1.0698, 0.9610,\n",
       "                       0.9959, 0.8857, 0.8674, 1.0904, 0.9460, 1.1021, 0.8845, 1.1908, 0.9727,\n",
       "                       0.9414, 0.9462, 0.8767, 0.8951, 0.8767, 1.0870, 0.9889, 0.9464, 1.0260,\n",
       "                       0.8112, 0.7924, 1.0159, 0.8706, 0.9318, 1.0553, 1.0830, 0.9906, 0.9703,\n",
       "                       0.9233, 0.8721, 0.9640, 1.1094, 1.1608, 0.9586, 1.0064, 0.9995, 0.8888,\n",
       "                       1.0602, 0.9145, 0.9555, 0.9612, 1.0192, 0.9691, 1.2006, 0.9664, 1.0581,\n",
       "                       0.8764, 0.9456, 1.0818, 1.0078, 0.9874, 0.9165, 0.9524, 0.9025, 0.9026,\n",
       "                       1.0472, 0.9455, 0.9862, 1.0481, 1.0101, 1.2084, 0.9555, 1.1446, 0.8704,\n",
       "                       1.0412, 0.9690, 1.0146, 0.8217, 0.8855, 1.1970, 1.1505, 1.0579, 0.9338,\n",
       "                       0.9593, 0.8604], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-4.1525e-02,  4.1166e-02, -1.0693e-02],\n",
       "                         [-3.4877e-02,  1.9022e-02,  4.4917e-03],\n",
       "                         [ 5.0756e-03,  3.9588e-02,  1.1603e-02]],\n",
       "               \n",
       "                        [[-1.1555e-02,  7.4816e-03, -1.0388e-01],\n",
       "                         [-8.5116e-02, -4.9209e-02, -6.2967e-02],\n",
       "                         [-8.3701e-02,  2.5713e-02, -3.7000e-02]],\n",
       "               \n",
       "                        [[-1.6934e-02, -3.2976e-02, -5.8814e-02],\n",
       "                         [-7.5569e-02, -1.1270e-02,  1.8583e-02],\n",
       "                         [ 5.5054e-02,  1.1552e-02, -5.5780e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3889e-02, -2.6815e-02, -2.1799e-02],\n",
       "                         [-5.5494e-02, -5.6621e-02, -1.2128e-02],\n",
       "                         [-3.0467e-02, -2.7448e-02, -3.8001e-05]],\n",
       "               \n",
       "                        [[ 5.8029e-02,  3.8783e-02, -1.7601e-03],\n",
       "                         [ 1.3730e-03, -6.3323e-03, -3.5290e-02],\n",
       "                         [ 7.6003e-02, -5.2702e-02,  1.8434e-02]],\n",
       "               \n",
       "                        [[-1.2185e-02,  8.2702e-02,  2.6252e-03],\n",
       "                         [ 9.0773e-03,  5.3265e-02,  2.5995e-02],\n",
       "                         [-1.1065e-02, -8.4747e-03,  5.2676e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.2045e-02, -6.6736e-02, -3.4031e-03],\n",
       "                         [ 4.1186e-02,  1.0996e-02, -2.4461e-02],\n",
       "                         [-1.8835e-02,  2.4474e-02,  7.7535e-03]],\n",
       "               \n",
       "                        [[-7.0804e-02,  1.9296e-02,  1.1802e-03],\n",
       "                         [-5.0786e-02,  1.9412e-02,  1.0250e-02],\n",
       "                         [-3.8149e-02, -3.1931e-02,  1.4512e-02]],\n",
       "               \n",
       "                        [[ 1.0785e-02,  1.7375e-03, -1.6972e-02],\n",
       "                         [-5.6058e-02,  1.8378e-02,  4.5187e-02],\n",
       "                         [-8.0434e-02, -1.0585e-02, -8.9635e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.2890e-02, -1.4853e-02,  2.4659e-02],\n",
       "                         [-3.5740e-02, -6.6334e-02, -6.8372e-03],\n",
       "                         [ 8.8266e-03,  1.7827e-02, -4.1943e-03]],\n",
       "               \n",
       "                        [[ 1.4818e-02, -1.5893e-02, -4.5808e-02],\n",
       "                         [ 5.4971e-02,  5.8339e-02,  1.9521e-03],\n",
       "                         [ 4.1070e-02,  2.3835e-02,  5.6787e-02]],\n",
       "               \n",
       "                        [[ 1.4745e-02, -5.7833e-02, -5.0993e-02],\n",
       "                         [ 2.5137e-02, -1.3691e-01, -4.8597e-02],\n",
       "                         [-7.1996e-02, -1.1972e-01, -3.9008e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.1257e-02,  3.9006e-03,  7.5891e-03],\n",
       "                         [ 7.8121e-04,  2.2968e-02, -6.1765e-02],\n",
       "                         [ 2.1461e-02,  1.6906e-02, -2.7438e-02]],\n",
       "               \n",
       "                        [[-1.5196e-02,  1.9470e-02, -4.5307e-02],\n",
       "                         [-5.3984e-04,  2.9238e-02, -1.9715e-03],\n",
       "                         [ 6.8928e-02,  9.7974e-03, -3.3205e-02]],\n",
       "               \n",
       "                        [[-3.2909e-02,  1.2674e-03, -5.5041e-02],\n",
       "                         [ 3.5558e-03, -1.0995e-02, -1.0252e-02],\n",
       "                         [ 6.3011e-03,  3.7037e-02,  5.4731e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.9455e-02,  5.7198e-02,  1.4949e-02],\n",
       "                         [ 7.3941e-03,  7.7611e-03,  1.4937e-03],\n",
       "                         [-9.7294e-03, -1.6017e-02, -2.0377e-02]],\n",
       "               \n",
       "                        [[ 6.2933e-03,  2.4126e-02,  4.3117e-02],\n",
       "                         [-5.5017e-02,  8.8736e-03, -1.7663e-02],\n",
       "                         [-1.4035e-02,  2.6913e-02, -1.8558e-02]],\n",
       "               \n",
       "                        [[-3.3197e-02, -2.9434e-02, -1.1912e-02],\n",
       "                         [-1.2998e-02,  2.8388e-02,  1.5388e-02],\n",
       "                         [ 3.7348e-02,  2.1048e-02,  8.3255e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.3634e-02,  1.6563e-02,  2.8142e-02],\n",
       "                         [ 2.5676e-02,  4.6806e-02,  2.9816e-02],\n",
       "                         [ 2.5337e-02, -2.0249e-02, -3.6372e-02]],\n",
       "               \n",
       "                        [[-2.0181e-02,  3.3070e-02,  5.3515e-02],\n",
       "                         [-7.1385e-02, -4.4268e-02, -2.3444e-02],\n",
       "                         [-3.3393e-02,  2.2075e-02, -1.1345e-02]],\n",
       "               \n",
       "                        [[ 4.8358e-02,  4.5835e-02,  6.7570e-02],\n",
       "                         [ 4.6716e-02,  6.5286e-03, -1.0667e-04],\n",
       "                         [-7.9515e-03, -4.6707e-03, -5.8623e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.8749e-03,  2.6921e-03,  2.9589e-02],\n",
       "                         [ 3.7259e-02, -3.0988e-02,  3.0639e-02],\n",
       "                         [-9.1648e-03, -1.9291e-02, -3.6211e-02]],\n",
       "               \n",
       "                        [[-2.7425e-02,  9.5722e-03,  8.7058e-03],\n",
       "                         [ 3.2233e-02,  2.6193e-02, -4.9082e-03],\n",
       "                         [-4.8171e-02,  3.7515e-02,  3.2829e-03]],\n",
       "               \n",
       "                        [[ 4.6909e-03, -4.1694e-02, -2.0260e-02],\n",
       "                         [ 8.6177e-03, -1.6899e-02,  7.3810e-03],\n",
       "                         [ 1.6468e-02,  3.2005e-02, -3.0057e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.9973e-02, -6.6497e-02,  3.3509e-02],\n",
       "                         [-2.6569e-02, -2.4488e-02,  4.6050e-02],\n",
       "                         [-2.3440e-02,  3.7759e-02, -1.3588e-02]],\n",
       "               \n",
       "                        [[ 7.5313e-02, -1.2594e-02,  4.6871e-02],\n",
       "                         [ 4.3842e-02,  2.7973e-02,  1.2730e-02],\n",
       "                         [ 2.4933e-02,  3.3269e-03,  7.7642e-02]],\n",
       "               \n",
       "                        [[-8.4563e-02,  6.7455e-02,  2.7613e-02],\n",
       "                         [-4.3820e-02, -9.2271e-04, -1.5282e-02],\n",
       "                         [-5.1621e-02,  1.9079e-02,  2.8615e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.0945e-03,  1.6221e-02,  7.3500e-02],\n",
       "                         [ 5.2099e-03,  2.6572e-02, -1.6716e-02],\n",
       "                         [-2.0950e-02, -2.4182e-02, -3.0895e-02]],\n",
       "               \n",
       "                        [[ 1.2917e-02,  2.8984e-02,  6.5342e-02],\n",
       "                         [-2.2244e-02, -1.4564e-02,  4.0817e-03],\n",
       "                         [-3.8239e-02, -9.7520e-03, -2.7790e-03]],\n",
       "               \n",
       "                        [[-4.9281e-02, -3.7684e-02, -3.7882e-02],\n",
       "                         [-6.0552e-02,  1.4015e-02, -8.6662e-03],\n",
       "                         [-2.7400e-02, -6.1758e-03,  6.2735e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.2594e-02, -4.9621e-02, -4.3396e-02],\n",
       "                         [ 6.2645e-02,  3.0074e-02, -2.9720e-02],\n",
       "                         [-2.9302e-02, -1.3148e-02, -3.8390e-02]],\n",
       "               \n",
       "                        [[-8.4238e-02,  1.3998e-02,  6.1122e-02],\n",
       "                         [ 4.7771e-02, -8.8073e-02, -6.2814e-02],\n",
       "                         [ 2.6058e-02, -7.5648e-02,  3.6544e-02]],\n",
       "               \n",
       "                        [[-2.7240e-02, -1.8860e-02,  5.3791e-02],\n",
       "                         [ 5.9964e-02,  3.3262e-02,  1.4743e-02],\n",
       "                         [ 5.0049e-02,  1.0566e-02,  2.4588e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.8548e-02,  5.4885e-03, -1.3560e-02],\n",
       "                         [ 4.2244e-02,  6.7388e-02, -1.8454e-02],\n",
       "                         [ 1.9817e-03, -7.5479e-03, -3.4716e-02]],\n",
       "               \n",
       "                        [[ 9.1183e-03, -5.5058e-02, -9.5625e-03],\n",
       "                         [ 8.5099e-02,  8.0008e-03, -1.4302e-03],\n",
       "                         [ 1.6680e-02, -7.0183e-03, -2.7136e-02]],\n",
       "               \n",
       "                        [[-1.3791e-02,  4.0915e-03, -1.5110e-02],\n",
       "                         [-1.0567e-02,  1.5128e-02,  4.3607e-02],\n",
       "                         [ 2.2022e-02, -2.1536e-02, -9.2822e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-2.9009e-04, -4.5466e-04, -3.4548e-04,  8.9681e-04, -3.9206e-04,\n",
       "                       -3.0242e-04, -9.9697e-04,  7.7944e-05,  3.7588e-04, -2.9496e-04,\n",
       "                       -3.7817e-04,  1.4482e-03,  9.3069e-04,  1.1873e-04, -3.8388e-04,\n",
       "                       -4.8079e-04,  2.8984e-04,  4.5521e-04, -6.2667e-05,  2.6725e-04,\n",
       "                       -5.4050e-04,  3.1782e-04,  4.0995e-04,  5.4079e-05,  1.2712e-03,\n",
       "                       -9.6077e-04,  2.0984e-04,  1.2221e-03,  8.5994e-04,  1.6901e-06,\n",
       "                       -7.3070e-04, -1.9852e-04, -1.8624e-04,  7.0018e-05,  7.9608e-04,\n",
       "                       -4.7300e-05, -7.4029e-05, -3.9657e-04, -5.7166e-04, -3.0145e-04,\n",
       "                        5.0392e-04,  3.2852e-04,  3.0715e-04,  3.6637e-04, -3.9448e-04,\n",
       "                        1.5697e-04,  3.4412e-04, -4.7126e-04, -9.9045e-04,  3.4102e-04,\n",
       "                       -2.9547e-04, -2.9699e-04, -4.9055e-04, -5.3419e-04,  9.0009e-04,\n",
       "                        1.1139e-04, -5.7257e-04, -5.6515e-04, -5.8524e-04, -4.2254e-04,\n",
       "                        9.2052e-04,  4.3737e-04,  3.0702e-04, -1.0713e-04, -1.5003e-04,\n",
       "                       -2.6941e-04, -7.0944e-05,  3.9045e-04,  2.7636e-05,  1.8226e-04,\n",
       "                       -4.4929e-04, -3.1214e-04, -5.9250e-04, -5.2528e-04, -2.5155e-04,\n",
       "                       -2.0019e-04, -4.1514e-04,  8.5445e-04, -8.8580e-04, -4.8283e-05,\n",
       "                        3.0398e-04,  4.2589e-04,  4.9043e-05,  2.5340e-04, -2.5293e-04,\n",
       "                        6.8543e-04, -5.3208e-04,  1.9240e-04, -7.2526e-06, -4.1935e-04,\n",
       "                        1.2796e-03, -1.2670e-04,  8.4438e-04,  2.5664e-04,  6.0156e-04,\n",
       "                        2.0592e-05, -1.0084e-04, -1.1482e-04,  3.9643e-04, -3.8495e-04,\n",
       "                       -2.6925e-04, -3.3503e-04,  2.3616e-04,  1.6464e-04, -1.5878e-04,\n",
       "                       -1.5580e-04, -1.8493e-04,  2.6248e-04,  6.0062e-04,  2.1943e-04,\n",
       "                       -3.8322e-05, -4.2878e-04, -3.6072e-04,  1.1506e-03, -4.7420e-04,\n",
       "                       -2.2587e-04, -2.1576e-06,  8.7128e-04, -1.8751e-05,  3.2934e-04,\n",
       "                       -8.5763e-04, -5.6781e-04, -4.3990e-04, -4.2181e-04, -2.7628e-04,\n",
       "                       -1.9164e-05,  6.9943e-04,  1.7943e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.1022, -0.1785, -0.3513, -0.2506, -0.1741, -0.1964, -0.2183, -0.1570,\n",
       "                       -0.2078, -0.1973, -0.1527, -0.2030, -0.1540, -0.2324,  0.0046, -0.1824,\n",
       "                       -0.1568, -0.1521, -0.1558, -0.2072, -0.2798, -0.2185, -0.1476, -0.1510,\n",
       "                       -0.0126, -0.0779, -0.2107, -0.1092, -0.2299, -0.1939, -0.2182, -0.2796,\n",
       "                       -0.1888, -0.1828, -0.2506, -0.1984, -0.0921, -0.0367, -0.0969, -0.1397,\n",
       "                       -0.1789, -0.0231, -0.2125, -0.1581, -0.1653, -0.2453, -0.2259, -0.1269,\n",
       "                       -0.0787, -0.1989, -0.2590, -0.2159, -0.3333, -0.1966, -0.1867, -0.2062,\n",
       "                       -0.1221, -0.0829, -0.2210, -0.1728, -0.1169, -0.2078, -0.1710, -0.0498,\n",
       "                       -0.0549, -0.1999, -0.0693, -0.2513, -0.2138, -0.2103, -0.2760, -0.1517,\n",
       "                       -0.1920, -0.1401, -0.1267, -0.1410, -0.2269, -0.2453, -0.1211, -0.1913,\n",
       "                       -0.1953, -0.1777, -0.1115, -0.0465, -0.1448, -0.3706, -0.0795, -0.1710,\n",
       "                       -0.0158, -0.1368, -0.3234, -0.1846, -0.0821, -0.0593, -0.2772, -0.1327,\n",
       "                       -0.1294, -0.1410, -0.3468, -0.2061, -0.1557, -0.0592, -0.1523, -0.2112,\n",
       "                       -0.1395, -0.1179, -0.1769, -0.2272, -0.2866, -0.2318, -0.1317, -0.2336,\n",
       "                       -0.1976, -0.2465, -0.2204, -0.1221, -0.2056, -0.2526, -0.1664, -0.2371,\n",
       "                       -0.1816, -0.0737, -0.1699, -0.0514, -0.1977, -0.3105, -0.2011, -0.1978],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.9462, 1.0067, 0.9432, 0.9207, 0.9356, 1.0100, 0.9532, 0.9714, 1.0234,\n",
       "                       1.0465, 0.9958, 1.0957, 0.9737, 1.0250, 1.0139, 0.9738, 0.8953, 0.9031,\n",
       "                       0.9707, 0.9401, 0.9924, 0.9529, 0.9877, 1.0441, 1.0661, 0.9480, 0.9546,\n",
       "                       0.9669, 0.9323, 0.9489, 0.9700, 1.0268, 0.9409, 1.0463, 0.9350, 1.0067,\n",
       "                       0.9710, 1.0096, 0.9928, 1.0057, 0.9081, 0.9968, 0.9535, 0.9370, 0.8815,\n",
       "                       1.0460, 0.9269, 0.9579, 1.1018, 0.9822, 1.0838, 0.8949, 0.9563, 1.0604,\n",
       "                       1.1557, 1.0069, 0.9455, 1.0209, 0.9833, 1.0016, 0.9830, 1.0134, 0.9787,\n",
       "                       1.0072, 0.9260, 0.9990, 1.0089, 0.9913, 0.8839, 0.9762, 0.9305, 0.9917,\n",
       "                       0.9837, 0.9763, 0.9880, 1.0605, 0.9786, 1.0327, 1.0576, 0.9906, 1.0421,\n",
       "                       0.8917, 1.0226, 0.9326, 0.9720, 1.0283, 0.9515, 0.9298, 1.0237, 1.0104,\n",
       "                       1.1967, 0.9726, 1.0188, 0.9387, 1.1383, 1.0015, 1.0035, 1.0735, 1.1163,\n",
       "                       0.9831, 1.0139, 0.9892, 0.9899, 1.0287, 0.9525, 0.9825, 1.0110, 1.0455,\n",
       "                       1.0537, 0.9488, 0.9929, 1.1242, 1.0799, 1.1307, 0.9058, 1.0292, 1.0454,\n",
       "                       1.0345, 0.9014, 1.0019, 0.9799, 1.0323, 0.9835, 0.9886, 0.9376, 0.9704,\n",
       "                       0.9915, 1.0195], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[ 7.6996e-03,  7.1835e-02,  2.3481e-02],\n",
       "                         [-6.7841e-02, -3.1117e-02, -8.8042e-03],\n",
       "                         [-1.6322e-02, -5.7288e-02,  2.2959e-02]],\n",
       "               \n",
       "                        [[-2.5072e-02, -8.4172e-02, -8.9290e-02],\n",
       "                         [-1.8313e-02,  2.9599e-02, -4.2791e-02],\n",
       "                         [-3.8004e-02, -2.0054e-02, -1.1942e-01]],\n",
       "               \n",
       "                        [[-6.1816e-02, -6.6988e-02, -2.3313e-02],\n",
       "                         [ 2.9269e-02,  5.0838e-04,  2.6230e-03],\n",
       "                         [-2.4870e-02,  3.8905e-03, -1.2983e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.6320e-02, -1.3210e-02,  5.0256e-03],\n",
       "                         [ 7.7381e-03,  2.3704e-02,  2.1591e-02],\n",
       "                         [-3.0638e-02, -5.4461e-03, -1.4204e-02]],\n",
       "               \n",
       "                        [[ 2.3976e-02, -3.4457e-02, -6.6776e-03],\n",
       "                         [ 2.7034e-02, -2.9805e-03,  1.2470e-03],\n",
       "                         [ 7.4687e-02,  6.3236e-02,  5.7873e-02]],\n",
       "               \n",
       "                        [[-3.6194e-02, -3.1387e-02,  6.2413e-02],\n",
       "                         [-1.7122e-02, -2.4275e-02, -4.6540e-02],\n",
       "                         [-4.6181e-02, -2.6005e-02,  5.2130e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.5011e-03,  4.5815e-03,  3.5608e-03],\n",
       "                         [ 1.6133e-02,  1.4983e-02, -3.1952e-02],\n",
       "                         [-3.7108e-02, -8.8508e-02, -3.6657e-02]],\n",
       "               \n",
       "                        [[ 1.5969e-02,  7.0582e-02,  1.2294e-02],\n",
       "                         [ 5.5731e-02,  4.8719e-02, -7.8540e-03],\n",
       "                         [-4.7073e-02, -2.6611e-03, -1.8572e-02]],\n",
       "               \n",
       "                        [[-4.1146e-02,  7.2141e-02, -3.0625e-03],\n",
       "                         [ 2.8683e-02,  1.6197e-02, -7.7520e-04],\n",
       "                         [ 6.4317e-02,  2.8805e-02,  2.9137e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.0993e-02,  5.9707e-02,  2.1861e-02],\n",
       "                         [-3.2668e-02, -5.7557e-02, -5.1898e-02],\n",
       "                         [-3.2223e-02, -3.6796e-03, -5.5295e-02]],\n",
       "               \n",
       "                        [[-3.7859e-03, -3.2716e-02, -1.3597e-02],\n",
       "                         [-3.3479e-02, -1.4671e-02, -2.7339e-02],\n",
       "                         [-8.9820e-03,  1.8855e-03,  1.2970e-02]],\n",
       "               \n",
       "                        [[ 7.7822e-02, -3.9491e-02, -4.1699e-03],\n",
       "                         [ 2.8286e-02,  5.7590e-02,  2.7581e-02],\n",
       "                         [ 1.7958e-02,  3.1244e-02, -6.8023e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.0459e-01,  3.9085e-02, -1.2303e-02],\n",
       "                         [ 2.6515e-02,  1.2675e-02,  7.2287e-02],\n",
       "                         [ 2.1898e-02,  8.3891e-03,  3.8511e-02]],\n",
       "               \n",
       "                        [[ 3.7160e-02,  6.5921e-02,  5.4115e-02],\n",
       "                         [ 1.6192e-02,  8.5110e-02,  4.6616e-02],\n",
       "                         [ 2.6059e-02,  7.9846e-02,  7.2477e-02]],\n",
       "               \n",
       "                        [[ 2.8662e-02, -3.1668e-02,  4.9614e-02],\n",
       "                         [-1.8962e-02,  3.8518e-02,  3.0415e-03],\n",
       "                         [-1.8247e-02,  1.4057e-02,  7.9016e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.8632e-02,  4.0916e-02, -1.1382e-02],\n",
       "                         [-1.5701e-02,  1.1905e-02, -5.6738e-03],\n",
       "                         [-5.1859e-02, -4.7849e-03,  4.7186e-02]],\n",
       "               \n",
       "                        [[-6.4189e-03, -2.4051e-02, -3.4358e-03],\n",
       "                         [ 5.3038e-02,  1.8107e-02,  1.4282e-02],\n",
       "                         [ 6.6071e-03,  2.2891e-03,  8.1593e-02]],\n",
       "               \n",
       "                        [[-7.6018e-03, -1.1851e-02, -1.3761e-02],\n",
       "                         [ 2.1884e-02,  5.0324e-02,  1.1908e-03],\n",
       "                         [-4.1738e-02, -2.4864e-02, -5.4845e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.6451e-02,  5.6146e-02, -1.3779e-03],\n",
       "                         [ 5.1724e-02,  1.4604e-03, -1.2797e-02],\n",
       "                         [ 6.1130e-03,  7.3391e-03,  2.9663e-02]],\n",
       "               \n",
       "                        [[ 2.0743e-02, -5.6367e-02, -1.2641e-02],\n",
       "                         [ 2.9373e-02,  2.0781e-02, -6.3512e-02],\n",
       "                         [ 7.5441e-03,  1.0455e-02,  1.3994e-02]],\n",
       "               \n",
       "                        [[ 2.6420e-03,  1.5788e-02, -3.8186e-02],\n",
       "                         [-9.0960e-02, -8.1472e-02,  1.3395e-02],\n",
       "                         [ 2.0103e-02, -1.2906e-02,  4.9593e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.6286e-02,  5.6542e-02,  7.2167e-03],\n",
       "                         [ 5.6086e-02,  5.8357e-02,  1.9538e-02],\n",
       "                         [-3.7025e-02,  2.5907e-03,  3.3186e-02]],\n",
       "               \n",
       "                        [[ 3.5130e-03, -2.0630e-02,  4.2730e-02],\n",
       "                         [-3.5803e-02, -3.1023e-02, -1.2824e-02],\n",
       "                         [-2.5569e-02,  3.1210e-03, -7.6187e-02]],\n",
       "               \n",
       "                        [[ 9.9838e-02, -2.5132e-02, -6.5010e-02],\n",
       "                         [ 2.1718e-02, -6.9263e-02, -5.8428e-02],\n",
       "                         [-3.9011e-02, -1.8887e-02, -8.2036e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.0953e-03,  5.5116e-02, -4.0451e-02],\n",
       "                         [ 1.3440e-02, -3.7230e-02, -3.3557e-02],\n",
       "                         [ 2.4562e-03,  3.8086e-02, -4.7116e-02]],\n",
       "               \n",
       "                        [[-2.1738e-03, -6.0370e-02,  1.2608e-02],\n",
       "                         [-2.4358e-02, -8.9904e-03, -1.3584e-03],\n",
       "                         [ 2.1600e-02,  5.0799e-02,  4.5800e-02]],\n",
       "               \n",
       "                        [[ 7.5068e-04, -2.3383e-02, -5.1693e-03],\n",
       "                         [ 3.8641e-02,  9.4556e-03,  3.4450e-02],\n",
       "                         [-5.2684e-03, -3.8201e-02, -2.1724e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.1830e-02, -1.5057e-02,  3.0492e-02],\n",
       "                         [-1.2295e-02,  5.5536e-02, -6.1304e-02],\n",
       "                         [-3.2246e-02, -3.0042e-02, -3.1644e-02]],\n",
       "               \n",
       "                        [[-7.2817e-02, -4.5324e-02, -4.7268e-02],\n",
       "                         [-3.5612e-02, -6.3792e-02, -1.2601e-02],\n",
       "                         [ 2.5810e-02, -3.9194e-02, -9.8126e-02]],\n",
       "               \n",
       "                        [[-5.9614e-04, -1.3092e-02, -8.8923e-02],\n",
       "                         [-3.1170e-02,  1.3127e-02, -8.8678e-02],\n",
       "                         [-3.8451e-02,  2.0926e-02, -3.1547e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.6331e-02,  3.8553e-02, -1.4874e-02],\n",
       "                         [ 3.3825e-04,  1.2699e-02,  3.0994e-02],\n",
       "                         [-6.3533e-02, -1.8549e-02, -3.3585e-04]],\n",
       "               \n",
       "                        [[ 9.6315e-03, -4.1125e-03, -3.6917e-02],\n",
       "                         [-1.1486e-03, -4.4110e-02, -7.9186e-02],\n",
       "                         [-4.2242e-02, -2.9142e-02, -7.5801e-02]],\n",
       "               \n",
       "                        [[-4.8929e-02, -3.5639e-02, -4.6786e-02],\n",
       "                         [-4.1771e-02, -7.4947e-03, -1.1517e-02],\n",
       "                         [-8.9842e-02, -5.5383e-03,  1.1657e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.3286e-02,  2.6374e-02, -2.8591e-02],\n",
       "                         [ 5.9258e-02,  4.9465e-02,  1.2669e-02],\n",
       "                         [ 2.1487e-03,  4.1206e-02,  2.1908e-02]],\n",
       "               \n",
       "                        [[-2.2028e-02, -5.5872e-02, -1.6000e-02],\n",
       "                         [-1.5978e-02, -1.6894e-02, -6.8963e-02],\n",
       "                         [ 1.7585e-02,  4.7981e-04, -7.0138e-02]],\n",
       "               \n",
       "                        [[ 1.1138e-02,  4.6544e-02,  4.2686e-03],\n",
       "                         [-1.8883e-02,  1.0960e-04, -3.1083e-02],\n",
       "                         [ 3.7975e-03,  1.5932e-02, -1.2507e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-6.0760e-06, -6.0640e-04,  9.2060e-04,  9.9303e-05,  8.0496e-05,\n",
       "                       -2.1454e-04,  1.3242e-04,  1.2033e-04, -4.5796e-04, -1.8945e-04,\n",
       "                        4.7504e-04, -2.4685e-04,  2.0184e-05, -9.9687e-05,  6.0538e-05,\n",
       "                        5.5098e-05,  3.2757e-04, -2.2810e-05,  5.3391e-04, -4.2414e-04,\n",
       "                        2.8890e-05, -1.8176e-04,  6.0750e-05, -3.9524e-04, -3.9297e-04,\n",
       "                       -4.0260e-04, -1.0981e-04, -5.5774e-04, -2.9551e-04,  9.5477e-05,\n",
       "                       -4.7203e-04,  2.7099e-04, -1.1706e-04, -1.4034e-04, -1.4366e-04,\n",
       "                       -8.2336e-06, -3.0238e-04,  2.4074e-05, -3.4844e-04, -1.7416e-04,\n",
       "                       -1.7936e-04,  1.5202e-04, -2.9810e-05,  5.6514e-05,  3.8287e-08,\n",
       "                        2.6531e-04, -4.4396e-04, -5.5978e-05,  1.1228e-05, -5.0739e-04,\n",
       "                       -1.2806e-04,  6.9467e-04, -6.8525e-05,  3.8770e-05, -2.0107e-04,\n",
       "                       -2.3724e-04, -2.0854e-04, -1.6167e-04, -2.1274e-05,  3.9527e-04,\n",
       "                        5.1499e-04, -1.7254e-04, -2.4746e-05, -4.2612e-04, -1.0228e-04,\n",
       "                        3.6573e-04, -5.2881e-05, -1.2913e-04,  1.9131e-04, -3.1437e-04,\n",
       "                       -1.9844e-04, -2.2708e-04,  3.1210e-04,  1.8244e-05, -6.7770e-05,\n",
       "                       -1.1535e-05, -9.9037e-05, -1.3788e-04,  9.4576e-05, -3.1666e-04,\n",
       "                       -3.8956e-04,  2.5680e-05,  6.9358e-04, -1.6438e-04, -2.1543e-04,\n",
       "                        8.3893e-05, -2.3111e-04,  1.7446e-04,  6.4534e-04,  9.4801e-05,\n",
       "                        3.7837e-05, -3.3500e-04, -1.6847e-04, -5.3772e-04, -2.6066e-05,\n",
       "                       -5.5981e-04, -8.6174e-05, -3.2947e-04, -3.0270e-04, -1.3441e-04,\n",
       "                       -1.1960e-04, -1.0694e-04, -5.9652e-04, -1.8422e-04, -7.5544e-04,\n",
       "                       -8.2487e-04, -3.4009e-04,  1.3242e-04, -3.8402e-04, -1.1065e-04,\n",
       "                        4.3006e-05, -2.6204e-04, -2.4343e-04, -7.2673e-04,  7.0624e-04,\n",
       "                       -1.4963e-04,  2.3615e-04, -2.8133e-04, -3.3923e-05,  4.6497e-06,\n",
       "                       -1.4010e-04,  1.6806e-04, -2.2280e-04,  5.0670e-04, -3.5284e-04,\n",
       "                       -1.5228e-04,  6.7031e-05, -7.3733e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.2545, -0.3339, -0.2949, -0.3492, -0.3268, -0.2987, -0.3776, -0.3204,\n",
       "                       -0.2708, -0.2964, -0.2921, -0.3515, -0.3752, -0.3228, -0.3826, -0.3040,\n",
       "                       -0.2832, -0.3361, -0.3270, -0.4511, -0.3159, -0.4156, -0.2098, -0.3048,\n",
       "                       -0.3182, -0.2784, -0.3151, -0.3059, -0.3298, -0.3592, -0.4080, -0.3326,\n",
       "                       -0.4100, -0.3258, -0.4418, -0.3802, -0.3186, -0.4337, -0.3721, -0.2806,\n",
       "                       -0.2830, -0.4583, -0.3002, -0.2784, -0.3627, -0.3278, -0.3641, -0.2827,\n",
       "                       -0.2662, -0.3985, -0.3455, -0.3791, -0.4142, -0.4069, -0.3515, -0.3176,\n",
       "                       -0.3610, -0.3834, -0.4276, -0.2909, -0.3022, -0.3559, -0.3131, -0.3406,\n",
       "                       -0.3571, -0.3248, -0.4169, -0.2717, -0.2853, -0.3547, -0.3204, -0.4044,\n",
       "                       -0.3760, -0.4110, -0.3788, -0.3094, -0.4236, -0.3745, -0.2247, -0.3493,\n",
       "                       -0.2689, -0.3146, -0.3203, -0.2223, -0.2556, -0.3115, -0.3844, -0.3085,\n",
       "                       -0.2784, -0.2869, -0.3329, -0.3732, -0.3059, -0.5500, -0.3718, -0.4086,\n",
       "                       -0.3110, -0.3236, -0.3088, -0.2687, -0.2836, -0.4036, -0.2605, -0.2914,\n",
       "                       -0.3214, -0.4615, -0.3711, -0.2473, -0.2521, -0.3091, -0.3152, -0.3665,\n",
       "                       -0.3046, -0.4005, -0.2946, -0.3237, -0.3226, -0.3331, -0.2781, -0.4326,\n",
       "                       -0.4805, -0.2961, -0.2506, -0.3597, -0.2888, -0.2891, -0.2791, -0.3250],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.7569, 0.8935, 0.9906, 0.7026, 0.9361, 0.8711, 0.9076, 0.9077, 0.8774,\n",
       "                       0.9255, 0.8855, 0.8995, 1.0075, 1.0531, 0.8019, 0.8421, 0.9659, 0.9469,\n",
       "                       0.9686, 1.0651, 0.8724, 1.0389, 1.0367, 0.8804, 0.8985, 0.7747, 0.9375,\n",
       "                       0.9312, 0.9021, 0.9576, 1.0891, 0.9093, 0.9015, 1.1370, 1.2488, 0.9054,\n",
       "                       1.0423, 1.0576, 0.8112, 0.9370, 0.9506, 1.0967, 0.9745, 0.9083, 0.8738,\n",
       "                       1.0165, 0.8261, 0.8985, 0.8353, 0.9330, 0.9811, 1.0591, 0.9531, 1.0168,\n",
       "                       0.8155, 0.9050, 0.9030, 0.8409, 0.8824, 0.8392, 0.9032, 0.9499, 1.0077,\n",
       "                       0.8514, 0.8903, 0.8984, 0.8129, 0.9630, 0.7213, 0.9280, 0.9502, 1.1001,\n",
       "                       1.0581, 0.8144, 0.8617, 0.8892, 1.0326, 0.9918, 0.8958, 0.9420, 0.8526,\n",
       "                       0.9858, 0.9688, 1.0088, 0.8758, 0.8650, 0.7856, 0.8768, 1.0020, 0.9060,\n",
       "                       0.7718, 0.9701, 0.9615, 1.0399, 0.8940, 0.9367, 0.8844, 0.8282, 0.9491,\n",
       "                       0.9741, 0.8008, 0.9845, 0.8498, 0.8869, 0.9215, 1.1950, 0.9330, 1.0151,\n",
       "                       0.8614, 0.8723, 0.8959, 0.9471, 0.9265, 1.0535, 1.0558, 0.9141, 1.0155,\n",
       "                       0.9878, 0.8402, 1.0087, 1.0964, 0.9689, 0.8798, 1.0420, 0.8519, 0.9261,\n",
       "                       0.8211, 0.8847], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-0.0590, -0.0416, -0.0730],\n",
       "                         [ 0.0043,  0.0021, -0.0518],\n",
       "                         [-0.0051,  0.0224,  0.0248]],\n",
       "               \n",
       "                        [[-0.0040, -0.0186, -0.0164],\n",
       "                         [-0.0278, -0.0275, -0.0353],\n",
       "                         [-0.0461, -0.0161, -0.0212]],\n",
       "               \n",
       "                        [[ 0.0006, -0.0648, -0.0448],\n",
       "                         [-0.0715, -0.0436,  0.0230],\n",
       "                         [-0.0694, -0.0175, -0.0770]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0354, -0.0193, -0.0760],\n",
       "                         [ 0.0236,  0.0168,  0.0135],\n",
       "                         [ 0.0090,  0.0044, -0.0222]],\n",
       "               \n",
       "                        [[-0.0499, -0.0271, -0.0488],\n",
       "                         [ 0.0015, -0.0100, -0.0042],\n",
       "                         [ 0.0019, -0.0509, -0.0255]],\n",
       "               \n",
       "                        [[ 0.0635,  0.0148,  0.0308],\n",
       "                         [ 0.0142,  0.0064, -0.0080],\n",
       "                         [ 0.0430,  0.0202,  0.0038]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0519,  0.0324,  0.0444],\n",
       "                         [ 0.0251, -0.0043,  0.0704],\n",
       "                         [ 0.0389,  0.0368,  0.0770]],\n",
       "               \n",
       "                        [[-0.0430, -0.0168, -0.0528],\n",
       "                         [-0.0725, -0.0351, -0.0509],\n",
       "                         [-0.1319, -0.0143, -0.0518]],\n",
       "               \n",
       "                        [[-0.0943, -0.0428, -0.0471],\n",
       "                         [-0.0138, -0.0299, -0.0371],\n",
       "                         [-0.1087, -0.0518, -0.0965]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0306, -0.0251,  0.0111],\n",
       "                         [ 0.0234, -0.0124,  0.0497],\n",
       "                         [ 0.0405,  0.0398, -0.0457]],\n",
       "               \n",
       "                        [[ 0.0260,  0.0182,  0.0044],\n",
       "                         [ 0.0332,  0.0551,  0.0081],\n",
       "                         [ 0.0788,  0.0140,  0.0448]],\n",
       "               \n",
       "                        [[-0.0246, -0.0013, -0.0053],\n",
       "                         [-0.0275, -0.0399, -0.0201],\n",
       "                         [-0.0349, -0.0013, -0.0290]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0346,  0.0298,  0.0484],\n",
       "                         [-0.0040,  0.0243,  0.0235],\n",
       "                         [ 0.0416,  0.0376, -0.0268]],\n",
       "               \n",
       "                        [[ 0.0206,  0.0287,  0.0443],\n",
       "                         [ 0.0073,  0.0316,  0.0233],\n",
       "                         [ 0.0490,  0.0533,  0.0197]],\n",
       "               \n",
       "                        [[ 0.0462,  0.0249,  0.0102],\n",
       "                         [ 0.0119,  0.0513,  0.0396],\n",
       "                         [ 0.1390,  0.1495,  0.2380]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0427,  0.0281, -0.0148],\n",
       "                         [ 0.0327, -0.0343, -0.0097],\n",
       "                         [ 0.0637,  0.0974,  0.0864]],\n",
       "               \n",
       "                        [[-0.0111, -0.0069, -0.0447],\n",
       "                         [-0.0157, -0.0287, -0.0553],\n",
       "                         [-0.0587, -0.0421, -0.0915]],\n",
       "               \n",
       "                        [[-0.0937, -0.0023, -0.0149],\n",
       "                         [-0.0277,  0.0185, -0.0243],\n",
       "                         [-0.0733, -0.0364, -0.0581]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0371,  0.0090,  0.0327],\n",
       "                         [ 0.0477, -0.0046,  0.0633],\n",
       "                         [ 0.0266,  0.0414,  0.0065]],\n",
       "               \n",
       "                        [[-0.0425, -0.0100, -0.0522],\n",
       "                         [-0.0258, -0.0574, -0.0880],\n",
       "                         [-0.0512, -0.0559, -0.0125]],\n",
       "               \n",
       "                        [[-0.0600, -0.0297, -0.0604],\n",
       "                         [-0.0323, -0.0012, -0.0814],\n",
       "                         [-0.1157, -0.0560, -0.0955]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0032, -0.0017, -0.0228],\n",
       "                         [ 0.0336,  0.0255,  0.0259],\n",
       "                         [-0.0024, -0.0266, -0.0578]],\n",
       "               \n",
       "                        [[ 0.0063, -0.0132,  0.0117],\n",
       "                         [ 0.0100, -0.0131, -0.0112],\n",
       "                         [ 0.0317, -0.0102,  0.0200]],\n",
       "               \n",
       "                        [[-0.0283,  0.0190,  0.0050],\n",
       "                         [ 0.0211,  0.0128,  0.0395],\n",
       "                         [-0.0488, -0.0204, -0.0192]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0129,  0.0365,  0.0352],\n",
       "                         [ 0.0369,  0.0157,  0.0077],\n",
       "                         [ 0.0485,  0.0099,  0.0093]],\n",
       "               \n",
       "                        [[ 0.0288, -0.0047,  0.0045],\n",
       "                         [ 0.0367,  0.0137, -0.0083],\n",
       "                         [ 0.0642,  0.0175, -0.0038]],\n",
       "               \n",
       "                        [[ 0.0377,  0.0147, -0.0074],\n",
       "                         [-0.0917, -0.0730, -0.1190],\n",
       "                         [ 0.0509,  0.0838,  0.0375]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0221, -0.0290, -0.0184],\n",
       "                         [-0.0278, -0.0768, -0.0488],\n",
       "                         [-0.0440,  0.0097, -0.0771]],\n",
       "               \n",
       "                        [[ 0.0581, -0.0206,  0.0811],\n",
       "                         [ 0.0190,  0.0615,  0.0400],\n",
       "                         [ 0.0551, -0.0245,  0.0185]],\n",
       "               \n",
       "                        [[-0.0299, -0.0103, -0.0369],\n",
       "                         [-0.0747,  0.0094, -0.0615],\n",
       "                         [-0.0488, -0.0059, -0.0447]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0260, -0.0372, -0.0208],\n",
       "                         [ 0.0608,  0.0160,  0.0427],\n",
       "                         [-0.0105,  0.0261,  0.0190]],\n",
       "               \n",
       "                        [[-0.0271,  0.0153, -0.0099],\n",
       "                         [-0.0305, -0.0334, -0.0042],\n",
       "                         [ 0.0118,  0.0039, -0.0043]],\n",
       "               \n",
       "                        [[ 0.0273,  0.0310, -0.0026],\n",
       "                         [-0.0350,  0.0249,  0.0357],\n",
       "                         [-0.0567, -0.0370,  0.0167]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0147, -0.0794, -0.0239],\n",
       "                         [ 0.0304, -0.0181,  0.0499],\n",
       "                         [-0.0596, -0.0525, -0.0231]],\n",
       "               \n",
       "                        [[ 0.0284, -0.0373, -0.0196],\n",
       "                         [ 0.0211,  0.0046,  0.0080],\n",
       "                         [ 0.0421,  0.0332, -0.0179]],\n",
       "               \n",
       "                        [[ 0.0203,  0.0263,  0.0283],\n",
       "                         [ 0.0217,  0.0460, -0.0328],\n",
       "                         [ 0.0405, -0.0061,  0.0064]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 0.2668,  0.2763,  0.2576,  0.2678, -0.3147,  0.2755,  0.2803,  0.2784,\n",
       "                        0.2698, -0.4401, -0.3794,  0.2573,  0.2724, -0.3672,  0.2763, -0.4308,\n",
       "                       -0.2784, -0.3688, -0.2826,  0.2792,  0.2807, -0.2693,  0.2770, -0.2757,\n",
       "                       -0.2563,  0.1923,  0.2189, -0.4457, -0.2780, -0.2606, -0.4251, -0.4190,\n",
       "                       -0.2837,  0.2322, -0.3558, -0.4344, -0.3570, -0.3452,  0.2770,  0.2779,\n",
       "                        0.3043, -0.2774,  0.2567,  0.2623, -0.4302,  0.2526, -0.2803, -0.2812,\n",
       "                        0.2788, -0.2751, -0.4453, -0.4421,  0.2809, -0.3695, -0.4109,  0.2779,\n",
       "                        0.2451, -0.2803, -0.4169,  0.2786,  0.2197,  0.1988,  0.2325, -0.3651,\n",
       "                       -0.4862,  0.2741, -0.2756,  0.2804,  0.3006, -0.4849, -0.2829, -0.3745,\n",
       "                        0.2750, -0.4055, -0.2799, -0.2664, -0.2732, -0.3364, -0.4684, -0.2839,\n",
       "                        0.2817, -0.2517, -0.2810, -0.2840, -0.2632,  0.2781, -0.2756, -0.2902,\n",
       "                       -0.2828,  0.2790,  0.3128,  0.2703,  0.2755,  0.2853,  0.2792, -0.3024,\n",
       "                        0.2780,  0.2772,  0.2684,  0.2425,  0.2013,  0.2910, -0.3692,  0.2816,\n",
       "                        0.2196,  0.2836,  0.2735,  0.2819, -0.3998,  0.2505,  0.2923,  0.2817,\n",
       "                        0.2794, -0.2774, -0.2772, -0.2585,  0.2778,  0.2748,  0.1633,  0.2792,\n",
       "                        0.1490, -0.2819, -0.3614,  0.2626, -0.4147, -0.3759,  0.2799,  0.2877],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.5235, -0.5609, -0.5190, -0.3920, -0.6931, -0.4414, -0.4149, -0.3642,\n",
       "                       -0.4491, -0.6046, -0.5337, -0.4770, -0.3867, -0.5636, -0.4343, -0.4763,\n",
       "                       -0.3649, -0.5688, -0.3960, -0.3825, -0.3847, -0.4436, -0.7336, -0.4831,\n",
       "                       -0.3995, -0.5849, -0.4165, -0.7343, -0.4069, -0.4015, -0.4487, -0.5541,\n",
       "                       -0.3883, -0.4092, -0.4950, -0.4646, -0.5749, -0.5439, -0.3899, -0.5785,\n",
       "                       -0.6205, -0.5601, -0.4374, -0.4593, -0.5455, -0.5635, -0.4576, -0.3979,\n",
       "                       -0.3768, -0.4118, -0.7176, -0.5476, -0.4038, -0.6312, -0.6245, -0.4066,\n",
       "                       -0.4351, -0.3779, -0.4819, -0.3749, -0.6422, -0.4111, -0.4289, -0.4840,\n",
       "                       -0.5868, -0.4842, -0.4356, -0.5656, -0.6348, -0.4692, -0.3788, -0.6040,\n",
       "                       -0.4083, -0.4992, -0.3980, -0.4524, -0.4322, -0.5302, -0.4596, -0.4952,\n",
       "                       -0.3862, -0.3946, -0.3846, -0.4564, -0.4210, -0.4803, -0.4036, -0.4143,\n",
       "                       -0.3638, -0.4299, -0.5160, -0.4040, -0.5195, -0.6125, -0.4436, -0.5067,\n",
       "                       -0.6220, -0.3759, -0.5454, -0.4149, -0.6404, -0.5581, -0.5536, -0.6891,\n",
       "                       -0.4641, -0.5528, -0.4259, -0.3859, -0.4623, -0.5538, -0.5573, -0.3920,\n",
       "                       -0.4793, -0.3835, -0.4305, -0.3749, -0.4182, -0.4539, -0.4743, -0.3782,\n",
       "                       -0.4532, -0.3733, -0.5153, -0.4247, -0.4734, -0.5864, -0.3557, -0.5853],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.7641, 0.7554, 0.6998, 0.6643, 0.4067, 0.6350, 0.6337, 0.6277, 0.7018,\n",
       "                       0.9830, 0.8972, 0.6797, 0.6315, 0.7769, 0.6238, 0.8531, 0.6157, 0.7626,\n",
       "                       0.6389, 0.5909, 0.6660, 0.7120, 1.0202, 0.4944, 0.5785, 0.7824, 0.6555,\n",
       "                       1.1407, 0.6765, 0.6863, 0.8745, 1.0105, 0.6061, 0.6361, 0.8252, 0.8726,\n",
       "                       0.7681, 0.7384, 0.6741, 0.8939, 0.7699, 0.5229, 0.6529, 0.6882, 0.8198,\n",
       "                       0.7638, 0.6594, 0.6530, 0.6532, 0.6514, 1.1168, 0.7199, 0.6230, 0.6784,\n",
       "                       0.9706, 0.5645, 0.6851, 0.6482, 0.8764, 0.5903, 0.8492, 0.6058, 0.8402,\n",
       "                       0.6643, 0.8828, 0.5394, 0.6153, 0.7929, 1.1198, 0.8382, 0.6587, 0.8652,\n",
       "                       0.6313, 1.0579, 0.6326, 0.6232, 0.6633, 0.6121, 0.8403, 0.5779, 0.6364,\n",
       "                       0.6599, 0.6197, 0.4516, 0.5102, 0.4631, 0.6654, 0.7323, 0.5988, 0.6118,\n",
       "                       0.9502, 0.6496, 0.4729, 0.8918, 0.6622, 0.6629, 0.9211, 0.6630, 0.9476,\n",
       "                       0.6353, 0.9221, 0.8211, 0.8109, 1.0589, 0.5887, 0.9686, 0.4774, 0.5997,\n",
       "                       0.7760, 0.7521, 0.9056, 0.6592, 0.6518, 0.5834, 0.6539, 0.6217, 0.5830,\n",
       "                       0.8100, 0.8607, 0.6073, 0.8604, 0.6081, 0.7025, 0.7196, 0.8448, 0.7668,\n",
       "                       0.7312, 0.9562], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0031,  0.0061,  0.0159,  ..., -0.0066,  0.0081, -0.0077],\n",
       "                       [-0.0052, -0.0014, -0.0052,  ..., -0.0094,  0.0057, -0.0108],\n",
       "                       [-0.0010, -0.0029,  0.0007,  ..., -0.0055,  0.0086, -0.0073],\n",
       "                       [-0.0015, -0.0006,  0.0218,  ..., -0.0026,  0.0039, -0.0081],\n",
       "                       [-0.0047, -0.0091, -0.0006,  ..., -0.0029,  0.0025, -0.0073]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0012, -0.0057, -0.0131, -0.0043,  0.0187], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[ 1.9450e-01, -9.9795e-02,  5.8899e-02,  2.0421e-01, -6.8798e-02,\n",
       "                        -2.0007e-01, -1.7882e-01,  1.9901e-01,  7.3716e-02, -7.7455e-02,\n",
       "                        -1.7553e-01,  1.2926e-01, -2.1960e-01, -5.1855e-02,  1.6968e-01,\n",
       "                        -7.0568e-02, -1.7667e-01,  8.5367e-02, -1.8126e-01, -4.0493e-02],\n",
       "                       [ 1.7353e+00, -9.1854e-02,  3.1752e-01, -1.4802e-01,  3.8902e-01,\n",
       "                         1.2064e-01,  2.6899e-01,  1.2064e+00,  1.0344e+00,  9.4460e-03,\n",
       "                        -2.1750e+00,  4.8852e-02, -5.6482e-01,  9.7123e-02, -3.0906e-01,\n",
       "                         1.3036e-02, -8.0467e-01, -2.5727e-01, -1.8819e+00, -2.9549e-01],\n",
       "                       [-1.6827e-01, -1.0385e-02, -4.5335e-02,  5.0138e-03,  1.1525e-01,\n",
       "                        -1.6526e-01, -6.2435e-02, -1.1266e-02,  1.4281e-01,  2.2943e-02,\n",
       "                         4.6835e-02,  1.8987e-01,  1.8115e-01,  1.4512e-01,  2.0388e-01,\n",
       "                        -1.4252e-01,  1.0847e-01,  1.8634e-01, -1.0822e-02, -1.8472e-01],\n",
       "                       [-8.5229e-01, -4.7153e-01,  2.7821e-01, -4.8381e-01,  1.6806e-01,\n",
       "                        -4.4711e-01,  3.7168e-01, -2.8367e-01, -4.4902e-01, -2.2014e-01,\n",
       "                         7.3040e-01, -4.3841e-01, -2.9784e-01, -4.1960e-01, -3.4597e-01,\n",
       "                        -5.3547e-01,  1.1221e-01, -2.6471e-01,  5.8276e-01, -3.6614e-01],\n",
       "                       [-1.0481e+00, -7.2111e-01,  3.9942e-01, -3.9954e-01,  2.9476e-01,\n",
       "                        -7.2398e-01,  4.0017e-01, -3.4228e-01, -7.9460e-01, -6.6713e-01,\n",
       "                         7.9204e-01, -4.3145e-01, -2.9336e-01, -4.7095e-01, -1.0436e-01,\n",
       "                        -5.2424e-01,  3.8407e-03, -4.1258e-01,  1.0979e+00, -5.8595e-01],\n",
       "                       [-1.6199e-01, -4.6980e-01,  4.3900e-01, -5.6455e-01,  6.8942e-01,\n",
       "                        -5.0696e-01,  7.3493e-01,  3.4355e-01, -9.8068e-02, -7.2798e-01,\n",
       "                         3.7813e-02, -4.6629e-01, -7.5758e-01, -6.3892e-01, -4.9764e-01,\n",
       "                        -5.2737e-01, -4.6897e-01, -6.5699e-01, -4.5877e-01, -5.0287e-01],\n",
       "                       [-9.1209e-01, -4.5004e-01,  5.1553e-01, -5.4736e-01,  6.3686e-01,\n",
       "                        -6.8735e-01,  6.2826e-01, -4.3118e-01, -7.2347e-01, -7.0367e-01,\n",
       "                         7.5990e-01, -3.2092e-01, -1.6595e-01, -5.4322e-01, -4.8487e-01,\n",
       "                        -6.0961e-01,  9.8469e-02, -5.8235e-01,  7.6504e-01, -5.2159e-01],\n",
       "                       [ 2.9009e-01,  1.7531e-01, -1.1453e-01,  2.3866e-03,  2.2747e-01,\n",
       "                         2.2571e-01,  4.2080e-02,  4.1651e-01,  2.5253e-01, -1.6386e-01,\n",
       "                        -8.9159e-01, -1.8848e-01, -1.8397e-02, -5.2680e-02,  6.3115e-02,\n",
       "                         1.5641e-01, -4.6105e-01,  2.4882e-02, -5.8931e-01,  4.5383e-02],\n",
       "                       [-7.9067e-01, -4.9062e-01,  3.9284e-01, -6.0223e-01,  4.3311e-01,\n",
       "                        -6.3251e-01,  3.8817e-01, -6.4926e-01, -8.1020e-01, -5.8495e-01,\n",
       "                         9.9257e-01, -2.6383e-01, -3.8121e-01, -3.9272e-01, -7.3732e-03,\n",
       "                        -5.8554e-01,  1.5052e-01, -3.3789e-01,  1.0906e+00, -2.6436e-01],\n",
       "                       [-1.0283e+00, -6.8587e-01,  1.7270e-01, -5.4408e-01,  3.7154e-01,\n",
       "                        -5.4464e-01,  2.0197e-01, -5.3396e-01, -9.3041e-01, -2.5700e-01,\n",
       "                         1.1418e+00, -3.8200e-01, -4.4176e-01, -4.6435e-01, -3.4223e-01,\n",
       "                        -4.7282e-01,  7.8894e-02, -5.3351e-01,  9.2016e-01, -2.6287e-01],\n",
       "                       [-1.1070e+00, -5.4437e-01,  5.6777e-01, -2.6170e-01,  4.4844e-01,\n",
       "                        -4.7415e-01,  3.0653e-01, -3.4535e-01, -6.0394e-01, -4.5009e-01,\n",
       "                         1.0528e+00, -4.3845e-01, -1.6572e-01, -3.9694e-01,  9.7298e-03,\n",
       "                        -4.9304e-01,  3.0261e-01, -4.0828e-01,  7.4209e-01, -4.4445e-01],\n",
       "                       [ 9.7307e-02, -1.1309e-01, -5.3454e-02,  3.0533e-01,  1.1127e-01,\n",
       "                         1.0012e-01, -2.5072e-01,  2.0497e-01,  2.2407e-03, -6.9251e-03,\n",
       "                         9.9663e-02,  1.1022e-01,  1.7607e-01, -8.4756e-02,  1.6582e-01,\n",
       "                         8.9532e-03,  1.4399e-01,  2.6388e-02,  8.4423e-02,  7.2213e-02],\n",
       "                       [ 2.1460e-01, -1.2244e-01, -4.7370e-03,  1.3690e-01,  1.8198e-01,\n",
       "                         4.9313e-02, -2.0272e-01,  1.8716e-02,  1.1511e-01,  4.4500e-02,\n",
       "                         1.0384e-01,  2.3530e-01,  8.3163e-02,  1.1300e-01,  9.8999e-02,\n",
       "                         1.8313e-01, -1.0734e-01,  1.8990e-01,  1.5302e-01, -5.7541e-03],\n",
       "                       [-3.5130e-02, -6.1225e-02, -1.7263e-01,  2.2230e-01, -1.1839e-01,\n",
       "                         1.5746e-01,  2.5898e-02, -1.9704e-01,  1.5753e-01,  1.5394e-01,\n",
       "                         2.0103e-01,  6.9307e-02,  1.9744e-01,  1.9463e-01, -1.8095e-01,\n",
       "                         2.2054e-01,  8.0090e-02, -1.1284e-01, -5.4173e-02, -1.9982e-01],\n",
       "                       [-9.6418e-01, -5.6401e-01,  4.0396e-01, -3.1266e-01,  7.6867e-02,\n",
       "                        -6.1605e-01,  4.8461e-01, -4.4598e-01, -9.7391e-01, -4.4509e-01,\n",
       "                         9.1621e-01, -2.4127e-01, -1.3111e-01, -4.6517e-01, -2.5690e-01,\n",
       "                        -3.9934e-01,  2.0945e-01, -5.9430e-01,  8.2230e-01, -5.1802e-01],\n",
       "                       [ 6.2884e-02,  1.5268e-01, -9.8051e-02, -7.9451e-02,  5.4087e-02,\n",
       "                        -8.8575e-02, -2.0254e-01,  7.8244e-03, -2.1483e-01,  1.6323e-01,\n",
       "                         5.5022e-02,  1.6895e-01,  1.5126e-01, -4.8578e-04, -1.3752e-01,\n",
       "                         1.3657e-02,  8.7678e-02,  7.2777e-02, -2.0556e-01,  1.6001e-01],\n",
       "                       [-1.7184e-01,  1.5199e-01, -1.4167e-01,  4.3341e-02, -1.1432e-01,\n",
       "                         1.8257e-02,  1.5661e-01,  6.9146e-03,  1.4682e-01,  1.5712e-01,\n",
       "                        -1.8079e-01, -2.4925e-02,  9.8675e-02,  1.8632e-01,  7.6713e-02,\n",
       "                        -7.8911e-02, -9.5569e-02, -1.8848e-01,  2.3741e-02, -1.2041e-01],\n",
       "                       [-1.4656e-01,  7.4668e-02,  3.5199e-02,  2.4825e-02,  7.0286e-02,\n",
       "                         1.3211e-01, -2.7986e-02,  1.9289e-01,  1.9805e-01,  1.9401e-01,\n",
       "                        -3.6612e-02,  1.4258e-01, -2.3074e-02,  1.4368e-01, -1.5427e-01,\n",
       "                        -8.2087e-02,  5.0667e-02,  2.0349e-01,  1.6160e-01, -8.7803e-02],\n",
       "                       [-6.4075e-01, -5.2026e-01,  2.4185e-01, -6.8232e-01,  2.5460e-01,\n",
       "                        -5.9572e-01,  4.3895e-01, -1.1340e-02, -4.7128e-01, -3.9733e-01,\n",
       "                         4.4866e-01, -6.5520e-01, -1.6905e-01, -6.3746e-01, -4.0997e-01,\n",
       "                        -6.6564e-01, -2.5717e-01, -6.5931e-01,  4.0509e-01, -6.5789e-01],\n",
       "                       [-1.1403e-01, -3.5787e-02, -2.3004e-01, -2.6748e-02, -7.9291e-03,\n",
       "                        -1.0953e-01,  1.1231e-01,  1.5424e-01, -1.6711e-01, -5.1077e-02,\n",
       "                        -1.1215e-02,  2.0834e-01, -2.2533e-01, -1.0289e-01, -6.3570e-02,\n",
       "                        -1.6608e-01,  1.6110e-01,  1.5337e-01, -9.8804e-02, -1.1561e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([-0.1579,  0.3317, -0.2266,  0.1376,  0.3147,  0.6708,  0.5496,  0.0762,\n",
       "                        0.3255,  0.5074,  0.4359,  0.1290,  0.1084, -0.1700,  0.3683,  0.1238,\n",
       "                        0.1505,  0.0867,  0.4810, -0.1023], device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 0.1263, -0.3344,  0.1453, -0.5822, -0.2961, -0.5934, -0.6665,  0.1498,\n",
       "                        -0.4193, -0.4439, -0.3855,  0.1673,  0.2106,  0.1204, -0.2755,  0.0328,\n",
       "                         0.0779, -0.0944, -0.2348, -0.0505],\n",
       "                       [ 0.0947,  0.5150, -0.1515,  0.5087,  0.4123,  0.5249,  0.3443,  0.3109,\n",
       "                         0.4761,  0.4943,  0.2503, -0.0277,  0.1094,  0.0985,  0.4945, -0.0959,\n",
       "                        -0.1545,  0.2038,  0.3788, -0.1774],\n",
       "                       [-0.1231, -0.5512, -0.1389, -0.5851, -0.3872, -0.2641, -0.5559, -0.1714,\n",
       "                        -0.3891, -0.6839, -0.3157,  0.0028,  0.0339,  0.0062, -0.5346,  0.0933,\n",
       "                         0.0714, -0.0312, -0.4261, -0.0237],\n",
       "                       [ 0.2137,  0.6503,  0.1578,  0.4865,  0.1649,  0.3847,  0.3863,  0.4489,\n",
       "                         0.1918,  0.4767,  0.2793, -0.0916, -0.0052,  0.0060,  0.1676,  0.0615,\n",
       "                        -0.1830, -0.0083,  0.3891,  0.1752],\n",
       "                       [-0.1682, -0.1868, -0.1246, -0.4531, -0.5128, -0.2832, -0.1628,  0.1761,\n",
       "                        -0.4474, -0.8021, -0.2750,  0.1195, -0.1405, -0.0798, -0.3772,  0.0231,\n",
       "                         0.0909,  0.0590, -0.1927, -0.0339],\n",
       "                       [ 0.1320,  0.7142,  0.1656,  0.6402,  0.6090,  0.5877,  0.3136,  0.5334,\n",
       "                         0.3913,  0.3434,  0.2637,  0.1621,  0.0653,  0.0616,  0.4950, -0.0616,\n",
       "                         0.2075,  0.2232,  0.3148, -0.2000],\n",
       "                       [ 0.0186,  0.7161, -0.0520,  0.5408,  0.8011,  0.8012,  0.8519, -0.0051,\n",
       "                         0.4469,  0.4728,  0.5148, -0.0371,  0.0579, -0.1367,  0.4937,  0.0342,\n",
       "                         0.1900,  0.1087,  0.8313, -0.1684],\n",
       "                       [-0.0348,  1.6996, -0.2019,  1.3769,  1.4634,  1.3800,  1.3068,  1.0588,\n",
       "                         1.5199,  1.4969,  1.5522, -0.1445,  0.1233,  0.1053,  1.2408, -0.2108,\n",
       "                         0.0877, -0.1953,  1.1804,  0.0776],\n",
       "                       [-0.0676, -0.6053, -0.1941,  0.4563,  0.6788,  0.0842,  0.4916, -0.6939,\n",
       "                         0.5072,  0.6108,  0.5006, -0.1416, -0.1245, -0.0395,  0.5281, -0.1009,\n",
       "                         0.0885, -0.0804,  0.3023, -0.1967],\n",
       "                       [ 0.0187, -0.7021,  0.0039, -0.6063, -0.5650, -0.5172, -0.5664,  0.0619,\n",
       "                        -0.6036, -0.3312, -0.6623,  0.0109, -0.0110,  0.0980, -0.4196, -0.0094,\n",
       "                         0.2114, -0.0088, -0.2525,  0.0999]], device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.4890,  0.4480, -0.4067,  0.3688, -0.3490,  0.4760,  0.5701,  1.2780,\n",
       "                        0.3255, -0.1266], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.6312753443717956,\n",
       "   1.5536453297138215,\n",
       "   1.523911440372467,\n",
       "   1.499477592945099,\n",
       "   1.486074610233307,\n",
       "   1.4615552172660828,\n",
       "   1.4352952117919922,\n",
       "   1.4117356083393098,\n",
       "   1.3937308223247529,\n",
       "   1.3660124487876892,\n",
       "   1.3410863075256347,\n",
       "   1.323280165195465,\n",
       "   1.3148709342479705,\n",
       "   1.3010779201984406,\n",
       "   1.2839238045215606,\n",
       "   1.2748850593566894,\n",
       "   1.2559942799806594,\n",
       "   1.2500510506629945,\n",
       "   1.2352914267778397,\n",
       "   1.2235034726858138,\n",
       "   1.2188936455249786,\n",
       "   1.215208762049675,\n",
       "   1.1991394428014754,\n",
       "   1.1936199412345887,\n",
       "   1.1794605705738068,\n",
       "   1.1725759319067002,\n",
       "   1.170899636745453,\n",
       "   1.1576161487102508,\n",
       "   1.1564540531635283,\n",
       "   1.1360031201839447,\n",
       "   1.136114946961403,\n",
       "   1.1196761959791184,\n",
       "   1.1314458951950073,\n",
       "   1.107127965927124,\n",
       "   1.1041757291555405,\n",
       "   1.10244908452034,\n",
       "   1.0973484139442444,\n",
       "   1.07775164103508,\n",
       "   1.0772982504367827,\n",
       "   1.0764302415847777,\n",
       "   1.0622951929569244,\n",
       "   1.0652987344264984,\n",
       "   1.047271199464798,\n",
       "   1.046766982793808,\n",
       "   1.0322010674476623,\n",
       "   1.0385592976808549,\n",
       "   1.015462129354477,\n",
       "   1.0306507573127746,\n",
       "   1.0199654269218446,\n",
       "   1.001055035829544,\n",
       "   0.9956076159477234,\n",
       "   0.9915678917169571,\n",
       "   0.9886687263250351,\n",
       "   0.9976781339645385,\n",
       "   0.9847062610387802,\n",
       "   0.9809195554256439,\n",
       "   0.9653531548976898,\n",
       "   0.9697326718568802,\n",
       "   0.9603524369001388,\n",
       "   0.9583376731872558,\n",
       "   0.9626984648704529,\n",
       "   0.9360564488172531,\n",
       "   0.9371786391735077,\n",
       "   0.9232260868549347,\n",
       "   0.9318730368614196,\n",
       "   0.9233466293811798,\n",
       "   0.9206965967416764,\n",
       "   0.9013555799722671,\n",
       "   0.8973877140283585,\n",
       "   0.89515362906456,\n",
       "   0.8954488372802735,\n",
       "   0.8938073664903641,\n",
       "   0.8883875104188919,\n",
       "   0.8851192671060563,\n",
       "   0.8736684693098068,\n",
       "   0.8767604328393936,\n",
       "   0.8670074696540833,\n",
       "   0.8709033648967743,\n",
       "   0.8776839482784271,\n",
       "   0.8561808477044106,\n",
       "   0.8526581404209137,\n",
       "   0.8522894896268844,\n",
       "   0.8490510226488114,\n",
       "   0.8419514431357383,\n",
       "   0.8306779575943947,\n",
       "   0.8370874986052513,\n",
       "   0.818301858663559,\n",
       "   0.8318635807037353,\n",
       "   0.8305096046328545,\n",
       "   0.8263716456890107,\n",
       "   0.812418635904789,\n",
       "   0.8102063025832176,\n",
       "   0.8097334576845169,\n",
       "   0.806390151977539,\n",
       "   0.8001925750970841,\n",
       "   0.7956064458489418,\n",
       "   0.8003029662370682,\n",
       "   0.7935896825194358,\n",
       "   0.7792422060370445,\n",
       "   0.7870164322853088,\n",
       "   0.7800704671144485,\n",
       "   0.7731958082914352,\n",
       "   0.7573743429780007,\n",
       "   0.7621203933358193,\n",
       "   0.7579243068695068,\n",
       "   0.7619101973772049,\n",
       "   0.750242599606514,\n",
       "   0.7490658296346664,\n",
       "   0.742469374537468,\n",
       "   0.7454377554655075,\n",
       "   0.7411725878715515,\n",
       "   0.7407792394161224,\n",
       "   0.7376013640761375,\n",
       "   0.7245420916676522,\n",
       "   0.732714619576931,\n",
       "   0.7293270953297615,\n",
       "   0.7261229116916657,\n",
       "   0.7281058864593506,\n",
       "   0.7309482480883598,\n",
       "   0.7122141485810279,\n",
       "   0.7245344784855843,\n",
       "   0.711326936841011,\n",
       "   0.7155838780403138,\n",
       "   0.7117281124591828,\n",
       "   0.700309777200222,\n",
       "   0.7013217418193817,\n",
       "   0.6939922206997872,\n",
       "   0.6918741597533226,\n",
       "   0.6857633302211762,\n",
       "   0.6804469656348229,\n",
       "   0.6801684402227401,\n",
       "   0.6912091225981712,\n",
       "   0.6844610158801079,\n",
       "   0.6684100223779679,\n",
       "   0.6564512103796005,\n",
       "   0.6718824481368065,\n",
       "   0.6615818604230881,\n",
       "   0.6636605249047279,\n",
       "   0.6621590116024018,\n",
       "   0.6559794107079506,\n",
       "   0.659119460761547,\n",
       "   0.6499888167977333,\n",
       "   0.6454217532277107,\n",
       "   0.6618763255476952,\n",
       "   0.6481747549176217,\n",
       "   0.636088679254055,\n",
       "   0.6301739387512207,\n",
       "   0.637675487279892],\n",
       "  'train_loss_std': [0.09245625152620476,\n",
       "   0.05431498872198672,\n",
       "   0.06403686572345477,\n",
       "   0.06881304420361085,\n",
       "   0.07331014654068113,\n",
       "   0.07688629780016662,\n",
       "   0.08103098835209477,\n",
       "   0.08605979617297742,\n",
       "   0.08913261634872616,\n",
       "   0.09284755679521971,\n",
       "   0.09418142127381496,\n",
       "   0.09636385867009054,\n",
       "   0.09777757958036082,\n",
       "   0.09880834326937396,\n",
       "   0.10090273702342745,\n",
       "   0.10447686779137208,\n",
       "   0.10362605257686132,\n",
       "   0.10532023921983699,\n",
       "   0.11099878309506209,\n",
       "   0.10407305396207367,\n",
       "   0.11508054110550273,\n",
       "   0.11035334492463031,\n",
       "   0.11151179860269061,\n",
       "   0.11434996954731064,\n",
       "   0.11146147821499856,\n",
       "   0.11550849382752458,\n",
       "   0.12612609717947557,\n",
       "   0.12018523249979496,\n",
       "   0.11541844018462492,\n",
       "   0.11962672671535189,\n",
       "   0.11867079884835745,\n",
       "   0.11894588731286002,\n",
       "   0.11140343260291057,\n",
       "   0.12614266471257102,\n",
       "   0.11591846984181058,\n",
       "   0.11598336164351686,\n",
       "   0.1244304446290607,\n",
       "   0.12226650766081637,\n",
       "   0.1219736818230656,\n",
       "   0.1270422299875038,\n",
       "   0.12293041329006393,\n",
       "   0.12280023271630663,\n",
       "   0.12376754007772178,\n",
       "   0.12596900867387115,\n",
       "   0.12574329304993026,\n",
       "   0.12321648634212234,\n",
       "   0.13119353012460178,\n",
       "   0.13345060677968756,\n",
       "   0.12280744488672218,\n",
       "   0.12881212128120295,\n",
       "   0.12442238992669635,\n",
       "   0.12700504637166282,\n",
       "   0.1301440375379684,\n",
       "   0.1225948799006876,\n",
       "   0.1314354783818133,\n",
       "   0.12820730961831062,\n",
       "   0.12414032093931221,\n",
       "   0.13419673330274395,\n",
       "   0.12840126715661215,\n",
       "   0.12606774799001716,\n",
       "   0.12749948734516953,\n",
       "   0.12773265309789472,\n",
       "   0.12532826897919028,\n",
       "   0.13100200366084364,\n",
       "   0.12572368129127398,\n",
       "   0.12605952564403047,\n",
       "   0.1327964619235423,\n",
       "   0.12487836845359447,\n",
       "   0.13858238008811954,\n",
       "   0.1356259785481087,\n",
       "   0.12861157640949777,\n",
       "   0.13297074162117772,\n",
       "   0.12606241373289556,\n",
       "   0.1279403583684044,\n",
       "   0.12691085242306915,\n",
       "   0.1353111109776787,\n",
       "   0.12921480272049957,\n",
       "   0.13272276108800757,\n",
       "   0.1281231963375653,\n",
       "   0.13581961646045845,\n",
       "   0.12919584596126085,\n",
       "   0.13201866636353934,\n",
       "   0.13298571852662983,\n",
       "   0.13187184752201594,\n",
       "   0.12720278823918715,\n",
       "   0.13295245417764695,\n",
       "   0.12912041363080407,\n",
       "   0.12699982482080607,\n",
       "   0.13261507488906252,\n",
       "   0.12665781075519475,\n",
       "   0.13447199389431377,\n",
       "   0.1300578317168336,\n",
       "   0.12538481210584196,\n",
       "   0.13806450586306063,\n",
       "   0.13018908852197594,\n",
       "   0.12786465562097246,\n",
       "   0.13446761576757266,\n",
       "   0.1284930005947963,\n",
       "   0.12434299643091913,\n",
       "   0.13346227073706685,\n",
       "   0.12599957519543728,\n",
       "   0.13471636877816792,\n",
       "   0.12699072892136642,\n",
       "   0.1294762572420477,\n",
       "   0.128859956688055,\n",
       "   0.13036798934777563,\n",
       "   0.13447963795063525,\n",
       "   0.1284147004321635,\n",
       "   0.1322420442973943,\n",
       "   0.13050792759276839,\n",
       "   0.13210080580977468,\n",
       "   0.12667346309367408,\n",
       "   0.1226536266116478,\n",
       "   0.12791581110361733,\n",
       "   0.12524506852330314,\n",
       "   0.12014348137911013,\n",
       "   0.1312707930200373,\n",
       "   0.1312983588980172,\n",
       "   0.12162720539730441,\n",
       "   0.12544537378018195,\n",
       "   0.12985604560388841,\n",
       "   0.12606320758390896,\n",
       "   0.12190498763069793,\n",
       "   0.12071367443226899,\n",
       "   0.1323124557921808,\n",
       "   0.12881564732368758,\n",
       "   0.12337391213817488,\n",
       "   0.12247710101014654,\n",
       "   0.1298130112966631,\n",
       "   0.1286653732678364,\n",
       "   0.12158214632371175,\n",
       "   0.12512173879401725,\n",
       "   0.12012055175685746,\n",
       "   0.12266621213823276,\n",
       "   0.1241450393995035,\n",
       "   0.12998113815795082,\n",
       "   0.12791106188205423,\n",
       "   0.12741131976298373,\n",
       "   0.11935393576981639,\n",
       "   0.12349631419276516,\n",
       "   0.1248312657753292,\n",
       "   0.12112067637230059,\n",
       "   0.12383489415720256,\n",
       "   0.11959194625798943,\n",
       "   0.12335504359122627,\n",
       "   0.12936401210210524,\n",
       "   0.1296476860662694,\n",
       "   0.12389954792704433],\n",
       "  'train_accuracy_mean': [0.26842000007629396,\n",
       "   0.3109533336162567,\n",
       "   0.33402000048756597,\n",
       "   0.35203999969363214,\n",
       "   0.3607599996626377,\n",
       "   0.3771933328509331,\n",
       "   0.39587999975681304,\n",
       "   0.40996666610240934,\n",
       "   0.4199133332967758,\n",
       "   0.4358799997568131,\n",
       "   0.4509066665172577,\n",
       "   0.4597666670680046,\n",
       "   0.4632133322358131,\n",
       "   0.4719066669344902,\n",
       "   0.4793466659784317,\n",
       "   0.48401999825239184,\n",
       "   0.49289333283901215,\n",
       "   0.49695999825000764,\n",
       "   0.5035666663050652,\n",
       "   0.508166665673256,\n",
       "   0.5134533323049545,\n",
       "   0.5127133322358132,\n",
       "   0.5194066653847694,\n",
       "   0.5225333324670791,\n",
       "   0.5312266644239426,\n",
       "   0.529973332464695,\n",
       "   0.534239999473095,\n",
       "   0.5389333317875862,\n",
       "   0.5401466658711433,\n",
       "   0.5497466650605202,\n",
       "   0.5507333332896233,\n",
       "   0.5577399990558625,\n",
       "   0.5507266665697098,\n",
       "   0.5627799994945526,\n",
       "   0.5644266663193702,\n",
       "   0.5660933334827423,\n",
       "   0.5676199997663498,\n",
       "   0.5754599995613098,\n",
       "   0.5753466649055481,\n",
       "   0.5766866657733917,\n",
       "   0.5833133320212365,\n",
       "   0.5809866663813591,\n",
       "   0.589579999923706,\n",
       "   0.5902200002074242,\n",
       "   0.5959799988269806,\n",
       "   0.5925466670989991,\n",
       "   0.6023533335328102,\n",
       "   0.5949399996995925,\n",
       "   0.600986667394638,\n",
       "   0.6103466661572456,\n",
       "   0.6122999994754791,\n",
       "   0.6138200008273125,\n",
       "   0.6151866652965545,\n",
       "   0.6103666663169861,\n",
       "   0.6172133340835572,\n",
       "   0.6203066667914391,\n",
       "   0.6253133323192597,\n",
       "   0.6240333334803582,\n",
       "   0.6280600018501282,\n",
       "   0.6278466678857804,\n",
       "   0.6246666669845581,\n",
       "   0.6370066678524018,\n",
       "   0.6380666663646698,\n",
       "   0.6429066669940948,\n",
       "   0.6388933326601982,\n",
       "   0.6433133337497711,\n",
       "   0.6452733327746392,\n",
       "   0.653913334786892,\n",
       "   0.655099998652935,\n",
       "   0.6558866666555405,\n",
       "   0.6543199996352196,\n",
       "   0.6553133335709572,\n",
       "   0.657106667459011,\n",
       "   0.6605533334612846,\n",
       "   0.6643800014853477,\n",
       "   0.663013334274292,\n",
       "   0.6680600000023842,\n",
       "   0.6660066667199135,\n",
       "   0.6623400000929832,\n",
       "   0.6718000005483628,\n",
       "   0.6746600001454354,\n",
       "   0.6737533346414566,\n",
       "   0.6754200010299682,\n",
       "   0.6787200002670288,\n",
       "   0.6811200004220009,\n",
       "   0.6798133335709572,\n",
       "   0.689646666765213,\n",
       "   0.6832799994945526,\n",
       "   0.6819666667580605,\n",
       "   0.6835400006771087,\n",
       "   0.6909400009512902,\n",
       "   0.6896733341217041,\n",
       "   0.691933333516121,\n",
       "   0.6932000021934509,\n",
       "   0.695286668419838,\n",
       "   0.6974666670560837,\n",
       "   0.6951199985742569,\n",
       "   0.6997866661548614,\n",
       "   0.7039533331394195,\n",
       "   0.701753333568573,\n",
       "   0.7029599996805191,\n",
       "   0.7083400003314019,\n",
       "   0.7132066680192948,\n",
       "   0.7112599989175796,\n",
       "   0.712440000295639,\n",
       "   0.7127333338260651,\n",
       "   0.7175200011730194,\n",
       "   0.7170399997234345,\n",
       "   0.7195733326673508,\n",
       "   0.7189066668748856,\n",
       "   0.7211400008201599,\n",
       "   0.7195666673183441,\n",
       "   0.7231333343982697,\n",
       "   0.7283666672706604,\n",
       "   0.7242399997711182,\n",
       "   0.7256266677379608,\n",
       "   0.7274733327627182,\n",
       "   0.7259533334970474,\n",
       "   0.7241600006818771,\n",
       "   0.7333933311700821,\n",
       "   0.726266667842865,\n",
       "   0.7335999995470047,\n",
       "   0.7307266663312912,\n",
       "   0.7337066661119461,\n",
       "   0.7366066657304764,\n",
       "   0.7372200006246566,\n",
       "   0.741186665892601,\n",
       "   0.7408199994564056,\n",
       "   0.7427733340263367,\n",
       "   0.746813332915306,\n",
       "   0.7476799998283387,\n",
       "   0.7406333338022232,\n",
       "   0.7451266677379608,\n",
       "   0.7522466661930084,\n",
       "   0.7566266676187515,\n",
       "   0.750046667098999,\n",
       "   0.7548000015020371,\n",
       "   0.7526266658306122,\n",
       "   0.7532533346414566,\n",
       "   0.7567800004482269,\n",
       "   0.7544533338546753,\n",
       "   0.7599933327436447,\n",
       "   0.759866665482521,\n",
       "   0.7534266659021378,\n",
       "   0.7597533338069916,\n",
       "   0.7639199998378754,\n",
       "   0.7691399984359741,\n",
       "   0.7627000002861023],\n",
       "  'train_accuracy_std': [0.03968337432334797,\n",
       "   0.04197250524953474,\n",
       "   0.045237590723798536,\n",
       "   0.0484127460909841,\n",
       "   0.04855100591608849,\n",
       "   0.050604022375648997,\n",
       "   0.05053407041481656,\n",
       "   0.05304148230632834,\n",
       "   0.051397937729363255,\n",
       "   0.05471829861025683,\n",
       "   0.05193265033978923,\n",
       "   0.05519189815795109,\n",
       "   0.05476847286824963,\n",
       "   0.05631348917078804,\n",
       "   0.053798138377891054,\n",
       "   0.05632835854207846,\n",
       "   0.056752343807259215,\n",
       "   0.05545130449247343,\n",
       "   0.059385099906595656,\n",
       "   0.05519938615106621,\n",
       "   0.05906105159962144,\n",
       "   0.05845752490459978,\n",
       "   0.05791740412372757,\n",
       "   0.06095010686719509,\n",
       "   0.05873524195893209,\n",
       "   0.060193755869709396,\n",
       "   0.06296895297870722,\n",
       "   0.06186774412940492,\n",
       "   0.05929793592166246,\n",
       "   0.05989678358999355,\n",
       "   0.06222750391243451,\n",
       "   0.05842776332831158,\n",
       "   0.05653459943955778,\n",
       "   0.062446996130139584,\n",
       "   0.05934011431338027,\n",
       "   0.058249885752006896,\n",
       "   0.06168343887471859,\n",
       "   0.06242568619544669,\n",
       "   0.05981148107223847,\n",
       "   0.06258345313546763,\n",
       "   0.06154474071532347,\n",
       "   0.05890711981928294,\n",
       "   0.0606712575448222,\n",
       "   0.061694017350271455,\n",
       "   0.060444609428495596,\n",
       "   0.06162073150880352,\n",
       "   0.06415706257961086,\n",
       "   0.06524941020884156,\n",
       "   0.059588626620637096,\n",
       "   0.061450359691018724,\n",
       "   0.06060967179040227,\n",
       "   0.06291128541649246,\n",
       "   0.06341283273657522,\n",
       "   0.06001610094031987,\n",
       "   0.06275570902984061,\n",
       "   0.06125389292877822,\n",
       "   0.058123735707429264,\n",
       "   0.06413301411194561,\n",
       "   0.061211045131891385,\n",
       "   0.0606201921900051,\n",
       "   0.05973980680402527,\n",
       "   0.061143511437017274,\n",
       "   0.06080457006377663,\n",
       "   0.06139848148917735,\n",
       "   0.06004496400780153,\n",
       "   0.057905859238758295,\n",
       "   0.06354764403986407,\n",
       "   0.056845184875867594,\n",
       "   0.06685416122788387,\n",
       "   0.06359448268979073,\n",
       "   0.06059376555162922,\n",
       "   0.06129647164454763,\n",
       "   0.06056388468839661,\n",
       "   0.060647473976914335,\n",
       "   0.059793665443692655,\n",
       "   0.06267577859133845,\n",
       "   0.059750899571810207,\n",
       "   0.06048570043261071,\n",
       "   0.06133344188978242,\n",
       "   0.06297198644718487,\n",
       "   0.059129199068153795,\n",
       "   0.06075507632386632,\n",
       "   0.062254864696217375,\n",
       "   0.06168563141169507,\n",
       "   0.05809217758775086,\n",
       "   0.06174867182997094,\n",
       "   0.06038641854075568,\n",
       "   0.05921183665378568,\n",
       "   0.06054327194144866,\n",
       "   0.05899399008489147,\n",
       "   0.0603142192548032,\n",
       "   0.059581913605404366,\n",
       "   0.05953930594353224,\n",
       "   0.06406649257680815,\n",
       "   0.06120481260041143,\n",
       "   0.05880914476833396,\n",
       "   0.06154607164964872,\n",
       "   0.06028265792287274,\n",
       "   0.05802713977570749,\n",
       "   0.061671650395766095,\n",
       "   0.05961687409234236,\n",
       "   0.06226056336799943,\n",
       "   0.05961995802217064,\n",
       "   0.059530674680109724,\n",
       "   0.058577219675111356,\n",
       "   0.05991620019081443,\n",
       "   0.060953758684003725,\n",
       "   0.058742701302196235,\n",
       "   0.06049569319897116,\n",
       "   0.060597433919088,\n",
       "   0.06023924919362413,\n",
       "   0.05839607094851679,\n",
       "   0.056611776747367476,\n",
       "   0.05910422946778505,\n",
       "   0.055830102138150255,\n",
       "   0.05623627205848195,\n",
       "   0.05922306511180926,\n",
       "   0.059831263432306867,\n",
       "   0.05595618215620283,\n",
       "   0.05696370005472326,\n",
       "   0.05950999776388421,\n",
       "   0.05707320717225638,\n",
       "   0.05675742590416029,\n",
       "   0.05514581246310404,\n",
       "   0.05870166213776254,\n",
       "   0.05948953661865057,\n",
       "   0.055710088015984253,\n",
       "   0.057305951855635306,\n",
       "   0.05871094588674984,\n",
       "   0.05875391783152132,\n",
       "   0.056587748134155326,\n",
       "   0.058148650699461596,\n",
       "   0.05609323141721217,\n",
       "   0.056413918237393425,\n",
       "   0.057699590934585994,\n",
       "   0.057720186641614456,\n",
       "   0.057419159892819614,\n",
       "   0.058592666454442735,\n",
       "   0.055207428832892404,\n",
       "   0.05632809031723292,\n",
       "   0.05680601499477403,\n",
       "   0.054930053930014745,\n",
       "   0.05756642505773494,\n",
       "   0.05581787966528174,\n",
       "   0.056080944508913134,\n",
       "   0.05794067942958215,\n",
       "   0.056288882060633705,\n",
       "   0.05732149919152669],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.6046970876057942,\n",
       "   1.5749306297302246,\n",
       "   1.561995029449463,\n",
       "   1.5600267108281454,\n",
       "   1.5385781168937682,\n",
       "   1.534354329109192,\n",
       "   1.5028958161671957,\n",
       "   1.4934051473935446,\n",
       "   1.4787775206565856,\n",
       "   1.458341458638509,\n",
       "   1.449342987537384,\n",
       "   1.4370915794372559,\n",
       "   1.425681365331014,\n",
       "   1.41751957654953,\n",
       "   1.4255064217249553,\n",
       "   1.4043161821365358,\n",
       "   1.3830989027023315,\n",
       "   1.3844980128606161,\n",
       "   1.3951738452911377,\n",
       "   1.3892926216125487,\n",
       "   1.3623798632621764,\n",
       "   1.3687306968371074,\n",
       "   1.354328678448995,\n",
       "   1.3421893541018168,\n",
       "   1.3686388572057089,\n",
       "   1.344886039098104,\n",
       "   1.3452581389745077,\n",
       "   1.3471073603630066,\n",
       "   1.331680347919464,\n",
       "   1.3291320665677389,\n",
       "   1.3254357266426087,\n",
       "   1.3169755919774373,\n",
       "   1.307908645470937,\n",
       "   1.313577837149302,\n",
       "   1.309145795504252,\n",
       "   1.321841200987498,\n",
       "   1.3058741323153178,\n",
       "   1.3000436147054037,\n",
       "   1.3016735219955444,\n",
       "   1.3024899339675904,\n",
       "   1.279579153060913,\n",
       "   1.2847012011210124,\n",
       "   1.2782184465726216,\n",
       "   1.2953647247950235,\n",
       "   1.290355130036672,\n",
       "   1.268283536831538,\n",
       "   1.2864303692181904,\n",
       "   1.2804109998544058,\n",
       "   1.2635193498929342,\n",
       "   1.2788482411702473,\n",
       "   1.2794405428568523,\n",
       "   1.274326229095459,\n",
       "   1.2583651145299275,\n",
       "   1.2762661345799764,\n",
       "   1.2578219048182169,\n",
       "   1.2482620747884114,\n",
       "   1.2552852908770242,\n",
       "   1.2507553211847942,\n",
       "   1.24133443236351,\n",
       "   1.2445635747909547,\n",
       "   1.2438773584365845,\n",
       "   1.2640845902760824,\n",
       "   1.2665600315729777,\n",
       "   1.2434406113624572,\n",
       "   1.2264581779638926,\n",
       "   1.246945999066035,\n",
       "   1.2349410859743755,\n",
       "   1.2387561734517416,\n",
       "   1.2245617592334748,\n",
       "   1.2314104231198628,\n",
       "   1.2353096803029378,\n",
       "   1.251856857140859,\n",
       "   1.2359461204210918,\n",
       "   1.2375411335627238,\n",
       "   1.2477174905935924,\n",
       "   1.2399725166956583,\n",
       "   1.2385031720002493,\n",
       "   1.2416342043876647,\n",
       "   1.2287102059523265,\n",
       "   1.248939692179362,\n",
       "   1.2241936167081198,\n",
       "   1.2288015592098236,\n",
       "   1.2490050276120503,\n",
       "   1.247981745004654,\n",
       "   1.22911221464475,\n",
       "   1.259045092264811,\n",
       "   1.2238044051329295,\n",
       "   1.2434468960762024,\n",
       "   1.2350315996011099,\n",
       "   1.2327227767308553,\n",
       "   1.2359634701410929,\n",
       "   1.2248387145996094,\n",
       "   1.2266197466850282,\n",
       "   1.236441603899002,\n",
       "   1.2270518891016642,\n",
       "   1.2346954540411632,\n",
       "   1.2142287468910218,\n",
       "   1.2263327197233835,\n",
       "   1.2177448344230652,\n",
       "   1.252651938199997,\n",
       "   1.2180018325646718,\n",
       "   1.227766762971878,\n",
       "   1.2457132863998412,\n",
       "   1.2207239588101706,\n",
       "   1.2386607086658479,\n",
       "   1.2372861961523691,\n",
       "   1.2197141571839651,\n",
       "   1.2535343782107036,\n",
       "   1.2409058614571888,\n",
       "   1.2223083829879762,\n",
       "   1.2471735453605652,\n",
       "   1.2317876438299815,\n",
       "   1.2272677703698476,\n",
       "   1.2340474196275075,\n",
       "   1.2520764621098837,\n",
       "   1.2409818228085836,\n",
       "   1.2267267787456513,\n",
       "   1.2340124841531117,\n",
       "   1.247551896572113,\n",
       "   1.2092069300015766,\n",
       "   1.2345958499113718,\n",
       "   1.2243945542971293,\n",
       "   1.226966172059377,\n",
       "   1.2333331314722698,\n",
       "   1.2263166789213817,\n",
       "   1.2086418199539184,\n",
       "   1.2224453576405843,\n",
       "   1.2300163257122039,\n",
       "   1.2548588514328003,\n",
       "   1.2323463308811187,\n",
       "   1.242492240667343,\n",
       "   1.23200736562411,\n",
       "   1.246898940006892,\n",
       "   1.2492689808209738,\n",
       "   1.2371675646305085,\n",
       "   1.243919161160787,\n",
       "   1.2260397187868755,\n",
       "   1.2423175001144409,\n",
       "   1.276805213689804,\n",
       "   1.2284063478310903,\n",
       "   1.2523314873377482,\n",
       "   1.2444554090499877,\n",
       "   1.27312997341156,\n",
       "   1.2456596871217092,\n",
       "   1.2528362941741944,\n",
       "   1.2566611178716023,\n",
       "   1.2473744761943817,\n",
       "   1.2220812074343363],\n",
       "  'val_loss_std': [0.04074155998748468,\n",
       "   0.0483766956284729,\n",
       "   0.05381249387443052,\n",
       "   0.06012777739493711,\n",
       "   0.06075902298574891,\n",
       "   0.06706227936325107,\n",
       "   0.07329684349614869,\n",
       "   0.07681986033276217,\n",
       "   0.07956859297465216,\n",
       "   0.08279304535362922,\n",
       "   0.08711080797968909,\n",
       "   0.08536047911383073,\n",
       "   0.09046749991441273,\n",
       "   0.08608389745433676,\n",
       "   0.09165949111876047,\n",
       "   0.09265573828543856,\n",
       "   0.09866415303872195,\n",
       "   0.09469666702601169,\n",
       "   0.0976931710129081,\n",
       "   0.09996239149598965,\n",
       "   0.09598494549731917,\n",
       "   0.09471939796193476,\n",
       "   0.10084761059226054,\n",
       "   0.09799903231598256,\n",
       "   0.09918006374792325,\n",
       "   0.10625478107095244,\n",
       "   0.09904456114145535,\n",
       "   0.10015887519423952,\n",
       "   0.10148041226787934,\n",
       "   0.10046723812109522,\n",
       "   0.1026697561430162,\n",
       "   0.10677738271218569,\n",
       "   0.1027718877614228,\n",
       "   0.11139962349411744,\n",
       "   0.10605043122776456,\n",
       "   0.11219092806714977,\n",
       "   0.1086103630407464,\n",
       "   0.11018187647188367,\n",
       "   0.11432456952955122,\n",
       "   0.11218107534737355,\n",
       "   0.10949810533941512,\n",
       "   0.11331187350189939,\n",
       "   0.11207341581545073,\n",
       "   0.1126191626810924,\n",
       "   0.11721046699106018,\n",
       "   0.11209157822640303,\n",
       "   0.11766738999375291,\n",
       "   0.11643274745276423,\n",
       "   0.10719650725887114,\n",
       "   0.1191746823086576,\n",
       "   0.11932034804788501,\n",
       "   0.1220059248862573,\n",
       "   0.1122220715218425,\n",
       "   0.11541443612096243,\n",
       "   0.11571251321465588,\n",
       "   0.11601547243577257,\n",
       "   0.11590503570047408,\n",
       "   0.11033994717371856,\n",
       "   0.12123153788926062,\n",
       "   0.11768756664910733,\n",
       "   0.11591384821151715,\n",
       "   0.11600414702388151,\n",
       "   0.12010926341116587,\n",
       "   0.12135588423047287,\n",
       "   0.11857358867024086,\n",
       "   0.12178704851807286,\n",
       "   0.11837751544096584,\n",
       "   0.115989463641455,\n",
       "   0.11976497409460625,\n",
       "   0.125877275772161,\n",
       "   0.11687507407880646,\n",
       "   0.11844072602425973,\n",
       "   0.12225381223369763,\n",
       "   0.12135298875240136,\n",
       "   0.1275328347118676,\n",
       "   0.11828403562147677,\n",
       "   0.11874656895489014,\n",
       "   0.12045150772743823,\n",
       "   0.12420688143833146,\n",
       "   0.12605127931640286,\n",
       "   0.12624069328225046,\n",
       "   0.1224910532112027,\n",
       "   0.12782695392291743,\n",
       "   0.1286374475112951,\n",
       "   0.12584611203994464,\n",
       "   0.1175379537859122,\n",
       "   0.12703483666089183,\n",
       "   0.1223590081626362,\n",
       "   0.12757302379930596,\n",
       "   0.1225644005847149,\n",
       "   0.11981542852918198,\n",
       "   0.12145667800033373,\n",
       "   0.12492480702983955,\n",
       "   0.12510510449905055,\n",
       "   0.1240809939482231,\n",
       "   0.12192385904077532,\n",
       "   0.12319234169733134,\n",
       "   0.1262749813879971,\n",
       "   0.12219687282049216,\n",
       "   0.12546660749909183,\n",
       "   0.1254935342913889,\n",
       "   0.12319168908242062,\n",
       "   0.12520233534420872,\n",
       "   0.11753593585187888,\n",
       "   0.12934942831944587,\n",
       "   0.12367470412616675,\n",
       "   0.12097362969942968,\n",
       "   0.12413100921303248,\n",
       "   0.12553611363718298,\n",
       "   0.12727683896352512,\n",
       "   0.1214608279739341,\n",
       "   0.13318430562167968,\n",
       "   0.12618798644315513,\n",
       "   0.13599399373617338,\n",
       "   0.13557528856583048,\n",
       "   0.13354429403069457,\n",
       "   0.13018167487099624,\n",
       "   0.13405084567367836,\n",
       "   0.1270067138599317,\n",
       "   0.1278549918789463,\n",
       "   0.1337261989854272,\n",
       "   0.12222427945536714,\n",
       "   0.12345126429295815,\n",
       "   0.13092572963060684,\n",
       "   0.12516772154435427,\n",
       "   0.12663164198887567,\n",
       "   0.13056168342771737,\n",
       "   0.13049811090551278,\n",
       "   0.13961864190000994,\n",
       "   0.13241770570593184,\n",
       "   0.136223338527556,\n",
       "   0.12560042785850192,\n",
       "   0.13300131678180674,\n",
       "   0.13252573147018368,\n",
       "   0.1340136782857201,\n",
       "   0.1324058651889505,\n",
       "   0.13586510346571073,\n",
       "   0.13695081283572255,\n",
       "   0.1361350003121348,\n",
       "   0.12521834363389456,\n",
       "   0.12827143987949025,\n",
       "   0.13336657704521193,\n",
       "   0.132774310048588,\n",
       "   0.13143806597393642,\n",
       "   0.1305681530938398,\n",
       "   0.1292270990868265,\n",
       "   0.13273710677737072,\n",
       "   0.12546363015843537],\n",
       "  'val_accuracy_mean': [0.27615555514891943,\n",
       "   0.2996000000834465,\n",
       "   0.3094444441795349,\n",
       "   0.3126888888080915,\n",
       "   0.3285111113389333,\n",
       "   0.335244444111983,\n",
       "   0.3553333334128062,\n",
       "   0.3630444441239039,\n",
       "   0.37211111108462014,\n",
       "   0.3866222228606542,\n",
       "   0.3945555551846822,\n",
       "   0.39917777756849926,\n",
       "   0.40388888875643414,\n",
       "   0.4086888891458511,\n",
       "   0.40786666711171465,\n",
       "   0.4172444434960683,\n",
       "   0.4263999996582667,\n",
       "   0.4251111110051473,\n",
       "   0.4232666669289271,\n",
       "   0.42439999918142957,\n",
       "   0.43428888897101087,\n",
       "   0.43075555622577666,\n",
       "   0.442822221716245,\n",
       "   0.4465111098686854,\n",
       "   0.4328222237030665,\n",
       "   0.44255555391311646,\n",
       "   0.44573333104451496,\n",
       "   0.444799998998642,\n",
       "   0.4495555559794108,\n",
       "   0.45226666688919065,\n",
       "   0.45515555640061695,\n",
       "   0.4565777776638667,\n",
       "   0.4615777748823166,\n",
       "   0.4601555550098419,\n",
       "   0.4620444430907567,\n",
       "   0.46037777662277224,\n",
       "   0.466977777282397,\n",
       "   0.4680666667222977,\n",
       "   0.4676888877153397,\n",
       "   0.46922222117582957,\n",
       "   0.47431110858917236,\n",
       "   0.47284444252649943,\n",
       "   0.48042222241560617,\n",
       "   0.4733777775367101,\n",
       "   0.473977777560552,\n",
       "   0.4822444432973862,\n",
       "   0.4783777765432994,\n",
       "   0.4791999995708466,\n",
       "   0.48793333053588867,\n",
       "   0.480333333214124,\n",
       "   0.48215555210908256,\n",
       "   0.4844888877868652,\n",
       "   0.4890666667620341,\n",
       "   0.4820666656891505,\n",
       "   0.49106666723887127,\n",
       "   0.493311109940211,\n",
       "   0.49286666492621106,\n",
       "   0.49311111013094583,\n",
       "   0.4991111085812251,\n",
       "   0.4937333323558172,\n",
       "   0.496888888279597,\n",
       "   0.49088888585567475,\n",
       "   0.48744444410006205,\n",
       "   0.49911110937595365,\n",
       "   0.5014666666587194,\n",
       "   0.4972444425026576,\n",
       "   0.5023333313067754,\n",
       "   0.49820000052452085,\n",
       "   0.504577776392301,\n",
       "   0.5051777764161428,\n",
       "   0.49864444375038147,\n",
       "   0.49204444309075673,\n",
       "   0.5001999980211258,\n",
       "   0.5043111101786295,\n",
       "   0.4992444431781769,\n",
       "   0.5021111090977987,\n",
       "   0.5007999982436498,\n",
       "   0.4986666643619537,\n",
       "   0.5045555563767751,\n",
       "   0.4992666655778885,\n",
       "   0.5078888871272405,\n",
       "   0.5048666658004125,\n",
       "   0.4991555547714233,\n",
       "   0.5016444422801336,\n",
       "   0.5078888879219691,\n",
       "   0.49333333373069765,\n",
       "   0.5100666666030884,\n",
       "   0.5017555544773737,\n",
       "   0.5056222192446391,\n",
       "   0.5002666650215785,\n",
       "   0.5011999974648158,\n",
       "   0.5066444430748621,\n",
       "   0.5072888892889023,\n",
       "   0.4984444433450699,\n",
       "   0.5057333332300186,\n",
       "   0.5030444419384003,\n",
       "   0.5136444435516994,\n",
       "   0.508222220937411,\n",
       "   0.5113555552562078,\n",
       "   0.4962888878583908,\n",
       "   0.5120666639010112,\n",
       "   0.5047111082077026,\n",
       "   0.5043111105759939,\n",
       "   0.5063111086686453,\n",
       "   0.5045999974012375,\n",
       "   0.5059333342313767,\n",
       "   0.5052444422245026,\n",
       "   0.4963999992609024,\n",
       "   0.503977777560552,\n",
       "   0.5079777763287227,\n",
       "   0.4993111095825831,\n",
       "   0.5062000002463659,\n",
       "   0.5056666664282481,\n",
       "   0.5111777764558793,\n",
       "   0.5009111096461614,\n",
       "   0.5033555547396342,\n",
       "   0.5089333323637645,\n",
       "   0.5061555544535319,\n",
       "   0.4980888867378235,\n",
       "   0.5110222206513086,\n",
       "   0.507666667898496,\n",
       "   0.5075999987125397,\n",
       "   0.5026666635274887,\n",
       "   0.5062222210566203,\n",
       "   0.5069333332777023,\n",
       "   0.5149999976158142,\n",
       "   0.5076444439093272,\n",
       "   0.508266664147377,\n",
       "   0.5073777749141057,\n",
       "   0.5109333330392838,\n",
       "   0.5037111093600591,\n",
       "   0.5044444429874421,\n",
       "   0.501599998275439,\n",
       "   0.5031333321332931,\n",
       "   0.5072888867060343,\n",
       "   0.505222218632698,\n",
       "   0.5112444436550141,\n",
       "   0.5073999987045924,\n",
       "   0.4960222212473551,\n",
       "   0.5069333336750667,\n",
       "   0.4989555561542511,\n",
       "   0.5043999967972438,\n",
       "   0.5035333323478699,\n",
       "   0.5045999987920126,\n",
       "   0.5027555551131566,\n",
       "   0.49977777640024823,\n",
       "   0.5063333306709925,\n",
       "   0.5128444451093673],\n",
       "  'val_accuracy_std': [0.034479702202173966,\n",
       "   0.03673714381410219,\n",
       "   0.03835780290187463,\n",
       "   0.04029832641928901,\n",
       "   0.041312784683485274,\n",
       "   0.04131244474942229,\n",
       "   0.04541332925983555,\n",
       "   0.04615422877036368,\n",
       "   0.04611807149330246,\n",
       "   0.044843738947096416,\n",
       "   0.047019959362475036,\n",
       "   0.04640232448298967,\n",
       "   0.04898286310218608,\n",
       "   0.046389489392906595,\n",
       "   0.04734003137412382,\n",
       "   0.04916148228063835,\n",
       "   0.05121272959957578,\n",
       "   0.05008979593736317,\n",
       "   0.04988649374296687,\n",
       "   0.05151960266878505,\n",
       "   0.05058082814288116,\n",
       "   0.0525922768940457,\n",
       "   0.053603116264585075,\n",
       "   0.05363157763460924,\n",
       "   0.04916598762621819,\n",
       "   0.053956656054538414,\n",
       "   0.050492564442395395,\n",
       "   0.05428386242850527,\n",
       "   0.05192682338139348,\n",
       "   0.05111978574442964,\n",
       "   0.05277848445837143,\n",
       "   0.054800574493154355,\n",
       "   0.053532538800371426,\n",
       "   0.054515151675390235,\n",
       "   0.0539791267312936,\n",
       "   0.05251389500946291,\n",
       "   0.05420994356520428,\n",
       "   0.05173618245728176,\n",
       "   0.055933751532575966,\n",
       "   0.05430014812150145,\n",
       "   0.0533840681643052,\n",
       "   0.05202191026656392,\n",
       "   0.052742978791345756,\n",
       "   0.05324296066294008,\n",
       "   0.054315534207756345,\n",
       "   0.05469984905934048,\n",
       "   0.057185514847906495,\n",
       "   0.05550666205779826,\n",
       "   0.05267604583670833,\n",
       "   0.055390531302922597,\n",
       "   0.053035466689983296,\n",
       "   0.05682981276276064,\n",
       "   0.05425880233496859,\n",
       "   0.053780995102247146,\n",
       "   0.054763222006924836,\n",
       "   0.05350458184033496,\n",
       "   0.05307720385439705,\n",
       "   0.051613472911589374,\n",
       "   0.0528459198810618,\n",
       "   0.05584491710310964,\n",
       "   0.053452182081328764,\n",
       "   0.05222127468188676,\n",
       "   0.05248691755722236,\n",
       "   0.053373132508849175,\n",
       "   0.05590531779248134,\n",
       "   0.05589374296289903,\n",
       "   0.05494744674530052,\n",
       "   0.053023586722813536,\n",
       "   0.05501184516610771,\n",
       "   0.05607076948677366,\n",
       "   0.052551345536757846,\n",
       "   0.05250155102336222,\n",
       "   0.05369354623475422,\n",
       "   0.05246169206770029,\n",
       "   0.056039862904729544,\n",
       "   0.05469296901643505,\n",
       "   0.054553649817787195,\n",
       "   0.05375389691666373,\n",
       "   0.05455600770181319,\n",
       "   0.05342474101316737,\n",
       "   0.053570884080261574,\n",
       "   0.054438044997503356,\n",
       "   0.054763040836784876,\n",
       "   0.05442595508240145,\n",
       "   0.054610291538227315,\n",
       "   0.05414520865890652,\n",
       "   0.055177114243511315,\n",
       "   0.055753022707664794,\n",
       "   0.052878807314222974,\n",
       "   0.053126711541797954,\n",
       "   0.05230859930539471,\n",
       "   0.05181519397176034,\n",
       "   0.055768550927893956,\n",
       "   0.05695829083922019,\n",
       "   0.0538545631144952,\n",
       "   0.054456146564165434,\n",
       "   0.053624685567681116,\n",
       "   0.054835331044573006,\n",
       "   0.052278592280253916,\n",
       "   0.05710878104604865,\n",
       "   0.05484884260582021,\n",
       "   0.05163143849712813,\n",
       "   0.05326460540821777,\n",
       "   0.053883474834804074,\n",
       "   0.05310879137128916,\n",
       "   0.05402999180066558,\n",
       "   0.05639063144175979,\n",
       "   0.0547851300451878,\n",
       "   0.05382921625974198,\n",
       "   0.05489234859809065,\n",
       "   0.04921351238393225,\n",
       "   0.05534542027629907,\n",
       "   0.054960927617797056,\n",
       "   0.0561375778732042,\n",
       "   0.05600515446397141,\n",
       "   0.05603239172440508,\n",
       "   0.051744843715715756,\n",
       "   0.05492450816142384,\n",
       "   0.05428777308968377,\n",
       "   0.05438674419035077,\n",
       "   0.053637672404295476,\n",
       "   0.052274375590763145,\n",
       "   0.05295700558444531,\n",
       "   0.05325364303028254,\n",
       "   0.054922868758833336,\n",
       "   0.054013373196108605,\n",
       "   0.056230406445885624,\n",
       "   0.05490560148014518,\n",
       "   0.0550145930964455,\n",
       "   0.05491963188375157,\n",
       "   0.05510062967853624,\n",
       "   0.054419041387010904,\n",
       "   0.05302650504434408,\n",
       "   0.05276534954594383,\n",
       "   0.05665671352585133,\n",
       "   0.053062388132578846,\n",
       "   0.05502800186125478,\n",
       "   0.05452402825389666,\n",
       "   0.05494602984047878,\n",
       "   0.056419749656828916,\n",
       "   0.05376373065217694,\n",
       "   0.056110103918094144,\n",
       "   0.051801945073861946,\n",
       "   0.05406535169244666,\n",
       "   0.05408326354902095,\n",
       "   0.05397072768213325,\n",
       "   0.0545917498298945,\n",
       "   0.05367582682706924],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb176",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb0c68",
   "metadata": {},
   "source": [
    "### 1.1 MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2a4a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d164b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     #print(key)\n",
    "#     if value.requires_grad:\n",
    "#         print(key)\n",
    "#         print(value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a599c8",
   "metadata": {},
   "source": [
    "### 1.2 Arbiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ebc67ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = arbiter_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = arbiter_system.state['best_epoch']\n",
    "\n",
    "state = arbiter_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "arbiter_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484a472",
   "metadata": {},
   "source": [
    "# 2. Data를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "569eeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0531d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a86b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "\n",
    "x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "\n",
    "\n",
    "x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task = next(zip(x_support_set,y_support_set,x_target_set, y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cdeb442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "647183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbiter_x_support_set, arbiter_x_target_set, arbiter_y_support_set, arbiter_y_target_set, seed = train_sample\n",
    "\n",
    "arbiter_x_support_set = torch.Tensor(arbiter_x_support_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_x_target_set = torch.Tensor(arbiter_x_target_set).float().to(device=arbiter_system.model.device)\n",
    "arbiter_y_support_set = torch.Tensor(arbiter_y_support_set).long().to(device=arbiter_system.model.device)\n",
    "arbiter_y_target_set = torch.Tensor(arbiter_y_target_set).long().to(device=arbiter_system.model.device)\n",
    "\n",
    "\n",
    "arbiter_x_support_set_task, arbiter_y_support_set_task, arbiter_x_target_set_task, arbiter_y_target_set_task = next(zip(arbiter_x_support_set,arbiter_y_support_set,arbiter_x_target_set, arbiter_y_target_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ce1c0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbiter_y_support_set_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd4d6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = arbiter_system.model.get_inner_loop_parameter_dict(arbiter_system.model.classifier.named_parameters())\n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = arbiter_x_target_set_task.shape\n",
    "\n",
    "arbiter_x_support_set_task = arbiter_x_support_set_task.view(-1, c, h, w)\n",
    "arbiter_y_support_set_task = arbiter_y_support_set_task.view(-1)\n",
    "arbiter_x_target_set_task = arbiter_x_target_set_task.view(-1, c, h, w)\n",
    "arbiter_y_target_set_task = arbiter_y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = arbiter_system.model.net_forward(\n",
    "            x=arbiter_x_support_set_task,\n",
    "            y=arbiter_y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "    \n",
    "    if arbiter_system.model.args.arbiter:\n",
    "        support_loss_grad = torch.autograd.grad(support_loss, names_weights_copy.values(),\n",
    "                                                retain_graph=True)\n",
    "\n",
    "        names_grads_copy = dict(zip(names_weights_copy.keys(), support_loss_grad))\n",
    "\n",
    "        per_step_task_embedding = []\n",
    "\n",
    "        for key, weight in names_weights_copy.items():\n",
    "            weight_norm = torch.norm(weight, p=2)\n",
    "            per_step_task_embedding.append(weight_norm)\n",
    "\n",
    "        for key, grad in names_grads_copy.items():\n",
    "            gradient_l2norm = torch.norm(grad, p=2)\n",
    "            per_step_task_embedding.append(gradient_l2norm)\n",
    "\n",
    "        per_step_task_embedding = torch.stack(per_step_task_embedding)\n",
    "\n",
    "        per_step_task_embedding = (per_step_task_embedding - per_step_task_embedding.mean()) / (\n",
    "                    per_step_task_embedding.std() + 1e-12)\n",
    "\n",
    "        generated_gradient_rate = arbiter_system.model.arbiter(per_step_task_embedding)\n",
    "\n",
    "        g = 0\n",
    "        for key in names_weights_copy.keys():\n",
    "            generated_alpha_params[key] = generated_gradient_rate[g]\n",
    "            g += 1\n",
    "\n",
    "    names_weights_copy,names_grads_copy = arbiter_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                      support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_arbiter.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=arbiter_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in arbiter_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d16650bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        \n",
    "num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "names_weights_copy = {\n",
    "    name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "        [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "    name, value in names_weights_copy.items()}\n",
    "\n",
    "n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "y_support_set_task = y_support_set_task.view(-1)\n",
    "x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "y_target_set_task = y_target_set_task.view(-1)\n",
    "\n",
    "# Inner-loop (Adaptation 과정을 수행한 후, loss function을 구해야하나?)\n",
    "num_steps=5\n",
    "for num_step in range(num_steps):            \n",
    "    support_loss, support_preds, support_loss_seperate, fetaure_map = maml_system.model.net_forward(\n",
    "            x=x_support_set_task,\n",
    "            y=y_support_set_task,\n",
    "            weights=names_weights_copy,\n",
    "            backup_running_statistics=num_step == 0,\n",
    "            training=True,\n",
    "            num_step=num_step,\n",
    "            training_phase='test',\n",
    "            epoch=0,\n",
    "        )\n",
    "\n",
    "    generated_alpha_params = {}\n",
    "\n",
    "\n",
    "    names_weights_copy,names_grads_copy = maml_system.model.apply_inner_loop_update(loss=support_loss,\n",
    "                                                                   support_loss_seperate=support_loss_seperate,\n",
    "                                                      names_weights_copy=names_weights_copy,\n",
    "                                                      alpha=generated_alpha_params,\n",
    "                                                      use_second_order=args_maml.second_order,\n",
    "                                                      current_step_idx=num_step,\n",
    "                                                      current_iter=maml_system.state['current_iter'],\n",
    "                                                      training_phase='test')\n",
    "\n",
    "\n",
    "for name, param in maml_system.model.classifier.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"norm_layer\" not in name:\n",
    "            param.data = names_weights_copy[name].squeeze().to(device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575454f0",
   "metadata": {},
   "source": [
    "## landscape 함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aec9618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape==  torch.Size([25, 3, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "tensor([[[[-1.0470e-03,  2.4492e-04,  5.1956e-04],\n",
      "          [-1.5472e-04,  8.6737e-04,  1.2578e-03],\n",
      "          [ 1.8392e-03,  5.3939e-04,  1.1473e-03]],\n",
      "\n",
      "         [[ 2.4967e-03,  4.1150e-04,  1.6876e-03],\n",
      "          [ 4.5193e-04,  4.2022e-04,  1.4201e-04],\n",
      "          [-9.1321e-04,  5.8100e-04,  7.5031e-04]],\n",
      "\n",
      "         [[ 1.8139e-03,  1.9078e-03,  1.2796e-03],\n",
      "          [ 1.6686e-03,  2.0005e-04,  1.5461e-03],\n",
      "          [ 1.6523e-03,  1.6434e-03,  1.1721e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3853e-03, -1.7529e-03, -1.7198e-03],\n",
      "          [-2.7772e-03, -2.2413e-03, -1.5451e-03],\n",
      "          [-2.1628e-03, -9.8456e-04, -8.4579e-04]],\n",
      "\n",
      "         [[ 1.5467e-03, -2.0402e-04, -2.9578e-04],\n",
      "          [ 5.7907e-04, -9.6326e-04, -1.4304e-03],\n",
      "          [-1.1161e-03, -3.5120e-04, -8.5586e-04]],\n",
      "\n",
      "         [[ 1.3277e-03,  1.6414e-03,  1.9287e-03],\n",
      "          [ 1.7993e-03,  1.6667e-03,  2.0753e-03],\n",
      "          [ 1.3622e-03,  1.3784e-03,  1.2772e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9687e-03,  4.4857e-03,  2.2451e-03],\n",
      "          [ 3.5948e-03,  1.7938e-03,  2.8535e-03],\n",
      "          [-3.7609e-05,  2.7372e-03,  9.4495e-04]],\n",
      "\n",
      "         [[ 2.4444e-03,  5.2587e-03,  2.9993e-03],\n",
      "          [ 1.2608e-03,  3.3743e-03, -5.1888e-04],\n",
      "          [ 2.1420e-03,  7.3916e-04, -1.0421e-03]],\n",
      "\n",
      "         [[ 2.7688e-03,  2.0234e-03,  1.3670e-03],\n",
      "          [-6.1668e-04,  2.9478e-03,  9.0013e-04],\n",
      "          [-1.4800e-03,  5.6207e-04, -2.5718e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6439e-04, -8.4628e-04, -2.1548e-03],\n",
      "          [-1.6585e-04, -1.0023e-03, -2.4515e-03],\n",
      "          [-2.1947e-04, -2.0562e-04, -2.1998e-03]],\n",
      "\n",
      "         [[ 8.5336e-04,  3.5615e-03,  2.1599e-03],\n",
      "          [-9.2467e-04,  9.9291e-04, -1.6397e-03],\n",
      "          [-1.9330e-03, -1.6481e-03, -3.1538e-03]],\n",
      "\n",
      "         [[ 1.6532e-03,  2.0440e-03,  2.0098e-03],\n",
      "          [ 1.1648e-03,  1.8869e-03,  2.3479e-03],\n",
      "          [ 8.1047e-04,  1.7862e-03,  1.6084e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.9670e-03, -2.6864e-03, -5.3401e-03],\n",
      "          [-2.6370e-03, -4.8922e-03, -4.5592e-03],\n",
      "          [-3.7261e-03, -4.6824e-03, -3.7404e-03]],\n",
      "\n",
      "         [[-1.1574e-03, -2.1400e-03, -8.7991e-04],\n",
      "          [-3.5325e-03, -2.4762e-03, -1.3352e-03],\n",
      "          [-2.3030e-03, -1.4141e-03, -2.6223e-03]],\n",
      "\n",
      "         [[-1.9003e-03, -1.6378e-03, -1.6627e-03],\n",
      "          [ 2.4618e-04, -1.9508e-03, -4.7758e-03],\n",
      "          [-2.5605e-03, -3.3368e-03, -1.9380e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5856e-03, -2.0689e-03, -3.1575e-03],\n",
      "          [-1.1051e-03, -3.5433e-03, -3.3841e-03],\n",
      "          [-2.2426e-03, -1.9396e-03, -8.5613e-04]],\n",
      "\n",
      "         [[-1.3972e-03, -4.8372e-04,  5.6506e-06],\n",
      "          [-1.0331e-03, -2.5309e-04,  8.1141e-04],\n",
      "          [-1.5326e-03, -1.1844e-03,  2.7278e-04]],\n",
      "\n",
      "         [[ 4.0499e-04,  6.3053e-05, -1.9324e-04],\n",
      "          [ 1.4300e-03,  7.5511e-04,  7.2609e-04],\n",
      "          [ 5.1608e-04,  1.2881e-03,  1.9987e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.9933e-03, -2.5510e-03, -2.7535e-03],\n",
      "          [ 5.2699e-04, -1.6406e-03, -1.6895e-03],\n",
      "          [-2.2646e-03, -3.3243e-03,  1.2481e-03]],\n",
      "\n",
      "         [[-2.6070e-03, -4.2614e-03, -5.1378e-03],\n",
      "          [-1.7954e-03, -1.4338e-03, -1.2458e-03],\n",
      "          [-1.9705e-03,  1.4173e-03, -9.1631e-04]],\n",
      "\n",
      "         [[-5.2009e-03, -3.1994e-03, -2.6602e-03],\n",
      "          [-2.2145e-03, -1.9100e-03, -9.8458e-05],\n",
      "          [ 2.4704e-03, -4.2814e-03, -8.0913e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6544e-04, -9.1625e-04,  6.8431e-04],\n",
      "          [ 2.1950e-03,  1.5667e-04, -1.4167e-03],\n",
      "          [ 1.6949e-03, -8.0798e-04, -2.1493e-03]],\n",
      "\n",
      "         [[-1.7418e-03,  1.4085e-03,  2.4884e-03],\n",
      "          [-1.5169e-03,  1.4883e-03,  4.0656e-03],\n",
      "          [ 4.4696e-04,  1.8282e-03,  2.0067e-03]],\n",
      "\n",
      "         [[-4.0823e-03, -3.3613e-03, -2.0348e-03],\n",
      "          [-1.8399e-03, -1.4406e-03, -1.0556e-03],\n",
      "          [-3.6562e-03, -2.0597e-03, -7.2092e-04]]],\n",
      "\n",
      "\n",
      "        [[[-6.7414e-04, -3.8785e-04,  1.9969e-04],\n",
      "          [-1.0424e-03,  1.5266e-04, -7.2555e-04],\n",
      "          [ 7.2048e-05,  7.2151e-05,  1.2062e-03]],\n",
      "\n",
      "         [[-1.0585e-03, -1.9119e-03, -3.6415e-04],\n",
      "          [-1.0453e-03, -6.5376e-04, -9.0487e-04],\n",
      "          [-2.1941e-03, -1.1921e-03, -1.2804e-03]],\n",
      "\n",
      "         [[-2.2767e-03, -6.6500e-05, -3.1024e-04],\n",
      "          [-1.8022e-03, -4.8665e-04,  4.4407e-05],\n",
      "          [-6.2351e-04, -4.5733e-04,  1.0523e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4570e-04,  3.0459e-04,  7.0008e-05],\n",
      "          [ 2.3372e-04,  4.7313e-04,  8.0607e-04],\n",
      "          [-4.2532e-04, -1.5534e-04,  9.3664e-04]],\n",
      "\n",
      "         [[-1.5514e-03, -9.1101e-04, -1.9643e-04],\n",
      "          [-6.9783e-04, -2.0778e-03, -7.7409e-04],\n",
      "          [ 1.4288e-04, -1.7572e-03, -1.0365e-04]],\n",
      "\n",
      "         [[ 1.7675e-03,  2.4776e-03,  2.5282e-03],\n",
      "          [ 2.1474e-03,  1.9858e-03,  2.3994e-03],\n",
      "          [ 2.3636e-03,  2.1199e-03,  2.6441e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.6875e-03,  4.8794e-03,  2.6436e-03],\n",
      "          [ 5.2839e-03,  4.6463e-03,  4.7379e-03],\n",
      "          [ 4.0662e-03,  3.9129e-03,  3.8167e-03]],\n",
      "\n",
      "         [[ 2.9796e-03,  1.7852e-03,  1.8088e-03],\n",
      "          [ 1.9168e-03,  2.1400e-03,  1.9041e-03],\n",
      "          [ 4.0303e-03,  4.0362e-03,  2.7118e-03]],\n",
      "\n",
      "         [[ 2.2258e-03,  4.2170e-03,  1.3913e-03],\n",
      "          [ 4.1832e-03,  3.4397e-03,  3.3412e-03],\n",
      "          [ 2.8257e-03,  2.8222e-03,  1.9015e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.8970e-03,  5.5161e-03,  5.7858e-03],\n",
      "          [ 6.1011e-03,  5.8433e-03,  7.1934e-03],\n",
      "          [ 3.7976e-03,  5.8762e-03,  7.2405e-03]],\n",
      "\n",
      "         [[ 1.7358e-03, -1.2321e-04, -1.1081e-03],\n",
      "          [ 1.1354e-03,  1.1493e-03,  1.2355e-03],\n",
      "          [-2.1404e-03, -1.3290e-04,  2.7090e-04]],\n",
      "\n",
      "         [[-4.8023e-04, -8.3382e-04, -1.0741e-03],\n",
      "          [ 8.6281e-04, -5.5860e-04, -6.1926e-04],\n",
      "          [-6.7408e-04, -8.6509e-04, -7.8526e-04]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 4.9477e-10, -8.1491e-10, -6.4028e-10,  5.8208e-11,  6.4028e-10,\n",
      "         1.4552e-10,  1.0477e-09,  4.6566e-10,  5.8208e-11, -6.4028e-10,\n",
      "        -1.1642e-10, -6.9849e-10,  9.8953e-10, -6.9849e-10,  6.4028e-10,\n",
      "         6.9849e-10,  8.1491e-10, -2.3283e-10,  2.0373e-10,  4.6566e-10,\n",
      "        -3.4925e-10,  7.5670e-10,  2.3283e-10,  1.1642e-10,  5.8208e-10,\n",
      "         0.0000e+00,  1.1642e-10,  2.9104e-10, -1.1642e-10,  1.7462e-10,\n",
      "        -4.0745e-10, -1.2224e-09,  2.3283e-10,  5.8208e-11, -1.1642e-10,\n",
      "        -4.0745e-10,  9.8953e-10,  4.6566e-10, -4.0745e-10,  8.7311e-11,\n",
      "        -1.1642e-10, -8.7311e-11,  8.1491e-10, -4.0745e-10, -3.7835e-10,\n",
      "         1.1642e-10,  1.7462e-10, -1.4552e-10, -6.9849e-10,  5.8208e-11,\n",
      "         2.6193e-10, -1.4552e-10,  1.4552e-10,  1.7462e-10, -1.4552e-11,\n",
      "        -2.9104e-10,  0.0000e+00, -1.0477e-09, -1.1642e-10,  5.8208e-11,\n",
      "        -2.6193e-10, -2.3283e-10, -5.8208e-11, -4.6566e-10, -7.5670e-10,\n",
      "        -4.6566e-10, -3.4925e-10,  0.0000e+00, -1.7462e-10, -9.3132e-10,\n",
      "        -4.0745e-10, -2.3283e-10, -4.6566e-10,  1.1642e-10, -1.1642e-10,\n",
      "        -9.3132e-10,  5.6025e-10,  5.8208e-10, -1.7462e-10,  6.1846e-11,\n",
      "        -2.3283e-10,  5.2387e-10,  0.0000e+00,  2.3283e-10, -3.7107e-10,\n",
      "         0.0000e+00,  3.4925e-10,  8.1491e-10, -2.9104e-10,  2.9104e-10,\n",
      "        -4.6566e-10,  2.3283e-10,  0.0000e+00, -5.8208e-11, -2.0373e-10,\n",
      "         7.2760e-11, -6.4028e-10,  6.4028e-10,  1.1642e-10,  1.1642e-10,\n",
      "         8.1491e-10, -6.4028e-10,  2.6193e-10,  6.9849e-10, -3.4925e-10,\n",
      "         5.8208e-11,  5.5297e-10,  0.0000e+00,  0.0000e+00, -5.2387e-10,\n",
      "         8.7311e-11,  1.1642e-10,  5.8208e-11, -2.0373e-10,  5.8208e-10,\n",
      "         5.5297e-10,  9.8953e-10,  7.2760e-11, -5.8208e-11, -6.1118e-10,\n",
      "         1.1642e-10,  5.8208e-11, -5.8208e-11,  2.3283e-10,  8.7311e-11,\n",
      "         0.0000e+00, -4.6566e-10, -4.3656e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 8.1226e-05,  1.1114e-02, -6.8242e-05, -2.6560e-03, -9.1189e-04,\n",
      "        -6.5994e-04,  7.6086e-03, -6.2664e-04, -1.7046e-05,  4.8045e-03,\n",
      "         1.5743e-03, -5.0233e-03,  2.7761e-03, -2.9537e-03,  1.4017e-03,\n",
      "        -1.5154e-03, -4.3699e-03,  1.9420e-03, -2.6958e-03,  1.6286e-04,\n",
      "        -1.0515e-03, -2.2993e-05,  3.5536e-03,  3.9239e-03, -1.0234e-02,\n",
      "        -1.7458e-03,  1.8906e-03, -3.7707e-03, -2.9531e-03,  1.3618e-03,\n",
      "         1.8621e-03,  2.9693e-03, -5.8099e-03,  9.1541e-03,  1.2448e-03,\n",
      "        -3.3346e-03,  1.9832e-04, -2.2788e-03, -9.7870e-04,  1.6984e-03,\n",
      "        -2.0495e-03,  8.3194e-04, -1.5183e-03,  3.5986e-03,  2.5769e-03,\n",
      "         3.0377e-04, -4.2716e-05, -2.6538e-03,  3.2966e-03, -4.8980e-03,\n",
      "         4.8676e-04, -1.1530e-04, -6.2022e-04,  5.2602e-03,  8.6146e-04,\n",
      "        -2.9092e-05,  3.5676e-03, -1.2784e-03, -5.2355e-03,  4.7900e-03,\n",
      "        -7.2753e-03,  1.4321e-03,  1.1011e-03, -1.5149e-03, -2.0646e-03,\n",
      "         3.0377e-03,  2.9323e-04,  2.2361e-03,  2.8125e-03, -9.8942e-03,\n",
      "         4.3553e-03, -2.4239e-03,  1.3575e-03,  2.1173e-03, -2.4038e-03,\n",
      "         1.6929e-03,  3.4865e-03,  3.2123e-03,  2.2770e-05, -3.5277e-03,\n",
      "        -3.2821e-03,  5.4901e-03,  3.0669e-03, -1.7944e-03, -1.8046e-03,\n",
      "         4.3444e-03,  1.0795e-03,  1.1268e-03, -1.2963e-03,  6.6455e-03,\n",
      "         3.0085e-04, -9.9359e-05,  1.5986e-03, -2.1394e-03,  6.4388e-03,\n",
      "         5.0791e-03, -1.1744e-03,  4.7119e-03, -4.6933e-04,  1.1169e-03,\n",
      "         1.2219e-03, -9.3574e-03, -6.8527e-03,  7.3493e-04,  6.2214e-04,\n",
      "         5.3682e-03, -8.2270e-04,  2.4535e-03,  3.7668e-03, -9.4693e-05,\n",
      "        -2.7732e-04, -3.0779e-03, -3.2997e-03, -6.1201e-03,  4.1123e-03,\n",
      "         7.3712e-03, -2.5665e-03, -2.3582e-03, -1.4000e-03, -1.0396e-02,\n",
      "         1.1333e-03,  8.2739e-04,  3.4347e-03,  2.1852e-03, -2.5649e-03,\n",
      "        -4.2730e-03, -2.9637e-03, -5.7373e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[[[-1.1509e-03, -1.9602e-03, -1.3901e-03],\n",
      "          [-2.4847e-03, -8.4556e-04, -7.6122e-04],\n",
      "          [-1.9314e-03, -1.3379e-03,  1.6994e-03]],\n",
      "\n",
      "         [[ 2.7347e-03,  1.8074e-03,  6.7978e-04],\n",
      "          [ 1.4561e-03, -1.5492e-04, -1.2227e-03],\n",
      "          [ 1.3077e-03,  3.0402e-04, -3.4288e-06]],\n",
      "\n",
      "         [[-2.7937e-04,  2.0764e-04, -7.5533e-04],\n",
      "          [ 8.4946e-04,  5.8945e-04,  6.8064e-04],\n",
      "          [ 6.3220e-04, -9.8194e-04, -1.0827e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5725e-05, -5.6081e-04,  1.5036e-04],\n",
      "          [-1.8801e-03, -2.8955e-03, -3.4405e-05],\n",
      "          [ 1.2403e-03, -3.3391e-03,  2.3072e-03]],\n",
      "\n",
      "         [[-3.2735e-04, -1.4440e-03, -1.6212e-03],\n",
      "          [-4.5349e-04, -3.8692e-04, -8.0733e-04],\n",
      "          [-3.2338e-04,  9.2647e-04, -1.0031e-03]],\n",
      "\n",
      "         [[ 2.2439e-03, -9.8575e-04,  3.6027e-03],\n",
      "          [-3.2319e-04,  5.2198e-04,  1.6117e-03],\n",
      "          [ 2.1532e-03,  1.3828e-03,  2.2535e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.0845e-03, -9.3805e-04, -7.8042e-04],\n",
      "          [-9.4796e-04, -1.0192e-04, -7.8393e-04],\n",
      "          [-3.0314e-03, -2.2051e-03, -6.7724e-04]],\n",
      "\n",
      "         [[ 1.1861e-03,  1.2409e-03,  1.0198e-03],\n",
      "          [-1.5794e-03, -9.4160e-04, -7.7253e-04],\n",
      "          [-2.0799e-03, -1.4045e-03, -1.6433e-03]],\n",
      "\n",
      "         [[ 7.2787e-04,  1.0336e-03,  1.0893e-03],\n",
      "          [ 1.2573e-03,  2.0593e-03,  2.3696e-04],\n",
      "          [ 2.2276e-03,  2.0009e-03, -4.1251e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9418e-03,  1.6125e-03,  5.6321e-04],\n",
      "          [ 1.8893e-03,  3.6623e-03,  3.3217e-03],\n",
      "          [ 1.6092e-03,  3.9732e-03,  2.6851e-03]],\n",
      "\n",
      "         [[-1.1484e-03,  5.2626e-05, -3.0900e-04],\n",
      "          [-1.4987e-03,  1.7330e-04, -2.3157e-04],\n",
      "          [ 1.0264e-05, -1.1929e-03, -1.4706e-03]],\n",
      "\n",
      "         [[-1.1653e-03,  7.4489e-04,  1.2092e-03],\n",
      "          [ 6.5438e-04, -7.1717e-04, -1.0524e-03],\n",
      "          [-2.6877e-04,  4.0407e-04, -9.7316e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.8158e-04,  1.0955e-04, -1.8550e-03],\n",
      "          [-1.9147e-03, -2.5305e-03, -1.7137e-03],\n",
      "          [-1.4639e-03, -2.4327e-03, -3.5422e-03]],\n",
      "\n",
      "         [[ 1.9181e-03,  2.4607e-03,  1.9107e-03],\n",
      "          [ 2.9169e-03,  1.7998e-03,  1.7617e-04],\n",
      "          [ 8.8838e-04, -1.3376e-04, -1.1008e-03]],\n",
      "\n",
      "         [[ 2.1443e-04, -3.6434e-04,  1.9482e-03],\n",
      "          [ 3.5368e-04,  4.5697e-04,  1.5377e-04],\n",
      "          [-1.2386e-03, -6.3938e-05,  1.4463e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.6050e-03,  3.7864e-03,  1.9268e-03],\n",
      "          [-7.2068e-04, -2.2287e-04,  4.3094e-04],\n",
      "          [ 2.7338e-03,  1.8851e-04,  4.0881e-04]],\n",
      "\n",
      "         [[-1.0638e-03, -1.8712e-03, -3.1464e-04],\n",
      "          [-3.5487e-03, -3.4098e-03, -1.9882e-04],\n",
      "          [-3.8436e-03, -2.0925e-03, -1.4958e-03]],\n",
      "\n",
      "         [[ 6.3406e-04,  1.3769e-03, -1.5052e-03],\n",
      "          [ 1.6278e-03,  1.5845e-03, -2.7785e-03],\n",
      "          [ 2.7061e-03,  6.3758e-04, -3.4725e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.7148e-03, -2.6517e-03, -6.1620e-04],\n",
      "          [-3.6370e-03, -2.9656e-03,  3.7476e-04],\n",
      "          [-1.1529e-03, -2.8513e-03,  3.4958e-03]],\n",
      "\n",
      "         [[-2.4228e-03, -4.3440e-03, -3.5272e-03],\n",
      "          [-5.6465e-03, -2.5549e-03, -2.3901e-03],\n",
      "          [-4.7434e-03, -3.9192e-03,  1.4952e-03]],\n",
      "\n",
      "         [[-9.4593e-05, -1.0977e-03,  2.1272e-03],\n",
      "          [ 3.4281e-04, -3.8430e-03, -6.4357e-03],\n",
      "          [-1.1203e-03, -8.9046e-05, -7.2948e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.8197e-04,  1.4682e-03,  1.2382e-03],\n",
      "          [-4.0396e-03, -2.4019e-03, -3.7017e-03],\n",
      "          [-1.7538e-03,  4.8563e-04,  8.0985e-04]],\n",
      "\n",
      "         [[-1.0550e-03,  1.6153e-03, -1.9088e-03],\n",
      "          [-1.0695e-03, -3.7081e-03, -2.7927e-03],\n",
      "          [-3.2961e-03, -8.5372e-04, -1.4522e-03]],\n",
      "\n",
      "         [[-3.5068e-03, -2.0775e-03,  6.3821e-04],\n",
      "          [-1.8578e-03,  1.2762e-03, -3.9275e-03],\n",
      "          [ 2.0783e-03, -3.2713e-03, -4.2136e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.7257e-03, -1.5530e-03, -1.4803e-03],\n",
      "          [-1.7186e-03, -2.1617e-03, -3.4449e-03],\n",
      "          [-3.0925e-03, -2.1973e-03, -2.8873e-03]],\n",
      "\n",
      "         [[ 1.0317e-03,  3.7003e-03,  1.6328e-03],\n",
      "          [ 2.3763e-04,  8.4255e-04,  1.9329e-03],\n",
      "          [-1.8086e-03,  1.5555e-03,  2.1450e-03]],\n",
      "\n",
      "         [[ 3.0727e-03,  2.2570e-03,  3.4877e-03],\n",
      "          [ 4.6506e-03,  3.5076e-03,  4.2520e-03],\n",
      "          [ 1.0967e-03,  2.4348e-04,  2.2846e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.1435e-03,  1.3453e-04, -1.2370e-03],\n",
      "          [ 4.0115e-03,  2.7901e-03,  1.8327e-03],\n",
      "          [ 4.1028e-03,  1.0894e-03, -2.0508e-03]],\n",
      "\n",
      "         [[-3.2902e-04,  3.9258e-04, -6.1980e-04],\n",
      "          [ 9.7023e-04,  6.5915e-04,  1.2131e-03],\n",
      "          [ 1.1946e-04,  4.2255e-04,  2.1819e-03]],\n",
      "\n",
      "         [[-2.8006e-03, -2.0911e-03,  1.0865e-04],\n",
      "          [-1.2316e-03,  2.2073e-04,  8.0767e-04],\n",
      "          [ 2.8689e-03,  1.6269e-03, -4.8918e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3127e-03,  3.6371e-03,  4.4780e-03],\n",
      "          [ 6.3680e-03,  7.4074e-03,  5.2322e-03],\n",
      "          [ 3.1447e-04,  9.7717e-03,  6.2962e-03]],\n",
      "\n",
      "         [[-4.8283e-03, -3.5234e-03,  1.4263e-03],\n",
      "          [-5.6445e-03,  4.2031e-04,  2.5429e-03],\n",
      "          [ 4.7809e-04,  2.7517e-03,  7.1795e-04]],\n",
      "\n",
      "         [[-3.9818e-03, -8.7953e-03, -7.7234e-03],\n",
      "          [-3.7462e-03, -6.3122e-03, -9.1985e-03],\n",
      "          [-5.2230e-03,  7.2812e-04, -5.2089e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1537e-02, -4.2770e-03, -1.1355e-02],\n",
      "          [-6.5393e-03,  1.6916e-03, -3.2237e-03],\n",
      "          [-3.4119e-03,  7.6402e-04, -5.6598e-03]],\n",
      "\n",
      "         [[-2.9176e-03,  1.4196e-03, -2.5406e-03],\n",
      "          [-2.5973e-03, -1.8371e-03,  3.8224e-03],\n",
      "          [-2.3488e-03,  5.9574e-03, -2.7214e-04]],\n",
      "\n",
      "         [[-4.3011e-03, -3.7809e-03, -3.0612e-04],\n",
      "          [-4.7971e-03, -2.5985e-03, -6.3738e-03],\n",
      "          [-1.0023e-02, -1.5587e-02, -5.2087e-03]]]], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 6.9849e-10,  1.1642e-10,  5.8208e-10,  2.3283e-10,  4.6566e-10,\n",
      "        -3.4925e-10,  3.4925e-10,  2.3283e-10, -1.3970e-09,  0.0000e+00,\n",
      "         8.7311e-11, -3.4925e-10,  1.2224e-09,  2.3283e-10,  1.4552e-10,\n",
      "        -9.3132e-10,  1.1642e-10,  1.7462e-10, -6.9849e-10,  2.0373e-10,\n",
      "         2.0373e-10, -1.1642e-09, -3.4925e-10,  5.8208e-10, -1.1642e-10,\n",
      "         1.1059e-09, -5.8208e-10,  1.2806e-09,  4.6566e-10,  6.9849e-10,\n",
      "         1.7462e-09, -7.5670e-10,  3.4925e-10,  1.0186e-10,  8.7311e-11,\n",
      "         4.6566e-10,  6.9849e-10, -5.5297e-10, -1.9791e-09,  4.6566e-10,\n",
      "         0.0000e+00,  4.3656e-10,  0.0000e+00, -2.3283e-10, -1.8626e-09,\n",
      "         6.9849e-10,  0.0000e+00, -1.8626e-09, -3.7835e-10,  9.3132e-10,\n",
      "         1.5425e-09, -3.4925e-10, -4.6566e-10,  5.2387e-10, -6.9849e-10,\n",
      "         8.7311e-11, -6.9849e-10,  1.9791e-09,  4.6566e-10,  4.6566e-10,\n",
      "         1.7462e-10,  6.9849e-10, -6.9849e-10, -8.7311e-10,  1.2806e-09,\n",
      "        -4.0745e-10, -1.1642e-10, -5.8208e-10,  1.1642e-10, -8.1491e-10,\n",
      "         4.6566e-10,  6.9849e-10, -2.6776e-09, -5.8208e-10, -2.3283e-10,\n",
      "        -4.0745e-10,  2.3283e-10,  5.2387e-10, -2.9104e-11,  5.8208e-11,\n",
      "        -2.3283e-10,  1.8626e-09,  2.3283e-10, -2.3283e-10,  2.3283e-10,\n",
      "         1.8626e-09,  2.6193e-10, -2.3283e-10, -2.3283e-10,  6.9849e-10,\n",
      "        -2.3283e-10, -1.1642e-10,  4.0745e-10, -6.9849e-10,  3.4925e-10,\n",
      "         4.6566e-10,  0.0000e+00, -1.7462e-10, -1.1642e-09,  1.2806e-09,\n",
      "         1.7462e-10,  2.9104e-10,  0.0000e+00,  1.7462e-10,  9.3132e-10,\n",
      "         4.6566e-10, -4.6566e-10, -1.0477e-09,  1.0768e-09,  6.9849e-10,\n",
      "         2.3283e-10,  2.3283e-10, -5.8208e-10, -2.3283e-10, -2.2119e-09,\n",
      "         5.2387e-10, -4.6566e-10, -2.9104e-10, -9.8953e-10, -5.8208e-10,\n",
      "         3.7835e-10,  0.0000e+00,  2.3283e-10,  2.3283e-10, -4.6566e-10,\n",
      "         2.0955e-09, -8.7311e-10, -6.9849e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 3.4925e-10,  2.9104e-10, -2.9104e-10,  0.0000e+00,  6.4028e-10,\n",
      "        -1.4552e-10,  1.6007e-09, -1.4552e-10,  6.9849e-10,  1.1642e-10,\n",
      "        -2.3283e-10, -1.1642e-10, -5.8208e-10,  0.0000e+00, -1.4552e-10,\n",
      "        -2.0373e-10, -2.3283e-10, -1.7462e-10,  2.9104e-10, -5.8208e-10,\n",
      "         5.8208e-11,  1.1642e-10,  5.8208e-10, -2.9104e-11,  5.8208e-11,\n",
      "        -2.6193e-10,  1.4552e-10, -5.8208e-11,  2.9104e-10,  0.0000e+00,\n",
      "        -2.9104e-10, -2.3283e-10, -1.7462e-10,  0.0000e+00,  0.0000e+00,\n",
      "        -1.2369e-10, -5.8208e-11, -5.8208e-11, -1.0477e-09, -2.3283e-10,\n",
      "         2.0373e-10,  8.7311e-10, -1.1642e-10, -7.2760e-11,  2.3283e-10,\n",
      "         5.8208e-11, -4.6566e-10,  5.8208e-11, -4.6566e-10,  1.3970e-09,\n",
      "         2.3283e-10,  5.8208e-10, -3.4925e-10,  1.7462e-09,  1.1642e-10,\n",
      "         0.0000e+00,  4.6566e-10,  2.6193e-10,  4.9477e-10,  0.0000e+00,\n",
      "         1.1642e-10,  0.0000e+00,  0.0000e+00,  2.6193e-10, -4.3656e-11,\n",
      "        -1.4552e-10, -7.2760e-11, -2.3283e-10,  1.7462e-10,  1.2806e-09,\n",
      "        -1.1642e-10,  1.1642e-10,  2.9104e-10, -3.4925e-10, -3.7835e-10,\n",
      "         5.8208e-10,  0.0000e+00,  1.7462e-10,  2.3283e-10, -2.9104e-10,\n",
      "         2.9104e-11,  1.0477e-09, -7.2760e-11,  5.8208e-11,  2.5466e-11,\n",
      "        -1.1059e-09, -7.2760e-10,  1.1933e-09,  3.4925e-10, -8.7311e-11,\n",
      "         1.3970e-09, -6.4028e-10,  4.9477e-10, -2.9831e-10, -8.1491e-10,\n",
      "        -1.1642e-09, -6.9849e-10,  5.8208e-10, -1.1642e-10, -3.4925e-10,\n",
      "        -9.3132e-10,  1.4552e-10, -8.7311e-11,  5.8208e-11, -1.7462e-10,\n",
      "        -2.9104e-11,  1.7462e-10,  0.0000e+00,  5.8208e-11, -5.8208e-11,\n",
      "         2.1828e-10, -1.8917e-10,  5.8208e-11,  1.4552e-10, -4.5111e-10,\n",
      "        -4.6566e-10, -8.7311e-11, -1.1642e-10,  2.3283e-10, -2.9104e-11,\n",
      "         0.0000e+00,  3.4925e-10,  2.6776e-09, -2.9104e-11, -8.7311e-11,\n",
      "         3.4925e-10,  1.1642e-10,  2.3283e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.2232e-04,  1.2817e-04,  2.9403e-04,  4.2408e-04,  1.8909e-05,\n",
      "        -4.0042e-06,  1.5330e-04,  3.2445e-04,  4.2816e-04,  1.2521e-04,\n",
      "        -1.4465e-05,  2.6969e-04,  1.2614e-04,  2.2323e-04, -1.3264e-05,\n",
      "         2.8215e-04, -7.8954e-05, -9.9249e-05,  5.1991e-04,  5.1950e-04,\n",
      "         1.0956e-04,  3.4070e-04,  5.1087e-04,  1.9436e-04,  5.3669e-04,\n",
      "        -2.7286e-04, -8.0193e-05,  4.4893e-05,  3.0302e-05,  2.0439e-04,\n",
      "         1.7944e-04,  2.9272e-04, -1.4615e-04,  3.2077e-04,  2.7465e-04,\n",
      "        -1.6188e-04,  6.0707e-05,  1.5834e-04,  4.3190e-05,  1.4109e-04,\n",
      "         5.0740e-05, -2.5492e-04, -2.1726e-04, -4.6410e-04,  2.0244e-04,\n",
      "        -3.6147e-04,  1.8157e-04,  3.6622e-04, -2.1704e-04, -2.3478e-04,\n",
      "         2.6045e-04, -1.5370e-04,  3.5552e-04,  2.5545e-04,  5.5995e-04,\n",
      "         5.0738e-05,  1.7345e-04,  4.1149e-04, -5.2276e-05,  5.8571e-04,\n",
      "        -3.5174e-04,  5.0504e-04,  1.3666e-04,  4.2777e-05,  1.6228e-05,\n",
      "         1.1181e-04,  8.5436e-05,  4.6496e-04,  1.0327e-04,  2.8142e-04,\n",
      "         9.7259e-05,  5.7284e-05,  3.8446e-04, -2.2814e-04, -7.6184e-05,\n",
      "         3.0491e-04,  1.6015e-04, -1.4677e-04, -4.5845e-04,  3.2040e-04,\n",
      "         4.5270e-04, -1.1648e-04,  1.8380e-04, -2.2805e-04, -1.3976e-04,\n",
      "         4.1423e-05,  1.9781e-04, -4.3220e-05, -1.7044e-04,  3.8109e-04,\n",
      "         3.4397e-05, -8.6116e-05,  3.2897e-04,  5.9569e-05,  2.1787e-04,\n",
      "        -4.9206e-05,  5.0088e-04, -3.6242e-05,  2.5387e-04, -7.6458e-05,\n",
      "         1.0150e-03,  2.8361e-04,  4.7841e-04,  3.2406e-04,  6.4686e-05,\n",
      "         2.5548e-04,  2.8245e-04,  4.9180e-04,  2.3291e-04,  2.7949e-04,\n",
      "        -2.3550e-04,  8.5778e-04,  8.5185e-05, -1.3196e-04, -4.0035e-05,\n",
      "        -2.0182e-04,  4.4558e-04,  2.2787e-04,  2.7291e-04,  5.1212e-04,\n",
      "         2.3113e-04,  2.1397e-04, -5.0320e-04,  1.4936e-04,  1.5921e-04,\n",
      "         3.5506e-05,  1.2073e-04,  1.1497e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.5752e-04,  1.5227e-04,  5.0188e-04,  1.9606e-04, -1.7024e-04,\n",
      "        -1.9858e-04,  1.2825e-05,  3.1175e-04,  2.9057e-04, -4.8331e-05,\n",
      "        -5.3951e-05,  1.1001e-04, -7.6828e-05,  1.3551e-04, -4.0729e-04,\n",
      "        -4.9940e-05,  5.0578e-06, -5.1023e-04, -4.7057e-05,  8.9739e-05,\n",
      "         8.5539e-06,  3.3164e-04,  4.5990e-04,  2.1391e-04,  3.5377e-04,\n",
      "        -2.7899e-04, -2.7621e-04,  1.7633e-04,  1.1856e-04, -5.5311e-05,\n",
      "         1.9375e-04,  1.1762e-05, -4.0369e-04,  3.6656e-04,  2.3664e-04,\n",
      "         9.1233e-05, -2.5184e-04, -2.3888e-05, -5.4639e-05, -7.4606e-05,\n",
      "        -6.2543e-04, -5.7516e-04, -2.9716e-04, -2.8128e-04, -7.3264e-04,\n",
      "        -4.7941e-04,  1.6231e-04,  4.0062e-04,  8.5974e-05, -1.4164e-04,\n",
      "         2.9948e-04, -5.0922e-05, -2.8296e-04,  1.4786e-04,  2.7671e-04,\n",
      "        -4.1200e-05,  1.7909e-04, -1.5926e-04, -4.1595e-05,  4.8331e-04,\n",
      "        -5.5703e-05,  8.0895e-05, -4.2472e-05, -6.2214e-05, -1.2784e-04,\n",
      "         1.1617e-04,  3.5068e-04,  2.9318e-04, -1.9488e-04,  3.1150e-04,\n",
      "         1.3540e-05, -1.4755e-06,  4.1656e-04, -5.8009e-05, -1.1866e-04,\n",
      "         2.7084e-04,  3.6181e-04, -4.0136e-04, -2.2502e-04,  2.9518e-04,\n",
      "         1.7883e-04, -7.4942e-05,  2.6404e-04, -5.1726e-05, -1.2554e-03,\n",
      "        -1.7441e-04, -6.4699e-05,  3.9238e-05, -3.1339e-04,  4.4723e-05,\n",
      "        -3.2901e-04, -1.5550e-04,  2.4311e-04, -3.8168e-05, -1.1170e-04,\n",
      "         1.6970e-05, -2.3828e-04, -3.2393e-04,  6.6024e-05,  2.6354e-04,\n",
      "         1.0794e-03, -2.6429e-04,  1.7981e-04,  7.1681e-05,  4.7315e-04,\n",
      "        -8.9448e-06,  1.2534e-04, -2.9018e-05,  3.0683e-04,  7.3840e-05,\n",
      "        -2.4626e-04,  6.9786e-04, -2.7200e-05, -3.0057e-04, -2.9324e-04,\n",
      "        -1.8859e-04,  5.4836e-04,  1.7593e-04,  3.3882e-04,  3.9948e-05,\n",
      "         1.0103e-04, -1.8224e-04, -4.7384e-04, -9.0316e-04, -3.3744e-04,\n",
      "        -1.1133e-05,  1.3717e-04, -1.4937e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 6.9122e-11,  1.8190e-11, -1.4552e-11,  3.6380e-11, -2.5466e-11,\n",
      "        -2.1828e-11, -2.1828e-11, -2.5466e-11, -1.0914e-11, -4.7294e-11,\n",
      "         4.3656e-11,  2.9104e-11,  3.6380e-11,  0.0000e+00,  2.9104e-11,\n",
      "        -2.1828e-11,  2.1828e-11, -2.3647e-11, -1.6371e-11, -2.0009e-11,\n",
      "        -6.5484e-11,  2.5466e-11,  1.4552e-11,  2.9104e-11,  2.9104e-11,\n",
      "        -3.2742e-11,  6.9122e-11,  7.2760e-11,  1.8190e-12,  5.4570e-11,\n",
      "        -6.5484e-11,  1.4552e-11,  2.9104e-11,  1.4552e-10, -3.8199e-11,\n",
      "         7.2760e-12,  3.2742e-11, -5.0932e-11,  7.2760e-12, -6.0027e-11,\n",
      "         2.9104e-11,  4.7294e-11,  7.2760e-12,  9.0949e-12, -4.3656e-11,\n",
      "         4.3656e-11, -3.0923e-11, -8.1855e-12, -4.1837e-11, -1.2733e-11,\n",
      "        -2.9104e-11, -5.8208e-11, -1.0914e-11,  6.1846e-11, -2.5466e-10,\n",
      "         2.2737e-11, -4.7294e-11,  6.9122e-11, -3.6380e-12, -1.8190e-11,\n",
      "         3.6380e-12, -6.0936e-11,  7.0941e-11,  2.9104e-11, -1.4552e-11,\n",
      "         3.6380e-11,  0.0000e+00,  3.6380e-12,  1.8190e-11,  1.4552e-11,\n",
      "         9.0949e-12,  3.2742e-11,  4.5475e-11, -1.2733e-11, -2.1828e-11,\n",
      "         5.4570e-11, -4.9113e-11, -1.2733e-11, -5.4570e-11, -7.2760e-12,\n",
      "         3.6380e-12,  1.0914e-11, -2.5466e-11,  4.0018e-11, -1.4552e-11,\n",
      "         4.0018e-11,  7.2760e-12,  9.2768e-11, -3.3651e-11, -6.5484e-11,\n",
      "         8.0036e-11, -2.9104e-11, -1.4552e-11,  2.9104e-11,  1.2187e-10,\n",
      "        -6.1846e-11, -5.0932e-11, -2.3647e-11, -2.9104e-11, -3.6380e-11,\n",
      "         0.0000e+00,  2.9104e-11,  1.4552e-11, -3.2742e-11,  4.5475e-11,\n",
      "         5.4570e-11, -5.8208e-11,  3.2742e-11,  4.1837e-11, -1.4552e-11,\n",
      "        -3.6380e-11, -7.2760e-12, -3.6380e-12,  2.7285e-11,  3.2742e-11,\n",
      "        -4.9113e-11,  6.5484e-11,  1.4552e-11, -3.6380e-12,  0.0000e+00,\n",
      "         0.0000e+00,  3.6380e-11, -2.1828e-11,  1.4552e-11,  0.0000e+00,\n",
      "        -1.8190e-12,  4.1837e-11,  1.0914e-11], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 6.1733e-04, -3.2700e-04, -1.1799e-04,  2.6064e-04,  3.6621e-04,\n",
      "        -1.6246e-04, -4.9313e-04,  8.1243e-04, -2.6842e-04,  5.3203e-04,\n",
      "         1.7215e-04, -3.6868e-05,  4.9019e-04,  3.5417e-04, -3.1704e-05,\n",
      "         1.0560e-04, -2.6646e-04,  9.0381e-05,  5.7140e-04,  4.0497e-04,\n",
      "        -2.8439e-04, -2.2332e-06,  2.6071e-05,  7.8791e-04, -8.1835e-05,\n",
      "         9.1952e-04,  1.7304e-04,  6.8330e-04,  5.5312e-04,  4.7758e-04,\n",
      "         2.4884e-04,  8.7539e-06,  4.8466e-04,  4.0953e-04,  2.6139e-04,\n",
      "        -5.9761e-04,  7.7978e-04, -1.1444e-04,  3.5033e-04,  4.1656e-04,\n",
      "         9.5969e-04,  5.0739e-04, -5.1835e-04,  4.4995e-04,  5.1104e-04,\n",
      "        -2.5978e-04,  7.8767e-05,  3.4365e-04, -1.5498e-04,  1.5520e-04,\n",
      "        -8.2198e-05, -2.8478e-04,  3.4831e-04,  1.0222e-04, -5.3145e-04,\n",
      "        -7.6512e-06,  5.8071e-05,  3.2810e-04, -1.1772e-04,  3.3855e-04,\n",
      "         6.0050e-04, -1.8071e-05,  3.0973e-04,  1.3774e-04, -4.0825e-04,\n",
      "        -2.7225e-05,  2.4747e-05,  5.2234e-04,  1.0471e-03,  1.5283e-04,\n",
      "        -2.3113e-04,  1.0221e-04,  1.0769e-04, -1.2796e-04,  2.9200e-04,\n",
      "         2.2161e-04,  3.9047e-04, -8.2911e-05, -3.1594e-04,  2.3467e-04,\n",
      "         1.6711e-04,  2.6015e-04,  8.1388e-04,  5.5878e-04,  2.9340e-04,\n",
      "         3.2098e-04,  3.1838e-04, -1.3455e-04,  5.1403e-04, -1.5623e-04,\n",
      "         3.5142e-04, -3.2701e-04,  6.5218e-04,  2.8803e-04,  5.5673e-04,\n",
      "        -6.7151e-05,  8.8034e-04, -4.7300e-04,  3.1559e-04,  2.9104e-04,\n",
      "         5.1684e-04,  3.1528e-04,  2.1195e-04, -5.1274e-04,  3.6118e-04,\n",
      "        -6.6256e-05, -2.0041e-04,  3.5334e-04,  5.0348e-04,  5.3190e-04,\n",
      "        -2.7718e-06,  6.4925e-04, -1.6618e-04,  1.7260e-04,  1.8192e-04,\n",
      "        -4.7610e-05, -9.3971e-05, -2.7887e-04, -1.2540e-04, -3.5778e-04,\n",
      "        -3.6929e-04,  4.2359e-04, -6.1002e-04,  9.1670e-05,  1.3441e-04,\n",
      "        -1.8012e-04,  5.3609e-05,  1.6020e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([ 1.9690e-04, -4.8823e-05, -5.7735e-04,  2.7854e-05,  1.6153e-04,\n",
      "        -2.8168e-04, -1.0263e-03,  6.4720e-04, -4.6542e-04,  1.5354e-04,\n",
      "         7.8667e-05,  8.8313e-04,  1.8173e-04,  6.5432e-04, -2.9023e-04,\n",
      "         7.7115e-05, -5.5422e-04,  2.3481e-04,  1.2353e-04,  1.1799e-04,\n",
      "        -3.0227e-05,  6.4834e-05, -1.7358e-04,  7.3744e-04,  1.0619e-06,\n",
      "         3.3529e-04, -2.5800e-04,  3.2936e-04,  5.1488e-04,  1.7796e-04,\n",
      "        -8.8433e-05, -2.9628e-04,  1.7761e-04,  6.0302e-04,  2.5234e-04,\n",
      "        -4.2978e-04,  8.6005e-04, -1.5407e-04,  1.1570e-04,  6.6087e-04,\n",
      "         7.3828e-04,  4.3783e-04, -1.4220e-03,  2.3361e-05,  3.4046e-04,\n",
      "        -7.4521e-05, -9.0576e-04,  2.7015e-04, -1.4422e-04, -3.1847e-04,\n",
      "        -2.1870e-04, -9.2429e-04,  3.1370e-04,  2.1025e-04, -8.3503e-04,\n",
      "        -4.2610e-05, -1.4351e-04, -6.8124e-05, -5.8142e-04, -2.9399e-05,\n",
      "         2.6924e-04,  1.0221e-04, -9.5272e-05, -2.7288e-04, -3.6940e-04,\n",
      "         2.1502e-05, -3.5033e-04,  3.6336e-04,  7.4390e-04,  3.9815e-04,\n",
      "        -3.3868e-04,  4.4936e-05, -8.8984e-05, -4.9283e-04, -2.5117e-04,\n",
      "         1.6979e-04,  4.1297e-04, -1.5987e-04, -3.1085e-04, -8.0730e-06,\n",
      "        -1.5599e-04, -5.6053e-05,  2.6983e-04, -1.4961e-04, -5.2475e-05,\n",
      "         7.7513e-04,  1.4751e-04, -3.6126e-04, -1.6552e-04, -3.4468e-04,\n",
      "         1.4244e-03, -5.0434e-04,  1.6345e-04, -8.3280e-05,  2.2995e-03,\n",
      "        -5.2531e-04,  5.5101e-04,  8.1822e-06,  8.0402e-04,  2.4451e-04,\n",
      "         2.1005e-04, -1.5341e-05, -3.6384e-04, -8.9878e-04,  2.5115e-04,\n",
      "        -3.9329e-04, -2.4021e-04,  7.0508e-04,  7.0659e-04,  1.2437e-04,\n",
      "        -3.1271e-04,  1.7243e-04, -3.5559e-05,  5.1651e-04, -1.2973e-04,\n",
      "        -3.8216e-04, -8.6332e-04, -1.6671e-04, -2.7067e-04, -3.1706e-04,\n",
      "        -1.6142e-04,  4.3809e-04, -6.3207e-04, -3.0955e-04,  2.9463e-04,\n",
      "        -4.5976e-04, -6.8403e-05,  1.2240e-04], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-1.4552e-11, -7.2760e-12,  2.0009e-11, -9.0949e-13,  3.6380e-12,\n",
      "        -2.8422e-13,  5.6843e-14, -6.8212e-13,  1.3642e-12, -1.7462e-10,\n",
      "         0.0000e+00,  1.7053e-13,  2.2737e-13,  7.2760e-12,  4.2633e-14,\n",
      "         2.3283e-10,  2.2737e-13,  1.4552e-11,  1.1369e-13, -4.5475e-13,\n",
      "         1.3642e-12, -1.1369e-13, -9.0949e-12,  1.1369e-13,  5.6843e-14,\n",
      "         0.0000e+00,  0.0000e+00,  1.7462e-10,  1.1369e-13,  1.1369e-13,\n",
      "        -1.7462e-10,  1.4552e-10, -2.2737e-12,  1.1369e-13, -4.0018e-11,\n",
      "        -1.1642e-10, -5.4570e-12, -3.6380e-11, -5.6843e-13,  1.0914e-11,\n",
      "        -7.2760e-12, -1.3642e-12, -1.4211e-12, -3.5527e-14, -3.2742e-11,\n",
      "         8.1491e-10, -1.7764e-13,  2.2737e-13, -6.8212e-13,  7.4607e-14,\n",
      "        -2.2555e-10, -1.0186e-10,  0.0000e+00, -7.0941e-11, -5.8208e-11,\n",
      "         2.2737e-13, -5.6843e-14,  3.1832e-12, -7.2760e-11, -4.2633e-14,\n",
      "        -1.8190e-11,  2.8422e-14,  1.7462e-10, -1.0914e-11,  6.9122e-11,\n",
      "         1.1369e-13,  7.1054e-14,  0.0000e+00,  5.8208e-11,  1.4552e-10,\n",
      "        -5.6843e-13, -7.2760e-12, -8.5265e-14,  8.7311e-11,  4.5475e-13,\n",
      "        -1.4211e-14,  4.5475e-13, -1.4552e-11, -1.4552e-10,  8.6402e-12,\n",
      "         1.7053e-13, -4.5475e-13, -3.4106e-13,  4.5475e-13, -2.8422e-14,\n",
      "         8.5265e-14,  9.0949e-13, -9.0949e-13,  7.1054e-14, -7.1054e-15,\n",
      "         0.0000e+00,  2.2737e-13,  6.8212e-13,  1.0914e-11, -2.2737e-13,\n",
      "         6.3665e-12, -5.4570e-12,  4.5475e-13,  0.0000e+00,  7.9581e-13,\n",
      "        -1.4552e-10, -2.5466e-11, -1.8190e-12, -4.3656e-11, -1.4552e-11,\n",
      "         7.2760e-12,  0.0000e+00,  1.7764e-14,  3.4925e-10, -3.6380e-12,\n",
      "        -2.1828e-11, -4.5475e-13, -1.1369e-13, -1.7053e-13, -1.4211e-13,\n",
      "         3.4106e-13, -2.7285e-12, -1.7280e-11, -2.3283e-10,  7.8160e-14,\n",
      "         4.6566e-10,  5.6843e-13,  7.2760e-12,  9.0949e-13, -1.1642e-10,\n",
      "         9.4587e-11,  1.3642e-12,  1.6735e-10], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([[-5.2392e-06, -2.9593e-06, -3.8639e-06,  ...,  3.4761e-05,\n",
      "          6.2362e-04,  4.8098e-04],\n",
      "        [ 8.6199e-07,  2.5804e-06,  2.3868e-06,  ..., -3.1005e-04,\n",
      "         -6.5855e-04, -7.5635e-04],\n",
      "        [ 2.6195e-06,  1.6502e-06,  1.2573e-06,  ..., -4.7963e-04,\n",
      "         -6.6945e-04, -9.1863e-05],\n",
      "        [-6.6330e-07, -1.0089e-06,  1.1033e-06,  ...,  4.4770e-04,\n",
      "          3.9785e-04,  2.0256e-04],\n",
      "        [ 2.4210e-06, -2.6246e-07, -8.8355e-07,  ...,  3.0722e-04,\n",
      "          3.0653e-04,  1.6468e-04]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([ 7.8226e-04, -3.7994e-04, -2.7052e-04, -1.5809e-04,  2.6303e-05],\n",
      "       device='cuda:0', grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAHACAYAAAC8i1LrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABuHklEQVR4nO3dd1hTZxsG8DsEZKiAigtFcC+0WvcoYt17jzpRW0erdqi1tg6wddZWbW2ttiK0ihuwblHBPeuqExcWcaIylBXI+f54P1BkyAg5J8n9u65cyMm68xrCw3mXSpIkCUREREQGxEzuAERERES5xQKGiIiIDA4LGCIiIjI4LGCIiIjI4LCAISIiIoPDAoaIiIgMDgsYIiIiMjgsYIiIiMjgsIAhIiIig8MChoiIiAyOIguYGzduYODAgShfvjxsbGxQo0YNzJ49G3FxcXJHIyIiIgVQKW0vpPDwcNStWxd2dnYYO3YsihcvjuPHj8PHxwfdu3fH1q1b5Y5IREREMjOXO8Cb/vrrL0RFReHIkSOoXbs2AGD06NHQarX4888/8fz5cxQrVkzmlERERCQnxXUhxcTEAABKly6d7njZsmVhZmaGQoUKyRGLiIiIFERxZ2Dc3d2xYMECjBo1Cl5eXihRogSOHTuG5cuXY+LEiShcuHCm90tMTERiYmLa91qtFs+ePUOJEiWgUqn0FZ+IiIjyQZIkxMbGwtHREWZm2ZxnkRTo22+/laytrSUAaZdvvvkm2/vMmjUr3e154YUXXnjhhRfDvYSHh2f7e19xg3gBYM2aNVizZg369OmDEiVKYMeOHVi9ejV++uknjB8/PtP7vHkGJjo6GhUqVMCdO3dQtGhRfUVXLI1Gg+DgYLRu3RoWFhZyxzFabGf9YDvrhym18+PHQK1a5vj55xR88IF+fy2aUjvnRGxsLCpWrIioqCjY2dlleTvFdSGtX78eo0ePRmhoKMqXLw8A6N27N7RaLaZOnYoPPvgAJUqUyHA/S0tLWFpaZjhevHhx2NraFnhupdNoNLCxsUGJEiX4A1KA2M76wXbWD1NqZ39/wMwMGDgQyORXTIEypXbOidQ2eNvwD8UN4v31119Rv379tOIlVffu3REXF4dz587JlIyIiIxVYCDg5gY4OMidhHJKcQXMo0ePkJKSkuG4RqMBACQnJ+s7EhERGbGYGGDfPqBnT7mTUG4oroCpVq0azp07h9DQ0HTH161bBzMzM9StW1emZEREZIx27waSkljAGBrFjYGZMmUKdu3ahffeew/jx49HiRIlsH37duzatQsffvghHB0d5Y5IRERGJCAAqF8fcHaWOwnlhuLOwLi5ueHYsWNo0KABfv31V3z22We4desW5syZg+XLl8sdj4iIjEhiIrBjB8++GCLFnYEBgMaNG2Pnzp1yxyAiIiMXEgLExgK9esmdhHJLkQUMkTHTaDSZDlSnnNNoNDA3N0dCQgLbMp8sLCygVqvljiGbgACgUiXA1VXuJJRbLGCI9CQmJgaRkZHpFlykvJEkCWXKlEF4eDi3CsknlUoFOzs7lClTxuTaUqsFtm4FBg8GTOylGwUWMER6EBMTg4iICBQpUgQODg6wsLAwuV8WuqTVavHixQsUKVIk+71SKFuSJOHly5d48uQJrK2tYW9vL3ckvTp5Enj4kONfDBULGCI9iIyMRJEiRVC+fHkWLjqg1WqRlJQEKysrFjD5ZG1tjcTERDx+/Bh2dnYm9f4MDARKlgSaNZM7CeUFf/KJCphGo0FiYqLJ/XIgw2Fra4uUlBSTGk8kSWL8S48egAkPATJoLGCICljqLwXucUJKZW4uTsab0krnV68CN26w+8iQsYAh0hOefSGlMsX3ZmAgUKQI0KaN3Ekor1jAEBGRyQkIADp1Aqys5E5CecUChoiITEp4OHDmDBevM3QsYIiIyKRs3QpYWACdO8udhPKDBQwREZmUwECgdWvAzk7uJJQfLGCIqMCFhYVBpVJBpVKhTJkyWc52uXr1atrtXFxcMr2NJEmoVq0aihUrhq5du2b7vKmPZWlpiadPn2Z6m+fPn8Pa2jrttq8LCQmBSqXC2LFj3/4iySA8eyb2P2L3keFjAUNEemNubo5Hjx5luVnrqlWrYGZmlu3idCEhIbh16xZUKhX27t2L+/fvv/U5k5KSsHbt2kyvX7t2LRISEtKmEpNx27EDSEkBuneXOwnlFwsYItKb5s2bw87ODt7e3hmuS05Oxpo1a9C2bdts18xZtWoVAOCTTz5BSkoKfHx8sn3OypUro1q1ali9enWm13t7e6N69eqoXLlyzl8IGayAAKBpU8DRUe4klF8sYIhIb6ytrTFw4EDs2LEDjx8/Tnfd9u3b8ejRI4wcOTLL+0dFRWHLli1wdXXF119/jaJFi8Lb2xuSJGX7vCNGjMD58+dx9uzZdMcvXLiAc+fOYcSIEXl/UWQw4uKA3bu5eJ2xYAFDRHo1cuRIJCcn46+//kp33NvbG8WLF0fPbH67+Pn5ISEhAUOHDoW1tTX69OmDW7du4eDBg9k+5/Dhw6FWqzOchVm1ahXUajWGDRuW59dDhmPfPiA+ngWMsWCnL5HM4uKAa9fkTpG9GjUAGxvdPFbjxo3h6uqK1atXY9KkSQCAhw8fYteuXRg3bhwsLS2zvG/qGJlBgwYBAIYMGQIfHx+sWrUK7u7uWd6vbNmy6NSpE/z8/LBo0SJYWloiMTERa9euRefOnVG2bFndvDhStIAAoGZNoHp1uZOQLrCAIZLZtWtAgwZyp8jeP/8A776ru8cbOXIkvvjiC5w8eRJNmjSBr68vkpOTs+0+Su0CateuHRwdHRETEwN3d3dUqFABW7ZswbJly2CXzbzYkSNHYvv27QgMDMSAAQMQGBiIZ8+eZfucZDySk4Ft24DRo+VOQrrCAoZIZjVqiAJByWrU0O3jDRkyBFOnToW3tzeaNGmC1atXo379+qhXr16W9/njjz8AIF13j0qlwpAhQzB37lz4+flh3LhxWd6/a9euKFWqFLy9vTFgwAB4e3ujVKlSb52KTcbhyBHg6VNOnzYmLGCIZGZjo9uzG4agZMmS6NatG9avX49+/frh+vXr+Pnnn7O8fUJCAtauXYsiRYqgd+/e6a4bNmwY5s6dC29v72wLGAsLCwwZMgRLlizBsWPHsG/fPnz++eecPm0iAgOBcuWUf7aTco6DeIlIFqNGjUJMTAw8PDxgZWWFwYMHZ3lbf39/REVF4cWLFyhcuDDUajWKFSsGtVqNGv8/PXTmzBlcvHjxrc+p1WrRv39/aLVajBo1SqeviZRJksT4l549gWyWGCIDwz89iEgWHTp0QLly5RAREYGBAweiWLFiWd42de2Xfv36wdbWFpIkQaPRwMLCAiqVCvfu3cOePXuwatUqLF26NMvHqVWrFpo0aYKTJ0+iadOmqFmzps5fFynP+fPAf/9x9pGxYQFDRLJQq9UIDAzEvXv3sh37cufOHQQHB8PFxQUbNmyASqWCVqtFTEwMbG1tYWZmhujoaJQtWxZr1qzBwoULs53J5O3tjdDQUFSrVq0AXhUpUWAgYG8PtGoldxLSJRYwRCSbhg0bomHDhtneJnWhuuHDh2fYqyiVnZ0devXqBT8/v7RZRlmpVasWatWqlaucwcHB8PDwyPS6li1b4sMPP8zV45F+BQQAXbuKHajJeLCAISLF0mq18PHxgUqlwvDhw7O97YgRI+Dn54dVq1ZlW8DkRWhoKEJDQ7O8ngWMct26Bfz7LzBrltxJSNdYwBBRgXNxcXnrcv+vS0hISPt3eHh4ju7Ttm3bDM+Rm+e8lslqgu7u7rl6DFKewEDAygro2FHuJKRrHI9NRERGKzAQaNcOKFxY7iSkayxgiIjIKD16BBw9ytlHxooFDBERGaVt2wCVCujWTe4kVBBYwBARkVEKDARatgRKlpQ7CRUEFjBERGR0YmOBoCDufWTMWMAQEZHR2b0bSEri+BdjprgCxsPDAyqVKstLRESE3BGJiEjhAgKAevUAFxe5k1BBUdw6MGPGjEHbtm3THZMkCWPHjoWLiwvKlSsnUzIiIjIESUnAjh3AF1/InYQKkuIKmGbNmqFZs2bpjh05cgRxcXHZ7lZLREQEAMHBQEwMu4+MneK6kDLj5+cHlUqFQYMGyR2FiIgULjAQqFgRqFtX7iRUkBR3BuZNGo0GGzduRPPmzeGSTWdmYmIiEhMT076PiYlJu79GoynomIqX2gZsi4KVWTtrNBpIkgStVgutVitXNKOSurx/artS/mi1WkiSBI1GA7VanXbcED83tFpg61Zz9O+vRXKyYbw3DLGdC1JO20HxBcyePXvw9OnTt3YfzZs3D15eXhmO7927FzY2NgUVz+AEBQXJHcEkvN7O5ubmKFOmDF68eIGkpCQZUxmf2NhYuSMYhaSkJMTHx+PQoUNITk7OcL0hfW5cv14MDx64oVSpY9i585nccXLFkNq5IMXFxeXodipJ4TuVDRo0CJs3b8aDBw9QokSJLG+X2RkYJycnREZGwtbWVh9RFU2j0SAoKAjt2rWDBfeULzCZtXNCQgLCw8Ph4uICKysrmRMaB0mSEBsbi6JFi0KlUskdx+AlJCQgLCwMTk5O6d6jhvi5MW2aGXx9zRAenozXTiYpmiG2c0GKiYmBg4MDoqOjs/39regzMC9evMDWrVvRoUOHbIsXALC0tISlpWWG4xYWFnxDvIbtoR+vt3NKSgpUKhXMzMxgZmYQw850LiwsDBUrVgQAlC5dGvfu3YO5ecaPn6tXr6JWrVoAAGdnZ4SFhWX6eKndRqnt+iZPT094eXkhODgY7u7uunkROXDr1i388ssvOHDgAO7evYsXL17A3t4eNWvWRNu2bTF8+HA4Ozunu4+Liwvu3r0LBwcH3L59G0WLFs3wuFZWVihTpkyW7SFJEqpWrYpbt26hc+fO2LFjR65ym5mZQaVSZfn5YCifG5IEbN0qBu9aWSk/75sMpZ0LWk7bQNGfpoGBgZx9RGREzM3N8ejRI+zcuTPT61etWmWwhd6PP/6IGjVqYPHixbC2tsaQIUPw5Zdfom/fvoiPj4enpyeqVq2K06dPZ3r/yMhILFy4ME/PHRISglu3bkGlUmHPnj24f/9+fl6Kwbp0Cbh5E+jdW+4kpA+K/pRYu3YtihQpgu7du8sdhYh0oHnz5rCzs4O3t3eG65KTk7FmzRq0bdvW4P4KXbFiBSZNmgQnJyecPn0ax48fx88//4w5c+Zg+fLlOH36NK5du4bevXunTTB4nYWFBSpUqIDFixfj4cOHuX7+VatWAQAmTZqElJQU+Pj45PclGSR/f8DWFnj/fbmTkD4otoB58uQJ9u3bh169enEQLpGRsLa2xsCBA7Fjxw48fvw43XXbt2/Ho0ePMHLkyEzvK0kSvL290aJFC9jb28PR0RGNGzfOUAy5u7unDehv3bp12irer89iDA4OxsiRI1G9enUUKVIERYoUQcOGDbFy5cpcv6bnz5/jyy+/hKWlJXbt2oWGDRtmertq1aph/fr1aNWqVYbrzMzM4OXlhZcvX2Y6GSE7UVFR2LJlC1xdXTF79mwULVoU3t7eUPjwxgLh7w907QpkMpqAjJBiC5gNGzYgOTmZ3UdERmbkyJFITk7GX3/9le64t7c3ihcvjp6ZrD4mSRIGDx6MUaNG4cmTJ/jggw8wdOhQvHz5EqNGjcLkyZPTbuvh4ZFWJAwfPhyzZs3CrFmz8Nlnn6XdZsGCBTh06BAaNWqE8ePHY8iQIYiMjMSYMWMwadKkXL2ezZs3IyYmBv369UP16tXfevvMxv4AwLBhw+Dq6oo//vgDoaGhOX5+Pz8/JCQkYNiwYbC2tkbfvn1x69YtHDx4MMePYQxu3gQuXmT3kUmRFKpp06ZSqVKlpOTk5DzdPzo6WgIgRUdH6ziZYUpKSpICAwOlpKQkuaMYtczaOT4+Xrpy5YoUHx8vYzJ53blzRwIgdejQQZIkSXJ1dZVq166ddv2DBw8kc3NzacKECZIkSZKlpaXk7Oycdv3KlSslANKIESOkpKQkKSUlRXr+/LkUHx8vdevWTQIgnTlzJu32s2bNkgBIwcHBmea5fft2hmMajUZq166dpFarpbt37+b4tY0YMUICIK1atSrH93mds7OzZGlpKUmSJG3fvl0CIPXp0yfdbd5sj9e9++67kpmZmRQRESFJkiQdOHBAAiANGTIkxxmyeo8a0ufGwoWSZGUlSS9eyJ0k9wypnfUhp7+/FTsL6fjx43JHINKPuDjg2jW5U2SvRg1Ah125I0eOxBdffIGTJ0+iSZMm8PX1RXJycpbdR8uWLUPhwoXxyy+/wMLCIm0WUqFChTBnzhxs27YN69atQ4MGDXL0/Kkzol5nbm6OsWPHIigoCMHBwRg+fHiOHit1zIqjo2OG686fP4/AwMB0x+rVq5fpWSYA6NKlC9zc3LBlyxacOnUKjRs3zva5z58/j7Nnz6Jdu3Zpz+/u7o4KFSpgy5YtWLZsGezs7HL0Ogydvz/QsSNQuLDcSUhfFFvAEJmMa9eAHP7ilc0//wDvvquzhxsyZAimTp0Kb29vNGnSBKtXr0b9+vVRr169DLeNi4vDv//+C0dHRyxYsACA6FJKTEyEpaVl2sJr13JRBMbGxmLRokUIDAzErVu38PLly3TXvz6Lx8fHJ8P05Z49e2aa9U3nz5/PMKZl+PDhWRYwALBw4UI0bdoUU6dORXBwcLaP/8cffwAQ3U+pVCoVhgwZgrlz58LPzw/jxo17a05DFxEBnDgB/Pmn3ElIn1jAEMmtRg1RIChZjRo6fbiSJUuiW7duWL9+Pfr164fr16/j559/zvS2z58/hyRJiIiIyHaA65tFSFaSkpLg7u6Os2fPon79+hg6dChKlCgBc3NzhIWFwdfXN92imD4+PhnGk7i4uKQVMKVLlwaATKcue3h4wMPDAwBw4sSJDBvVZqZJkybo3bs3/P39sXPnTnTu3DnT2yUkJKTN1Oz9xsCPYcOGYe7cufD29jaJAiYwEDA3FwN4yXSwgCGSm42NTs9uGIpRo0bB398fHh4esLKyynLAfupKnA0aNMCZM2cAiIXsYmJiYGtrm+s1Y7Zu3YqzZ89i1KhRaWcwUq1fvx6+vr7pjoWEhGT7eM2bN4ePj0/azCZdmDt3Lv7++2989dVX6NixY6a38ff3R1RUFACgcBb9JmfOnMHFixdR18h3NfT3F1OnixWTOwnpEwsYIpJFhw4dUK5cOURERGDgwIEolsVvn6JFi6JmzZq4evUqoqKiYG9v/9bHTt2QMCUlJcN1t27dAgD06NEjw3WHDx/OxSsQ+vbti0mTJmHTpk2YOXMmqlatmuvHeFP16tUxatQorFixIsNsrVSpa7/069cv0+XW7927hz179mDVqlVYunRpvjMpVWQkcPAg8MsvcichfWMBQ0SyUKvVCAwMxL179946nmTixIkYN24cPvroI/j4+MDa2jrd9Xfu3Em31kvx4sUBAOHh4RkeK3Up/yNHjqBbt25pxw8ePIjff/8916+jWLFi+P777zF27Fh06tQJGzZsyHQwcerZkpzy9PTEX3/9hZkzZ2bYcfvOnTsIDg6Gi4sLNmzYkOl+UNHR0ShbtizWrFmDhQsXZrrVijHYtk3sQJ3NsCIyUixgiEg2DRs2zHLht9eNGTMGJ06cgK+vL44ePYo2bdqgRIkSiIqKwvXr13Hy5En4+fmlFTCpC9h9/fXXuHz5Muzs7GBvb4/x48ejW7ducHFxwcKFC3Hp0iW4urri+vXr2L59O3r16oXNmzfn+nWMGTMGL168wNSpU9GwYUM0a9YMDRo0gK2tLZ4+fYpr167h0KFDsLCwQJMmTXL0mGXKlMHnn3+OOXPmZLgudaG64cOHZ7mZpZ2dHXr16gU/Pz8EBgZiwIABuX5dhsDfH2jZEvj/UCQyIYpdyI6IKJVKpYKPjw82bNiA2rVrY8eOHfj111+xb98+WFlZYdGiRWjbtm3a7WvVqoXVq1fDwcEBP//8M2bMmIFFixYBAIoUKYIDBw6gT58+OH36NJYtW4b79+9j7dq1+OSTT/KccdKkSbh27Ro+++wzvHz5En/++ScWLlyIzZs3IyUlBTNnzsSNGzdyNaj2yy+/hIODQ7pjWq0WPj4+UKlUb53qPWLECACvupuMTWwssHcvF68zVTwDQ0QFzsXFJVdL2yckJGR6vH///ujfv3+OBvEOHz48y1/wFStWzPJMS25yvqlKlSpYvHhxru6T1Q7TgBjA/OTJk3THzMzMMu0ay0zbtm2NekuBnTuBpCSgVy+5k5AceAaGiIgM0pYtYgml/w9rIhPDAoaIiAxOfLw4A8PuI9PFAoaIiAxOUBDw8iULGFPGAoaIiAyOvz9Qs6bOF4kmA8IChoiIDIpGA/z9N8++mDoWMLl07hzQujUQEyN3EiIi03TwIPD8OQsYU8cCJpdKlQKOHQOWL5c7CRkaY57OSobN0N6b/v5i5lH9+nInITmxgMmlcuUADw/gxx/FKHiit0ndl0ej0cichChzycnJAABzc+UvDabVAgEB4uxLFosQk4lgAZMHU6cCT58CRrq4JemYhYUFLC0tER0dbXB/6ZJpiImJgVqtTiu2lezECeDhQ3YfEVfizZNKlYAPPgAWLgRGjwYKFZI7ESmdg4MDIiIicO/ePdjZ2cHCwiLLPWzo7bRaLZKSkpCQkJDlSrz0dpIk4eXLl4iJiUHZsmUN4j3p7y/2PWrWTO4kJDcWMHn01VfAmjXiMnKk3GlI6WxtbQEAkZGRiIiIkDmN4ZMkCfHx8bC2tjaIX7pKplKpYG9vDzs7O7mjvJUkiQKmVy/AAE4WUQFjAZNHtWuLH6L584Hhw/nDRG9na2sLW1tbaDQapKSkyB3HoGk0Ghw6dAhubm6wsLCQO45Bs7CwMIiuIwC4cAG4c4fdRySwgMmHb74BGjYENm8GjHSneioAFhYW/KWbT2q1GsnJybCysmJbmhB/f8DeHnB3lzsJKQE7j/OhQQOgQwdg7lxxapOIiAqOvz/QvTvAmpUAFjD59vXXwMWLwPbtcichIjJe168Dly+z+4heYQGTT25uQMuWwJw5PAtDRFRQ/P0BGxugfXu5k5BSsIDRgW++AU6eBIKD5U5CRGSc/P2Bzp0Ba2u5k5BSsIDRgQ4dgHffFWdhiIhIt/77Dzhzht1HlB4LGB1QqcRYmAMHxCqRRESkOwEBYsHQLl3kTkJKwgJGR3r1AmrW5FkYIiJd8/cH2rYF/r8eJBEAFjA6Y2YGTJsmZiNduCB3GiIi4/DoEXD4MLuPKCMWMDr0wQeAiwswb57cSYiIjMPff4tu+u7d5U5CSsMCRofMzcVO1Rs3AqGhcqchIjJ8/v5iuYqSJeVOQkrDAkbHPDyAMmXEHklERJR3UVHA/v3sPqLMKbaAOXv2LLp3747ixYvDxsYGrq6u+Omnn+SO9VZWVsDkycBff4mpf0RElDc7dgAajZgkQfQmRRYwe/fuRbNmzfD48WPMmDEDS5cuRdeuXXHv3j25o+XI6NGAnR3w/fdyJyEiMlz+/kCTJkD58nInISVS3G7UMTExGDZsGLp06YLNmzfDzEyRNVa2ihQBPv1UbPI4fTpQurTciYiIDEtcHLBrF+DpKXcSUirFVQd+fn549OgR5syZAzMzM7x8+RJarVbuWLk2frzYMfXHH+VOQkRkePbsAeLj2X1EWVNcAbNv3z7Y2toiIiIC1atXR5EiRWBra4tx48YhISFB7ng5VqwY8MknwK+/As+fy52GiMiw+PsDdeoAVavKnYSUSnFdSDdu3EBycjJ69OiBUaNGYd68eQgJCcHPP/+MqKgorFu3LtP7JSYmIjExMe37mJgYAIBGo4FGo9FL9jeNHw8sXWqOJUu0mD5d3rNIqW0gV1uYCrazfrCd9UOudk5KArZtM8eECVpoNIZ3Bj63+H5OL6ftoJIkSSrgLLlSuXJl3L59G2PHjsXy5cvTjo8dOxYrVqxAaGgoqmZSknt6esLLyyvDcT8/P9jY2BRo5uz88YcrDh50wsqVe2FtnSJbDiIiQ3H2bCnMnt0MS5YEw8UlRu44pGdxcXEYNGgQoqOjYZvN/hGKK2BcXV1x+fJlHDx4EG5ubmnHDx06hFatWsHX1xfDhg3LcL/MzsA4OTkhMjIy2wYoaOHhQI0a5vj2Wy2++EK+vyQ0Gg2CgoLQrl07WFhYyJbD2LGd9YPtrB9ytfO4cWqEhKhw5UoyVCq9Pa1s+H5OLyYmBg4ODm8tYBTXheTo6IjLly+j9BtTd0qVKgUAeJ7FgBJLS0tYWlpmOG5hYSHrG6JSJWDYMGDJEjU+/VQNKyvZogCQvz1MBdtZP9jO+qHPdk5JEdsHeHgAhQqZ1v8t389CTttAcYN4GzRoAACIiIhId/z+/fsAgJIGuJ70V18Bjx8Dq1fLnYSISNmOHgWePOHqu/R2iitg+vfvDwBYtWpVuuN//PEHzM3N4e7uLkOq/KlSBRgwAFiwQKwqSUREmfP3BxwdgcaN5U5CSqe4LqT69etj5MiR8Pb2RnJyMlq1aoWQkBBs2rQJ06ZNg6Ojo9wR82TaNKBuXcDPDxg+XO40RETKI0migOnVCzDANUxJzxRXwADAb7/9hgoVKmD16tUICAiAs7MzFi9ejM8++0zuaHlWp47YDn7ePGDIEECtljsREZGy/POPmPjA7iPKCUXWuBYWFpg1axbCwsKQlJSEGzduGHTxkuqbb4Dr14GAALmTEBEpj78/ULw48NoEVKIsKbKAMVaNGwNt24o9kpQ1eZ2ISF6SBGzZAvToAZgrsm+AlIYFjJ59/TVw7pzYpIyIiISrV4HQUHYfUc6xgNEzd3egWTNgzhyehSEiSuXvDxQtKs5SE+UECxg9U6nEWJhjx4BDh+ROQ0SkDP7+QJcukH2xTzIcLGBk0Lkz8M474iwMEZGpu3NHdK2z+4hygwWMDFQqMRYmKAg4fVruNERE8goIACwtgU6d5E5ChoQFjEz69AGqVxczkoiITJm/P9ChA1CkiNxJyJCwgJGJWi32SAoMBC5dkjsNEZE8HjwQYwLZfUS5xQJGRoMHA87OYnVeIiJTFBgotg3o1k3uJGRoWMDIyMIC+PJLYP164OZNudMQEemfvz/QurVYgZcoN1jAyGzECKBkSWDhQrmTEBHp17NnQHAwu48ob1jAyMzaGpg0CfDxAe7dkzsNEZH+bNsGaLVAz55yJyFDxAJGAcaOFaPvFy2SOwkRkf74+4uVycuWlTsJGSIWMApQtCjw6afAypXA48dypyEiKngvXgB79rD7iPKOBYxCTJggplYvWSJ3EiKigrdrF5CYCPTqJXcSMlQsYBSieHHg44+BZcvEwDYiImPm7w/UqwdUqiR3EjJULGAUZNIkIDkZWLpU7iRERAUnIQHYsYPdR5Q/LGAUpFQpYNw4UcBERcmdhoioYOzZA8TGii1ViPKKBYzCTJki+oV//lnuJEREBWPjRsDVFahVS+4kZMhYwChMmTLAmDHA4sVATIzcaYiIdCs+Hvj7b2DAALmTkKFjAaNAX34JxMWJAb1ERMZk1y4xhbp/f7mTkKFjAaNAjo7Ahx8CP/wg+omJiIzFxo1i9lG1anInIUPHAkahpk4Vxcvy5XInISLSjZcvxfYBPPtCusACRqGcnICRI8X2Ai9fyp2GiCj/du4U3eMsYEgXWMAo2LRpwPPnwIoVcichIsq/DRuABg2AypXlTkLGgAWMgjk7Ax4ewMKF4q8WIiJD9eKFWLyOZ19IV1jAKNy0aUBkJPD773InISLKu+3bxQq8LGBIV1jAKFylSsDQocCCBeKHn4jIEG3YADRuDLi4yJ2EjAULGAPw9dfAo0fAqlVyJyEiyr2YGLH+C8++kC6xgDEAVasCgwYB8+eLbQaIiAzJtm3is6tfP7mTkDFhAWMgpk8HIiKA1avlTkJElDsbNgDNmgEVKsidhIwJCxgDUb06MHAgMG8ekJQkdxoiopyJihK7T7P7iHSNBYwB+eYbIDwc+PNPuZMQEeXM1q3ijy52H5GuKa6ACQkJgUqlyvRy4sQJuePJqnZtoG9fYO5cQKOROw0R0dtt3Ai0bAmUKyd3EjI25nIHyMrEiRPRqFGjdMeqVKkiUxrlmD4deOcdYO1ascgdEZFSPX8O7N0LLF4sdxIyRootYN577z307dtX7hiKU7cu0KsX8N13wJAhgLli/weJyNQFBAApKUCfPnInIWOkuC6k18XGxiI5OVnuGIozcyZw6xawbp3cSYiIsrZxI+DmBpQtK3cSMkaK/ft9xIgRePHiBdRqNd577z18//33aNiwYZa3T0xMROJri6TExMQAADQaDTRGNmCkdm2ga1c1vv1WhX79kqFWv/0+qW1gbG2hNGxn/WA760d+2jkyEti3zxxLlmih0Wh1Hc2o8P2cXk7bQSVJklTAWXLl2LFj+PHHH9G5c2c4ODjgypUrWLRoEV6+fIljx46hfv36md7P09MTXl5eGY77+fnBxsamoGPr3c2b9pg8uRW++OIM3Nwi5I5DRJTO3r3O+O23d+DtvQf29lyBk3IuLi4OgwYNQnR0NGxtbbO8neIKmMzcvHkTdevWhZubG3bv3p3pbTI7A+Pk5ITIyMhsG8CQ9eihxp07Kpw/nwyzt3QGajQaBAUFoV27drCwsNBPQBPEdtYPtrN+5KedO3VSQ5KA3btTCiid8eD7Ob2YmBg4ODi8tYBRbBfS66pUqYIePXrA398fKSkpUGfSZ2JpaQlLS8sMxy0sLIz2DTFzpljdcutWixwvEmXM7aEkbGf9YDvrR27b+fFjIDgYWL4csLBQ9FBLReH7WchpGxjMO8vJyQlJSUl4+fKl3FEUo2lToH174NtvAS27mIlIIfz9AZUK6N1b7iRkzAymgLl9+zasrKxQpEgRuaMoysyZwKVLQGCg3EmIiISNG4E2bQAHB7mTkDFTXAHz5MmTDMcuXLiAv//+G+3bt4fZ2wZ7mJgWLcQHxezZgPJHMxGRsXv4EDh4kHsfUcFT3BiYAQMGwNraGs2bN0epUqVw5coVrFy5EjY2Npg/f77c8RRp5kygVSuxZX337nKnISJTtmULYGYmFtwkKkiKO53Rs2dPREZG4scff8THH3+MDRs2oHfv3jhz5gxq1qwpdzxFcnMTBYyXF8/CEJG8NmwA2rUDiheXOwkZO8WdgZk4cSImTpwodwyDM3Om6ErauRPo0kXuNERkiiIigCNHAG9vuZOQKVDcGRjKm9atxXgYjoUhIrls2SL2Z+vZU+4kZApYwBgJlUqchTl1Suz+SkSkbxs2AB06APb2cichU8ACxoi0ayfWhuFYGCLSt/Bw4Ngxzj4i/WEBY0RSz8IcPw7s3y93GiIyJZs3A5aWQI8ecichU8ECxsh07Ag0bMizMESkXxs2iM8fI916jhSIBYyRST0Lc+SIWEyKiKighYUBJ08CAwbInYRMCQsYI9S1K1CvnpiRRERU0DZtAqysxGcPkb6wgDFCqWdhgoOBw4flTkNExm7jRqBzZ6BoUbmTkClhAWOkevQA6tYVO1UTERWU27eBM2fYfUT6xwLGSJmZATNmAEFBYlYSEVFB2LgRsLbmCuCkfyxgjFjv3kCtWhwLQ0QFZ+NGMfalcGG5k5CpYQFjxFLPwuzeDZw+rZI7DhEZmRs3gHPn2H1E8mABY+T69QOqVwfmzOF/NRHp1saN4sxLp05yJyFTxN9qRk6tBqZPB3buNMPNm/ZyxyEiI7JxI9C9O2BjI3cSMkUsYEzABx8A1apJ8POrIXcUIjIS164BFy9y7yOSDwsYE6BWAzNnpuDs2dI4doxjYYgo/zZuFOu+dOwodxIyVSxgTETfvhJcXKLh6cn/ciLKv40bxXpTVlZyJyFTxd9mJsLMDBg06BpCQsy4UzUR5cvly+LC7iOSEwsYE9Ko0UM0bKjF9OncqZqI8m7jRsDODmjfXu4kZMpYwJgQlQrw8tLixAlg50650xCRIZIkYMMGoGdPwNJS7jRkyvJVwISHh+PAgQOIi4tLO6bVarFgwQK0aNECbdu2xY4dO/IdknSnbVsJbm5igTutVu40RGRo/v0XuH6d3UckP/P83HnGjBnYtm0bHj58mHZszpw5mDVrVtr3Bw8exLFjx9CoUaP8PBXpiEolNnhs1QoICAD69JE7EREZko0bgWLFgLZt5U5Cpi5fZ2COHj2Ktm3bwsLCAgAgSRKWLVuGGjVq4L///sOpU6dQuHBhfP/99zoJS7rh5ib6rmfMAFJS5E5DRIYitfuoVy+gUCG505Cpy1cB8/jxYzg7O6d9f/78eTx58gQTJkxA+fLl0bBhQ/Ts2ROnT5/Od1DSrW+/Ba5eBdatkzsJERmK8+eBmzfZfUTKkK8CRqvVQvvaQIqQkBCoVCq8//77acfKlSuXrouJlKFxY7EEuKcnoNHInYaIDMHGjUCJEsBrH/FEsslXAVOhQgWcOnUq7fvAwECULVsW1atXTzv28OFD2Nvb5+dpqIB8+y1w6xbg6yt3EiJSutTuo969gf+PGiCSVb4KmD59+uDo0aPo27cvhgwZgiNHjqDPG6NCr1y5gkqVKuUrJBWMunWBAQOA2bOBxES50xCRkv3zD3DnjvjMIFKCfBUwkydPRqNGjeDv7w8/Pz/UqVMHnp6eadffvXsXp06dgru7ez5jUkHx9AQiIoCVK+VOQkRKtmEDULKkmMFIpAT5mkZta2uLEydO4NKlSwCAmjVrQq1Wp7uNv78/GjZsmJ+noQJUowYwdCgwZw4wahRgYyN3IiJSGkkS41/69AHM8/Vbg0h3dLISr6urK1xdXTMUL87OzujRowfKlSuni6ehAjJzJvD0KfDLL3InISIlOn1ahf/+Y/cRKUu+CpjY2Fjcvn0bmjemsWzYsAGDBw/Ghx9+iHPnzuUrIBW8SpWADz8EFiwAYmLkTkNESrNpkwplygDvvSd3EqJX8lXAfPnll3jnnXfSFTDLly/HoEGDsG7dOnh7e6Nly5a4du1avoNSwfrmG+DFC2DpUrmTEJGSaLXAli1m6NsXeOMkO5Gs8lXAHDx4EG3btoXNawMn5s+fj3LlyuHQoUPYuHEjJEnK10q8c+bMgUqlgqura36i0luULw+MGwcsWgQ8eyZ3GiJSiuvXi+PePRUXryPFyVcB8+DBA1SsWDHt+6tXryI8PBwTJ05Ey5Yt0bdvX3Tv3h2HDh3K0+Pfu3cPc+fOReHChfMTk3Loq6+A5GRRxBARAcDRo45wdJTQooXcSYjSy1cBk5iYiEKvbYhx8OBBqFQqtG/fPu1YpUqVEBERkafHnzx5Mpo2bcpZTHpSujTw6aeiG+nxY7nTEJHctFrg6NFy6NNHCzOdTPkg0p18vSXLly+Pixcvpn2/fft2FC9eHHXr1k079vTpUxQpUiTXj33o0CFs3rwZS5YsyU9EyqXJk8U0yfnz5U5CRHI7elSF58+t0LevJHcUogzyNaO/U6dO+OWXXzB58mRYWVlh9+7dGDZsWLrbhIaGokKFCrl63JSUFEyYMAEffvgh6tSpk6P7JCYmIvG15WRj/j+dRqPRZJglZYpS2+BtbVG0KPDZZ2ZYsMAMEycmgzPgcyen7Uz5w3bWjw0bgBIl4vHuuxI0GhYxBYXv5/Ry2g4qSZLy/K58+PAhmjdvjrCwMABA2bJlcfLkSZQvXx6A2K26fPnyGD9+PH788cccP+4vv/yCb775Bjdu3EDJkiXh7u6OyMjItAXzMuPp6QkvL68Mx/38/NINMqa3i4szx5gx7dCiRQTGjr349jsQkdHRaMwwYkQHtG9/F8OGXZE7DpmQuLg4DBo0CNHR0bC1tc3ydvkqYAAgPj4e+/fvBwC4ubmle7IrV64gKCgIHTp0QI0aNXL0eE+fPkW1atXw9ddfY9KkSQCQowImszMwTk5OiIyMzLYBTIVGo0FQUBDatWsHixzsxLZokRlmzjTD5cvJeG2cNr1FbtuZ8obtXPD8/VUYONAcP/+8HyNHNmM7FyC+n9OLiYmBg4PDWwuYfC8KbW1tja5du2Z6Xa1atVCrVq1cPd706dNRvHhxTJgwIVf3s7S0hKWlZYbjFhYWfEO8JqftkTqYd948C6xerYdgRobvO/1gOxectWuBhg21cHJ6wXbWE7azkNM20NmuFhERETh//jxiYmJga2uLevXq5XoLgRs3bmDlypVYsmQJ7t+/n3Y8ISEBGo0GYWFhsLW1RfHixXUVm7JgYyMWt/vsMzG9unp1uRMRkb48egTs3AksXsxxL6Rc+Z4Yd/PmTbRr1w4VKlRA9+7dMWTIEHTv3h0VKlRA+/btcfPmzRw/VkREBLRaLSZOnIiKFSumXU6ePInQ0FBUrFgRs2fPzm9kyqHRo4Fy5cSO1URkOvz8xKq7/ftr5Y5ClKV8nYEJDw9Hy5Yt8fjxY9SoUQNubm4oW7YsHj58iEOHDmHfvn147733cOrUKTg5Ob318VxdXREQEJDh+PTp0xEbG4ulS5eicuXK+YlMuWBpCcyYIQqZadOA12bHE5ER8/UFunUDeLKblCxfBYyXlxceP36MX3/9FWPGjIFKpUp3/YoVKzBu3DjMnj0bv//++1sfz8HBAT179sxwPHUtmMyuo4Ll4SE2eZw5EwgMlDsNERW08+eBCxeA776TOwlR9vLVhbRnzx5069YNY8eOzVC8AMCYMWPQrVs37Nq1Kz9PQzKysBBdSFu3AqdPy52GiAqary9QqhTQoYPcSYiyl68C5vHjx2/dZNHV1RVPnjzJz9MgJCQk2ynUVLA++ACoWVN0JxGR8dJoxOyjwYPFHy9ESpavAqZkyZK4ciX7BY6uXLmCkiVL5udpSGZqNTB7NrBnD3D4sNxpiKig7NoFPHkiuo6JlC5fBUyHDh3w999/Y9WqVZle7+3tjW3btqFjx475eRpSgN69gXr1gOnTgfwtfUhESuXrK37OOWCfDEG+BvHOmjUL27Ztw+jRo7FkyRK0atUKpUuXxqNHj3Do0CFcvnwZJUqUwKxZs3SVl2RiZiYG9XXtCuzbB7RrJ3ciItKlp0+BbduAhQvlTkKUM/kqYCpUqICjR49izJgxCAkJweXLl9Nd37p1a/z22285mkJNyte5M9C0qTgL07YtkMm4bSIyUOvWibOrgwbJnYQoZ/K9Em/VqlVx4MABhIeHZ1iJ18nJCQsWLMDevXvT9ksiw6VSibMwbdsC27eLdSKIyDj4+oo/UkqVkjsJUc7obCsBJyenTM+0XLt2DSEhIbp6GpLZ++8D7u5iRlKXLqJriYgM2+XLwJkzwJYtcichyjn++qFcST0Lc+ECP+yIjIWvL1CihBjjRmQoWMBQrrVoAXTqJFbnTUmROw0R5UdyMrBmjVjvqVAhudMQ5RwLGMqTb78Frl0Ti14RkeHatw948AAYPlzuJES5wwKG8qRBA6BXL7HNgEYjdxoiyisfH6B2bfEzTWRIWMBQnnl5AWFhwOrVcichoryIihKbtA4fzmURyPDkehZS586dc3X7f//9N7dPQQaiTh1g4EBRyAweDBQuLHciIsqNDRvEGdQhQ+ROQpR7uS5gdu/enesnyWynajIOc+YANWoAP/wgBvUSkeHw9RW7TpctK3cSotzLdQFz586dgshBBqpiRWDiRLH8+Ecf8YOQyFCEhgLHjwPr18udhChvcl3AODs7F0QOMmDffAN4e4szML//LncaIsoJX1/Azg7o0UPuJER5w0G8lG/29sCsWaKI4ZAnIuXTaoG//hJj2Kys5E5DlDcsYEgnxo4FKlcGpkyROwkRvU1wMBAezrVfyLCxgCGdKFQIWLAA2LNHXIhIuXx8gGrVxO7yRIaKBQzpTM+ewHvvibMw3GKASJliYwF/f679QoaPBQzpjEolplP/+y8XtyNSqs2bgfh4YOhQuZMQ5Q8LGNKpRo2AQYOAGTOAFy/kTkNEb/LxAdq0AZyc5E5ClD8sYEjn5s4Fnj8Hvv9e7iRE9Lo7d4BDhzh4l4wDCxjSOWdn4LPPRAETESF3GiJK9eefQJEiYiNWIkPHAoYKxLRpYm+kGTPkTkJEgFj7xdcX6N+f+5aRcWABQwXCzg7w9BT97RcuyJ2GiI4cEV1I7D4iY8EChgrM6NFirYnJkwFJkjsNkWnz9RV7l7VsKXcSIt1gAUMFxsJCbPK4bx+wa5fcaYhM18uXwMaN4uyLGT/1yUjwrUwFqls3wN1dLG6XnCx3GiLTFBAgljUYNkzuJES6wwKGCpRKBSxaBFy5AqxaJXcaItPk6wu4uYkuJCJjwQKGClyDBmLVz5kzxTLmRKQ/4eHA/v2Ah4fcSYh0iwUM6cWcOUBMjNjwkYj056+/AGtroG9fuZMQ6RYLGNILJyfgiy/EXkn37smdhsg0SJLoPurdGyhaVO40RLqluALm8uXL6NevHypVqgQbGxs4ODjAzc0N27Ztkzsa5dNXXwG2tsA338idhMg0nDgBhIay+4iMk+IKmLt37yI2NhbDhw/H0qVLMeP/S7l2794dK1eulDkd5UfRooCXl1jO/OxZudMQGT9fX3H2s3VruZMQ6Z653AHe1LlzZ3Tu3DndsfHjx6NBgwb48ccfMXr0aJmSkS58+CHw00/ApEnAgQNilhIR6V5CArB+PfDJJ1z7hYyTQbyt1Wo1nJycEBUVJXcUyidzc7HJY0gIsH273GmIjNfWrUB0NLcOIOOluDMwqV6+fIn4+HhER0fj77//xq5duzBgwIAsb5+YmIjExMS072NiYgAAGo0GGo2mwPMqXWobKKEt2rUD3n9fjcmTVWjTJhkWFnIn0h0ltbMxYzu/nY+PGk2bAhUrpiCvzcR21g+2c3o5bQeVJClzl5qxY8dixYoVAAAzMzP07t0bK1euRLFixTK9vaenJ7y8vDIc9/Pzg42NTYFmpdy7fdsWkya546OPLqJz5zC54xAZlWfPLPHhhx0wZswFdOhwV+44RLkSFxeHQYMGITo6Gra2tlneTrEFzLVr13Dv3j3cv38fGzduRKFChbB8+XKULl0609tndgbGyckJkZGR2TaAqdBoNAgKCkK7du1goZBTHh9+qMbOnSpcvZoMOzu50+iGEtvZGLGds/fDD2bw9DRDeHgy7O3z/jhsZ/1gO6cXExMDBweHtxYwiu1CqlGjBmrUqAEAGDZsGNq3b49u3brh5MmTUGUy8tPS0hKWlpYZjltYWPAN8RoltcfcucCmTcAPP1hg3jy50+iWktrZmLGdM5IkYM0aoGdPoGRJ3bQN21k/2M5CTtvAIAbxAkDfvn1x+vRphIaGyh2FdKRcOWDyZGDxYuAuz3IT6cTZs8Dlyxy8S8bPYAqY+Ph4AEB0dLTMSUiXpkwB7O25uB2Rrvj4AGXLisHyRMZMcQXM48ePMxzTaDT4888/YW1tjVq1asmQigpK0aLAt98Ca9cCp0/LnYbIsCUmAn5+wJAhYskCImOmuLf4mDFjEBMTAzc3N5QrVw4PHz7E2rVrce3aNfzwww8oUqSI3BFJx0aOFIvbTZ4s1ofh4nZEebNjB/DsGbuPyDQoroAZMGAAVq1aheXLl+Pp06coWrQoGjRogAULFqB79+5yx6MCoFaLxe06dRKLb/XsKXciIsPk6ws0bAjUri13EqKCp7gCZuDAgRg4cKDcMUjPOnYE2rcHvvwS6NIFRrW4HZE+PH4M7NwpBsUTmQLFjYEh0/X998DNm8Bvv8mdhMjw+PmJ7tcPPpA7CZF+sIAhxahbV4yH8fICuO0VUe74+gLdugElSsidhEg/WMCQosyeDcTHi0XuiChnLlwAzp/n4F0yLSxgSFEcHcU4mKVLgTt35E5DZBh8fYGSJcVAeCJTwQKGFGfyZHEa/Ouv5U5CpHwajVhHafBgDn4n08IChhSncGHgu++A9euB48flTkOkbIGBYgaSh4fcSYj0iwUMKdLw4UCDBsC4cUBystxpiJRryRKgVSvgnXfkTkKkXyxgSJHUajGd+t9/xSq9RJTRqVPAsWPA55/LnYRI/1jAkGI1bAh88gkwcybw339ypyFSniVLgEqVgK5d5U5CpH8sYEjRvv0WsLUFPv1U7iREynLvHrBpk/jZUKvlTkOkfyxgSNHs7MSU6sBA4O+/5U5DpBy//ALY2AAjRsidhEgeLGBI8fr2FetbjB8PvHghdxoi+b18CaxYAXz4IVC0qNxpiOTBAoYUT6UCli0DnjwR2wwQmbq//gKio4EJE+ROQiQfFjBkECpVEoN5Fy8Wy6YTmSqtVnSr9uoFuLjInYZIPixgyGBMmgRUrw6MHSs+xIlM0Z49wLVrwGefyZ2ESF4sYMhgFCoELF8OnDgB/P673GmI5LFkiVjksUULuZMQyYsFDBkUNzcx6+Krr4BHj+ROQ6Rfly8De/eKhetUKrnTEMmLBQwZnIULxboXkybJnYRIv5YuBcqWBfr1kzsJkfxYwJDBcXAAvv9e7MC7b5/caYj0IzJSzD4aP150pxKZOhYwZJA8PER30scfAwkJcqchKngrVoivo0fLm4NIKVjAkEFSqcRmj2FhwPz5cqchKlhJSWLl3WHDxBlIImIBQwasZk1gyhRg3jwgNFTuNEQFZ+NG4MED7glG9DoWMGTQpk8HypcHxo0DJEnuNES6J0liAccOHYBateROQ6QcLGDIoFlbi1PrBw4Afn5ypyHSvSNHgLNnuXAd0ZtYwJDB69gR6N8f+OIL4PlzudMQ6daSJUCNGkD79nInIVIWFjBkFBYvFrORvvpK7iREunPnDhAYKM6+mPHTmigd/kiQUXB0BObMAVauBI4dkzsNkW78/DNgbw8MHSp3EiLlYQFDRmPcOKBhQ7HZo0Yjdxqi/ImJAf74AxgzBrCxkTsNkfKwgCGjoVaLxb4uXxZLrhMZstWrgfh44JNP5E5CpEwsYMiovPsuMGECMGsWcPeu3GmI8iYlRRTh/fsD5crJnYZImVjAkNGZPVuMG5g4Ue4kRHmzbZsYwMup00RZYwFDRsfWFvjpJ+Dvv8UMDiJDs2QJ0KIF0KiR3EmIlEtxBczp06cxfvx41K5dG4ULF0aFChXQv39/hHKteMqF3r2Bzp1Fd1JsrNxpiHLu3Dng4EGefSF6G8UVMAsWLMCWLVvQpk0bLF26FKNHj8ahQ4fw7rvv4tKlS3LHIwOhUgHLlgFPnwKennKnIcq5JUsAZ2egZ0+5kxApm7ncAd70xRdfwM/PD4UKFUo7NmDAANSpUwfz58/HmjVrZExHhqRiRWDmTLFf0tChQL16ciciyt6DB8C6dWKDUnPFfToTKYvizsA0b948XfECAFWrVkXt2rVx9epVmVKRoZo0SSzDPmaMmNlBpGTLlwOFCgGjRsmdhEj5DKLGlyQJjx49Qu3atbO8TWJiIhITE9O+j4mJAQBoNBpouKpZWhuYYlssW6ZC69bmWL48BWPGaAv0uUy5nfXJGNs5Ph5YvtwcHh5aFC6sVcRijMbYzkrEdk4vp+1gEAXM2rVrERERgdmzZ2d5m3nz5sHLyyvD8b1798KGy1imCQoKkjuCLNq2rYepUx1RpMh+FCuW+PY75JOptrO+GVM7BwVVwNOn9eDqGoydO1/KHScdY2pnJWM7C3FxcTm6nUqSJKmAs+TLtWvX0KRJE9SuXRuHDx+GWq3O9HaZnYFxcnJCZGQkbG1t9RVXsTQaDYKCgtCuXTtYWFjIHUfvnj4F6tQxR5s2Ev76q+D6kky9nfXF2NpZkoD69c1RqZIEf3/l9HUaWzsrFds5vZiYGDg4OCA6Ojrb39+KPgPz8OFDdOnSBXZ2dti8eXOWxQsAWFpawtLSMsNxCwsLviFeY6rtUaYMsGgR4OGhwqhRZmjXrmCfz1TbWd+MpZ337QOuXBHdnRYWihuaaDTtrHRsZyGnbaC8n5T/i46ORqdOnRAVFYXdu3fD0dFR7khk4IYNA1q1Aj7+GEhIkDsN0StLlgDvvAO4u8udhMhwKLKASUhIQLdu3RAaGort27ejVq1ackciI6BSAb/9JvZImjtX7jREwvXrwI4dYuE6lUruNESGQ3EFTEpKCgYMGIDjx49j06ZNaNasmdyRyIjUqAF89ZVYZ+PUKbnTEIltL0qVAj74QO4kRIZFcWNgJk2ahL///hvdunXDs2fPMixcN2TIEJmSkbGYMQPYvRsYNEgs2160qNyJyFQ9ewb4+ABffglkMoSPiLKhuALm/PnzAIBt27Zh27ZtGa5nAUP5ZWEB+PkB9esD48cDvr5yJyJT9ccfQHIyMHas3EmIDI/iupBCQkIgSVKWFyJdqFIF+OUX4M8/RTFDpG8aDfDzz8DgwUDp0nKnITI8iitgiPRl6FDRjTR2LHD7ttxpyNT4+wP37gGffip3EiLDxAKGTJZKJfaecXAQhQxX8SZ9WrwYeP99MX2aiHKPBQyZNFtb0YV05gyQyU4URAXixAng5EkxdZqI8oYFDJm8pk1F8TJ3LhASIncaMgWLF4txWF26yJ2EKI9SUoDwcFkjKG4WEpEcvvpKLOc+ZAhw8SJQvLjcichY/fcfsGWLWH3XjH9CklJJEvDoEXDnDhAWJr6+fvnvPzGFLjYWKFJElogsYIgAqNXAX38BdesCH34ofsFwVVQqCL/8Ij7vPTzkTkImLyoqY2GSegkLA+LjX922eHGgYkVxqV//1b9l3LuJBQzR/5UvD6xaBfTuDaxcCYwZI3ciMjYvXoj31kcfyfZHK5kSjQa4dUtcMitQoqJe3bZwYVGQuLgAbdu+KlBSL9nsCi0XFjBEr+nVSxQun38OvPcewG24SJf+/FOccZ8wQe4kZFSio8WmWteuAVeviq/XrgE3b4puHkCcKXF2FsVI48bAgAHpCxQHB4M77cwChugNP/4IHDok9qY5eRKwspI7ERkDrVaMe+ndG6hQQe40ZHAkCYiIeFWcvF6o3L//6nZOTkDNmkD79sDEiWIDuCpVAEdH0VduRFjAEL3BxgZYt078kTJ1KrB0qdyJyBjs2gXcuCH2PiLKUlKSOHPyZpFy7ZrogwSAQoWAqlVFoTJypChSatYEqlUzqb5JFjBEmXjnHeD778Uqqe3bc7or5Y8kiR3QGzcGmjWTOw0pglYrxqKcPw+zf/5B4/37YT5lilgWPCVF3MbeXhQmdesC/fu/KlRcXABz/vpmCxBlYcIEYM8eMVvk4kWgbFm5E5Gh2rwZOHoU2LvX4IYZkC4kJgKXLwPnz6e/xMYCAMzKlIG6TBloO3aEunZtUajUqAGUKsU3TDZYwBBlQaUCVq8Wf/wMHw7s3s11Oyj3EhKAL78EunYF2rWTOw0VuOfPgQsXgHPnXhUqV66IwbQqlejmqVcP6NxZfK1XD8klSuD4zp3o3Lkz1DJOSzY0LGCIslGqlJg50qGDGNw7ebLcicjQLFkiNm3cvVvuJKRTkiQWc0stUlILlrt3xfVWVuKvn6ZNxY6x9eqJ7wsXzvhY3IgtT1jAEL1F+/aicPn6a6B1a6BBA7kTkaF49EhsUfHJJ0D16nKnoTyTJDGw9sSJ9GdWnj8X1zs4iMXd+vdPO6uCatU4TqWAsXWJcmDOHODAATG1+uxZkxroT/kwY4b4HTZzptxJKFdevABOnQKOHxdFy4kTQGSkuK5yZVGsTJr0qlhxdORYFRmwgCHKgUKFxNTqd98VSyt4e8udiJTuwgXgjz/ENHzuraVgkgSEhooi5fhxcbl0ScwSsrMDmjQRp9CaNhX/LlZM7sT0fyxgiHKoWjXg55/FsgsdOoiFLIkyI0liNefq1cXwB1KQmJiMZ1eePRNnUGrVEoXKxIlivnuNGhy5r2AsYIhywcNDDMYcM0b8MebiInciUqK//waCg4Ht22Xd6460WrHE/utnVy5fFhWmvb0oVj79VHxt3FgcI4PBAoYoF1QqYMUK0e09eDBw8CDH6VF6SUli0Hf79mKmLOlRYqIoUg4efHV2JSpK/OC6uoqzKl98Ib5Wq8azKwaOH71EuWRvD6xdC7i5Ad9+C3h5yZ2IlGTZMrGYakAAx3UWOEkC/v0XCAoC9u0Tm5jFxYlBR02bioG2qWdXFLibMuUPCxiiPGjRApg1SxQvbduKnauJIiOB2bNFF6Orq9xpjNS9e68Kln37gMePxZorbm6Ap6dYLbBuXZ5dMQEsYIjy6Ouvxefo4MFixgknJ9CsWeIrz8rpUHQ0EBIiipWgIDGmRaUSCzKNHCkKlubNuW28CWIBQ5RH5uaiK+mdd4DRo4GNG+VORHK6ckWMj1qwAChZUu40BiwpCTh58tVZllOnxOaGlSqJYuW778SKkiVKyJ2UZMYChigfKlQAfv8d6NdPrA0zbJjciUgukyaJWWnjx8udxMBIkqj+UguWkBDg5UsxjqVNGzH1r21bUcAQvYYFDFE+9e0LjBollo5o3FjuNCSHXbvE9PqAAMDSUu40BuDpU2DnTrE99759wMOHouFatgSmTxdnWurVA9RquZOSgrGAIdKBpUuBI0eAoUPNMX06Bw+aEo1GnH1xdwd69JA7jYLduQNs3QoEBgKHD4s1WurXB4YOFQVLixaAjY3cKcmAsIAh0oHChcVWA02bAr/9Vhfdu8udiPRlxQrg2jXAz4/TptORJLHhYWCgKFwuXBB7crRpAyxfDnTrBpQtK3dKMmAsYIh0pH594LffUjBypDPmzk2Bp6fciaigPX8uZh6NHCl6PEyeRiPOrqQWLf/9J/YT6tIF+OYboGNHoGhRuVOSkWABQ6RDQ4ZI2LfvKry8aqJSJQ7qNXazZ4tJM999J3cSGb14AezZI4qW7dvFyrflywM9e4o+tVatuJ8CFQgWMEQ61q9fKCwtq2PUKDOUKyfOmJPxCQ0Vq+7Ong2UKSN3Gj179AjYtk0ULfv2iSX869QBJkwQhUv9+uxPowLHAoZIx1QqYNmyFNy/b4bevYGjR7kqqzGaMgUoV07sOm0SQkNfDcI9fly80Vu2BObNE2daOM2Z9Exx0yVevHiBWbNmoWPHjihevDhUKhV8fHzkjkWUKxYWYmG7ihWBTp2AiAi5E5Eu7d8vdpxeuNDIF4C9cAE1//oL5nXrAtWriwE/pUqJRY8ePRKbJn7+OYsXkoXiCpjIyEjMnj0bV69exTvvvCN3HKI8s7UFduwQ/+7SBYiNlTcP6UZKivid3aKFWMDQ6Lx4AaxaBTRpAotGjeC8dy+kJk3EmZfISLHYjYcH4OAgd1IycYrrQipbtiwePHiAMmXK4MyZM2jUqJHckYjyrFw5sV5Xy5bil922bRzPaOhWrRIbIJ86ZWTDPM6dA1auFPtjvHgBdOqE5E2bsEelQqfu3WHGNy4pjOLOwFhaWqKMyY2II2NWpw7g7y+6HT7+WCyPQYYpOlosFDt0KGAUf1vFxoq9MBo1At59V/SLffaZWHRuxw5IPXpAMlfc37lEABR4BobIGLVpI/5yHz5c7JfzzTdyJ6K8mDtXbNMzd67cSfLpn3/E2RY/PyAuDujcWQzQ7dxZ7FJKZACM5p2amJiIxMTEtO9jYmIAABqNBhqNRq5YipHaBmyLgpVdO3/wAXDrlhmmT1ejXLlkDB7MUzF5Jcf7+fZtYMkSc0ydqkXp0loY3I9STAzM1q+H2R9/QHX+PKTy5aH9/HNoPTwAJydxG0nC6y+Mnxv6wXZOL6ftYDQFzLx58+Dl5ZXh+N69e2HD/TXSBAUFyR3BJGTVzvXrA23a1MNHHzkhPPw46taN1HMy46LP9/PChQ1RtGhx1K69Hzt3pujtefNFkmB/4wZc9u5FucOHYabR4GHDhrg7fToe1a8vNkv8919xyQY/N/SD7SzExcXl6HYqSVJuj3zqIN7Vq1fDw8Mj29tmdgbGyckJkZGRsLW1LeCkyqfRaBAUFIR27drBgoPxCkxO2lmjAXr0UOP0aRVCQpJRu7aeQxoBfb+fDx9WoU0bc6xebSBnzqKjYbZunTjbcvEipAoVoB0xAtrhw8UquTnEzw39YDunFxMTAwcHB0RHR2f7+9tozsBYWlrCMpN97C0sLPiGeA3bQz+ya2cLC2DLFuC994Du3S1w4gTg6KjngEZCH+9nrVYsWteoETBsmDnMFDf14f8kCTh5Uoxt2bBBrI7brRswfz5U7dtDrVZDnceH5ueGfrCdhZy2gVJ/FImMmq2tmF4tSUDXrlwjRsn+/BM4exZYsgTKLF4SEoBffgHeeQdo1gw4cAD4+muxkWJAgFhJUZ3X0oVIuZT440hkEsqVEwvd3bwJ9O8PJCfLnYje9OKFqAUGDACaN5c7zRu0WjGLqHp14NNPgapVgd27gVu3xDQ3ntYjI6fILqRly5YhKioK9+/fBwBs27YN9+7dAwBMmDABdnZ2csYj0pm6dUV3UufOYo2YFSuMbHE0A7dgAfDsmfiqKIcPA5MmAadPi32I9u4VhQyRCVFkAbNo0SLcvXs37Xt/f3/4+/sDAIYMGcIChoxKu3ZiLbERI8TeSdOmyZ2IANEDs2iRqBOcneVO8383bgBTp4quoQYNgJAQoFUruVMRyUKRBUxYWJjcEYj0ysMDuHtXdFdUqAAMHix3Ipo2DbC3B776Su4kAJ4+Bb79Vox1KVsW+OsvYNAghQ7KIdIPRRYwRKZo5kwgLEyciXF0BFq3ljuR6TpxQgwvWbUKKFpUxiCJicCyZcB334ldJGfPFkv9W1vLGIpIGVi+EymESiVmwLZqBfTqBVy+LHci0yRJokaoX19s/SBbiI0bgZo1RZfRBx+I0d7TprF4Ifo/FjBECmJhAWzeLLqROncGHjyQO5HpWbdOLKeyeLFMs4+PHwdatBBTn2rXFqvk/vorUKqUDGGIlIsFDJHC2NmJNWJSUsQaMS9eyJ3IdLx8Kca89Oolw9jY27fFfPrmzYH4eGDfPmDbNnEWhogyYAFDpEDly4s1Ym7cEH+Ic42YgpeUBPTrJ8bLfv+9Hp/4+XNg8mRRqBw7Bvj4iN2i27TRYwgiw8MChkih3nlHdCft3Qt88okYFkEFIyUFGDIE2L8f2LoVqFxZD0+alAQsXQpUqQL89hswfToQGioG3nB2EdFb8aeESMHatxcDe1euBObNkzuNcZIkYMwYwN9fbCHUtq0entDfX4xv+eILoE8fMUB3xgzAxqaAn5zIeHAadW6dPy/+HC5eHChW7NXX7P5dqJDcqcmAjRghFlX75hvg8WOxuJo5f3J1QpLEQnWrVok9j3r2LOAnPHVKdBcdPgx07CgWpHN1LeAnJTJO/BjMrUKFxCnf58+BO3dEX/Xz52K98cTEzO9TuHDWBc6bx16/2NryVDIBAGbNAkqWBCZOBK5cEWcKihWTO5Xhmz1bzDb65Rdg6NACfKIXL8Tc7FWrgDp1gD17xOk1IsozFjC5VasW4Oub+XXx8a+KmefP0//7zWOXL6c/lpKS8fHMzDIvbN52sbfnn+hG6OOPgRo1xEDTJk2Av/8W31PeLFkCeHoCc+eKti0wFy+Kkdjh4WKsy4cfcndoIh3gbzldsrYWl9zuAitJQGzsq2Lm6VNR3GR2CQ8HLlwQ/376NOuzPnZ26Yoatb096r54AbNTp8R6EiVKAA4O4mvqxdaWOwkq3Pvvi16I7t2Bpk2B9etFTwTljrc38PnnYo24Att7SpLE7pyffSY2WvznH264SKRDLGCUQKUSxYOtbe53jYuPz7rYef3y5AmKh4XB7OJFUfgkJGR8LHPz9AXNmwVOZt8XK8a/JvWscmWx1tngwUCXLmLK7+efs/bMqU2bgI8+AsaNK8CB0dHR4kk2bRKnd374AbCyKqAnIzJNLGAMnbU1UK6cuGQjRaNByM6d6Ny5MywsLIC4OFHIREaKr6mXN78/f/7Vv2NiMj6wSiW6rIoXF0XN611Z2X1vb8/CJx9sbYHAQDHzdtIksVjrb78BlpZyJ1O2XbtE4ffBB2KLoQIp+k6dAgYOFH84bN4sZhkRkc6xgDFVNjbi4uSU8/skJb3qunqz4Hm96ysiQvT7Z9fN9Xrhk12xU6rUq0vJkmKtfQIg6r9588QkllGjgOvXxezcMmXkTqZMhw+LWqJzZ2D16gIYH6/VihHBX30FNGggFpWpWFHHT0JEqVjAUM4VKiR+O+b2N2R8fPpxPW+O8clN4VO8OFC6tChoUr++/u/XvxYubBL9KoMHA9WqAT16AI0aiYXY3n1X7lTK8s8/orutWTMxbkjndXBkJODhIZZPnjIFmDOHxTZRAWMBQwXP2lqsjV++fO7uFxcHPHkCPHokFkB58+vjx2JO8ePH4heIVpvxebMqbsqUEeONKlQQ/zbw6eqNGgFnzoh1TFq2FKvR9+8vdypluHIF6NBBTCDcurUAhqIcOgQMGiQK7p07gU6ddPwERJQZFjCkXDY2osjIycDmlBRx1iazQif167//in8/epR+EHOhQqIrrUKFV8/3+r+dnAxicImjI3DwoJilO2AAcOmSmCZs4LVZvty5A7RrJ9pm506gSBEdPnhKipiD7ekJuLkBa9fmfgYiEeUZCxgyDmr1q+6kt61sKklilsjdu2KJ27t3X/376lVg927g4cP093n9jE1mRY6dnSK6q6ytgTVrgLp1xfTgS5fECrM6/cVtIO7fF9sC2NiI/aSKF9fhgz94IDZPCgkBZs4Uo6k5KJ1Ir1jAkOlJHUBsby92TMxMYqJYc+f14ib132fPiu81mle3L1oUcHaGukIFuKrVUN2/LwqpWrXEgGQ9UqnE+ia1aomejRYtRNeJi4teY8gqMlKcedFoxOBdnQ5s3rNHLNtrbi4G6rq76/DBiSinWMAQZcbSUmwZUaVK5tdrteIszZtncO7cQekzZ6DesePVmJySJYGaNTNeypcv0LM23boBJ06IRe8aNRIzlN57r8CeTjFiYsTifk+eiOIlt0srZUmjERsuLlggxrn4+or/WyKSBQsYorwwMxPjHRwdxZK4/5ei0WD/zp3o/P77sAgLE11SqZfjx8Xo2tTZVUWKiL0AatYUp0tSC5tKlXS2FUTt2mJZkn79gDZtgF9/FWNkjFVcnCjcbt4UvTs6W/j27l2xeMzp02LlwC++MO3BRUQKwAKGqCBYWYlN++rUSX88JQV4vbC5ckV83br11UKBhQoBVatmPGNTvboY5JJLJUqIXo/PPhOLw168CPz4o/Ftl5WUBPTtK2ZjBQUB9erp6IEDA8WW4Pb2wJEjYiMqIpKdkX2EESmcWi32AqhcGeja9dVxSRIDQ18/Y3P1KvD7768GFJubizE7TZu+ulSunKNuKAsLseNynTrAhAnioTds0PHAVhmlpIgxtfv3i6VYmjfXwYMmJIg1XZYtEyvg/fGHKGKISBFYwBApgUr1qkuqTZv010VFiYrjwgXg5Elg3z5RjQBiP6rXC5rGjcWA4iyMHSt6rfr2fbWjdc2aBfey9EGSgDFjxBifTZvEzKN8Cw0Vc9GvXhX9bmPHKmKWGRG9wgKGSOns7cUSss2aiV+kgFit+NQpMa7mxAkxLiM6WvySdXV9VdA0aya6nl4br+HuLu7ao4e4ybp1Ynl9QyRJYi+oVavEmNpevXTwoGvXinZ2dBQFY1Yz1YhIVixgiAxR8eJiqk3HjuJ7rVZshpRa0Jw4Ibo8JEmsUdOkyauCpnFjVKpUHMeOiW6Xrl3FdgR9+gDt24t1UwzFnDlmWLxY9PIMG5bPBzt1Siyec+CAeLBffjHNBXSIDAQLGCJjYGb2arDvyJHiWEyMmDWTWtD88gswe7a4rnp1FG3aFAEdm2FNlab4YY8r1qxRw9pazBDu1UsUNkoe8rFtWyWsWqXGnDnAJ5/k44GuXBEL0QUEiGlbW7eKuedEpGgsYIiMla2tGE+TOqZGkoBbt14VNMePw2zNGgxLScEwe3u8aNMKJ21aw/vG+xg2tDbU5mZo00YUMz16KGOXa41GrC4cGGiGVavqYNKkFEyblscVcMPCxDYAf/0lVlX+80+x8h9X1CUyCCxgiEyFSvVqcb4hQ8SxuDgx7/jgQRQ5cABt9nyJNklJ+LNESdyp4I5tYe9jycfvY9zYqmjeQoXevUVBU7FiwceVJLE24MmTry5nz4rNzc3NzdCt2y3MnVsBKlUuC47Hj8Vu0cuXi664pUuB0aPF9HUiMhgsYIhMmY2N2IjQzU2sMhsfDxw/DvWBA6hy4AA+/3cCPtcm46W9I07dfR/rp7ZG60nvo1g9F/TqBfTuLXpddDFBJ7XH6/WC5dEjcZ2zsxjG07u3+FqnTjKCgy9BpaqQ8yeIjgYWLQIWLxZT0j09gU8/BQoXzn94ItI7FjBE9Iq1NfD+++ICALGxwJEjKBwcjNYHDsA9Yi1UkPD4RkXs+a415s16H2EurdGyvyN69RKzuHOyQG1ysugKer1YuXpVnHWxtRVbH4waJYqVJk2A0qXT3//1bajeKj5ejP+ZN0+ccZo4UWwWZSyL4BCZKBYwRJS1okXFqN5OnQAAqufPgUOHUOrAAQzefwBDL3sDYcCNH6ojaOH78C72Por1ckf7QQ5wcxML6EkScO9e+mLln39ELaFWi8X13nsPmDxZFCs1auholX6NBli9WgxcfvRI7KEwY4aYHk1EBo8FDBHlXLFiYkRvjx4wA8R4kpAQVD4QjHI798MmfDngDVzwrotVlq3xsKY7dkTUx5knFQCo4OQkihQvL/H13XcLoAdHqxUr2s2YAdy4IfYwmj076405icggKXI3ssTEREydOhWOjo6wtrZGkyZNEBQUJHcsInpTqVJA//4w+205bP67Dty7B+nPv1CuewMMsAqE5/leOP3EBUk29khs/B7+6z4em9qtxOT3TuK9d1/qtniRJGDXLqBhQ2DgQKBaNeD8ecDPj8ULkRFS5BkYDw8PbN68GZ999hmqVq0KHx8fdO7cGcHBwWjZsqXc8YgoK+XKQTV0CByG/n+WU0QEcOECLC5eFFshBAcDv/0mNi9SqcReTu+8A9St++qri0vuRwUfOyYWoTt0CGjZEjh8WHwlIqOluALm1KlTWL9+Pb7//ntMnjwZADBs2DC4urriyy+/xLFjx2ROSEQ5Vq6cuLy+V0FCglg8LrWouXgR+Okn4OlTcX3RoqKQeb2ocXXNfI+nixfFbKLt28Vtd+wQ43W4bxGR0VNcAbN582ao1WqMHj067ZiVlRVGjRqFr7/+GuHh4XBycpIxIRHli5WVGPzy7ruvjqXuxv16UXPoELBypThbA4izNf8valTVquHdFStgfugQUKmS6CYaMEBHo3+JyBAoroA5d+4cqlWrBltb23THGzduDAA4f/58pgVMYmIiEhMT076Pjo4GADx79gyaXM25NE4ajQZxcXF4+vQpLCws5I5jtNjO+WBpKeZPN2r06lhiInD9OlSXL0N15Yr4+tNPUD17Bkt7ezydPx+qYcPEdKfnz+XLbqT4ftYPtnN6sbGxAABJkrK9neIKmAcPHqBs2bIZjqceu3//fqb3mzdvHry8vDIcr6iPJUOJSP+iosR6LlOnyp2EiApAbGws7OzssrxecQVMfHw8LC0tMxy3srJKuz4z06ZNwxdffJH2vVarxbNnz1CiRAmo2B+OmJgYODk5ITw8PMPZLdIdtrN+sJ31g+2sH2zn9CRJQmxsLBzfsmaT4goYa2vrdF1BqRISEtKuz4ylpWWGwsdeyVvpysTW1pY/IHrAdtYPtrN+sJ31g+38SnZnXlIpbsRb2bJl8eDBgwzHU4+9rSIjIiIi46e4AqZevXoIDQ1FTExMuuMnT55Mu56IiIhMm+IKmL59+yIlJQUrV65MO5aYmIjVq1ejSZMmnEKdR5aWlpg1a1am44tId9jO+sF21g+2s36wnfNGJb1tnpIM+vfvj4CAAHz++eeoUqUKfH19cerUKezfvx9ubm5yxyMiIiKZKbKASUhIwIwZM7BmzRo8f/4cdevWxbfffosOHTrIHY2IiIgUQJEFDBEREVF2FDcGhoiIiOhtWMAQERGRwWEBY6SioqIwevRolCxZEoULF0br1q1x9uzZXD+ORqNBrVq1oFKpsGjRogJIatjy2s5arRY+Pj7o3r07nJycULhwYbi6uuK7775LW7TRFCUmJmLq1KlwdHSEtbU1mjRpgqCgoBzdNyIiAv3794e9vT1sbW3Ro0cP3L59u4ATG6a8trO/vz8GDBiASpUqwcbGBtWrV8ekSZMQFRVV8KENUH7ez69r164dVCoVxo8fXwApDZhERiclJUVq3ry5VLhwYcnT01NatmyZVKtWLalo0aJSaGhorh7rhx9+kAoXLiwBkL7//vsCSmyY8tPOsbGxEgCpadOm0nfffSetXLlSGjFihGRmZia5u7tLWq1WT69CWQYOHCiZm5tLkydPllasWCE1a9ZMMjc3lw4fPpzt/WJjY6WqVatKpUqVkhYsWCD9+OOPkpOTk1S+fHkpMjJST+kNR17buUSJElKdOnWkGTNmSL///rs0ceJEqVChQlKNGjWkuLg4PaU3HHlt59dt2bIl7TP4k08+KcC0hocFjBHasGGDBEDatGlT2rHHjx9L9vb20gcffJDjx3n06JFkZ2cnzZ49mwVMJvLTzomJidLRo0czHPfy8pIASEFBQTrPq3QnT57M8D6Lj4+XKleuLDVr1izb+y5YsEACIJ06dSrt2NWrVyW1Wi1NmzatwDIbovy0c3BwcIZjvr6+EgDp999/13VUg5afdn799i4uLmmfwSxg0mMBY4T69esnlS5dWkpJSUl3fPTo0ZKNjY2UkJCQo8cZMWKE1LhxY+n27dssYDKhq3Z+3cWLFyUA0k8//aSrmAZjypQpklqtlqKjo9Mdnzt3rgRA+u+//7K8b6NGjaRGjRplON6+fXupcuXKOs9qyPLTzpmJiYmRAEhffPGFLmMaPF20s5eXl1ShQgUpLi6OBUwmOAbGCJ07dw7vvvsuzMzS//c2btwYcXFxCA0NfetjnDp1Cr6+vliyZAl3886CLtr5TQ8fPgQAODg46CSjITl37hyqVauWYTO7xo0bAwDOnz+f6f20Wi0uXryIhg0bZriucePGuHXrFmJjY3We11DltZ2zYsrv2ezkt53/++8/zJ8/HwsWLMhyE2NTxwLGCD148ABly5bNcDz12P3797O9vyRJmDBhAgYMGIBmzZoVSEZjkN92zszChQtha2uLTp065Tufoclrez579gyJiYk6/78wVrp+3y5YsABqtRp9+/bVST5jkd92njRpEurXr4+BAwcWSD5jYC53AMqeVqtFUlJSjm5raWkJlUqF+Pj4TPfUsLKyAgDEx8dn+zg+Pj74999/sXnz5twHNlBytPOb5s6di3379uHXX3+Fvb19ru5rDPLanqnHdfl/Ycx0+b718/PDqlWr8OWXX6Jq1ao6y2gM8tPOwcHB2LJlS9omxpQ5noFRuEOHDsHa2jpHl+vXrwMArK2tkZiYmOGxUqfnZnc6MiYmBtOmTcOUKVNMauNMfbfzmzZs2IDp06dj1KhRGDdunG5elIHJa3umHtfV/4Wx09X79vDhwxg1ahQ6dOiAOXPm6DSjMchrOycnJ2PixIkYOnQoGjVqVKAZDR3PwChcjRo1sHr16hzdNvXUZNmyZfHgwYMM16cec3R0zPIxFi1ahKSkJAwYMABhYWEAgHv37gEAnj9/jrCwMDg6OqJQoUK5eRmKp+92fl1QUBCGDRuGLl264LfffsthYuNTtmxZREREZDj+tvYsXrw4LC0tdfJ/YQry2s6vu3DhArp37w5XV1ds3rwZ5ub8VfKmvLbzn3/+ievXr2PFihVpn8GpYmNjERYWhlKlSsHGxkbnmQ2O3KOISff69u2b6eyYjz766K2zY4YPHy4ByPZy7ty5An4FhiE/7ZzqxIkTUuHChaXmzZub/DoakydPznTWxpw5c946a6Nhw4aZzkJq166dVKlSJZ1nNWT5aWdJkqSbN29KZcqUkapVqyY9fvy4IKMatLy286xZs976GRwQEKCHV6B8LGCM0Pr16zOsT/LkyRPJ3t5eGjBgQLrb3rx5U7p582ba9//8848UEBCQ7rJixQoJgOTh4SEFBARIUVFRenstSpafdpYkSbpy5YpUokQJqXbt2tKzZ8/0klnJTpw4kWG6fkJCglSlShWpSZMmacfu3r0rXb16Nd1958+fLwGQTp8+nXbs2rVrklqtlqZOnVrw4Q1Iftr5wYMHUqVKlSRHR0fpzp07+opskPLazlevXs3wGRwQECABkDp37iwFBARI9+/f1+trUSruRm2EUlJS0LJlS1y6dAlTpkyBg4MDfv31V/z33384ffo0qlevnnZbFxcXAMhwqvJ1YWFhqFixIr7//ntMnjy5gNMbjvy0c2xsLGrXro2IiAjMnTsX5cqVS/fYlStXNskZYP3790dAQAA+//xzVKlSBb6+vjh16hT2798PNzc3AIC7uzsOHjyI1z+6YmNjUb9+fcTGxmLy5MmwsLDAjz/+iJSUFJw/fx4lS5aU6yUpUl7buV69erhw4QK+/PJL1KlTJ91jli5dGu3atdPr61C6vLZzZlQqFT755BMsW7ZMH9ENg6zlExWYZ8+eSaNGjZJKlCgh2djYSK1atUr312kqZ2dnydnZOdvHunPnDheyy0Je2zm1TbO6DB8+XH8vQkHi4+OlyZMnS2XKlJEsLS2lRo0aSbt37053m1atWkmZfXSFh4dLffv2lWxtbaUiRYpIXbt2lW7cuKGv6AYlr+2c3Xu2VatWenwFhiE/7+c3gQvZZcAzMERERGRwOI2aiIiIDA4LGCIiIjI4LGCIiIjI4LCAISIiIoPDAoaIiIgMDgsYIiIiMjgsYIiIiMjgsIAhIiIig8MChoiIiAwOCxgiMjghISFQqVTw9PQ0yecnIhYwREYlLCwMKpUq3aVQoUJwcnLCoEGDcPHixQJ5XmP8ha5SqeDu7i53DCLKgrncAYhI9ypXrowhQ4YAAF68eIETJ05g3bp18Pf3x/79+9GiRQuZExq2xo0b4+rVq3BwcJA7CpHJYgFDZISqVKmS4WzI9OnTMWfOHHzzzTcICQmRJZexsLGxQY0aNeSOQWTS2IVEZCImTJgAADh9+nTasa1bt6JNmzYoVqwYrKys4OrqikWLFiElJSXdfX18fKBSqeDj44Nt27ahRYsWKFq0KFxcXODp6YnWrVsDALy8vNJ1X4WFhQEA3N3doVKpMs3l4eGR7rZve743HTlyBO7u7ihatCjs7e3Rp08f3Lx5M8PtgoODMXLkSFSvXh1FihRBkSJF0LBhQ6xcuTLd7VK7wwDg4MGD6V6Pj49Puttk1mV26dIl9O/fH6VKlYKlpSUqVqyIzz77DE+fPs1wWxcXF7i4uODFixf49NNP4ejoCEtLS9StWxebN2/OtL2ISOAZGCITk/rLedq0aZg/fz7KlSuH3r17w87ODocPH8aUKVNw8uRJbNq0KcN9N23ahL1796Jr1674+OOPERMTA3d3d4SFhcHX1xetWrVKN27E3t4+X1kze77XnThxAvPmzUPHjh0xYcIEXL58GQEBATh8+DBOnDiBSpUqpd12wYIFuHnzJpo2bYpevXohKioKu3fvxpgxY3D9+nX88MMPAERRMWvWLHh5ecHZ2RkeHh5pj1GvXr1s8x45cgQdOnRAUlIS+vbtCxcXFxw/fhxLly7F9u3bceLEiQzdThqNBu3bt8fz58/Rp08fxMXFYf369ejfvz92796N9u3b56sNiYyWRERG486dOxIAqUOHDhmumzlzpgRAat26tbR3796027148SLtNlqtVho7dqwEQNq8eXPa8dWrV0sAJDMzMykoKCjDYwcHB0sApFmzZmWaq1WrVlJWHzfDhw+XAEh37tzJ9fMBkH777bd01/32228SAKlr167pjt++fTvD42g0Gqldu3aSWq2W7t69m+46AFKrVq0yzZzZ601JSZEqV64sAZB2796d7vZTpkyRAEgjR45Md9zZ2VkCIPXo0UNKTExMO75v374s/x+JSGAXEpERunnzJjw9PeHp6YkpU6bAzc0Ns2fPhpWVFebMmYNly5YBAFauXInChQun3U+lUmH+/PlQqVRYt25dhsft0aMH2rZtq7fX8bbnq1atGj766KN0xz766CNUrVoVO3bswJMnT9KOV6xYMcP9zc3NMXbsWKSkpCA4ODhfWY8ePYpbt26hU6dO6NChQ7rrZs6cieLFi8PPzw9JSUkZ7rt48WIUKlQo7fs2bdrA2dk5XXcfEaXHLiQiI3Tr1i14eXkBACwsLFC6dGkMGjQIX331FerUqYMTJ06gcOHC8Pb2zvT+1tbWuHbtWobjjRs3LtDcuX2+Fi1awMws/d9hZmZmaNGiBW7cuIELFy6kFUCxsbFYtGgRAgMDcevWLbx8+TLd/e7fv5+vrOfOnQOATKdep4632bt3L65fv446deqkXWdvb59pcVW+fHkcP348X5mIjBkLGCIj1KFDB+zevTvL6589e4bk5OS0Iiczb/6CB4DSpUvrJF9Ove35sro+9Xh0dDQAICkpCe7u7jh79izq16+PoUOHokSJEjA3N08bv5OYmJivrKnjc7LKVLZs2XS3S2VnZ5fp7c3NzaHVavOViciYsYAhMkG2trZQqVSIjIzM1f2ymkn0NqlnSZKTk2Funv5jJ7XIyMvzPXr0KNvjqcXB1q1bcfbsWYwaNQp//PFHutuuX78evr6+2b+AHLC1tc0208OHD9Pdjojyh2NgiExQkyZN8PTpU9y4cUMnj6dWqwEgw/TrVMWKFQMAREREpDuu1Wpx4cKFPD/v0aNHM5yl0Gq1OHbsGFQqFd555x0AoksNEGNq3nT48OFMH9vMzCzL15OZ+vXrA0Cma+y8fPkSZ86cgbW1NapXr57jxySirLGAITJBEydOBACMHDky0/VJHj58iKtXr+b48YoXLw4ACA8Pz/T6Ro0aAUDaOiqpfvzxR9y5cyfHz/Om0NBQ/P777+mO/f777wgNDUWXLl1QsmRJAICzszMAMc35dQcPHsxw/1TFixfHvXv3cpylRYsWqFy5Mnbt2oV9+/alu+67777D06dP8cEHH6QbrEtEeccuJCIT1LFjR8yYMQPffvstqlSpgo4dO8LZ2RlPnz7FzZs3cfjwYXz33XeoWbNmjh6vRo0acHR0xPr162FpaYny5ctDpVJhwoQJsLOzw4gRI7Bw4UJ4enri/PnzqFy5Ms6cOYNLly6hVatWOHjwYJ5eR4cOHTBx4kTs3LkTtWvXxuXLl7Ft2zY4ODhg6dKlabfr1q0bXFxcsHDhQly6dAmurq64fv06tm/fjl69emW6aNz777+PjRs3omfPnqhfvz7UajW6d++OunXrZprFzMwMPj4+6NChAzp37ox+/frB2dkZx48fR0hICCpXroz58+fn6XUSUUYsYIhM1OzZs+Hm5oaffvoJ+/fvR1RUFEqUKIGKFSvC09MTgwcPzvFjqdVq+Pv7Y+rUqVi3bh1iY2MBAEOGDIGdnR1Kly6N4OBgTJo0CXv37oW5uTlat26NEydO4LvvvstzAdO0aVNMnz4d06dPx08//QS1Wo2ePXti4cKF6RaxK1KkCA4cOIApU6bg0KFDCAkJQe3atbF27VqULl060wImtQA6cOAAtm3bBq1Wi/Lly2dZwABAy5YtceLECcyePRt79+5FdHQ0HB0d8emnn2L69OncO4lIh1SSJElyhyAiIiLKDY6BISIiIoPDAoaIiIgMDgsYIiIiMjgsYIiIiMjgsIAhIiIig8MChoiIiAwOCxgiIiIyOCxgiIiIyOCwgCEiIiKDwwKGiIiIDA4LGCIiIjI4LGCIiIjI4PwPx07Qk1z3Ts0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = loss_landscape_join.landscape(maml_system.model.classifier, arbiter_system.model.classifier, args_arbiter)\n",
    "ls.show_2djoin(x_support_set_task, y_support_set_task, title=title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
