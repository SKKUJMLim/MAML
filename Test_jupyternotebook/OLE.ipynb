{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cda16bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures_OLE import VGGReLUNormNetwork, ResNet12\n",
    "from prompters import padding\n",
    "from utils.parser_utils import get_args\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "869f1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DATASET_DIR'] = os.path.join(os.getcwd(), \"datasets\")\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"alfa+maml\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "    \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"attenuate\": False,\n",
    "  \"alfa\": True,\n",
    "  \"random_init\": False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "   \"loss_function\": \"Softmax\",\n",
    "  \"ole\": True,\n",
    "  \"arbiter\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "def get_inner_loop_parameter_dict(params):\n",
    "\n",
    "    param_dict = dict()\n",
    "    for name, param in params:\n",
    "        if param.requires_grad:\n",
    "            param_dict[name] = param.to(device=device)\n",
    "\n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbba0c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([25, 3, 84, 84])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
      "        4], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(84),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR100(\"./data\", transform=preprocess,\n",
    "                          download=True, train=True)\n",
    "\n",
    "val_dataset = CIFAR100(\"./data\", transform=preprocess,\n",
    "                        download=True, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=25, pin_memory=True,\n",
    "                          num_workers=16, shuffle=True)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "targets = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4])\n",
    "targets = torch.Tensor(targets)\n",
    "targets = targets.type(torch.LongTensor)\n",
    "targets = targets.to(device)\n",
    "\n",
    "print(images.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0123c723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 84, 84])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 42, 42])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 21, 21])\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "torch.Size([2, 48, 10, 10])\n",
      "No inner loop params\n",
      "(VGGReLUNormNetwork) meta network params\n",
      "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
      "layer_dict.conv0.conv.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv0.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv1.conv.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv1.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv2.conv.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv2.norm_layer.weight torch.Size([48])\n",
      "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
      "layer_dict.conv3.conv.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_mean torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.running_var torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.bias torch.Size([48])\n",
      "layer_dict.conv3.norm_layer.weight torch.Size([48])\n",
      "layer_dict.linear.weights torch.Size([5, 1200])\n",
      "layer_dict.linear.bias torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "class MAMLFewShotClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, im_shape, device, args):\n",
    "        \n",
    "        super(MAMLFewShotClassifier, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.batch_size = args.batch_size\n",
    "        self.use_cuda = args.use_cuda\n",
    "        self.im_shape = im_shape\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.classifier = VGGReLUNormNetwork(im_shape=self.im_shape, num_output_classes=self.args.\n",
    "                                                 num_classes_per_set,\n",
    "                                                 args=args, device=device, meta_classifier=True).to(device=self.device)        \n",
    "        \n",
    "        self.optimizer = optim.Adam(self.trainable_parameters(), lr=args.meta_learning_rate, amsgrad=False)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=self.optimizer, T_max=self.args.total_epochs,\n",
    "                                                              eta_min=self.args.min_learning_rate)\n",
    "        \n",
    "    def trainable_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                yield param\n",
    "                \n",
    "    def get_inner_loop_parameter_dict(self, params):\n",
    "        \n",
    "        param_dict = dict()\n",
    "        for name, param in params:\n",
    "            if param.requires_grad:\n",
    "                param_dict[name] = param.to(device=device)\n",
    "\n",
    "        return param_dict\n",
    "                \n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        loss, preds = self.net_forward(x,y)\n",
    "        \n",
    "        return loss, preds\n",
    "    \n",
    "    \n",
    "    def net_forward(self, x, y):\n",
    "        \n",
    "        names_weights_copy = self.get_inner_loop_parameter_dict(self.classifier.named_parameters())\n",
    "\n",
    "\n",
    "        names_weights_copy = {\n",
    "                        name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                            [1] + [1 for i in range(len(value.shape))]) for\n",
    "                        name, value in names_weights_copy.items()}\n",
    "        \n",
    "        preds = self.classifier.forward(x, params=names_weights_copy, label=y, num_step=4)\n",
    "        \n",
    "        \n",
    "        print(\"preds == \", preds)\n",
    "        \n",
    "        loss = F.cross_entropy(input=preds, target=y)\n",
    "        \n",
    "        \n",
    "        grads = torch.autograd.grad(loss, names_weights_copy.values(), allow_unused=True)\n",
    "        \n",
    "        \n",
    "        return loss, preds, grads\n",
    "    \n",
    "    \n",
    "\n",
    "model = MAMLFewShotClassifier(args=args, device=device, im_shape=(2, 3, args.image_height, args.image_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26061063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds ==  (tensor([[-1.8017e+00,  1.5729e+00, -1.7459e+00,  4.2573e-02,  1.6444e+00],\n",
      "        [-4.2895e+00,  7.6371e-01, -3.1353e+00,  2.3147e+00, -7.0321e-02],\n",
      "        [-1.9116e+00,  8.5897e-01, -5.1459e-01, -2.9106e-01,  2.2475e-01],\n",
      "        [-3.0528e+00,  2.0867e-01, -3.2818e+00,  1.1803e+00,  3.0848e+00],\n",
      "        [-2.6788e+00,  1.3603e+00, -1.1568e+00,  6.7295e-02,  1.0021e+00],\n",
      "        [-1.7062e-01,  6.1720e-01, -2.1559e+00,  9.9997e-01,  1.2348e+00],\n",
      "        [-1.4104e+00,  1.8730e+00, -2.6200e+00,  3.0145e-01,  6.2916e-01],\n",
      "        [-1.3554e-01, -1.1797e+00, -2.2811e+00,  1.3671e+00, -9.0685e-02],\n",
      "        [-6.6618e-01, -4.6128e-01, -2.2296e+00,  1.9067e+00,  1.0700e+00],\n",
      "        [-2.0651e+00,  6.4732e-02, -3.2230e-01,  1.6418e-01, -6.5449e-01],\n",
      "        [-2.6879e+00,  5.3137e-01, -2.6987e+00,  8.6043e-01, -6.2171e-01],\n",
      "        [-6.1310e-01, -2.6689e-01, -2.2061e+00,  7.1541e-01,  6.0186e-01],\n",
      "        [-2.7209e+00,  8.6165e-01, -1.8286e+00, -9.4500e-01, -1.1977e+00],\n",
      "        [-2.3251e+00,  4.5386e-01, -1.8936e+00,  8.7753e-02, -1.4197e+00],\n",
      "        [-1.4857e-01, -2.0850e+00, -3.4061e+00,  3.3329e+00, -2.0780e-01],\n",
      "        [-1.6602e+00, -1.0589e+00, -2.9906e+00,  5.8727e-01,  3.1491e-01],\n",
      "        [-1.7160e+00,  9.6197e-01, -1.5036e+00,  1.3836e-01,  5.8429e-02],\n",
      "        [-1.9209e+00, -2.9200e-01, -1.1572e+00,  3.9748e-01, -2.7030e-01],\n",
      "        [-9.9175e-01,  3.8160e-03, -1.0195e+00, -4.9032e-01, -6.1718e-01],\n",
      "        [-1.6482e-01, -7.6428e-01, -3.3742e+00, -1.1369e-01,  3.8754e-01],\n",
      "        [-3.5506e+00, -1.1188e+00, -3.1191e+00,  1.4357e+00,  8.2421e-02],\n",
      "        [-2.7649e+00,  1.2465e+00, -1.9303e+00,  5.4088e-01,  8.9222e-01],\n",
      "        [-1.0347e+00,  4.3723e-01, -6.0136e-01,  1.1537e+00, -1.0877e+00],\n",
      "        [-3.3487e+00,  9.7030e-02, -2.7912e+00,  2.3972e+00, -6.5126e-02],\n",
      "        [-2.1536e+00,  4.2536e-01, -9.4447e-01,  6.2959e-01, -5.0727e-01]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[ 0.2887,  0.2423,  1.0282,  ...,  0.9190,  0.2710,  1.2539],\n",
      "        [ 0.3042,  0.6119,  0.9257,  ...,  0.4538,  0.4645,  0.8667],\n",
      "        [ 0.7789,  1.3184,  2.4357,  ...,  0.4575,  0.2125,  0.9998],\n",
      "        ...,\n",
      "        [ 1.0630,  2.3481,  1.7428,  ...,  0.4764,  0.5487,  1.5232],\n",
      "        [ 0.1737,  1.0750,  0.7586,  ..., -0.0032, -0.0105,  1.6073],\n",
      "        [ 1.5956,  1.0913,  1.0641,  ...,  0.6637,  0.6914,  1.4791]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), tensor([[-1.8017e+00,  1.5729e+00, -1.7459e+00,  4.2573e-02,  1.6444e+00],\n",
      "        [-4.2895e+00,  7.6371e-01, -3.1353e+00,  2.3147e+00, -7.0321e-02],\n",
      "        [-1.9116e+00,  8.5897e-01, -5.1459e-01, -2.9106e-01,  2.2475e-01],\n",
      "        [-3.0528e+00,  2.0867e-01, -3.2818e+00,  1.1803e+00,  3.0848e+00],\n",
      "        [-2.6788e+00,  1.3603e+00, -1.1568e+00,  6.7295e-02,  1.0021e+00],\n",
      "        [-1.7062e-01,  6.1720e-01, -2.1559e+00,  9.9997e-01,  1.2348e+00],\n",
      "        [-1.4104e+00,  1.8730e+00, -2.6200e+00,  3.0145e-01,  6.2916e-01],\n",
      "        [-1.3554e-01, -1.1797e+00, -2.2811e+00,  1.3671e+00, -9.0685e-02],\n",
      "        [-6.6618e-01, -4.6128e-01, -2.2296e+00,  1.9067e+00,  1.0700e+00],\n",
      "        [-2.0651e+00,  6.4732e-02, -3.2230e-01,  1.6418e-01, -6.5449e-01],\n",
      "        [-2.6879e+00,  5.3137e-01, -2.6987e+00,  8.6043e-01, -6.2171e-01],\n",
      "        [-6.1310e-01, -2.6689e-01, -2.2061e+00,  7.1541e-01,  6.0186e-01],\n",
      "        [-2.7209e+00,  8.6165e-01, -1.8286e+00, -9.4500e-01, -1.1977e+00],\n",
      "        [-2.3251e+00,  4.5386e-01, -1.8936e+00,  8.7753e-02, -1.4197e+00],\n",
      "        [-1.4857e-01, -2.0850e+00, -3.4061e+00,  3.3329e+00, -2.0780e-01],\n",
      "        [-1.6602e+00, -1.0589e+00, -2.9906e+00,  5.8727e-01,  3.1491e-01],\n",
      "        [-1.7160e+00,  9.6197e-01, -1.5036e+00,  1.3836e-01,  5.8429e-02],\n",
      "        [-1.9209e+00, -2.9200e-01, -1.1572e+00,  3.9748e-01, -2.7030e-01],\n",
      "        [-9.9175e-01,  3.8160e-03, -1.0195e+00, -4.9032e-01, -6.1718e-01],\n",
      "        [-1.6482e-01, -7.6428e-01, -3.3742e+00, -1.1369e-01,  3.8754e-01],\n",
      "        [-3.5506e+00, -1.1188e+00, -3.1191e+00,  1.4357e+00,  8.2421e-02],\n",
      "        [-2.7649e+00,  1.2465e+00, -1.9303e+00,  5.4088e-01,  8.9222e-01],\n",
      "        [-1.0347e+00,  4.3723e-01, -6.0136e-01,  1.1537e+00, -1.0877e+00],\n",
      "        [-3.3487e+00,  9.7030e-02, -2.7912e+00,  2.3972e+00, -6.5126e-02],\n",
      "        [-2.1536e+00,  4.2536e-01, -9.4447e-01,  6.2959e-01, -5.0727e-01]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27920\\666101169.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27920\\1301266774.py\u001b[0m in \u001b[0;36mnet_forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preds == \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3026\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "loss, preds, grads = model.net_forward(images, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
