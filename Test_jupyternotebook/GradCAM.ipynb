{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb3f6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c51c0520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyhessian import hessian\n",
    "import numpy as np\n",
    "\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import easydict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b159aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "from utils import loss_landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2200d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # Register hook for gradients and activations\n",
    "        self.hook_layers()\n",
    "\n",
    "    def hook_layers(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        # Forward pass\n",
    "        output = self.forward(x)\n",
    "\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(output)\n",
    "\n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        output[:, class_idx].backward()\n",
    "\n",
    "        # Pool the gradients across the width and height dimensions\n",
    "        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n",
    "\n",
    "        # Weight the activations with the pooled gradients\n",
    "        for i in range(pooled_gradients.size(0)):\n",
    "            self.activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # Compute the mean of the activations across the channels\n",
    "        heatmap = torch.mean(self.activations, dim=1).squeeze()\n",
    "\n",
    "        # Relu on the heatmap (because we are only interested in positive influences)\n",
    "        heatmap = F.relu(heatmap)\n",
    "\n",
    "        # Normalize the heatmap between 0 and 1\n",
    "        heatmap -= heatmap.min()\n",
    "        heatmap /= heatmap.max()\n",
    "\n",
    "        return heatmap.detach().cpu().numpy()\n",
    "\n",
    "def apply_heatmap(heatmap, image):\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    superimposed_img = heatmap * 0.4 + image\n",
    "    return superimposed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6799c007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML+Arbiter_5way_5shot_filter64_7177\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":64,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": True,\n",
    "  \"use_bias\": True,\n",
    "  \"smoothing\": False,\n",
    "  \"knowledge_distillation\": False,\n",
    "  \"momentum\": \"SGD\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "728ae0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "No inner loop params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "log_dir ===  C:\\Users\\JM\\PycharmProjects\\MAML\\MAML+Arbiter_5way_5shot_filter64_7177\n",
      "attempting to find existing checkpoint\n",
      "dataset_splits ==  dict_keys(['test', 'train', 'val'])\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70f45f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6617333311835925,\n",
       " 'best_val_iter': 29000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 58,\n",
       " 'train_loss_mean': 0.3813422092795372,\n",
       " 'train_loss_std': 0.11591314818354599,\n",
       " 'train_accuracy_mean': 0.858093334197998,\n",
       " 'train_accuracy_std': 0.05075264943448804,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.9604903495311737,\n",
       " 'val_loss_std': 0.1663930915163558,\n",
       " 'val_accuracy_mean': 0.638777776658535,\n",
       " 'val_accuracy_std': 0.06109242444902527,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 9.6518e-02, -4.1286e-01,  2.3131e-01],\n",
       "                         [-1.4095e-01, -2.4668e-02,  5.6443e-02],\n",
       "                         [-1.2072e-01,  4.0870e-01, -1.1026e-01]],\n",
       "               \n",
       "                        [[ 2.3994e-01, -4.1264e-01,  2.2670e-01],\n",
       "                         [-7.2736e-02,  3.8435e-02,  4.4961e-02],\n",
       "                         [-2.1231e-01,  2.9264e-01, -1.4149e-01]],\n",
       "               \n",
       "                        [[ 3.2136e-01, -2.5902e-01,  4.1119e-02],\n",
       "                         [ 6.0019e-02,  1.2725e-01, -1.6326e-01],\n",
       "                         [-1.9750e-01,  2.5471e-01, -1.7664e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.2674e-01,  4.1193e-01,  1.0951e-01],\n",
       "                         [ 2.9547e-01, -1.4481e-01, -1.5562e-01],\n",
       "                         [-2.1227e-01, -3.4246e-01, -1.0721e-01]],\n",
       "               \n",
       "                        [[-1.7717e-01,  1.2557e-01, -6.4691e-02],\n",
       "                         [ 2.0832e-01, -1.5972e-01, -1.3837e-01],\n",
       "                         [ 5.0811e-02, -8.6246e-02,  2.6788e-01]],\n",
       "               \n",
       "                        [[-2.5491e-01,  3.8355e-02, -1.2175e-01],\n",
       "                         [ 3.9995e-02, -2.0600e-02,  7.5007e-02],\n",
       "                         [ 5.1562e-03,  7.5210e-02,  1.9765e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.1350e-12,  5.7701e-12,  7.3418e-12],\n",
       "                         [ 4.9463e-12,  5.7191e-12,  7.1803e-12],\n",
       "                         [ 4.7922e-12,  4.5284e-12,  5.8525e-12]],\n",
       "               \n",
       "                        [[ 2.5770e-12,  2.7285e-12,  3.3251e-12],\n",
       "                         [ 2.4321e-12,  2.7106e-12,  3.0411e-12],\n",
       "                         [ 3.3703e-12,  2.5826e-12,  2.7330e-12]],\n",
       "               \n",
       "                        [[-1.6590e-12, -2.8083e-12, -2.9706e-12],\n",
       "                         [-2.3134e-12, -3.5608e-12, -4.3996e-12],\n",
       "                         [-1.9806e-12, -3.4872e-12, -3.9671e-12]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.2981e-28,  1.0177e-28,  5.9082e-30],\n",
       "                         [ 2.5761e-28,  3.3315e-28,  2.0465e-28],\n",
       "                         [ 2.5047e-28,  1.3011e-28,  1.7277e-29]],\n",
       "               \n",
       "                        [[ 6.9094e-32,  1.3484e-29, -1.8687e-29],\n",
       "                         [-1.8053e-32,  7.3880e-33,  9.8066e-30],\n",
       "                         [ 5.9477e-32,  1.0110e-29, -1.5955e-29]],\n",
       "               \n",
       "                        [[ 1.9231e-29,  9.0464e-27,  1.0335e-25],\n",
       "                         [ 1.7352e-29,  5.6917e-29,  5.7276e-29],\n",
       "                         [ 8.8129e-29, -1.8685e-26,  3.9491e-26]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.6420e-01, -7.9228e-02, -1.4184e-01],\n",
       "                         [ 1.3276e-01, -4.7937e-02,  4.4284e-02],\n",
       "                         [-1.1222e-01,  3.0138e-01,  2.3320e-01]],\n",
       "               \n",
       "                        [[-1.3508e-01,  3.6352e-02, -1.3426e-02],\n",
       "                         [ 1.8677e-01, -2.5663e-01, -1.7175e-01],\n",
       "                         [-7.4531e-02,  7.0938e-02, -1.7699e-02]],\n",
       "               \n",
       "                        [[-3.4340e-02,  3.6623e-01,  1.9697e-01],\n",
       "                         [ 2.5568e-01, -1.1257e-01, -3.2239e-02],\n",
       "                         [-1.4803e-01, -1.0960e-01, -1.3113e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.1227e-15,  4.8734e-16,  1.5529e-14],\n",
       "                         [ 7.6488e-15,  1.2028e-15,  1.3987e-14],\n",
       "                         [ 4.6932e-15, -1.7574e-14, -4.8958e-15]],\n",
       "               \n",
       "                        [[-3.0412e-14,  2.5040e-13,  5.8847e-13],\n",
       "                         [ 1.6973e-13,  3.4781e-13,  6.4594e-13],\n",
       "                         [ 3.8230e-14,  2.2525e-13,  4.2798e-13]],\n",
       "               \n",
       "                        [[-2.8477e-16,  3.5315e-15,  1.1120e-14],\n",
       "                         [-3.0282e-14, -2.6756e-14, -3.0628e-14],\n",
       "                         [-2.9158e-14, -8.3768e-15, -8.1623e-15]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-3.0511e-03, -8.6655e-05,  1.2166e-15, -2.0458e-03,  6.3764e-04,\n",
       "                       -1.4794e-03, -5.2500e-12, -8.2863e-04, -2.8173e-03, -1.0595e-01,\n",
       "                        1.4689e-11,  1.7210e-15, -2.0363e-03, -1.5961e-07, -4.7478e-04,\n",
       "                       -1.0389e-03,  3.3261e-04, -1.4614e-06,  1.9089e-04,  8.7997e-05,\n",
       "                       -9.6723e-03,  9.9511e-09, -1.5407e-06,  4.1866e-04,  1.5046e-03,\n",
       "                        3.3969e-07,  7.4022e-06, -6.8840e-06,  2.7429e-03, -5.6621e-04,\n",
       "                       -2.2246e-03, -9.2189e-04,  5.5266e-04,  6.1269e-03,  2.3267e-02,\n",
       "                        7.0601e-16,  3.2188e-09,  1.2678e-03, -8.1637e-04, -2.4845e-12,\n",
       "                       -7.7996e-05,  7.1147e-04,  4.4637e-04, -2.0091e-03, -5.3441e-15,\n",
       "                        2.0745e-03,  9.5743e-04,  3.4327e-04,  1.9231e-04,  6.4435e-08,\n",
       "                       -5.5868e-03,  3.9275e-12, -9.5649e-05,  3.4872e-04,  1.8659e-03,\n",
       "                       -9.5329e-10,  4.3621e-04, -4.3469e-03,  7.9983e-12, -8.1725e-04,\n",
       "                       -6.0914e-08,  9.7693e-25,  5.5569e-04, -1.4887e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 7.0501e-02,  4.6988e-01, -1.9562e-07, -5.0761e-02, -1.2087e-01,\n",
       "                        1.1365e-01, -3.3570e-06,  3.6637e-01, -1.1250e-02, -3.4213e-01,\n",
       "                       -2.5606e-10, -5.6889e-08,  7.7629e-01, -5.0325e-02, -4.0210e-02,\n",
       "                       -2.4891e-01, -1.4657e-01, -6.7287e-02, -3.2791e-01, -3.8358e-01,\n",
       "                        2.5447e-01, -1.9791e-06, -2.5914e-02, -2.3702e-02,  2.6083e-02,\n",
       "                       -1.1793e-02, -3.1245e-02,  9.8977e-01, -5.9683e-02, -1.5673e-01,\n",
       "                       -4.4701e-02, -8.3262e-02, -3.0436e-02, -1.8305e-02, -5.1191e-02,\n",
       "                       -4.3471e-09, -4.9165e-11,  1.1629e-01, -3.0544e-01, -2.2258e-06,\n",
       "                       -4.0223e-01, -2.3050e-02, -4.9754e-01,  1.1354e-01, -2.8455e-08,\n",
       "                       -4.0639e-02, -6.4169e-01, -4.4654e-01, -2.5870e-01, -1.4586e-02,\n",
       "                        4.6797e-03, -1.2934e-10,  1.0775e-01, -6.7175e-01, -7.1939e-01,\n",
       "                       -2.9767e-03, -2.9600e-01,  6.4334e-03, -4.4158e-07,  1.7886e-02,\n",
       "                       -4.0753e-02, -1.0388e-09, -3.7723e-02, -3.2897e-10], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([ 3.8525e-01,  2.7976e-01, -4.3828e-07,  5.9550e-01,  1.6507e-01,\n",
       "                        2.5299e-01,  1.5815e-06,  4.6193e-01,  7.2687e-04,  4.0503e-01,\n",
       "                       -1.4943e-06, -1.2495e-07,  6.0997e-02,  1.1676e-02,  4.1959e-01,\n",
       "                        3.8386e-01,  2.2576e-01,  1.5824e-02,  4.3706e-01,  4.7323e-01,\n",
       "                        3.3055e-01,  2.1001e-05, -2.9399e-03,  4.8946e-01,  1.8725e-01,\n",
       "                        3.5524e-03,  8.1251e-03,  2.3122e-03,  4.0193e-01,  2.1323e-01,\n",
       "                        4.6875e-01,  1.4861e-01,  5.2050e-01,  4.0841e-01, -6.2190e-03,\n",
       "                        4.7986e-14, -4.0365e-13,  1.8856e-01,  3.6550e-01, -8.7930e-06,\n",
       "                        2.6131e-01,  5.9838e-01,  3.7358e-01,  4.4811e-01,  1.6748e-23,\n",
       "                        5.0787e-01,  5.3384e-01,  1.9870e-01,  2.3167e-01,  4.1041e-03,\n",
       "                        3.9277e-01,  4.2810e-10,  4.2272e-01,  4.1405e-01,  4.8938e-01,\n",
       "                        3.1147e-04,  2.4365e-01,  5.2232e-01,  6.9597e-07,  3.9178e-02,\n",
       "                       -3.5066e-03, -1.1097e-32,  2.6890e-01, -2.3200e-11], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-3.3177e-02, -6.6276e-02, -1.9599e-02],\n",
       "                         [-3.5394e-02, -1.4106e-01, -1.1250e-01],\n",
       "                         [-1.0069e-01,  2.0636e-01,  7.3052e-02]],\n",
       "               \n",
       "                        [[-4.3130e-02,  3.7964e-02, -5.8966e-03],\n",
       "                         [-1.0560e-01, -3.3924e-02, -6.9316e-02],\n",
       "                         [ 2.7613e-02, -6.1355e-02, -9.2837e-03]],\n",
       "               \n",
       "                        [[-6.8838e-09, -4.4581e-09, -8.8244e-09],\n",
       "                         [-2.0919e-09, -1.5030e-11, -4.9021e-09],\n",
       "                         [-3.6586e-09, -1.7636e-09, -7.7927e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.3838e-10, -6.1078e-10, -5.8199e-10],\n",
       "                         [ 1.5314e-11, -6.8630e-13, -2.1501e-10],\n",
       "                         [ 1.9294e-11, -7.6880e-12, -4.7433e-10]],\n",
       "               \n",
       "                        [[-1.1014e-01, -1.4492e-01, -5.7979e-02],\n",
       "                         [-2.3952e-01, -1.1920e-01, -1.4566e-01],\n",
       "                         [ 3.6207e-02,  1.6807e-02,  7.2562e-02]],\n",
       "               \n",
       "                        [[-4.8665e-11, -3.0214e-11, -1.3606e-10],\n",
       "                         [-5.7305e-12, -5.7036e-14, -1.1438e-10],\n",
       "                         [-1.5577e-11, -1.0569e-11, -1.2266e-10]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.6817e-02, -1.4069e-02, -9.9153e-02],\n",
       "                         [ 3.6874e-02, -1.0419e-01, -5.7113e-02],\n",
       "                         [ 1.2559e-01,  6.8282e-02, -2.4031e-04]],\n",
       "               \n",
       "                        [[ 2.2098e-02,  3.6243e-05,  4.9132e-02],\n",
       "                         [-7.0068e-02, -5.8072e-02,  3.2187e-02],\n",
       "                         [-1.0543e-02, -1.9938e-03,  4.6586e-02]],\n",
       "               \n",
       "                        [[-9.7754e-11, -4.1323e-10,  2.7797e-09],\n",
       "                         [ 3.0696e-10,  9.3693e-12,  3.1953e-09],\n",
       "                         [ 4.8782e-10,  1.5858e-10,  3.3743e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.0210e-11,  6.7535e-13,  4.8182e-10],\n",
       "                         [-3.0717e-11,  3.2120e-13,  2.2182e-10],\n",
       "                         [-3.4829e-12, -6.7749e-11, -1.5707e-10]],\n",
       "               \n",
       "                        [[-8.6362e-02, -3.8219e-02,  5.3125e-02],\n",
       "                         [ 8.1450e-03, -7.6519e-02,  2.2695e-02],\n",
       "                         [ 2.4349e-02, -1.3499e-01, -3.2909e-02]],\n",
       "               \n",
       "                        [[ 2.2762e-12,  2.2613e-13,  2.0330e-11],\n",
       "                         [ 8.8019e-13,  3.8722e-14,  1.9994e-11],\n",
       "                         [ 9.9406e-12,  6.9537e-12,  2.5140e-11]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0919e-02,  5.3899e-02,  2.3517e-02],\n",
       "                         [ 1.0041e-01,  5.8491e-02,  1.7205e-01],\n",
       "                         [ 9.5477e-02,  1.8420e-01,  9.8242e-02]],\n",
       "               \n",
       "                        [[ 6.9382e-02,  2.6590e-02, -1.0125e-01],\n",
       "                         [ 5.8807e-02,  2.2384e-02,  1.0497e-01],\n",
       "                         [ 1.0139e-01,  4.8644e-02,  1.5142e-02]],\n",
       "               \n",
       "                        [[-3.4797e-09, -2.4727e-09, -1.4477e-09],\n",
       "                         [-1.2254e-09, -6.7060e-13,  7.9760e-10],\n",
       "                         [-4.8075e-09, -3.9270e-09, -2.8818e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.4683e-10, -3.9766e-10,  3.9255e-10],\n",
       "                         [-1.9356e-10, -2.8715e-13,  6.6495e-10],\n",
       "                         [-5.4611e-10, -1.4077e-10,  7.7542e-10]],\n",
       "               \n",
       "                        [[ 2.9196e-02, -3.0019e-02, -4.8887e-02],\n",
       "                         [-7.0499e-03, -1.8971e-02,  4.3129e-02],\n",
       "                         [-6.5686e-02, -2.9485e-02,  9.5457e-02]],\n",
       "               \n",
       "                        [[ 2.3870e-11,  1.1137e-11,  4.2354e-12],\n",
       "                         [ 1.1860e-11,  1.0981e-13, -3.0576e-12],\n",
       "                         [ 2.2152e-11,  2.5591e-12, -2.9130e-12]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.8394e-02, -1.2264e-01, -3.5637e-02],\n",
       "                         [-6.5626e-02,  8.5449e-03,  7.3998e-04],\n",
       "                         [-7.1380e-02, -6.6283e-02,  2.7089e-02]],\n",
       "               \n",
       "                        [[ 2.4821e-02, -5.7017e-02,  8.1589e-02],\n",
       "                         [ 2.4374e-02,  4.7393e-02, -1.4394e-02],\n",
       "                         [-9.2947e-03,  6.5693e-02, -1.4857e-01]],\n",
       "               \n",
       "                        [[-1.1312e-08,  3.1459e-09,  1.1734e-08],\n",
       "                         [-1.5909e-08, -1.7383e-11,  8.5642e-09],\n",
       "                         [-2.6415e-08, -7.4808e-09, -1.1930e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1391e-09, -6.3065e-11,  4.3369e-12],\n",
       "                         [ 2.0030e-11,  2.9225e-13, -3.2219e-11],\n",
       "                         [ 5.7775e-10, -3.7636e-10, -5.2667e-10]],\n",
       "               \n",
       "                        [[-2.1312e-02,  5.7356e-04, -1.0271e-01],\n",
       "                         [ 1.7651e-02,  2.3202e-02, -8.8844e-02],\n",
       "                         [-7.0330e-02, -1.2412e-02, -6.8904e-02]],\n",
       "               \n",
       "                        [[ 1.7670e-10,  1.2359e-11,  1.6519e-11],\n",
       "                         [ 1.7231e-10,  1.3839e-13,  7.8552e-12],\n",
       "                         [ 3.1581e-10,  1.6951e-10,  1.7940e-10]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.5044e-02,  4.0207e-02, -2.1266e-01],\n",
       "                         [ 1.6475e-01,  3.9017e-02, -9.6593e-02],\n",
       "                         [-2.3896e-02,  8.2161e-02, -1.9517e-01]],\n",
       "               \n",
       "                        [[ 1.1893e-01, -1.4771e-03, -2.0202e-01],\n",
       "                         [ 6.0730e-02,  1.2401e-01, -1.3503e-01],\n",
       "                         [ 6.8079e-02,  3.8523e-02, -1.4090e-01]],\n",
       "               \n",
       "                        [[ 5.2236e-09,  4.7631e-09,  1.4777e-08],\n",
       "                         [ 4.1978e-10,  3.4706e-12,  1.0289e-08],\n",
       "                         [-6.3177e-09, -6.9101e-09,  2.6716e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.8881e-10,  2.5682e-10,  1.3138e-11],\n",
       "                         [ 1.7180e-10,  1.1610e-12, -2.5780e-10],\n",
       "                         [ 1.8217e-10,  6.8667e-10, -3.2042e-11]],\n",
       "               \n",
       "                        [[ 6.1891e-02,  3.4518e-02, -1.6407e-01],\n",
       "                         [ 1.0514e-01,  2.1985e-03, -7.3447e-02],\n",
       "                         [ 4.3710e-02, -6.2312e-02, -3.0990e-02]],\n",
       "               \n",
       "                        [[-5.0607e-12,  2.7338e-11,  4.4185e-11],\n",
       "                         [-1.8492e-11,  1.8046e-14, -2.3321e-12],\n",
       "                         [-5.1070e-11, -3.2433e-11, -3.5065e-11]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1391e-01,  1.2182e-01,  7.8000e-02],\n",
       "                         [ 8.2127e-02, -4.2121e-02,  1.3768e-01],\n",
       "                         [ 4.5743e-02,  9.6477e-02,  1.8028e-01]],\n",
       "               \n",
       "                        [[ 1.1772e-01, -8.0650e-02, -1.4270e-02],\n",
       "                         [ 3.3406e-02, -1.5700e-01, -4.9734e-02],\n",
       "                         [-2.1167e-02, -4.2047e-02,  1.8009e-02]],\n",
       "               \n",
       "                        [[-3.1402e-09, -1.0358e-10,  1.2639e-09],\n",
       "                         [-3.0487e-09, -2.2629e-12,  1.5967e-09],\n",
       "                         [ 3.4298e-09,  6.5272e-09,  7.6864e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.2250e-10,  8.4647e-13,  4.2393e-10],\n",
       "                         [ 4.5074e-10, -7.3948e-13,  3.0065e-10],\n",
       "                         [ 2.6776e-10, -1.7369e-10,  2.0971e-10]],\n",
       "               \n",
       "                        [[-4.7590e-02, -3.2086e-02, -1.0382e-01],\n",
       "                         [-6.9231e-02, -8.1556e-02, -1.1927e-01],\n",
       "                         [-2.3012e-02, -3.4477e-03,  4.7145e-02]],\n",
       "               \n",
       "                        [[ 2.7393e-11,  9.6687e-13,  4.1007e-11],\n",
       "                         [ 1.4633e-11, -2.6082e-14,  3.6183e-11],\n",
       "                         [ 4.6862e-11,  4.8886e-11,  7.6777e-11]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 1.4319e-03,  6.5791e-04, -6.2202e-04, -3.6500e-04, -1.0313e-03,\n",
       "                        5.7896e-05,  1.6942e-03, -1.1478e-03,  1.9141e-03,  1.3861e-03,\n",
       "                       -2.1465e-03,  6.4370e-03,  8.8558e-04, -3.9787e-04,  4.1703e-03,\n",
       "                       -4.1756e-04, -4.1133e-05,  1.3838e-04,  1.0606e-01,  2.3850e-03,\n",
       "                       -1.6543e-03, -1.2845e-03, -3.0096e-03, -6.6241e-04, -5.8452e-04,\n",
       "                       -7.1947e-04,  5.5374e-04, -1.8512e-03,  8.5349e-06, -2.7193e-03,\n",
       "                        1.4187e-03,  1.0027e-04,  7.0749e-04,  1.2013e-03, -1.0079e-03,\n",
       "                       -3.3577e-04,  1.8727e-03,  1.8623e-03, -2.0001e-04,  7.4711e-04,\n",
       "                       -8.2206e-05, -1.5794e-03,  7.6573e-04, -1.3078e-03,  1.4692e-03,\n",
       "                        7.1627e-04,  1.6850e-03, -1.8993e-03, -7.4857e-04, -4.0671e-04,\n",
       "                       -1.5788e-03, -2.5612e-03, -9.5477e-04,  2.1310e-04, -6.4991e-03,\n",
       "                       -5.0795e-03,  5.7819e-05,  2.3607e-03,  9.6753e-04,  7.5966e-04,\n",
       "                       -6.0232e-03, -9.8573e-04,  7.7085e-04,  4.3770e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.2780, -0.1950, -0.1771, -0.1396, -0.4147, -0.3061, -0.2442, -0.2346,\n",
       "                       -0.4372, -0.1658, -0.3120, -0.0735, -0.2197, -0.4413, -0.1155, -0.3183,\n",
       "                       -0.2112, -0.3380, -0.3141, -0.1255, -0.3649, -0.2278, -0.1764, -0.2179,\n",
       "                       -0.1428, -0.2253, -0.3937, -0.4993, -0.2698, -0.2098, -0.3255, -0.3099,\n",
       "                       -0.2977, -0.2059, -0.2094, -0.2615, -0.2529, -0.3091, -0.2174, -0.3083,\n",
       "                       -0.3558, -0.1935, -0.3550, -0.4475, -0.3182, -0.1844, -0.3268, -0.1800,\n",
       "                       -0.0994, -0.2182, -0.3011, -0.2602, -0.3146, -0.1516, -0.4073, -0.2031,\n",
       "                       -0.2168, -0.2829, -0.3712, -0.3392, -0.1162, -0.2966, -0.1799, -0.1198],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([ 0.3742,  0.3326,  0.3508,  0.3310,  0.4334,  0.2707,  0.4626,  0.3536,\n",
       "                        0.3925,  0.2839,  0.4176, -0.0047,  0.3539,  0.3628,  0.3381,  0.4440,\n",
       "                        0.2921,  0.3997,  0.4304,  0.3588,  0.4595,  0.4911,  0.3339,  0.3536,\n",
       "                        0.4808,  0.3496,  0.2896,  0.5237,  0.4000,  0.3694,  0.4357,  0.3733,\n",
       "                        0.4230,  0.4760,  0.3464,  0.2487,  0.3518,  0.3925,  0.3324,  0.4033,\n",
       "                        0.3371,  0.2704,  0.4949,  0.5148,  0.4751,  0.3514,  0.3802,  0.3827,\n",
       "                        0.3922,  0.3749,  0.3719,  0.4378,  0.4600,  0.3165,  0.9504,  0.4069,\n",
       "                        0.2486,  0.4330,  0.3085,  0.4416,  0.2872,  0.5327,  0.4725,  0.4609],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[ 5.6846e-02,  1.4157e-01,  4.9773e-02],\n",
       "                         [ 3.6349e-03, -5.5035e-03,  2.4299e-02],\n",
       "                         [ 5.6452e-03,  4.6911e-02, -1.1392e-02]],\n",
       "               \n",
       "                        [[-1.4079e-01, -3.4221e-02,  1.6551e-01],\n",
       "                         [ 9.8691e-02, -4.0693e-03, -4.3539e-02],\n",
       "                         [-3.1333e-02,  1.9683e-01,  2.2865e-02]],\n",
       "               \n",
       "                        [[ 1.8079e-02, -4.0008e-02,  4.2737e-02],\n",
       "                         [-8.5254e-02, -5.5959e-02, -5.7558e-02],\n",
       "                         [ 2.5236e-02, -8.1493e-02, -8.2467e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.3417e-02, -9.6951e-02, -4.7738e-02],\n",
       "                         [-6.9654e-02, -1.1906e-01, -9.3694e-02],\n",
       "                         [-1.7484e-02, -1.0873e-01, -2.2214e-02]],\n",
       "               \n",
       "                        [[ 2.9822e-02,  1.1499e-01, -2.3611e-02],\n",
       "                         [-1.2149e-01,  2.8510e-02,  1.1605e-01],\n",
       "                         [-6.6724e-02,  1.2458e-01,  3.4163e-03]],\n",
       "               \n",
       "                        [[-1.3072e-02,  5.4348e-02,  1.5633e-02],\n",
       "                         [-1.3426e-01, -6.2354e-02, -2.5316e-05],\n",
       "                         [-3.9838e-02, -8.3690e-02, -8.7673e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.1480e-02, -7.2586e-02, -5.0806e-02],\n",
       "                         [-1.2793e-01, -8.0301e-02, -1.5233e-01],\n",
       "                         [-7.4215e-02, -3.5109e-02, -6.4095e-02]],\n",
       "               \n",
       "                        [[-4.2147e-02, -5.9669e-02, -2.3129e-01],\n",
       "                         [ 5.8808e-02,  9.1493e-03, -4.6879e-02],\n",
       "                         [-1.3843e-01,  1.4313e-02, -4.4074e-02]],\n",
       "               \n",
       "                        [[-4.7675e-02,  1.3353e-01,  4.0883e-02],\n",
       "                         [ 8.8675e-03,  6.0647e-02,  6.3339e-02],\n",
       "                         [-8.7133e-02, -2.4709e-01, -1.2092e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.7548e-02, -6.7048e-02, -1.7503e-02],\n",
       "                         [-2.7709e-02, -4.6823e-02, -5.5717e-02],\n",
       "                         [-3.5300e-02, -3.8373e-02,  1.2334e-02]],\n",
       "               \n",
       "                        [[-9.0684e-02,  4.5097e-02, -3.1575e-02],\n",
       "                         [-4.3271e-02, -5.2855e-02, -1.1730e-01],\n",
       "                         [-7.5226e-03, -6.7461e-02, -6.1726e-02]],\n",
       "               \n",
       "                        [[-6.7172e-02,  1.6813e-02, -2.1872e-02],\n",
       "                         [-4.5642e-02, -1.1044e-01, -1.2587e-01],\n",
       "                         [ 2.6086e-02,  3.4735e-02,  7.2804e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.6389e-02,  1.2295e-01, -6.5035e-02],\n",
       "                         [ 2.9496e-02,  1.0384e-01, -3.6128e-02],\n",
       "                         [ 6.4192e-02,  4.0200e-02, -5.2788e-02]],\n",
       "               \n",
       "                        [[-3.8125e-02, -3.2333e-02,  4.9069e-02],\n",
       "                         [ 4.7176e-02, -7.6434e-02,  3.2560e-02],\n",
       "                         [-7.4863e-02, -1.2324e-01,  9.5155e-03]],\n",
       "               \n",
       "                        [[-5.1883e-03,  9.8166e-02, -1.3064e-01],\n",
       "                         [ 1.1987e-01,  1.9594e-01, -8.1519e-02],\n",
       "                         [ 9.1754e-03,  6.9366e-02,  1.0491e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.1346e-03,  1.0159e-01, -2.2518e-02],\n",
       "                         [-9.0187e-02, -8.4981e-03,  2.1359e-02],\n",
       "                         [-7.8611e-02, -3.9271e-02, -1.1058e-01]],\n",
       "               \n",
       "                        [[ 5.3114e-02, -4.6244e-02,  4.6426e-02],\n",
       "                         [ 2.5806e-02,  1.1044e-01,  3.8947e-02],\n",
       "                         [-7.2269e-02,  4.9677e-02, -1.0086e-02]],\n",
       "               \n",
       "                        [[ 3.2760e-02,  3.6360e-02, -1.5572e-01],\n",
       "                         [ 3.7840e-02,  9.6803e-02,  1.1313e-02],\n",
       "                         [ 2.2356e-02,  3.7168e-02,  1.3717e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 9.6723e-02, -6.9836e-02, -4.1994e-02],\n",
       "                         [ 3.4943e-03, -1.0548e-01,  9.6325e-04],\n",
       "                         [ 1.5467e-01,  1.1899e-02,  6.5481e-02]],\n",
       "               \n",
       "                        [[ 1.7588e-01,  4.8224e-03,  7.3881e-02],\n",
       "                         [ 4.4875e-02, -7.9207e-02,  1.0845e-01],\n",
       "                         [ 4.4067e-02, -9.4472e-02, -9.0702e-02]],\n",
       "               \n",
       "                        [[-3.8911e-02, -1.0833e-01, -4.6965e-02],\n",
       "                         [-5.5063e-02, -4.7697e-02, -1.3808e-01],\n",
       "                         [-8.6047e-02,  9.8907e-02, -3.1472e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.4292e-02,  3.4040e-02,  1.3890e-01],\n",
       "                         [-4.2187e-02, -1.5226e-01, -1.4390e-02],\n",
       "                         [ 6.0886e-02, -1.0225e-01, -6.9637e-02]],\n",
       "               \n",
       "                        [[-3.5716e-02,  3.1871e-02,  6.9762e-03],\n",
       "                         [ 4.2023e-02,  4.2229e-03, -3.0155e-02],\n",
       "                         [ 1.7904e-01,  1.1674e-01, -1.0151e-01]],\n",
       "               \n",
       "                        [[-2.1256e-02,  5.0400e-02,  6.7049e-02],\n",
       "                         [-2.8948e-02, -1.2061e-02, -1.8193e-02],\n",
       "                         [-4.3412e-03, -6.4962e-02, -1.3566e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.2056e-02, -1.2101e-02, -1.9785e-02],\n",
       "                         [ 5.1084e-03,  1.1984e-01,  6.8699e-03],\n",
       "                         [ 2.0399e-02,  1.2507e-01, -6.8749e-02]],\n",
       "               \n",
       "                        [[-3.7420e-02,  1.5664e-02,  6.9668e-02],\n",
       "                         [-1.1288e-02, -9.2266e-02, -3.3240e-02],\n",
       "                         [ 9.7098e-02, -1.1712e-01, -6.2658e-02]],\n",
       "               \n",
       "                        [[ 9.1784e-02,  2.2416e-02, -3.4674e-02],\n",
       "                         [ 2.4884e-02,  2.8088e-02, -6.6139e-02],\n",
       "                         [ 5.6285e-03, -2.7866e-02, -7.5527e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.9620e-03,  1.9656e-02,  4.2421e-02],\n",
       "                         [-7.9933e-03,  5.6591e-02,  9.1998e-02],\n",
       "                         [-1.1556e-02, -6.0949e-02,  3.0408e-02]],\n",
       "               \n",
       "                        [[-7.8718e-02,  7.3010e-02,  4.7494e-02],\n",
       "                         [ 6.0565e-03,  1.4085e-01,  6.7320e-02],\n",
       "                         [ 4.9074e-02,  1.6504e-01,  1.3903e-01]],\n",
       "               \n",
       "                        [[ 2.6308e-03, -4.9968e-02, -6.2877e-02],\n",
       "                         [ 6.6030e-02, -7.2584e-02, -3.9187e-03],\n",
       "                         [ 5.6337e-02, -2.8144e-02,  2.3234e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.5354e-02,  2.6879e-02, -6.3267e-03],\n",
       "                         [-1.2945e-01, -8.9093e-02, -6.8371e-03],\n",
       "                         [-3.4210e-02, -7.9968e-02, -5.3849e-02]],\n",
       "               \n",
       "                        [[-1.6801e-01, -8.3177e-02, -3.8461e-02],\n",
       "                         [-1.3995e-01, -4.7438e-02, -1.2734e-01],\n",
       "                         [-1.8953e-01, -8.8588e-02, -7.0681e-02]],\n",
       "               \n",
       "                        [[-1.4901e-01, -8.6289e-02,  1.2224e-02],\n",
       "                         [-2.1251e-01, -8.6898e-02, -5.8514e-02],\n",
       "                         [ 4.3238e-02, -3.8012e-02,  6.7278e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.3556e-02,  6.0574e-02,  1.1961e-01],\n",
       "                         [-1.0973e-01, -1.8093e-01, -8.5843e-02],\n",
       "                         [-1.6059e-02, -7.9260e-02, -1.4583e-01]],\n",
       "               \n",
       "                        [[ 4.2590e-02, -6.4622e-02,  1.8683e-02],\n",
       "                         [-9.5658e-02,  1.3986e-02,  1.3267e-01],\n",
       "                         [-2.2516e-01, -8.0586e-02,  2.3125e-02]],\n",
       "               \n",
       "                        [[ 1.1706e-01,  3.2252e-02,  8.5938e-02],\n",
       "                         [ 2.4577e-02, -4.4049e-02, -3.1827e-04],\n",
       "                         [-4.7288e-02, -1.1304e-01, -1.1965e-01]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 0.0001,  0.0006, -0.0009, -0.0003,  0.0019, -0.0008, -0.0009, -0.0011,\n",
       "                       -0.0002, -0.0001,  0.0009, -0.0010, -0.0017, -0.0031, -0.0033, -0.0027,\n",
       "                        0.0003, -0.0008, -0.0004, -0.0022, -0.0006, -0.0026,  0.0016, -0.0037,\n",
       "                       -0.0074, -0.0030, -0.0024, -0.0033, -0.0008,  0.0017, -0.0015,  0.0010,\n",
       "                       -0.0029, -0.0058,  0.0016, -0.0026,  0.0020,  0.0019,  0.0015, -0.0006,\n",
       "                       -0.0011,  0.0002,  0.0006, -0.0024,  0.0017,  0.0017, -0.0028, -0.0006,\n",
       "                       -0.0028, -0.0008,  0.0005, -0.0010,  0.0003,  0.0011, -0.0014,  0.0006,\n",
       "                        0.0006,  0.0019,  0.0002,  0.1036,  0.0021, -0.0006, -0.0010,  0.0018],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.2458, -0.3103, -0.6448, -0.3012, -0.4756, -0.4356, -0.3806, -0.4764,\n",
       "                       -0.2660, -0.4080, -0.3504, -0.3412, -0.3058, -0.3246, -0.4190, -0.5418,\n",
       "                       -0.3525, -0.2297, -0.2837, -0.3477, -0.4726, -0.3641, -0.2286, -0.2912,\n",
       "                       -0.6210, -0.5276, -0.4262, -0.2381, -0.4362, -0.5316, -0.2606, -0.5221,\n",
       "                       -0.3780, -0.3705, -0.7534, -0.3899, -0.2374, -0.8842, -0.1544, -0.3600,\n",
       "                       -0.2519, -0.1886, -0.3628, -0.2173, -0.4840, -0.3601, -0.7270, -0.5030,\n",
       "                       -0.3159, -0.6772, -0.5107, -0.6889, -0.2239, -0.3141, -0.2163, -0.7637,\n",
       "                       -0.3805, -0.3778, -0.4905, -1.1798, -0.2514, -0.3341, -0.2657, -0.3466],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.2018, 0.2359, 0.5364, 0.2551, 0.3692, 0.2895, 0.3281, 0.4515, 0.2252,\n",
       "                       0.2493, 0.2132, 0.3358, 0.2725, 0.2518, 0.2902, 0.4250, 0.3208, 0.2622,\n",
       "                       0.2257, 0.3272, 0.3500, 0.3110, 0.1943, 0.2801, 0.4413, 0.3520, 0.3844,\n",
       "                       0.2743, 0.3556, 0.3437, 0.2763, 0.3675, 0.3001, 0.3567, 0.4835, 0.2516,\n",
       "                       0.1918, 0.6465, 0.1296, 0.3112, 0.2042, 0.6539, 0.2695, 0.1668, 0.4819,\n",
       "                       0.2924, 0.3823, 0.4426, 0.3175, 0.6447, 0.4600, 0.5070, 0.2365, 0.2478,\n",
       "                       0.3020, 0.6652, 0.3375, 0.3056, 0.2934, 0.9185, 0.3300, 0.2769, 0.2881,\n",
       "                       0.2635], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[ 5.4177e-05,  1.0940e-05, -7.8813e-05],\n",
       "                         [-8.1466e-05,  6.2149e-06, -4.8549e-05],\n",
       "                         [-7.1148e-06,  7.9622e-06, -3.0051e-05]],\n",
       "               \n",
       "                        [[ 1.0858e-05,  9.3730e-05,  1.5722e-04],\n",
       "                         [-1.3839e-05,  4.9300e-05,  5.9186e-05],\n",
       "                         [ 6.5929e-05, -3.8234e-05, -2.5309e-05]],\n",
       "               \n",
       "                        [[-1.4788e-05,  2.1876e-05, -4.5963e-05],\n",
       "                         [-1.3850e-05, -6.7863e-05, -5.6396e-05],\n",
       "                         [-3.6613e-05,  1.5483e-05,  3.0362e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 5.9064e-05, -5.6993e-05,  5.2258e-06],\n",
       "                         [ 4.8892e-05,  1.4627e-05,  1.0686e-06],\n",
       "                         [ 1.8826e-05,  7.0126e-06, -9.1366e-06]],\n",
       "               \n",
       "                        [[ 2.1580e-05, -2.3893e-05,  3.6974e-05],\n",
       "                         [ 3.6754e-05,  3.4823e-05,  5.3091e-05],\n",
       "                         [ 2.4002e-05,  1.7840e-05,  5.3732e-05]],\n",
       "               \n",
       "                        [[-3.2070e-05,  1.5714e-05, -2.4717e-05],\n",
       "                         [-2.2117e-05,  3.0146e-05, -3.5543e-05],\n",
       "                         [-5.7094e-06, -1.0040e-05, -6.0430e-06]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.2667e-01, -7.4985e-02,  1.1839e-01],\n",
       "                         [ 8.0080e-02,  3.2111e-02,  4.6574e-03],\n",
       "                         [ 3.0110e-02,  5.0198e-02,  1.2945e-01]],\n",
       "               \n",
       "                        [[-3.9607e-02, -5.8149e-02, -1.5181e-02],\n",
       "                         [ 1.4875e-02, -8.2489e-02,  2.9126e-02],\n",
       "                         [-6.7170e-02, -4.1791e-02, -4.2324e-02]],\n",
       "               \n",
       "                        [[-1.0701e-02, -2.2814e-02, -7.9228e-02],\n",
       "                         [-3.4320e-02,  5.0753e-02, -1.0091e-02],\n",
       "                         [ 3.6162e-02, -2.9876e-02, -4.5763e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.9784e-02,  7.5840e-02, -1.9954e-02],\n",
       "                         [-4.6774e-03,  2.3887e-02, -3.6144e-02],\n",
       "                         [-6.3362e-03, -4.1429e-02,  6.4960e-03]],\n",
       "               \n",
       "                        [[ 1.8447e-02, -4.2024e-02,  3.2086e-02],\n",
       "                         [-5.0663e-02, -5.4501e-02, -9.1369e-02],\n",
       "                         [-9.1084e-02, -1.9918e-01, -9.8364e-02]],\n",
       "               \n",
       "                        [[-8.9885e-02, -5.8665e-02, -8.5599e-02],\n",
       "                         [-2.1684e-03, -1.6271e-02, -3.9982e-02],\n",
       "                         [ 3.0759e-02, -6.6219e-03, -1.6039e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.6698e-01, -3.9737e-02, -6.1322e-02],\n",
       "                         [-8.7206e-02, -5.4195e-02, -2.1172e-02],\n",
       "                         [-1.1167e-01, -5.2337e-02, -1.2589e-01]],\n",
       "               \n",
       "                        [[-5.8349e-02, -4.0392e-03,  6.5282e-02],\n",
       "                         [ 1.8018e-02, -5.1598e-03, -7.3728e-02],\n",
       "                         [-3.2374e-02, -2.7106e-02,  3.2710e-02]],\n",
       "               \n",
       "                        [[-2.4344e-02, -1.0009e-01, -3.7149e-03],\n",
       "                         [-1.1372e-01, -5.7549e-02, -4.8833e-02],\n",
       "                         [-7.0987e-02, -1.1777e-01, -1.2487e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.1625e-01,  5.8768e-02,  8.4992e-02],\n",
       "                         [ 2.2628e-02, -1.7806e-04,  5.0617e-02],\n",
       "                         [ 4.1607e-02,  6.7836e-02, -3.1130e-03]],\n",
       "               \n",
       "                        [[ 9.3625e-02,  5.4868e-02,  3.1988e-02],\n",
       "                         [ 4.1208e-03,  3.7839e-02, -2.4788e-02],\n",
       "                         [ 5.8355e-02,  3.1439e-02, -1.2349e-02]],\n",
       "               \n",
       "                        [[ 8.4438e-02, -1.8805e-02,  1.5981e-01],\n",
       "                         [ 1.1398e-01,  7.6718e-02,  1.9108e-01],\n",
       "                         [ 1.1310e-01, -2.8671e-02,  7.8397e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-4.8919e-02,  1.3891e-02, -3.2110e-02],\n",
       "                         [-3.0544e-02, -4.5406e-02, -6.4514e-02],\n",
       "                         [-8.6003e-02,  8.3615e-02, -7.3178e-02]],\n",
       "               \n",
       "                        [[ 1.2080e-01,  3.5127e-03, -1.2024e-02],\n",
       "                         [ 1.6391e-01, -4.7369e-02,  3.1280e-02],\n",
       "                         [ 1.3670e-01, -6.1186e-02, -3.8044e-02]],\n",
       "               \n",
       "                        [[-2.9785e-02, -5.9414e-02, -1.1089e-01],\n",
       "                         [-3.0195e-02,  2.0865e-02, -2.6772e-02],\n",
       "                         [-7.2572e-02, -1.0845e-01, -5.9176e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.4320e-02, -6.1034e-03, -8.7604e-02],\n",
       "                         [-1.0793e-01, -1.4200e-03, -9.7291e-02],\n",
       "                         [-9.3723e-02, -7.8392e-02, -1.8309e-01]],\n",
       "               \n",
       "                        [[ 1.9617e-03, -3.8624e-02, -3.5050e-02],\n",
       "                         [-4.6962e-02, -3.6210e-02,  6.4846e-03],\n",
       "                         [-1.1994e-01, -4.1755e-02, -4.2707e-02]],\n",
       "               \n",
       "                        [[-1.0730e-01,  2.7914e-02, -8.4031e-02],\n",
       "                         [-3.3424e-02, -5.4116e-02, -1.2347e-01],\n",
       "                         [-1.7695e-01, -3.3620e-02, -3.0310e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.7679e-04,  2.4601e-04, -1.6585e-04],\n",
       "                         [-9.5741e-04, -3.4529e-03, -4.1754e-04],\n",
       "                         [ 4.7928e-05, -1.8620e-03,  9.6495e-06]],\n",
       "               \n",
       "                        [[-3.3515e-05, -8.7419e-05,  2.2345e-09],\n",
       "                         [-2.2583e-05,  3.8272e-05,  7.1536e-04],\n",
       "                         [ 8.6373e-04,  1.1179e-04,  3.6830e-07]],\n",
       "               \n",
       "                        [[ 6.7003e-05, -1.4216e-04, -5.4984e-04],\n",
       "                         [ 8.1204e-08, -1.8028e-06, -6.3714e-06],\n",
       "                         [-3.1938e-04,  6.3665e-04, -3.8846e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.5033e-04,  4.4031e-04, -1.7756e-05],\n",
       "                         [ 2.4979e-05, -3.2709e-06,  8.9548e-06],\n",
       "                         [-6.4669e-04,  1.6312e-04,  7.8769e-06]],\n",
       "               \n",
       "                        [[-8.9710e-04, -1.9745e-04, -2.6877e-06],\n",
       "                         [ 4.3447e-06, -1.4444e-04,  1.3567e-07],\n",
       "                         [ 1.9513e-08,  1.2028e-05,  2.5166e-03]],\n",
       "               \n",
       "                        [[-7.8037e-04, -1.8145e-07,  2.4425e-08],\n",
       "                         [-1.0224e-04,  1.8666e-04, -5.7422e-05],\n",
       "                         [ 9.4292e-08, -3.2863e-06,  3.9774e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.1947e-06,  4.3152e-05, -2.2945e-05],\n",
       "                         [ 1.6091e-05,  2.9370e-05, -1.7058e-05],\n",
       "                         [ 2.4092e-05,  1.4548e-05, -2.7465e-05]],\n",
       "               \n",
       "                        [[-6.9560e-06, -4.1473e-06,  5.0518e-05],\n",
       "                         [-1.8760e-05, -1.8658e-05, -2.2500e-05],\n",
       "                         [-3.5737e-05,  7.0908e-06, -8.4639e-05]],\n",
       "               \n",
       "                        [[-6.4960e-06,  7.1307e-06, -1.4352e-05],\n",
       "                         [-1.5078e-05, -1.3473e-05, -1.0270e-04],\n",
       "                         [-5.7678e-06,  1.3559e-05, -3.1211e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.5491e-06,  2.7693e-05, -5.4309e-05],\n",
       "                         [ 7.0403e-06, -4.7192e-06,  4.9964e-05],\n",
       "                         [-1.0859e-05, -3.0498e-05, -1.0257e-05]],\n",
       "               \n",
       "                        [[ 6.5516e-06,  8.9703e-05,  2.7026e-05],\n",
       "                         [-2.5728e-05,  5.8412e-05,  2.1558e-05],\n",
       "                         [ 6.2697e-06, -2.6846e-06,  5.4676e-05]],\n",
       "               \n",
       "                        [[-2.6195e-05, -1.2137e-05, -3.3921e-05],\n",
       "                         [-5.9531e-06, -1.0707e-05,  2.3787e-05],\n",
       "                         [-1.8696e-05, -2.0476e-05, -6.3621e-05]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 4.3760e-06,  8.9508e-04, -5.7679e-03, -3.4092e-04, -4.1268e-03,\n",
       "                       -4.0108e-06, -2.2792e-04, -2.9018e-03,  8.5754e-09, -1.6912e-09,\n",
       "                       -4.2715e-05, -7.3985e-06, -9.0362e-04, -6.3604e-09, -1.8087e-03,\n",
       "                        1.9316e-07, -3.5870e-03,  3.3916e-03, -8.4296e-03, -1.9639e-03,\n",
       "                        2.2962e-05,  3.9368e-03, -1.6630e-03,  1.0428e-04, -1.0674e-03,\n",
       "                        1.0501e-03, -4.3247e-04,  1.3567e-06,  3.5871e-07,  2.7683e-03,\n",
       "                       -8.6406e-04, -1.2758e-03,  5.0466e-04,  1.6021e-02,  1.6171e-05,\n",
       "                       -3.5175e-03,  6.0717e-03,  5.0275e-08, -1.0535e-01, -3.7029e-03,\n",
       "                       -1.5735e-04, -3.4015e-07, -3.7422e-09, -7.4212e-04, -2.3926e-05,\n",
       "                       -1.1401e-03, -3.7369e-04,  8.6847e-06, -3.5780e-07, -1.5013e-03,\n",
       "                        2.1297e-03, -2.8779e-03, -6.2841e-03, -1.9414e-03,  4.2086e-08,\n",
       "                       -8.1576e-03, -2.6986e-07,  3.6500e-04, -3.6878e-06,  1.1765e-07,\n",
       "                        1.9491e-07, -3.8243e-03,  1.4882e-08,  2.1526e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-1.3104e-03, -2.6205e-01, -2.7848e-01, -2.2708e-01, -2.9279e-01,\n",
       "                       -4.4517e-03, -2.8797e-01, -8.8774e-05, -2.6643e-04, -5.5327e-04,\n",
       "                       -1.2498e-02, -3.0410e-02, -2.1254e-01, -2.4805e-03, -2.8326e-01,\n",
       "                       -1.0452e-02, -2.9994e-01, -2.0501e-01, -4.2764e-03, -1.4044e-04,\n",
       "                       -5.8718e-02, -1.8540e-01, -1.3722e-01, -5.0776e-02, -2.1670e-02,\n",
       "                       -2.3919e-01, -2.3816e-02, -5.5732e-03, -8.5113e-03, -8.5060e-03,\n",
       "                       -6.0826e-03, -3.7720e-01, -2.7376e-01, -3.0367e-02, -2.5113e-03,\n",
       "                       -2.3708e-01, -1.1969e-02, -2.1165e-02, -1.3768e-01, -9.4916e-03,\n",
       "                       -4.3132e-06, -5.7469e-04, -7.3985e-05, -2.8878e-01, -1.9314e-01,\n",
       "                       -2.6844e-01, -1.0745e-03, -3.5636e-02, -2.1405e-02, -2.6321e-01,\n",
       "                       -1.3107e-03, -6.9757e-02, -2.8675e-01, -3.5463e-01, -3.4923e-02,\n",
       "                       -3.0109e-01, -2.4056e-02,  1.7311e-04, -1.0122e-02, -4.9790e-03,\n",
       "                       -9.4508e-03, -1.9664e-01, -8.0597e-03, -8.1108e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([ 7.2146e-04,  4.8213e-01,  5.6755e-01,  4.4560e-01,  5.9407e-01,\n",
       "                       -2.1087e-03,  5.6990e-01, -2.4646e-05, -4.1842e-04,  1.2046e-03,\n",
       "                        8.7070e-04, -6.6557e-03,  4.8767e-01, -5.8704e-04,  5.5990e-01,\n",
       "                        1.0740e-03,  5.6336e-01,  4.5588e-01,  4.0617e-03, -6.9602e-05,\n",
       "                        2.6230e-02,  4.3351e-01,  3.5484e-01,  4.9240e-02,  3.8903e-03,\n",
       "                        4.8884e-01,  8.3929e-03, -9.1966e-04,  7.6174e-03,  7.2162e-04,\n",
       "                        1.9404e-03,  5.9814e-01,  5.2348e-01,  2.3380e-03,  5.7604e-04,\n",
       "                        4.2125e-01, -1.6131e-03, -1.9100e-03,  7.0945e-02,  3.5905e-03,\n",
       "                       -1.7526e-06,  6.1458e-05,  7.9112e-06,  5.1542e-01,  4.9328e-02,\n",
       "                        5.5515e-01,  1.1711e-04,  3.8079e-02,  6.7626e-03,  5.5204e-01,\n",
       "                       -9.3106e-06,  2.7388e-01,  5.5769e-01,  5.7771e-01,  3.6760e-03,\n",
       "                        6.0699e-01,  5.1562e-03,  1.6763e-01, -1.7001e-03, -3.7069e-04,\n",
       "                       -2.1286e-03,  4.5079e-01, -1.5193e-04,  2.5733e-03], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-1.9319e-05,  7.9597e-05,  3.0818e-05,  ..., -6.0808e-05,\n",
       "                        -7.4112e-05, -8.7713e-05],\n",
       "                       [-3.4661e-05, -9.9737e-05,  1.2073e-03,  ...,  2.4423e-05,\n",
       "                        -1.9174e-04, -1.2199e-04],\n",
       "                       [ 6.0094e-06, -1.9930e-06, -2.5542e-04,  ..., -9.6934e-05,\n",
       "                        -9.7765e-05, -7.1825e-05],\n",
       "                       [ 4.7639e-06, -4.6600e-05, -4.0066e-05,  ...,  2.9868e-05,\n",
       "                        -2.2476e-05, -1.0843e-05],\n",
       "                       [ 4.6005e-04,  7.0851e-04,  1.1747e-04,  ...,  7.1792e-05,\n",
       "                         2.8852e-04,  2.3685e-04]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0028, -0.0311,  0.0159, -0.0233,  0.0008], device='cuda:0')),\n",
       "              ('arbiter.linear1.weight',\n",
       "               tensor([[-1.8630e-06, -1.1432e-07, -5.1946e-08,  ..., -4.3350e-07,\n",
       "                        -5.5755e-07, -6.3185e-07],\n",
       "                       [ 2.4723e-01, -2.6617e-02, -7.2410e-02,  ..., -7.7168e-02,\n",
       "                        -7.7846e-02, -7.9531e-02],\n",
       "                       [ 2.6507e-01, -4.9423e-02, -6.6948e-02,  ..., -6.9920e-02,\n",
       "                        -7.0761e-02, -7.3102e-02],\n",
       "                       ...,\n",
       "                       [ 9.8988e-42,  1.0228e-41,  9.8273e-42,  ...,  1.0067e-41,\n",
       "                         1.0032e-41,  1.0239e-41],\n",
       "                       [-1.7751e-20,  1.0068e-41,  1.0772e-41,  ...,  9.8876e-42,\n",
       "                         9.8329e-42,  9.9268e-42],\n",
       "                       [ 1.6114e-01,  5.2494e-02, -9.2716e-02,  ..., -1.0217e-01,\n",
       "                        -1.0276e-01, -1.0180e-01]], device='cuda:0')),\n",
       "              ('arbiter.linear1.bias',\n",
       "               tensor([-1.7456e-06,  1.9854e-01,  1.9968e-01,  1.7073e-01,  1.2639e-01,\n",
       "                        1.7963e-01, -1.1221e-24,  3.3116e-37, -3.8655e-13,  1.6230e-01,\n",
       "                        1.9814e-01,  3.6686e-02, -3.3142e-17,  4.7671e-25, -1.6050e-35,\n",
       "                        1.3516e-01,  1.7139e-01,  1.7590e-01, -1.2009e-15, -2.7118e-13,\n",
       "                        1.8098e-01,  1.6630e-01,  9.9660e-42,  1.7289e-01, -1.1202e-41,\n",
       "                        5.3588e-06, -9.9478e-42, -8.8660e-16,  1.5641e-01, -5.8176e-09,\n",
       "                        1.7923e-01, -5.0425e-09,  1.4480e-01,  9.9440e-02,  1.7101e-01,\n",
       "                        1.6283e-01,  1.9850e-01, -9.8679e-42, -6.2976e-22,  1.8238e-01],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear2.weight',\n",
       "               tensor([[ 4.9412e-06,  4.3036e-02,  2.1652e-02,  7.5971e-02,  4.0596e-02,\n",
       "                         9.1145e-02,  2.5325e-12, -1.4202e-24, -1.0500e-04,  3.0871e-02,\n",
       "                        -1.4735e-02, -1.7165e-01, -7.9889e-05,  4.3502e-14, -3.5067e-07,\n",
       "                         1.1676e-01, -4.6649e-03,  5.5861e-02,  2.7557e-04,  4.9376e-04,\n",
       "                        -3.1612e-02, -9.6639e-02, -1.0122e-41,  2.2371e-02, -1.0180e-41,\n",
       "                        -3.2578e-07, -9.8357e-42,  5.7007e-04,  5.4519e-02, -6.3720e-11,\n",
       "                        -1.4690e-01, -5.6282e-04, -2.5243e-02, -9.5234e-02,  6.1502e-02,\n",
       "                        -1.2072e-01,  1.2879e-02,  8.6054e-42,  2.8694e-05,  3.2287e-02],\n",
       "                       [ 2.8753e-08, -1.7438e-03, -1.7349e-03, -1.7333e-03, -1.4137e-03,\n",
       "                        -1.5291e-03,  3.0281e-25,  1.1388e-39,  4.5730e-34, -1.5121e-03,\n",
       "                        -1.5664e-03, -4.6502e-03, -1.1697e-41,  1.1963e-25,  1.1160e-41,\n",
       "                        -1.4367e-03, -1.2480e-03, -1.3162e-03, -7.7645e-20,  8.4442e-42,\n",
       "                        -1.4912e-03, -1.5773e-03,  9.9184e-42, -1.2938e-03,  9.8105e-42,\n",
       "                         1.2838e-09,  1.4418e-08, -5.7673e-37, -1.4822e-03, -4.6626e-16,\n",
       "                        -1.2952e-03, -5.0488e-34, -1.0734e-03, -4.5402e-04, -1.4186e-03,\n",
       "                        -1.5438e-03, -1.6969e-03,  6.6099e-42,  1.0754e-41, -1.5136e-03],\n",
       "                       [-1.0889e-07, -5.0791e-02, -4.9350e-02, -3.7955e-02, -2.8706e-02,\n",
       "                        -4.5443e-02, -1.0320e-24,  1.6995e-38,  6.6261e-32, -3.6314e-02,\n",
       "                        -4.4660e-02, -5.6244e-03,  9.9576e-42, -8.4236e-25,  1.0543e-41,\n",
       "                        -3.0994e-02, -4.1736e-02, -4.3163e-02,  3.8209e-17,  9.9772e-42,\n",
       "                        -3.9932e-02, -3.6290e-02,  1.1557e-41, -4.1934e-02, -9.8862e-42,\n",
       "                         9.6552e-09,  9.9114e-42, -1.1984e-36, -3.4827e-02,  4.0888e-14,\n",
       "                        -4.1931e-02, -9.2242e-33, -3.5636e-02, -1.5224e-02, -4.1847e-02,\n",
       "                        -3.5627e-02, -5.0229e-02,  1.1800e-41,  9.9688e-42, -4.0919e-02],\n",
       "                       [-1.0154e-08, -4.1831e-03, -4.2774e-03, -4.9476e-03, -4.0237e-03,\n",
       "                        -4.2234e-03, -1.2719e-25, -3.8936e-40,  3.4072e-33, -4.5140e-03,\n",
       "                        -4.3702e-03, -5.9316e-03,  9.8091e-42, -1.7168e-25, -9.8511e-42,\n",
       "                        -4.2164e-03, -3.8467e-03, -3.9084e-03,  3.6523e-19, -1.2470e-41,\n",
       "                        -4.4313e-03, -4.6365e-03, -1.1803e-41, -3.9295e-03,  9.8259e-42,\n",
       "                         3.7585e-09,  1.2582e-41,  3.9635e-37, -4.4374e-03,  9.4829e-16,\n",
       "                        -3.9277e-03,  3.1495e-33, -3.6726e-03, -1.5226e-03, -4.1855e-03,\n",
       "                        -4.5628e-03, -4.1379e-03,  1.0253e-41,  9.8231e-42, -4.4491e-03],\n",
       "                       [-2.8063e-05, -6.4591e-02, -1.4659e-01,  7.0008e-01,  1.9336e-01,\n",
       "                        -2.6675e-02,  4.6394e-03, -5.4939e-07, -9.4605e-03,  3.2648e-01,\n",
       "                         1.1269e-01,  9.7961e-01, -1.3731e-03, -3.1996e-03,  9.9382e-06,\n",
       "                         2.2532e-01, -5.0685e-01, -4.8559e-01,  1.8808e-03, -1.5564e-03,\n",
       "                         2.0634e-01,  4.6944e-01, -9.8371e-42, -4.4342e-01,  9.8371e-42,\n",
       "                         1.7256e-05,  8.7427e-42, -3.8637e-03,  2.9252e-01, -4.0090e-08,\n",
       "                        -4.0743e-01, -9.9447e-03, -5.2718e-01, -1.5857e+00, -2.3070e-01,\n",
       "                         4.2181e-01, -4.5407e-02, -1.0207e-41, -2.0391e-03,  1.9083e-01],\n",
       "                       [-5.6043e-08, -1.7182e-03, -1.7215e-03,  3.4538e-04,  5.7810e-04,\n",
       "                        -1.2416e-03, -1.0383e-25,  4.2160e-39, -1.4892e-32,  9.1950e-05,\n",
       "                        -9.8202e-04,  3.2416e-03, -1.0469e-41, -8.4456e-26, -9.8441e-42,\n",
       "                         4.5290e-04, -1.6847e-03, -1.7222e-03, -3.2442e-18,  1.1080e-41,\n",
       "                        -4.3604e-04,  2.4694e-04, -9.1824e-07, -1.6023e-03,  1.1629e-41,\n",
       "                         1.2974e-09, -6.8001e-10, -1.6628e-36,  1.9309e-04, -6.0304e-16,\n",
       "                        -1.5925e-03, -2.8516e-33, -1.3637e-03, -2.2615e-03, -1.2829e-03,\n",
       "                         2.5618e-04, -1.6730e-03, -1.1104e-41, -1.0183e-41, -5.3204e-04],\n",
       "                       [ 1.2686e-06,  9.1205e-02,  5.7154e-02,  4.2998e-01,  1.2420e-01,\n",
       "                         1.0671e-01,  2.1333e-10,  1.8380e-36,  5.9199e-05,  2.3813e-01,\n",
       "                         1.9162e-01,  3.8828e-01,  2.2810e-08,  7.5820e-09, -1.2322e-10,\n",
       "                         1.7809e-01, -1.6493e-01, -1.0539e-01,  1.3655e-06,  7.2386e-06,\n",
       "                         1.8455e-01,  2.7463e-01,  9.9829e-42, -1.2347e-01,  1.0175e-41,\n",
       "                         2.2437e-07, -9.8427e-42, -6.7294e-06,  2.1502e-01, -8.1880e-11,\n",
       "                        -1.6106e-01,  5.0971e-04, -2.4014e-01, -7.0450e-01, -2.9689e-02,\n",
       "                         2.4377e-01,  6.7630e-02,  1.1220e-41, -1.7583e-05,  2.2716e-01],\n",
       "                       [-6.3166e-09, -3.7404e-03, -3.7545e-03, -3.6382e-03, -3.5924e-03,\n",
       "                        -3.5917e-03, -6.6342e-26,  1.0458e-39, -1.4839e-32, -3.5465e-03,\n",
       "                        -3.5880e-03, -4.3421e-03,  9.8427e-42, -1.4327e-25,  9.8217e-42,\n",
       "                        -3.5666e-03, -3.4838e-03, -3.5201e-03, -4.3693e-18, -1.2221e-41,\n",
       "                        -3.5325e-03, -3.5737e-03, -1.9290e-10, -3.5059e-03, -1.1167e-41,\n",
       "                        -1.8534e-09,  1.0512e-05, -5.1977e-37, -3.5427e-03, -3.4198e-16,\n",
       "                        -3.5034e-03, -5.8760e-34, -3.4151e-03, -3.6175e-03, -3.5646e-03,\n",
       "                        -3.5623e-03, -3.7001e-03, -9.8315e-42, -9.8231e-42, -3.5463e-03],\n",
       "                       [ 1.0735e-07,  1.3316e+00,  1.3507e+00,  9.8280e-01,  7.8243e-01,\n",
       "                         1.2070e+00, -1.6661e-23, -1.0534e-38,  4.7111e-14,  9.8063e-01,\n",
       "                         1.2148e+00,  1.2437e-01,  3.3078e-16, -4.2612e-25,  3.6483e-29,\n",
       "                         8.4789e-01,  1.2620e+00,  1.2976e+00,  6.7332e-16,  6.7530e-14,\n",
       "                         1.0881e+00,  9.6359e-01, -9.9674e-42,  1.2561e+00,  9.8820e-42,\n",
       "                         3.9299e-09,  9.9044e-42, -3.9847e-28,  9.4704e-01,  3.0855e-11,\n",
       "                         1.2451e+00, -5.4377e-12,  1.0890e+00,  4.0584e-01,  1.1990e+00,\n",
       "                         9.4842e-01,  1.3127e+00, -1.0123e-41, -7.4922e-15,  1.1155e+00],\n",
       "                       [-5.3074e-08, -3.2445e-02, -3.2354e-02, -1.9790e-02, -1.5210e-02,\n",
       "                        -2.8192e-02, -1.1557e-24,  1.2358e-38, -2.0231e-32, -1.9852e-02,\n",
       "                        -2.6720e-02,  7.7652e-05, -1.1552e-41,  9.2767e-25, -1.1257e-41,\n",
       "                        -1.6630e-02, -2.8744e-02, -2.9598e-02,  3.6196e-17, -1.2470e-41,\n",
       "                        -2.2940e-02, -1.9367e-02,  1.2155e-41, -2.8626e-02, -9.8273e-42,\n",
       "                        -1.5629e-08, -8.9459e-42, -1.7991e-36, -1.8962e-02, -4.6416e-12,\n",
       "                        -2.8542e-02, -9.0751e-34, -2.4543e-02, -1.0935e-02, -2.7622e-02,\n",
       "                        -1.9056e-02, -3.1887e-02, -1.0105e-41,  1.0082e-41, -2.3683e-02]],\n",
       "                      device='cuda:0')),\n",
       "              ('arbiter.linear2.bias',\n",
       "               tensor([-0.0262, -0.0023, -0.0068, -0.0034,  0.0809, -0.0003,  0.0003, -0.0043,\n",
       "                        0.1761, -0.0024], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.350129269361496,\n",
       "   1.1360058069229126,\n",
       "   1.04993595123291,\n",
       "   0.9883905935287476,\n",
       "   0.9399947162866592,\n",
       "   0.8995303221940995,\n",
       "   0.8758509140014649,\n",
       "   0.8351961605548859,\n",
       "   0.8087177600264549,\n",
       "   0.793303301692009,\n",
       "   0.7436333929896355,\n",
       "   0.7374898125529289,\n",
       "   0.7202764943838119,\n",
       "   0.6965293320417404,\n",
       "   0.6936158599257469,\n",
       "   0.7232751197218895,\n",
       "   0.6687140984535217,\n",
       "   0.664169134259224,\n",
       "   0.6676166779994964,\n",
       "   0.6414886571466922,\n",
       "   0.6388102241158485,\n",
       "   0.624561147749424,\n",
       "   0.6233343564867974,\n",
       "   0.6207933755517006,\n",
       "   0.6276247451305389,\n",
       "   0.6100650665163994,\n",
       "   0.5920156311392785,\n",
       "   0.6118202214837074,\n",
       "   0.6023207283616065,\n",
       "   0.6283379449248314,\n",
       "   0.5837065820097923,\n",
       "   0.5876298642158508,\n",
       "   0.5813308635652066,\n",
       "   0.5756083212196826,\n",
       "   0.5878902344405651,\n",
       "   0.5707722766697407,\n",
       "   0.5753514708280564,\n",
       "   0.5679792599081993,\n",
       "   0.5734763342738152,\n",
       "   0.5628393875062465,\n",
       "   0.5673242945075035,\n",
       "   0.559889204621315,\n",
       "   0.5597849978208542,\n",
       "   0.5656186723709107,\n",
       "   0.5549832828044892,\n",
       "   0.5385635884404183,\n",
       "   0.5414396000206471,\n",
       "   0.5435804533660412,\n",
       "   0.5390219500958919,\n",
       "   0.5317659276723862,\n",
       "   0.5352563015818596,\n",
       "   0.5119795972704887,\n",
       "   0.5213542311191559,\n",
       "   0.5037301463782787,\n",
       "   0.5104101197719574,\n",
       "   0.5061582255363465,\n",
       "   0.4939623585939407,\n",
       "   0.5291518195867538,\n",
       "   0.4993996959924698,\n",
       "   0.5063738747537136,\n",
       "   0.5159230885505677,\n",
       "   0.49288389724493026,\n",
       "   0.4994060781896114,\n",
       "   0.49387544935941696,\n",
       "   0.49874337312579153,\n",
       "   0.4932865569293499,\n",
       "   0.4841204156577587,\n",
       "   0.46500733745098116,\n",
       "   0.46350239908695223,\n",
       "   0.46918256604671477,\n",
       "   0.4560320879817009,\n",
       "   0.46634220403432847,\n",
       "   0.4514354290664196,\n",
       "   0.45181462889909746,\n",
       "   0.4405596172809601,\n",
       "   0.43551917520165445,\n",
       "   0.4424514462053776,\n",
       "   0.4413931300342083,\n",
       "   0.4309755008816719,\n",
       "   0.43482168710231783,\n",
       "   0.4314375641942024,\n",
       "   0.42838502329587935,\n",
       "   0.42994782105088236,\n",
       "   0.4210725578665733,\n",
       "   0.414782809227705,\n",
       "   0.4127816413640976,\n",
       "   0.41104804596304895,\n",
       "   0.40904395425319673,\n",
       "   0.4051317814290524,\n",
       "   0.40056779837608336,\n",
       "   0.4107719674408436,\n",
       "   0.4090171436071396,\n",
       "   0.3942751791626215,\n",
       "   0.4028638192713261,\n",
       "   0.39984815669059753,\n",
       "   0.4092846298068762,\n",
       "   0.3960004736185074,\n",
       "   0.3976752373725176,\n",
       "   0.38531122455000877],\n",
       "  'train_loss_std': [0.1710165138003402,\n",
       "   0.13583328829142197,\n",
       "   0.15134752238546811,\n",
       "   0.14015064618063058,\n",
       "   0.14230109175450525,\n",
       "   0.14521023606362787,\n",
       "   0.13813377418655823,\n",
       "   0.13201374662300577,\n",
       "   0.13545412351733568,\n",
       "   0.132167555122506,\n",
       "   0.13667833507709498,\n",
       "   0.15005713863630618,\n",
       "   0.1404322917531047,\n",
       "   0.13509466895638472,\n",
       "   0.13088732433945857,\n",
       "   0.15064708589681633,\n",
       "   0.13394902899984548,\n",
       "   0.13489747364385496,\n",
       "   0.13902348414624696,\n",
       "   0.13962428180177075,\n",
       "   0.1320638250897173,\n",
       "   0.13265601727415607,\n",
       "   0.1349879156235899,\n",
       "   0.13828576147344865,\n",
       "   0.13561111023776487,\n",
       "   0.1372463538373589,\n",
       "   0.1250832106888962,\n",
       "   0.1359434503238467,\n",
       "   0.13155249659686466,\n",
       "   0.14318441697299075,\n",
       "   0.13106068584672806,\n",
       "   0.1295406923527272,\n",
       "   0.12718383162315872,\n",
       "   0.13787699327013186,\n",
       "   0.13541290121821223,\n",
       "   0.1416709496545175,\n",
       "   0.13449134731380438,\n",
       "   0.13511192161838328,\n",
       "   0.14122737579657935,\n",
       "   0.13739838057299814,\n",
       "   0.1279599543648193,\n",
       "   0.13216839406307238,\n",
       "   0.12617922179615507,\n",
       "   0.1323675028698352,\n",
       "   0.1280305843588601,\n",
       "   0.13102490681748122,\n",
       "   0.13209125512981207,\n",
       "   0.12705077248725102,\n",
       "   0.12820175953267451,\n",
       "   0.12097121068117862,\n",
       "   0.12307734982990627,\n",
       "   0.12661103905877005,\n",
       "   0.12386555742424045,\n",
       "   0.12780862878648955,\n",
       "   0.1319097869522042,\n",
       "   0.12285937483944294,\n",
       "   0.12102233857719887,\n",
       "   0.157908853049909,\n",
       "   0.12483367263573271,\n",
       "   0.12851223182681232,\n",
       "   0.12721985597417412,\n",
       "   0.13158058785406782,\n",
       "   0.13032385081049475,\n",
       "   0.12601837372889454,\n",
       "   0.13067099594501502,\n",
       "   0.12847721578416357,\n",
       "   0.1341574793177453,\n",
       "   0.12630452010057996,\n",
       "   0.12512990397334098,\n",
       "   0.12755385708181208,\n",
       "   0.12045034636826002,\n",
       "   0.12690657753236764,\n",
       "   0.1250131087460597,\n",
       "   0.12374486263943818,\n",
       "   0.1297244543252253,\n",
       "   0.12389625584395161,\n",
       "   0.12845793261946023,\n",
       "   0.11978945269601968,\n",
       "   0.12214387876417572,\n",
       "   0.13164683585638756,\n",
       "   0.12344286444753093,\n",
       "   0.12449348461824018,\n",
       "   0.1323932908575007,\n",
       "   0.11963704946015258,\n",
       "   0.12290938263590806,\n",
       "   0.1162509783141531,\n",
       "   0.11840715637690324,\n",
       "   0.12272059800790948,\n",
       "   0.12252484356116179,\n",
       "   0.12124304124073823,\n",
       "   0.13033887771265035,\n",
       "   0.13333549644248305,\n",
       "   0.11290025249706474,\n",
       "   0.12578111710495413,\n",
       "   0.12236123888116293,\n",
       "   0.1334924606481204,\n",
       "   0.11134562055610141,\n",
       "   0.1285156602909087,\n",
       "   0.12278007647318157],\n",
       "  'train_accuracy_mean': [0.44898666656017305,\n",
       "   0.5560400004982948,\n",
       "   0.5961333338022232,\n",
       "   0.6208799984455109,\n",
       "   0.6434533325433731,\n",
       "   0.6636266661286354,\n",
       "   0.6713066669106483,\n",
       "   0.6887333331108093,\n",
       "   0.6988399995565414,\n",
       "   0.7058533344268799,\n",
       "   0.7259599992036819,\n",
       "   0.7289999991655349,\n",
       "   0.7353733323812485,\n",
       "   0.7443733341693878,\n",
       "   0.7455333325862884,\n",
       "   0.734466665148735,\n",
       "   0.7575600000619889,\n",
       "   0.7577066668272019,\n",
       "   0.7561466678380966,\n",
       "   0.7678266662359238,\n",
       "   0.7668799996376038,\n",
       "   0.7721333339214325,\n",
       "   0.7743600010871887,\n",
       "   0.7776266663074494,\n",
       "   0.772666666150093,\n",
       "   0.78,\n",
       "   0.7856666675806045,\n",
       "   0.7789733346700668,\n",
       "   0.782106665968895,\n",
       "   0.7715333331823349,\n",
       "   0.788773334145546,\n",
       "   0.7892133328914642,\n",
       "   0.7908933345079422,\n",
       "   0.7943066661357879,\n",
       "   0.7884266667366028,\n",
       "   0.7961733326911926,\n",
       "   0.7923466658592224,\n",
       "   0.7965333329439164,\n",
       "   0.7948666651248932,\n",
       "   0.799613333106041,\n",
       "   0.7967199997901917,\n",
       "   0.7990399998426437,\n",
       "   0.7999333320856095,\n",
       "   0.7959733345508575,\n",
       "   0.8005333327054978,\n",
       "   0.8059466673135758,\n",
       "   0.8067200002670288,\n",
       "   0.8066133345365525,\n",
       "   0.8091599998474122,\n",
       "   0.8091600006818771,\n",
       "   0.8074133327007293,\n",
       "   0.8185733338594436,\n",
       "   0.8129466673135758,\n",
       "   0.8189866673946381,\n",
       "   0.8159066678285599,\n",
       "   0.8158266673088074,\n",
       "   0.8214666666984558,\n",
       "   0.8111333339214325,\n",
       "   0.819399999499321,\n",
       "   0.8147333329916,\n",
       "   0.8112266671657562,\n",
       "   0.8179866679906845,\n",
       "   0.8159066671133042,\n",
       "   0.8180533335208893,\n",
       "   0.8173200011253356,\n",
       "   0.8184800000190735,\n",
       "   0.8214933316707611,\n",
       "   0.8266799991130829,\n",
       "   0.8288266671895981,\n",
       "   0.8253066675662994,\n",
       "   0.8306666659116745,\n",
       "   0.826266666173935,\n",
       "   0.8320266674757004,\n",
       "   0.8325466674566269,\n",
       "   0.8362266683578491,\n",
       "   0.8387999992370605,\n",
       "   0.8359599984884262,\n",
       "   0.8349866676330566,\n",
       "   0.8399600007534027,\n",
       "   0.8368000010251999,\n",
       "   0.8386933344602585,\n",
       "   0.8386000009775162,\n",
       "   0.840080001115799,\n",
       "   0.8428266679048538,\n",
       "   0.8444400016069412,\n",
       "   0.8465600012540817,\n",
       "   0.8463066673278808,\n",
       "   0.8473866671323776,\n",
       "   0.8487733337879181,\n",
       "   0.850960000872612,\n",
       "   0.8476399997472763,\n",
       "   0.8465999997854233,\n",
       "   0.8525066673755646,\n",
       "   0.8507733340263367,\n",
       "   0.8489066669940949,\n",
       "   0.8479733343124389,\n",
       "   0.8514400005340577,\n",
       "   0.8508666667938233,\n",
       "   0.8564400025606156],\n",
       "  'train_accuracy_std': [0.08807040087087095,\n",
       "   0.06890933241930236,\n",
       "   0.07573392122532062,\n",
       "   0.06814708868140452,\n",
       "   0.06969829559028724,\n",
       "   0.06835985387525814,\n",
       "   0.06665719643689237,\n",
       "   0.06374459542067755,\n",
       "   0.0644304016483245,\n",
       "   0.06174179718673486,\n",
       "   0.06482892349302435,\n",
       "   0.06851942063090752,\n",
       "   0.0640861791152692,\n",
       "   0.06150615563757435,\n",
       "   0.06175690715412806,\n",
       "   0.06753866397123076,\n",
       "   0.06279421081467966,\n",
       "   0.06134552962051049,\n",
       "   0.06307664387713734,\n",
       "   0.06329884025150373,\n",
       "   0.060598120894018746,\n",
       "   0.05970877512063864,\n",
       "   0.06127471247581772,\n",
       "   0.06074253948516077,\n",
       "   0.061405030402363635,\n",
       "   0.06201935223276333,\n",
       "   0.058453970953627886,\n",
       "   0.058078217097037955,\n",
       "   0.05957185867625439,\n",
       "   0.06467185550164184,\n",
       "   0.05846637934785356,\n",
       "   0.057429407030555234,\n",
       "   0.05687102054580791,\n",
       "   0.06124075967349111,\n",
       "   0.057737455004119295,\n",
       "   0.06146237355697217,\n",
       "   0.05899532772167387,\n",
       "   0.05862180089146666,\n",
       "   0.06152907186713663,\n",
       "   0.05921210027177611,\n",
       "   0.05594677443570288,\n",
       "   0.05804491188484762,\n",
       "   0.05598805537862625,\n",
       "   0.05912066245171999,\n",
       "   0.05638817893084672,\n",
       "   0.057525391275061996,\n",
       "   0.05789643535668297,\n",
       "   0.05502643216498359,\n",
       "   0.05703103498635201,\n",
       "   0.053395849946620874,\n",
       "   0.0564440568551057,\n",
       "   0.05608097524647175,\n",
       "   0.05314242345607787,\n",
       "   0.057781540817762544,\n",
       "   0.05700877145801448,\n",
       "   0.05387645259407807,\n",
       "   0.052813971862309815,\n",
       "   0.06437981055305801,\n",
       "   0.05653471124203627,\n",
       "   0.05623992500349081,\n",
       "   0.05670514260501338,\n",
       "   0.05920315295838916,\n",
       "   0.05698693924415381,\n",
       "   0.05625447702922239,\n",
       "   0.056716212993884914,\n",
       "   0.05567845053567431,\n",
       "   0.059088568111026,\n",
       "   0.05689991266867817,\n",
       "   0.05664039676267814,\n",
       "   0.05662130951331712,\n",
       "   0.054414052014437234,\n",
       "   0.05702568698736899,\n",
       "   0.054612002363095616,\n",
       "   0.05621074444123468,\n",
       "   0.05418718503857818,\n",
       "   0.054040150173091654,\n",
       "   0.05462387074130098,\n",
       "   0.051876135064515515,\n",
       "   0.05259064672374658,\n",
       "   0.05613222564969822,\n",
       "   0.054095219054371216,\n",
       "   0.05475679783571457,\n",
       "   0.05835365746917576,\n",
       "   0.052778879134613474,\n",
       "   0.053337688159409966,\n",
       "   0.051233561357891054,\n",
       "   0.05154979757859488,\n",
       "   0.05332347421563993,\n",
       "   0.05416688166150688,\n",
       "   0.05247190518654011,\n",
       "   0.056971215406527394,\n",
       "   0.05663995721055247,\n",
       "   0.04943800963067518,\n",
       "   0.05208904779505751,\n",
       "   0.05367851054811216,\n",
       "   0.05699808303926886,\n",
       "   0.049116346097276185,\n",
       "   0.05551580435107202,\n",
       "   0.05281449601184501],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.323536109526952,\n",
       "   1.2257801701625188,\n",
       "   1.160706576704979,\n",
       "   1.1099132430553436,\n",
       "   1.091327703197797,\n",
       "   1.053987174431483,\n",
       "   1.0795905768871308,\n",
       "   1.010670112768809,\n",
       "   0.9855351648728052,\n",
       "   0.9662644265095393,\n",
       "   0.9659364320834478,\n",
       "   0.9606020673116048,\n",
       "   0.9469107993443807,\n",
       "   0.9380261874198914,\n",
       "   0.9340542630354564,\n",
       "   0.9462760730584463,\n",
       "   0.9498479465643564,\n",
       "   0.9155812360843023,\n",
       "   0.9101461118459702,\n",
       "   0.9217838170131047,\n",
       "   0.9265365719795227,\n",
       "   0.9454715694983801,\n",
       "   0.9065263144175212,\n",
       "   0.9100324066480001,\n",
       "   0.9183974734942119,\n",
       "   0.902601024309794,\n",
       "   0.911728660662969,\n",
       "   0.9064143725236257,\n",
       "   0.9323409122228622,\n",
       "   0.9375276402632395,\n",
       "   0.9172970926761628,\n",
       "   0.9230657986799876,\n",
       "   0.9459368735551834,\n",
       "   0.9144858139753341,\n",
       "   0.9199253193537394,\n",
       "   0.9243928563594818,\n",
       "   0.8969421217838923,\n",
       "   0.8949695986509323,\n",
       "   0.901789034207662,\n",
       "   0.9043728230396907,\n",
       "   0.9407408271233241,\n",
       "   0.903457727432251,\n",
       "   0.9073624734083812,\n",
       "   0.9010190661748251,\n",
       "   0.8992503112554551,\n",
       "   0.9131758403778076,\n",
       "   0.9011026034752528,\n",
       "   0.9204938590526581,\n",
       "   0.8969170671701431,\n",
       "   0.895218501885732,\n",
       "   0.9142210859060288,\n",
       "   0.8997798732916514,\n",
       "   0.9225055515766144,\n",
       "   0.9023950495322546,\n",
       "   0.8830849015712738,\n",
       "   0.9036490019162496,\n",
       "   1.0179073411226272,\n",
       "   0.883631320198377,\n",
       "   0.9002438875039419,\n",
       "   0.9194537272055944,\n",
       "   0.920753236413002,\n",
       "   0.8976403170824051,\n",
       "   0.935405567685763,\n",
       "   0.9027149403095245,\n",
       "   0.933891243537267,\n",
       "   0.9186869426568349,\n",
       "   0.9258191762367884,\n",
       "   0.9055434167385101,\n",
       "   0.9331674041350683,\n",
       "   0.9159210038185119,\n",
       "   0.9285813277959823,\n",
       "   0.9256997934977214,\n",
       "   0.9520116790135702,\n",
       "   0.9320512652397156,\n",
       "   0.9703170307477316,\n",
       "   0.9315344975392024,\n",
       "   0.9412898532549541,\n",
       "   0.9767352394262949,\n",
       "   0.9699661248922348,\n",
       "   0.9358571634689967,\n",
       "   0.9324534829457601,\n",
       "   0.9363316001494726,\n",
       "   0.9290282732248306,\n",
       "   0.9720992374420167,\n",
       "   0.9434330914417902,\n",
       "   0.9548732986052831,\n",
       "   0.9557097305854162,\n",
       "   0.9429025018215179,\n",
       "   0.9733560254176458,\n",
       "   0.9485354016224543,\n",
       "   0.9665992619593938,\n",
       "   0.9757575617233912,\n",
       "   0.9721449019511541,\n",
       "   0.9441454627116521,\n",
       "   0.9737809340159098,\n",
       "   0.9706000556548436,\n",
       "   0.9405071425437928,\n",
       "   0.9548739947875341,\n",
       "   0.968724982937177],\n",
       "  'val_loss_std': [0.10150313941819156,\n",
       "   0.12284746787766095,\n",
       "   0.1173647456789128,\n",
       "   0.11651000013420386,\n",
       "   0.12594585238196868,\n",
       "   0.1266688320293397,\n",
       "   0.12243650847438083,\n",
       "   0.12912640469851175,\n",
       "   0.12694298761509865,\n",
       "   0.1345565669903,\n",
       "   0.12007894824484934,\n",
       "   0.13620162084343373,\n",
       "   0.12466546297930307,\n",
       "   0.13520402561412667,\n",
       "   0.13178611858381203,\n",
       "   0.12678023166193297,\n",
       "   0.13134352036362412,\n",
       "   0.1282998514580557,\n",
       "   0.13355739449129403,\n",
       "   0.13351123218454736,\n",
       "   0.13652744866433753,\n",
       "   0.13663868884358915,\n",
       "   0.13332174189638174,\n",
       "   0.12878486137641126,\n",
       "   0.134792926682904,\n",
       "   0.12580997851987688,\n",
       "   0.12854916356387766,\n",
       "   0.13229685925604023,\n",
       "   0.12633104806695247,\n",
       "   0.12768812142125913,\n",
       "   0.13683771279381274,\n",
       "   0.12907771411551497,\n",
       "   0.14429002099459695,\n",
       "   0.14096807032360387,\n",
       "   0.1399645295226597,\n",
       "   0.12654006563459566,\n",
       "   0.13831351482377688,\n",
       "   0.1396622098923457,\n",
       "   0.13153005168287843,\n",
       "   0.12980276754335485,\n",
       "   0.14141532455239722,\n",
       "   0.1397102506747609,\n",
       "   0.1379050299632047,\n",
       "   0.1395686565049043,\n",
       "   0.13681095343667685,\n",
       "   0.13571330243501015,\n",
       "   0.13520097647318371,\n",
       "   0.14643528824066301,\n",
       "   0.1312285078245626,\n",
       "   0.13919106726040215,\n",
       "   0.13039330131572782,\n",
       "   0.13981650372444795,\n",
       "   0.128473001870341,\n",
       "   0.13512924508760413,\n",
       "   0.14191703189547614,\n",
       "   0.13653424959938498,\n",
       "   0.17014665767198034,\n",
       "   0.1417580810571357,\n",
       "   0.13473839067982274,\n",
       "   0.1367871050906638,\n",
       "   0.13946343498591227,\n",
       "   0.1371581622801187,\n",
       "   0.13740208024599532,\n",
       "   0.1378625495870394,\n",
       "   0.13069175807618924,\n",
       "   0.13150639729044977,\n",
       "   0.14153563822602064,\n",
       "   0.14767932188083496,\n",
       "   0.15343667019486715,\n",
       "   0.1425788750186476,\n",
       "   0.14419936151734988,\n",
       "   0.14995406969641595,\n",
       "   0.1567298832781672,\n",
       "   0.15206443610110368,\n",
       "   0.14864858460520486,\n",
       "   0.16333338249334256,\n",
       "   0.16313057276529838,\n",
       "   0.16212273618832376,\n",
       "   0.15959207061526856,\n",
       "   0.1614343846290707,\n",
       "   0.1577834237535076,\n",
       "   0.1665072302259236,\n",
       "   0.1489706148362714,\n",
       "   0.1642797469551458,\n",
       "   0.1414290919003695,\n",
       "   0.1635292665421582,\n",
       "   0.16631608941397186,\n",
       "   0.1538855734922551,\n",
       "   0.1583678844397163,\n",
       "   0.15625284843518716,\n",
       "   0.1625516664911734,\n",
       "   0.15410315681653491,\n",
       "   0.16634067085217077,\n",
       "   0.16559705066216346,\n",
       "   0.16159802452264552,\n",
       "   0.16414265705492492,\n",
       "   0.15708547735514639,\n",
       "   0.16679352842280915,\n",
       "   0.17679395776396747],\n",
       "  'val_accuracy_mean': [0.46344444433848064,\n",
       "   0.5141111105680466,\n",
       "   0.5392222217718761,\n",
       "   0.5652222225069999,\n",
       "   0.5739999998609225,\n",
       "   0.5875777778029442,\n",
       "   0.579377776881059,\n",
       "   0.6062444433569908,\n",
       "   0.6185555555423101,\n",
       "   0.6257555555303892,\n",
       "   0.6291777769724528,\n",
       "   0.6302666650215785,\n",
       "   0.6353999988238017,\n",
       "   0.6390666656692823,\n",
       "   0.6383555557330449,\n",
       "   0.6369777764876684,\n",
       "   0.6334222217400869,\n",
       "   0.6499333319067955,\n",
       "   0.6485111086567243,\n",
       "   0.6478444440166156,\n",
       "   0.6450888879100481,\n",
       "   0.6358222206433614,\n",
       "   0.6517555542786916,\n",
       "   0.6491777782638868,\n",
       "   0.6464000002543131,\n",
       "   0.6547777780890465,\n",
       "   0.6487111119429271,\n",
       "   0.6545777759949366,\n",
       "   0.6424666671951612,\n",
       "   0.6393333318829536,\n",
       "   0.6468222227692604,\n",
       "   0.6443555528918902,\n",
       "   0.6394888893763224,\n",
       "   0.6511333334445953,\n",
       "   0.6472888878981272,\n",
       "   0.6469555546840032,\n",
       "   0.6559999991456668,\n",
       "   0.6583111102382342,\n",
       "   0.654644443889459,\n",
       "   0.6527555545171102,\n",
       "   0.6446888894836108,\n",
       "   0.6543111115694046,\n",
       "   0.6500000000993411,\n",
       "   0.6550222220023473,\n",
       "   0.6569999994834265,\n",
       "   0.6510222212473551,\n",
       "   0.653688887755076,\n",
       "   0.6455333302418391,\n",
       "   0.6550222225983937,\n",
       "   0.6582666662335396,\n",
       "   0.6478666655222575,\n",
       "   0.6553999990224838,\n",
       "   0.6441333323717118,\n",
       "   0.6519555554787317,\n",
       "   0.6595777766903241,\n",
       "   0.6488888879617055,\n",
       "   0.6303111096223195,\n",
       "   0.6617333311835925,\n",
       "   0.6479777753353119,\n",
       "   0.642266667385896,\n",
       "   0.6424222213029861,\n",
       "   0.6467333326737086,\n",
       "   0.6364888887604078,\n",
       "   0.6510222225387892,\n",
       "   0.6407333315412204,\n",
       "   0.6379333328207334,\n",
       "   0.6406888888279597,\n",
       "   0.6419111087918281,\n",
       "   0.638155554731687,\n",
       "   0.6434222212433816,\n",
       "   0.6402444433172544,\n",
       "   0.6430666667222976,\n",
       "   0.6357555561264356,\n",
       "   0.6407999987403552,\n",
       "   0.6231999986370405,\n",
       "   0.6448666656017303,\n",
       "   0.6427555541197459,\n",
       "   0.6228666671117147,\n",
       "   0.6299777766068776,\n",
       "   0.6430444438258807,\n",
       "   0.6457999999324481,\n",
       "   0.642511111398538,\n",
       "   0.6379555562138557,\n",
       "   0.6322444447875023,\n",
       "   0.6327777764201165,\n",
       "   0.6379333324233691,\n",
       "   0.6388666653633117,\n",
       "   0.6404222241044044,\n",
       "   0.6299111104011536,\n",
       "   0.6411333328485489,\n",
       "   0.6325333326061566,\n",
       "   0.6280666638414065,\n",
       "   0.6313555558522542,\n",
       "   0.6395333328843117,\n",
       "   0.6267555548747381,\n",
       "   0.6331999996304511,\n",
       "   0.6393777763843537,\n",
       "   0.6399111125866572,\n",
       "   0.6369999997814496],\n",
       "  'val_accuracy_std': [0.055690835612654595,\n",
       "   0.06042922507625035,\n",
       "   0.059070473024852965,\n",
       "   0.05761547680682845,\n",
       "   0.05998888713147845,\n",
       "   0.062262760972891414,\n",
       "   0.06190959556833182,\n",
       "   0.061248488845319145,\n",
       "   0.06022047900211955,\n",
       "   0.06565385886149633,\n",
       "   0.0596836555478954,\n",
       "   0.06318991975571772,\n",
       "   0.05934977394599706,\n",
       "   0.0618050944905067,\n",
       "   0.06206537488042029,\n",
       "   0.059240060486926135,\n",
       "   0.060957723042517745,\n",
       "   0.06236211109806395,\n",
       "   0.06163457224680419,\n",
       "   0.062005541841924625,\n",
       "   0.06390756875113496,\n",
       "   0.06579675471485268,\n",
       "   0.06126831920146834,\n",
       "   0.058027372986526535,\n",
       "   0.059261530481768625,\n",
       "   0.06060059490505672,\n",
       "   0.06151397055543334,\n",
       "   0.057880526157404404,\n",
       "   0.06177369805435048,\n",
       "   0.06046609259779359,\n",
       "   0.062092925313143595,\n",
       "   0.06290021285701877,\n",
       "   0.060930119926129,\n",
       "   0.06274738475302297,\n",
       "   0.060659814317613955,\n",
       "   0.061707116984419455,\n",
       "   0.06009622119238296,\n",
       "   0.060573490051262016,\n",
       "   0.06246231208424405,\n",
       "   0.0611806634955708,\n",
       "   0.058206016901147316,\n",
       "   0.06260226815334283,\n",
       "   0.061361106773288876,\n",
       "   0.0633166615418843,\n",
       "   0.05971072497765104,\n",
       "   0.06148284617519717,\n",
       "   0.06222914388317876,\n",
       "   0.06486033993282873,\n",
       "   0.06108251584712141,\n",
       "   0.06107884423215389,\n",
       "   0.05979753750616285,\n",
       "   0.06312852232130411,\n",
       "   0.06237126874491986,\n",
       "   0.06201694805609231,\n",
       "   0.061989004612074586,\n",
       "   0.061685783888662284,\n",
       "   0.061574823234192465,\n",
       "   0.06095015605511506,\n",
       "   0.061701068945253394,\n",
       "   0.06313222047379805,\n",
       "   0.06316280813285974,\n",
       "   0.06255187021992703,\n",
       "   0.06502283043177266,\n",
       "   0.06583250029213142,\n",
       "   0.058708787575325105,\n",
       "   0.061920105343407736,\n",
       "   0.06443118470733189,\n",
       "   0.06700729329653726,\n",
       "   0.06461514169215403,\n",
       "   0.06706431021156892,\n",
       "   0.062378293189658325,\n",
       "   0.06639781285638664,\n",
       "   0.0640178203000769,\n",
       "   0.06474757875341684,\n",
       "   0.06253755155812468,\n",
       "   0.06389705851146633,\n",
       "   0.06430108100789826,\n",
       "   0.0646374863261401,\n",
       "   0.0636576455475804,\n",
       "   0.06262225100397835,\n",
       "   0.061255904493349604,\n",
       "   0.06547879160410072,\n",
       "   0.06261201825868136,\n",
       "   0.0644371632503424,\n",
       "   0.0629107741763466,\n",
       "   0.06473441110327802,\n",
       "   0.0625558501536538,\n",
       "   0.06019941709136248,\n",
       "   0.06123778391416701,\n",
       "   0.06027139862932894,\n",
       "   0.06319831358026963,\n",
       "   0.065505184618344,\n",
       "   0.06678500526057388,\n",
       "   0.06326139931959605,\n",
       "   0.06295435996447903,\n",
       "   0.06336008921872494,\n",
       "   0.06740412907472264,\n",
       "   0.06318924735669927,\n",
       "   0.06753901419231846],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2ada4",
   "metadata": {},
   "source": [
    "# 1. 학습된 모델을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f54da463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
